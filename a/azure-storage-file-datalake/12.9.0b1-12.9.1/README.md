# Comparing `tmp/azure-storage-file-datalake-12.9.0b1.zip` & `tmp/azure-storage-file-datalake-12.9.1.zip`

## zipinfo {}

```diff
@@ -1,138 +1,138 @@
-Zip file size: 346914 bytes, number of entries: 136
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/tests/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/samples/
--rw-rw-r--  2.0 unx    15236 b- defN 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/PKG-INFO
--rw-rw-r--  2.0 unx     2654 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/setup.py
--rw-rw-r--  2.0 unx       38 b- defN 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/setup.cfg
--rw-rw-r--  2.0 unx     7675 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/GEN1_GEN2_MAPPING.md
--rw-rw-r--  2.0 unx     1073 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/LICENSE
--rw-rw-r--  2.0 unx     9608 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/CHANGELOG.md
--rw-rw-r--  2.0 unx    14420 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/README.md
--rw-rw-r--  2.0 unx      198 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/MANIFEST.in
--rw-rw-r--  2.0 unx    15236 b- defN 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/PKG-INFO
--rw-rw-r--  2.0 unx       79 b- defN 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/requires.txt
--rw-rw-r--  2.0 unx     5407 b- defN 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/SOURCES.txt
--rw-rw-r--  2.0 unx        1 b- defN 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/dependency_links.txt
--rw-rw-r--  2.0 unx        1 b- defN 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/not-zip-safe
--rw-rw-r--  2.0 unx        6 b- defN 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/top_level.txt
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/tests/settings/
--rw-rw-r--  2.0 unx    41924 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_file_system.py
--rw-rw-r--  2.0 unx    52302 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_file_async.py
--rw-rw-r--  2.0 unx    64679 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_directory.py
--rw-rw-r--  2.0 unx     7030 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_large_file.py
--rw-rw-r--  2.0 unx    64695 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_directory_async.py
--rw-rw-r--  2.0 unx    49674 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_file_system_async.py
--rw-rw-r--  2.0 unx    57438 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_file.py
--rw-rw-r--  2.0 unx    43356 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_quick_query.py
--rw-rw-r--  2.0 unx    18107 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_datalake_service_client_async.py
--rw-rw-r--  2.0 unx    17762 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_datalake_service_client.py
--rw-rw-r--  2.0 unx     7581 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_large_file_async.py
--rw-rw-r--  2.0 unx    11790 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_cpk_async.py
--rw-rw-r--  2.0 unx    12263 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/test_cpk.py
--rw-rw-r--  2.0 unx     1397 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/append.py
--rw-rw-r--  2.0 unx     1431 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/upload_from_file.py
--rw-rw-r--  2.0 unx        0 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/__init__.py
--rw-rw-r--  2.0 unx     1569 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/read.py
--rw-rw-r--  2.0 unx     3182 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/_test_base.py
--rw-rw-r--  2.0 unx     1256 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/upload.py
--rw-rw-r--  2.0 unx      665 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/settings/settings_fake.py
--rw-rw-r--  2.0 unx        0 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/settings/__init__.py
--rw-rw-r--  2.0 unx     2019 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/tests/settings/testcase.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/storage/
--rw-rw-r--  2.0 unx       78 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/__init__.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/
--rw-rw-r--  2.0 unx       78 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/__init__.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/
--rw-rw-r--  2.0 unx     4727 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_serialize.py
--rw-rw-r--  2.0 unx    50254 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_models.py
--rw-rw-r--  2.0 unx    36179 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_directory_client.py
--rw-rw-r--  2.0 unx     2664 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_download.py
--rw-rw-r--  2.0 unx    53954 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_path_client.py
--rw-rw-r--  2.0 unx    21949 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared_access_signature.py
--rw-rw-r--  2.0 unx        0 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/py.typed
--rw-rw-r--  2.0 unx     9942 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_deserialize.py
--rw-rw-r--  2.0 unx    28000 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_service_client.py
--rw-rw-r--  2.0 unx     4372 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_upload_helper.py
--rw-rw-r--  2.0 unx    13354 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_lease.py
--rw-rw-r--  2.0 unx      332 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_version.py
--rw-rw-r--  2.0 unx     3048 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/__init__.py
--rw-rw-r--  2.0 unx    46871 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_file_client.py
--rw-rw-r--  2.0 unx     2499 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_quick_query_helper.py
--rw-rw-r--  2.0 unx    51881 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_file_system_client.py
--rw-rw-r--  2.0 unx     7371 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_list_paths_helper.py
--rw-rw-r--  2.0 unx      668 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/constants.py
--rw-rw-r--  2.0 unx     9737 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/request_handlers.py
--rw-rw-r--  2.0 unx     6792 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/base_client_async.py
--rw-rw-r--  2.0 unx     8627 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/response_handlers.py
--rw-rw-r--  2.0 unx    15661 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/uploads_async.py
--rw-rw-r--  2.0 unx     1529 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/__init__.py
--rw-rw-r--  2.0 unx    20683 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/models.py
--rw-rw-r--  2.0 unx    22184 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/uploads.py
--rw-rw-r--  2.0 unx     9993 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/policies_async.py
--rw-rw-r--  2.0 unx    18032 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/base_client.py
--rw-rw-r--  2.0 unx    27627 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/policies.py
--rw-rw-r--  2.0 unx    10456 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/shared_access_signature.py
--rw-rw-r--  2.0 unx     5369 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/authentication.py
--rw-rw-r--  2.0 unx     1590 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/parser.py
--rw-rw-r--  2.0 unx     1988 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_models.py
--rw-rw-r--  2.0 unx    35630 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_file_client_async.py
--rw-rw-r--  2.0 unx    35406 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_directory_client_async.py
--rw-rw-r--  2.0 unx     4443 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_upload_helper.py
--rw-rw-r--  2.0 unx    49292 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_file_system_client_async.py
--rw-rw-r--  2.0 unx      967 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/__init__.py
--rw-rw-r--  2.0 unx    25811 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_service_client_async.py
--rw-rw-r--  2.0 unx    46829 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_path_client_async.py
--rw-rw-r--  2.0 unx    13393 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_lease_async.py
--rw-rw-r--  2.0 unx     7564 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_list_paths_helper.py
--rw-rw-r--  2.0 unx     2716 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_download_async.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/
--rw-rw-r--  2.0 unx     1169 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_vendor.py
--rw-rw-r--  2.0 unx     3409 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_configuration.py
--rw-rw-r--  2.0 unx      878 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/__init__.py
--rw-rw-r--  2.0 unx     4790 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_azure_data_lake_storage_restapi.py
--rw-rw-r--  2.0 unx     1530 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_patch.py
--rw-rw-r--  2.0 unx    77452 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_serialization.py
--rw-rw-r--  2.0 unx    38715 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/_models_py3.py
--rw-rw-r--  2.0 unx     2758 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/__init__.py
--rw-rw-r--  2.0 unx      791 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/_patch.py
--rw-rw-r--  2.0 unx     2241 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/_azure_data_lake_storage_restapi_enums.py
-drwxrwxr-x  2.0 unx        0 b- stor 22-Aug-23 20:45 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/
--rw-rw-r--  2.0 unx     3375 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/_configuration.py
--rw-rw-r--  2.0 unx      878 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/__init__.py
--rw-rw-r--  2.0 unx     4844 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/_azure_data_lake_storage_restapi.py
--rw-rw-r--  2.0 unx     1530 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/_patch.py
--rw-rw-r--  2.0 unx    31614 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_file_system_operations.py
--rw-rw-r--  2.0 unx      951 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/__init__.py
--rw-rw-r--  2.0 unx     7290 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_service_operations.py
--rw-rw-r--  2.0 unx   107271 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_path_operations.py
--rw-rw-r--  2.0 unx      791 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_patch.py
--rw-rw-r--  2.0 unx    42848 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_file_system_operations.py
--rw-rw-r--  2.0 unx      951 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/__init__.py
--rw-rw-r--  2.0 unx     9134 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_service_operations.py
--rw-rw-r--  2.0 unx   147178 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_path_operations.py
--rw-rw-r--  2.0 unx      791 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_patch.py
--rw-rw-r--  2.0 unx     4948 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_query.py
--rw-rw-r--  2.0 unx     3978 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control.py
--rw-rw-r--  2.0 unx     5271 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_service.py
--rw-rw-r--  2.0 unx     4451 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_upload_download_async.py
--rw-rw-r--  2.0 unx     1768 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_instantiate_client_async.py
--rw-rw-r--  2.0 unx     5962 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control_recursive_async.py
--rw-rw-r--  2.0 unx     5711 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control_recursive.py
--rw-rw-r--  2.0 unx     9654 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_file_system_async.py
--rw-rw-r--  2.0 unx     8514 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_file_system.py
--rw-rw-r--  2.0 unx     1618 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_instantiate_client.py
--rw-rw-r--  2.0 unx     3987 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_directory.py
--rw-rw-r--  2.0 unx     4141 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_directory_async.py
--rw-rw-r--  2.0 unx     2888 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/README.md
--rw-rw-r--  2.0 unx     4088 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control_async.py
--rw-rw-r--  2.0 unx     4177 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_upload_download.py
--rw-rw-r--  2.0 unx     5310 b- defN 22-Aug-23 20:44 azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_service_async.py
-136 files, 1821937 bytes uncompressed, 315020 bytes compressed:  82.7%
+Zip file size: 347403 bytes, number of entries: 136
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/samples/
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/tests/
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/
+-rw-rw-r--  2.0 unx     7675 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/GEN1_GEN2_MAPPING.md
+-rw-rw-r--  2.0 unx       38 b- defN 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/setup.cfg
+-rw-rw-r--  2.0 unx    14589 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/README.md
+-rw-rw-r--  2.0 unx     9952 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/CHANGELOG.md
+-rw-rw-r--  2.0 unx      198 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/MANIFEST.in
+-rw-rw-r--  2.0 unx    15416 b- defN 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/PKG-INFO
+-rw-rw-r--  2.0 unx     1073 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/LICENSE
+-rw-rw-r--  2.0 unx     2664 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/setup.py
+-rw-rw-r--  2.0 unx        1 b- defN 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/not-zip-safe
+-rw-rw-r--  2.0 unx        1 b- defN 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/dependency_links.txt
+-rw-rw-r--  2.0 unx    15416 b- defN 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/PKG-INFO
+-rw-rw-r--  2.0 unx     5407 b- defN 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/SOURCES.txt
+-rw-rw-r--  2.0 unx       76 b- defN 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/requires.txt
+-rw-rw-r--  2.0 unx        6 b- defN 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/top_level.txt
+-rw-rw-r--  2.0 unx     5916 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control_recursive_async.py
+-rw-rw-r--  2.0 unx     2888 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/README.md
+-rw-rw-r--  2.0 unx     9591 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_file_system_async.py
+-rw-rw-r--  2.0 unx     5711 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control_recursive.py
+-rw-rw-r--  2.0 unx     4095 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_directory_async.py
+-rw-rw-r--  2.0 unx     4948 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_query.py
+-rw-rw-r--  2.0 unx     4405 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_upload_download_async.py
+-rw-rw-r--  2.0 unx     5221 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_service_async.py
+-rw-rw-r--  2.0 unx     4042 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control_async.py
+-rw-rw-r--  2.0 unx     3987 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_directory.py
+-rw-rw-r--  2.0 unx     4177 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_upload_download.py
+-rw-rw-r--  2.0 unx     1618 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_instantiate_client.py
+-rw-rw-r--  2.0 unx     3978 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control.py
+-rw-rw-r--  2.0 unx     1720 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_instantiate_client_async.py
+-rw-rw-r--  2.0 unx     5271 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_service.py
+-rw-rw-r--  2.0 unx     8514 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/samples/datalake_samples_file_system.py
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/tests/perfstress_tests/
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/tests/settings/
+-rw-rw-r--  2.0 unx    17762 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_datalake_service_client.py
+-rw-rw-r--  2.0 unx    46623 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_quick_query.py
+-rw-rw-r--  2.0 unx    47395 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_file_system.py
+-rw-rw-r--  2.0 unx    74370 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_directory_async.py
+-rw-rw-r--  2.0 unx    11790 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_cpk_async.py
+-rw-rw-r--  2.0 unx    57438 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_file.py
+-rw-rw-r--  2.0 unx    18107 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_datalake_service_client_async.py
+-rw-rw-r--  2.0 unx    52302 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_file_async.py
+-rw-rw-r--  2.0 unx     7804 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_large_file_async.py
+-rw-rw-r--  2.0 unx    12263 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_cpk.py
+-rw-rw-r--  2.0 unx    75282 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_directory.py
+-rw-rw-r--  2.0 unx     7262 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_large_file.py
+-rw-rw-r--  2.0 unx    54628 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/test_file_system_async.py
+-rw-rw-r--  2.0 unx     1256 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/perfstress_tests/upload.py
+-rw-rw-r--  2.0 unx     3182 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/perfstress_tests/_test_base.py
+-rw-rw-r--  2.0 unx     1431 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/perfstress_tests/upload_from_file.py
+-rw-rw-r--  2.0 unx     1397 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/perfstress_tests/append.py
+-rw-rw-r--  2.0 unx     1569 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/perfstress_tests/read.py
+-rw-rw-r--  2.0 unx        0 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/perfstress_tests/__init__.py
+-rw-rw-r--  2.0 unx     2503 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/settings/testcase.py
+-rw-rw-r--  2.0 unx      776 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/settings/settings_fake.py
+-rw-rw-r--  2.0 unx        0 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/tests/settings/__init__.py
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/storage/
+-rw-rw-r--  2.0 unx       78 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/__init__.py
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/
+-rw-rw-r--  2.0 unx       78 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/__init__.py
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/
+-rw-rw-r--  2.0 unx    53954 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_path_client.py
+-rw-rw-r--  2.0 unx    28000 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_service_client.py
+-rw-rw-r--  2.0 unx    46870 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_file_client.py
+-rw-rw-r--  2.0 unx    51881 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_file_system_client.py
+-rw-rw-r--  2.0 unx     4372 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_upload_helper.py
+-rw-rw-r--  2.0 unx    21950 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared_access_signature.py
+-rw-rw-r--  2.0 unx     4727 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_serialize.py
+-rw-rw-r--  2.0 unx     7371 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_list_paths_helper.py
+-rw-rw-r--  2.0 unx      330 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_version.py
+-rw-rw-r--  2.0 unx     3048 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/__init__.py
+-rw-rw-r--  2.0 unx    36178 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_directory_client.py
+-rw-rw-r--  2.0 unx        0 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/py.typed
+-rw-rw-r--  2.0 unx     2680 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_download.py
+-rw-rw-r--  2.0 unx     2499 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_quick_query_helper.py
+-rw-rw-r--  2.0 unx    50256 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_models.py
+-rw-rw-r--  2.0 unx    13354 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_lease.py
+-rw-rw-r--  2.0 unx     9942 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_deserialize.py
+-rw-rw-r--  2.0 unx    27628 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/policies.py
+-rw-rw-r--  2.0 unx      586 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/constants.py
+-rw-rw-r--  2.0 unx     6792 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/base_client_async.py
+-rw-rw-r--  2.0 unx    21038 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/models.py
+-rw-rw-r--  2.0 unx    10511 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/shared_access_signature.py
+-rw-rw-r--  2.0 unx    15661 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/uploads_async.py
+-rw-rw-r--  2.0 unx     1590 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/parser.py
+-rw-rw-r--  2.0 unx    22186 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/uploads.py
+-rw-rw-r--  2.0 unx     1529 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/__init__.py
+-rw-rw-r--  2.0 unx     9993 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/policies_async.py
+-rw-rw-r--  2.0 unx     9738 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/request_handlers.py
+-rw-rw-r--  2.0 unx     5369 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/authentication.py
+-rw-rw-r--  2.0 unx    18032 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/base_client.py
+-rw-rw-r--  2.0 unx     8627 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/response_handlers.py
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/
+-rw-rw-r--  2.0 unx     1530 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_patch.py
+-rw-rw-r--  2.0 unx     3409 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_configuration.py
+-rw-rw-r--  2.0 unx     4790 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_azure_data_lake_storage_restapi.py
+-rw-rw-r--  2.0 unx      878 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/__init__.py
+-rw-rw-r--  2.0 unx     1169 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_vendor.py
+-rw-rw-r--  2.0 unx    77450 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_serialization.py
+-rw-rw-r--  2.0 unx      791 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_patch.py
+-rw-rw-r--  2.0 unx   147178 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_path_operations.py
+-rw-rw-r--  2.0 unx    42848 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_file_system_operations.py
+-rw-rw-r--  2.0 unx      951 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/__init__.py
+-rw-rw-r--  2.0 unx     9134 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_service_operations.py
+-rw-rw-r--  2.0 unx      791 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/_patch.py
+-rw-rw-r--  2.0 unx    38715 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/_models_py3.py
+-rw-rw-r--  2.0 unx     2241 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/_azure_data_lake_storage_restapi_enums.py
+-rw-rw-r--  2.0 unx     2758 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/__init__.py
+drwxrwxr-x  2.0 unx        0 b- stor 22-Oct-19 00:00 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/
+-rw-rw-r--  2.0 unx     1530 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/_patch.py
+-rw-rw-r--  2.0 unx     3375 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/_configuration.py
+-rw-rw-r--  2.0 unx     4844 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/_azure_data_lake_storage_restapi.py
+-rw-rw-r--  2.0 unx      878 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/__init__.py
+-rw-rw-r--  2.0 unx      791 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_patch.py
+-rw-rw-r--  2.0 unx   107271 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_path_operations.py
+-rw-rw-r--  2.0 unx    31614 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_file_system_operations.py
+-rw-rw-r--  2.0 unx      951 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/__init__.py
+-rw-rw-r--  2.0 unx     7290 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_service_operations.py
+-rw-rw-r--  2.0 unx    49288 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_file_system_client_async.py
+-rw-rw-r--  2.0 unx    35405 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_directory_client_async.py
+-rw-rw-r--  2.0 unx     4443 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_upload_helper.py
+-rw-rw-r--  2.0 unx    13393 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_lease_async.py
+-rw-rw-r--  2.0 unx     2732 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_download_async.py
+-rw-rw-r--  2.0 unx     7564 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_list_paths_helper.py
+-rw-rw-r--  2.0 unx      967 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/__init__.py
+-rw-rw-r--  2.0 unx    35629 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_file_client_async.py
+-rw-rw-r--  2.0 unx    46829 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_path_client_async.py
+-rw-rw-r--  2.0 unx     1988 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_models.py
+-rw-rw-r--  2.0 unx    25811 b- defN 22-Oct-18 23:59 azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_service_client_async.py
+136 files, 1857808 bytes uncompressed, 316053 bytes compressed:  83.0%
```

## zipnote {}

```diff
@@ -1,409 +1,409 @@
-Filename: azure-storage-file-datalake-12.9.0b1/
+Filename: azure-storage-file-datalake-12.9.1/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/
+Filename: azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/
+Filename: azure-storage-file-datalake-12.9.1/samples/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/
+Filename: azure-storage-file-datalake-12.9.1/tests/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/
+Filename: azure-storage-file-datalake-12.9.1/azure/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/PKG-INFO
+Filename: azure-storage-file-datalake-12.9.1/GEN1_GEN2_MAPPING.md
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/setup.py
+Filename: azure-storage-file-datalake-12.9.1/setup.cfg
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/setup.cfg
+Filename: azure-storage-file-datalake-12.9.1/README.md
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/GEN1_GEN2_MAPPING.md
+Filename: azure-storage-file-datalake-12.9.1/CHANGELOG.md
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/LICENSE
+Filename: azure-storage-file-datalake-12.9.1/MANIFEST.in
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/CHANGELOG.md
+Filename: azure-storage-file-datalake-12.9.1/PKG-INFO
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/README.md
+Filename: azure-storage-file-datalake-12.9.1/LICENSE
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/MANIFEST.in
+Filename: azure-storage-file-datalake-12.9.1/setup.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/PKG-INFO
+Filename: azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/not-zip-safe
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/requires.txt
+Filename: azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/dependency_links.txt
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/SOURCES.txt
+Filename: azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/PKG-INFO
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/dependency_links.txt
+Filename: azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/SOURCES.txt
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/not-zip-safe
+Filename: azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/requires.txt
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/top_level.txt
+Filename: azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/top_level.txt
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control_recursive_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/settings/
+Filename: azure-storage-file-datalake-12.9.1/samples/README.md
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_file_system.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_file_system_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_file_async.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control_recursive.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_directory.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_directory_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_large_file.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_query.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_directory_async.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_upload_download_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_file_system_async.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_service_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_file.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_quick_query.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_directory.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_datalake_service_client_async.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_upload_download.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_datalake_service_client.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_instantiate_client.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_large_file_async.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_cpk_async.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_instantiate_client_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/test_cpk.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_service.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/append.py
+Filename: azure-storage-file-datalake-12.9.1/samples/datalake_samples_file_system.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/upload_from_file.py
+Filename: azure-storage-file-datalake-12.9.1/tests/perfstress_tests/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/tests/settings/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/read.py
+Filename: azure-storage-file-datalake-12.9.1/tests/test_datalake_service_client.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/_test_base.py
+Filename: azure-storage-file-datalake-12.9.1/tests/test_quick_query.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/upload.py
+Filename: azure-storage-file-datalake-12.9.1/tests/test_file_system.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/settings/settings_fake.py
+Filename: azure-storage-file-datalake-12.9.1/tests/test_directory_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/settings/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/tests/test_cpk_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/tests/settings/testcase.py
+Filename: azure-storage-file-datalake-12.9.1/tests/test_file.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/
+Filename: azure-storage-file-datalake-12.9.1/tests/test_datalake_service_client_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/tests/test_file_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/
+Filename: azure-storage-file-datalake-12.9.1/tests/test_large_file_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/tests/test_cpk.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/
+Filename: azure-storage-file-datalake-12.9.1/tests/test_directory.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/
+Filename: azure-storage-file-datalake-12.9.1/tests/test_large_file.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/
+Filename: azure-storage-file-datalake-12.9.1/tests/test_file_system_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_serialize.py
+Filename: azure-storage-file-datalake-12.9.1/tests/perfstress_tests/upload.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_models.py
+Filename: azure-storage-file-datalake-12.9.1/tests/perfstress_tests/_test_base.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_directory_client.py
+Filename: azure-storage-file-datalake-12.9.1/tests/perfstress_tests/upload_from_file.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_download.py
+Filename: azure-storage-file-datalake-12.9.1/tests/perfstress_tests/append.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_path_client.py
+Filename: azure-storage-file-datalake-12.9.1/tests/perfstress_tests/read.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared_access_signature.py
+Filename: azure-storage-file-datalake-12.9.1/tests/perfstress_tests/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/py.typed
+Filename: azure-storage-file-datalake-12.9.1/tests/settings/testcase.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_deserialize.py
+Filename: azure-storage-file-datalake-12.9.1/tests/settings/settings_fake.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_service_client.py
+Filename: azure-storage-file-datalake-12.9.1/tests/settings/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_upload_helper.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_lease.py
+Filename: azure-storage-file-datalake-12.9.1/azure/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_version.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_file_client.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_quick_query_helper.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_file_system_client.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_list_paths_helper.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_path_client.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/constants.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_service_client.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/request_handlers.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_file_client.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/base_client_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_file_system_client.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/response_handlers.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_upload_helper.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/uploads_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared_access_signature.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_serialize.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/models.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_list_paths_helper.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/uploads.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_version.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/policies_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/base_client.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_directory_client.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/policies.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/py.typed
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/shared_access_signature.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_download.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/authentication.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_quick_query_helper.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/parser.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_models.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_models.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_lease.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_file_client_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_deserialize.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_directory_client_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/policies.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_upload_helper.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/constants.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_file_system_client_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/base_client_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/models.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_service_client_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/shared_access_signature.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_path_client_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/uploads_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_lease_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/parser.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_list_paths_helper.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/uploads.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_download_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/policies_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/request_handlers.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/authentication.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_vendor.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/base_client.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_configuration.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/response_handlers.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_azure_data_lake_storage_restapi.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_patch.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_serialization.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_patch.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/_models_py3.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_configuration.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_azure_data_lake_storage_restapi.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/_patch.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/_azure_data_lake_storage_restapi_enums.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_vendor.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_serialization.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/_configuration.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_patch.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_path_operations.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/_azure_data_lake_storage_restapi.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_file_system_operations.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/_patch.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_file_system_operations.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_service_operations.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/_patch.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_service_operations.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/_models_py3.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_path_operations.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/_azure_data_lake_storage_restapi_enums.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_patch.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_file_system_operations.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/__init__.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/_patch.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_service_operations.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/_configuration.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_path_operations.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/_azure_data_lake_storage_restapi.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_patch.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_query.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_patch.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_path_operations.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_service.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_file_system_operations.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_upload_download_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_instantiate_client_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_service_operations.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control_recursive_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_file_system_client_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control_recursive.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_directory_client_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_file_system_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_upload_helper.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_file_system.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_lease_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_instantiate_client.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_download_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_directory.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_list_paths_helper.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_directory_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/__init__.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/README.md
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_file_client_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_path_client_async.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_upload_download.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_models.py
 Comment: 
 
-Filename: azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_service_async.py
+Filename: azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_service_client_async.py
 Comment: 
 
 Zip file comment:
```

## Comparing `azure-storage-file-datalake-12.9.0b1/PKG-INFO` & `azure-storage-file-datalake-12.9.1/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 Metadata-Version: 2.1
 Name: azure-storage-file-datalake
-Version: 12.9.0b1
+Version: 12.9.1
 Summary: Microsoft Azure File DataLake Storage Client Library for Python
 Home-page: https://github.com/Azure/azure-sdk-for-python
 Author: Microsoft Corporation
 Author-email: ascl@microsoft.com
 License: MIT License
-Classifier: Development Status :: 4 - Beta
+Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
@@ -210,15 +210,18 @@
 ### Other client / per-operation configuration
 
 Other optional configuration keyword arguments that can be specified on the client or per-operation.
 
 **Client keyword arguments:**
 
 * __connection_timeout__ (int): The number of seconds the client will wait to establish a connection to the server.
-* __read_timeout__ (int): The number of seconds the client will wait, after the connections has been established, for the server to send a response.
+Defaults to 20 seconds.
+* __read_timeout__ (int): The number of seconds the client will wait, between consecutive read operations, for a
+response from the server. This is a socket level timeout and is not affected by overall data size. Client-side read 
+timeouts will be automatically retried. Defaults to 60 seconds.
 * __transport__ (Any): User-provided transport to send the HTTP request.
 
 **Per-operation keyword arguments:**
 
 * __raw_response_hook__ (callable): The given callback uses the response returned from the service.
 * __raw_request_hook__ (callable): The given callback uses the request before being sent to service.
 * __client_request_id__ (str): Optional user specified identification of the request.
```

## Comparing `azure-storage-file-datalake-12.9.0b1/setup.py` & `azure-storage-file-datalake-12.9.1/setup.py`

 * *Files 8% similar despite different names*

```diff
@@ -52,15 +52,15 @@
     long_description=open('README.md', 'r').read(),
     long_description_content_type='text/markdown',
     license='MIT License',
     author='Microsoft Corporation',
     author_email='ascl@microsoft.com',
     url='https://github.com/Azure/azure-sdk-for-python',
     classifiers=[
-        "Development Status :: 4 - Beta",
+        'Development Status :: 5 - Production/Stable',
         'Programming Language :: Python',
         "Programming Language :: Python :: 3 :: Only",
         'Programming Language :: Python :: 3',
         'Programming Language :: Python :: 3.7',
         'Programming Language :: Python :: 3.8',
         'Programming Language :: Python :: 3.9',
         'Programming Language :: Python :: 3.10',
@@ -72,11 +72,11 @@
         'azure',
         'azure.storage',
         'tests',
     ]),
     python_requires=">=3.7",
     install_requires=[
         "azure-core<2.0.0,>=1.24.2",
-        "msrest>=0.6.21",
-        "azure-storage-blob<13.0.0,>=12.14.0b1"
+        "msrest>=0.7.1",
+        "azure-storage-blob<13.0.0,>=12.14.1"
     ],
 )
```

## Comparing `azure-storage-file-datalake-12.9.0b1/GEN1_GEN2_MAPPING.md` & `azure-storage-file-datalake-12.9.1/GEN1_GEN2_MAPPING.md`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/LICENSE` & `azure-storage-file-datalake-12.9.1/LICENSE`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/CHANGELOG.md` & `azure-storage-file-datalake-12.9.1/CHANGELOG.md`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,22 @@
 # Release History
 
+## 12.9.1 (2022-10-18)
+
+### Bugs Fixed
+- Fixed possible `ValueError` for invalid content range that gets raised when downloading empty files through Azurite.
+
+## 12.9.0 (2022-10-11)
+
+### Features Added
+- Stable release of features from 12.9.0b1.
+
+### Other Changes
+- Changed the default value for `read_timeout` to 60 seconds for all clients.
+
 ## 12.9.0b1 (2022-08-23)
 
 This version and all future versions will require Python 3.7+. Python 3.6 is no longer supported.
 
 ### Features Added
 - Added support for `AzureNamedKeyCredential` as a valid `credential` type.
 - Added support for `flush` to `append_data` API, allowing for append and flush in one operation.
```

## Comparing `azure-storage-file-datalake-12.9.0b1/README.md` & `azure-storage-file-datalake-12.9.1/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -189,15 +189,18 @@
 ### Other client / per-operation configuration
 
 Other optional configuration keyword arguments that can be specified on the client or per-operation.
 
 **Client keyword arguments:**
 
 * __connection_timeout__ (int): The number of seconds the client will wait to establish a connection to the server.
-* __read_timeout__ (int): The number of seconds the client will wait, after the connections has been established, for the server to send a response.
+Defaults to 20 seconds.
+* __read_timeout__ (int): The number of seconds the client will wait, between consecutive read operations, for a
+response from the server. This is a socket level timeout and is not affected by overall data size. Client-side read 
+timeouts will be automatically retried. Defaults to 60 seconds.
 * __transport__ (Any): User-provided transport to send the HTTP request.
 
 **Per-operation keyword arguments:**
 
 * __raw_response_hook__ (callable): The given callback uses the response returned from the service.
 * __raw_request_hook__ (callable): The given callback uses the request before being sent to service.
 * __client_request_id__ (str): Optional user specified identification of the request.
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/PKG-INFO` & `azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 Metadata-Version: 2.1
 Name: azure-storage-file-datalake
-Version: 12.9.0b1
+Version: 12.9.1
 Summary: Microsoft Azure File DataLake Storage Client Library for Python
 Home-page: https://github.com/Azure/azure-sdk-for-python
 Author: Microsoft Corporation
 Author-email: ascl@microsoft.com
 License: MIT License
-Classifier: Development Status :: 4 - Beta
+Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
@@ -210,15 +210,18 @@
 ### Other client / per-operation configuration
 
 Other optional configuration keyword arguments that can be specified on the client or per-operation.
 
 **Client keyword arguments:**
 
 * __connection_timeout__ (int): The number of seconds the client will wait to establish a connection to the server.
-* __read_timeout__ (int): The number of seconds the client will wait, after the connections has been established, for the server to send a response.
+Defaults to 20 seconds.
+* __read_timeout__ (int): The number of seconds the client will wait, between consecutive read operations, for a
+response from the server. This is a socket level timeout and is not affected by overall data size. Client-side read 
+timeouts will be automatically retried. Defaults to 60 seconds.
 * __transport__ (Any): User-provided transport to send the HTTP request.
 
 **Per-operation keyword arguments:**
 
 * __raw_response_hook__ (callable): The given callback uses the response returned from the service.
 * __raw_request_hook__ (callable): The given callback uses the request before being sent to service.
 * __client_request_id__ (str): Optional user specified identification of the request.
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure_storage_file_datalake.egg-info/SOURCES.txt` & `azure-storage-file-datalake-12.9.1/azure_storage_file_datalake.egg-info/SOURCES.txt`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_file_system.py` & `azure-storage-file-datalake-12.9.1/tests/test_file_system.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,21 +1,19 @@
-# coding: utf-8
-
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
-import pytest
 import unittest
 from datetime import datetime, timedelta
+from time import sleep
 
+import pytest
 from azure.core import MatchConditions
 from azure.core.exceptions import HttpResponseError, ResourceNotFoundError
-
 from azure.storage.filedatalake import (
     AccessPolicy,
     AccountSasPermissions,
     DataLakeDirectoryClient,
     DataLakeFileClient,
     DataLakeServiceClient,
     EncryptionScopeOptions,
@@ -26,104 +24,128 @@
     generate_file_sas,
     generate_file_system_sas,
     PublicAccess,
     ResourceTypes
 )
 from azure.storage.filedatalake._models import FileSasPermissions
 
-from devtools_testutils.storage import StorageTestCase
+from devtools_testutils import recorded_by_proxy
+from devtools_testutils.storage import StorageRecordedTestCase
 from settings.testcase import DataLakePreparer
 
 # ------------------------------------------------------------------------------
 TEST_FILE_SYSTEM_PREFIX = 'filesystem'
 # ------------------------------------------------------------------------------
 
 
-class FileSystemTest(StorageTestCase):
+class TestFileSystem(StorageRecordedTestCase):
     def _setUp(self, account_name, account_key):
         url = self.account_url(account_name, 'dfs')
         self.dsc = DataLakeServiceClient(url, account_key)
         self.config = self.dsc._config
         self.test_file_systems = []
 
     def tearDown(self):
         if not self.is_playback():
             try:
                 for file_system in self.test_file_systems:
                     self.dsc.delete_file_system(file_system)
             except:
                 pass
 
-        return super(FileSystemTest, self).tearDown()
-
     # --Helpers-----------------------------------------------------------------
     def _get_file_system_reference(self, prefix=TEST_FILE_SYSTEM_PREFIX):
         file_system_name = self.get_resource_name(prefix)
         self.test_file_systems.append(file_system_name)
         return file_system_name
 
     def _create_file_system(self, file_system_prefix=TEST_FILE_SYSTEM_PREFIX):
         try:
             return self.dsc.create_file_system(self._get_file_system_reference(prefix=file_system_prefix))
         except:
             pass
 
+    def _is_almost_equal(self, first, second, delta):
+        if first == second:
+            return True
+        diff = abs(first - second)
+        if delta is not None:
+            if diff <= delta:
+                return True
+        return False
+
 
     # --Test cases for file system ---------------------------------------------
 
     @DataLakePreparer()
-    def test_create_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name)
         created = file_system_client.create_file_system()
 
         # Assert
-        self.assertTrue(created)
+        assert created
 
     @DataLakePreparer()
-    def test_create_file_system_extra_backslash(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_system_extra_backslash(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name + '/')
         created = file_system_client.create_file_system()
 
         # Assert
-        self.assertTrue(created)
+        assert created
 
     @DataLakePreparer()
-    def test_create_file_system_encryption_scope(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_system_encryption_scope(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
         encryption_scope = EncryptionScopeOptions(default_encryption_scope="hnstestscope1")
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name)
         file_system_client.create_file_system(encryption_scope_options=encryption_scope)
         props = file_system_client.get_file_system_properties()
 
         # Assert
-        self.assertTrue(props)
-        self.assertIsNotNone(props['encryption_scope'])
-        self.assertEqual(props['encryption_scope'].default_encryption_scope, encryption_scope.default_encryption_scope)
+        assert props
+        assert props['encryption_scope'] is not None
+        assert props['encryption_scope'].default_encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_create_file_system_encryption_scope_account_sas(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_system_encryption_scope_account_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
-        token = generate_account_sas(
+        token = self.generate_sas(
+            generate_account_sas,
             self.dsc.account_name,
             self.dsc.credential.account_key,
             ResourceTypes(service=True, file_system=True, object=True),
             permission=AccountSasPermissions(write=True, read=True, create=True, delete=True),
             expiry=datetime.utcnow() + timedelta(hours=5),
             encryption_scope="hnstestscope1")
         file_system_name = self._get_file_system_reference()
@@ -136,29 +158,33 @@
         fsc_sas = FileSystemClient(url, file_system_name, token)
         fsc_sas.create_file('file1')
         fsc_sas.create_directory('dir1')
         dir_props = fsc_sas.get_directory_client('dir1').get_directory_properties()
         file_props = fsc_sas.get_file_client('file1').get_file_properties()
 
         # Assert
-        self.assertTrue(dir_props)
-        self.assertIsNotNone(dir_props.encryption_scope)
-        self.assertEqual(dir_props.encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertTrue(file_props)
-        self.assertIsNotNone(file_props.encryption_scope)
-        self.assertEqual(file_props.encryption_scope, encryption_scope.default_encryption_scope)
+        assert dir_props
+        assert dir_props.encryption_scope is not None
+        assert dir_props.encryption_scope == encryption_scope.default_encryption_scope
+        assert file_props
+        assert file_props.encryption_scope is not None
+        assert file_props.encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_create_file_system_encryption_scope_file_system_sas(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_system_encryption_scope_file_system_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
         file_system_name = self._get_file_system_reference()
-        token = generate_file_system_sas(
+        token = self.generate_sas(
+            generate_file_system_sas,
             self.dsc.account_name,
             file_system_name,
             self.dsc.credential.account_key,
             permission=FileSystemSasPermissions(write=True, read=True, delete=True),
             expiry=datetime.utcnow() + timedelta(hours=5),
             encryption_scope="hnstestscope1")
         encryption_scope = EncryptionScopeOptions(default_encryption_scope="hnstestscope1")
@@ -170,29 +196,33 @@
         fsc_sas = FileSystemClient(url, file_system_name, token)
         fsc_sas.create_file('file1')
         fsc_sas.create_directory('dir1')
         dir_props = fsc_sas.get_directory_client('dir1').get_directory_properties()
         file_props = fsc_sas.get_file_client('file1').get_file_properties()
 
         # Assert
-        self.assertTrue(dir_props)
-        self.assertIsNotNone(dir_props.encryption_scope)
-        self.assertEqual(dir_props.encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertTrue(file_props)
-        self.assertIsNotNone(file_props.encryption_scope)
-        self.assertEqual(file_props.encryption_scope, encryption_scope.default_encryption_scope)
+        assert dir_props
+        assert dir_props.encryption_scope is not None
+        assert dir_props.encryption_scope == encryption_scope.default_encryption_scope
+        assert file_props
+        assert file_props.encryption_scope is not None
+        assert file_props.encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_create_file_system_encryption_scope_directory_sas(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_system_encryption_scope_directory_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
         file_system_name = self._get_file_system_reference()
-        token = generate_directory_sas(
+        token = self.generate_sas(
+            generate_directory_sas,
             self.dsc.account_name,
             file_system_name,
             'dir1',
             self.dsc.credential.account_key,
             permission=FileSasPermissions(write=True, read=True, delete=True),
             expiry=datetime.utcnow() + timedelta(hours=5),
             encryption_scope="hnstestscope1")
@@ -203,26 +233,30 @@
         file_system_client.create_file_system(encryption_scope_options=encryption_scope)
 
         dir_client = DataLakeDirectoryClient(url, file_system_name, 'dir1', credential=token)
         dir_client.create_directory()
         dir_props = dir_client.get_directory_properties()
 
         # Assert
-        self.assertTrue(dir_props)
-        self.assertIsNotNone(dir_props.encryption_scope)
-        self.assertEqual(dir_props.encryption_scope, encryption_scope.default_encryption_scope)
+        assert dir_props
+        assert dir_props.encryption_scope is not None
+        assert dir_props.encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_create_file_system_encryption_scope_file_sas(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_system_encryption_scope_file_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
         file_system_name = self._get_file_system_reference()
-        token = generate_file_sas(
+        token = self.generate_sas(
+            generate_file_sas,
             self.dsc.account_name,
             file_system_name,
             'dir1',
             'file1',
             self.dsc.credential.account_key,
             permission=FileSasPermissions(write=True, read=True, delete=True),
             expiry=datetime.utcnow() + timedelta(hours=5),
@@ -235,92 +269,117 @@
         file_system_client.create_directory('dir1')
 
         file_client = DataLakeFileClient(url, file_system_name, 'dir1/file1', token)
         file_client.create_file()
         file_props = file_client.get_file_properties()
 
         # Assert
-        self.assertTrue(file_props)
-        self.assertIsNotNone(file_props.encryption_scope)
-        self.assertEqual(file_props.encryption_scope, encryption_scope.default_encryption_scope)
+        assert file_props
+        assert file_props.encryption_scope is not None
+        assert file_props.encryption_scope == encryption_scope.default_encryption_scope
 
     @DataLakePreparer()
-    def test_file_system_exists(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_file_system_exists(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client1 = self.dsc.get_file_system_client(file_system_name)
         file_system_client2 = self.dsc.get_file_system_client("nonexistentfs")
         file_system_client1.create_file_system()
 
-        self.assertTrue(file_system_client1.exists())
-        self.assertFalse(file_system_client2.exists())
+        assert file_system_client1.exists()
+        assert not file_system_client2.exists()
 
     @DataLakePreparer()
-    def test_create_file_system_with_metadata(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_system_with_metadata(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         metadata = {'hello': 'world', 'number': '42'}
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name)
         created = file_system_client.create_file_system(metadata=metadata)
 
         # Assert
         meta = file_system_client.get_file_system_properties().metadata
-        self.assertTrue(created)
-        self.assertDictEqual(meta, metadata)
+        assert created
+        assert meta == metadata
 
     @DataLakePreparer()
-    def test_set_file_system_acl(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_set_file_system_acl(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+        variables = kwargs.pop('variables', {})
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Act
         file_system = self._create_file_system()
+        expiry_time = self.get_datetime_variable(variables, 'expiry_time', datetime.utcnow() + timedelta(hours=1))
+        start_time = self.get_datetime_variable(variables, 'start_time', datetime.utcnow())
         access_policy = AccessPolicy(permission=FileSystemSasPermissions(read=True),
-                                     expiry=datetime.utcnow() + timedelta(hours=1),
-                                     start=datetime.utcnow())
+                                     expiry=expiry_time,
+                                     start=start_time)
         signed_identifier1 = {'testid': access_policy}
         response = file_system.set_file_system_access_policy(signed_identifier1, public_access=PublicAccess.FileSystem)
 
-        self.assertIsNotNone(response.get('etag'))
-        self.assertIsNotNone(response.get('last_modified'))
+        assert response.get('etag') is not None
+        assert response.get('last_modified') is not None
         acl1 = file_system.get_file_system_access_policy()
-        self.assertIsNotNone(acl1['public_access'])
-        self.assertEqual(len(acl1['signed_identifiers']), 1)
+        assert acl1['public_access'] is not None
+        assert len(acl1['signed_identifiers']) == 1
 
         # If set signed identifier without specifying the access policy then it will be default to None
         signed_identifier2 = {'testid': access_policy, 'test2': access_policy}
         file_system.set_file_system_access_policy(signed_identifier2)
         acl2 = file_system.get_file_system_access_policy()
-        self.assertIsNone(acl2['public_access'])
-        self.assertEqual(len(acl2['signed_identifiers']), 2)
+        assert acl2['public_access'] is None
+        assert len(acl2['signed_identifiers']) == 2
+
+        return variables
 
     @DataLakePreparer()
-    def test_list_file_systems(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_file_systems(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
         file_system = self.dsc.create_file_system(file_system_name)
 
         # Act
         file_systems = list(self.dsc.list_file_systems())
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 1)
-        self.assertIsNotNone(file_systems[0])
+        assert file_systems is not None
+        assert len(file_systems) >= 1
+        assert file_systems[0] is not None
         self.assertNamedItemInContainer(file_systems, file_system.file_system_name)
-        self.assertIsNotNone(file_systems[0].has_immutability_policy)
-        self.assertIsNotNone(file_systems[0].has_legal_hold)
+        assert file_systems[0].has_immutability_policy is not None
+        assert file_systems[0].has_legal_hold is not None
 
     @DataLakePreparer()
-    def test_list_file_systems_encryption_scope(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_file_systems_encryption_scope(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name1 = self._get_file_system_reference(prefix='es')
         file_system_name2 = self._get_file_system_reference(prefix='es2')
         encryption_scope = EncryptionScopeOptions(default_encryption_scope="hnstestscope1")
         self.dsc.create_file_system(file_system_name1, encryption_scope_options=encryption_scope)
         self.dsc.create_file_system(file_system_name2, encryption_scope_options=encryption_scope)
@@ -328,336 +387,407 @@
         # Act
         file_systems = []
         for filesystem in self.dsc.list_file_systems():
             if filesystem['name'] in [file_system_name1, file_system_name2]:
                 file_systems.append(filesystem)
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertEqual(len(file_systems), 2)
-        self.assertEqual(file_systems[0].encryption_scope.default_encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertEqual(file_systems[1].encryption_scope.default_encryption_scope, encryption_scope.default_encryption_scope)
+        assert file_systems is not None
+        assert len(file_systems) == 2
+        assert file_systems[0].encryption_scope.default_encryption_scope == encryption_scope.default_encryption_scope
+        assert file_systems[1].encryption_scope.default_encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_list_file_systems_account_sas(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_file_systems_account_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
         file_system = self.dsc.create_file_system(file_system_name)
-        sas_token = generate_account_sas(
+        sas_token = self.generate_sas(
+            generate_account_sas,
             datalake_storage_account_name,
             datalake_storage_account_key,
             ResourceTypes(service=True),
             AccountSasPermissions(list=True),
             datetime.utcnow() + timedelta(hours=1),
         )
 
         # Act
         dsc = DataLakeServiceClient(self.account_url(datalake_storage_account_name, 'dfs'), credential=sas_token)
         file_systems = list(dsc.list_file_systems())
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 1)
-        self.assertIsNotNone(file_systems[0])
+        assert file_systems is not None
+        assert len(file_systems) >= 1
+        assert file_systems[0] is not None
         self.assertNamedItemInContainer(file_systems, file_system.file_system_name)
 
+    @pytest.mark.skip(reason="Feature not yet enabled. Make sure to record this test once enabled.")
     @DataLakePreparer()
-    def test_rename_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
-        if not self.is_playback():
-            return
+    @recorded_by_proxy
+    def test_rename_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         old_name1 = self._get_file_system_reference(prefix="oldcontainer1")
         old_name2 = self._get_file_system_reference(prefix="oldcontainer2")
         new_name = self._get_file_system_reference(prefix="newcontainer")
         filesystem1 = self.dsc.create_file_system(old_name1)
         self.dsc.create_file_system(old_name2)
 
         new_filesystem = self.dsc._rename_file_system(name=old_name1, new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             self.dsc._rename_file_system(name=old_name2, new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             filesystem1.get_file_system_properties()
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             self.dsc._rename_file_system(name="badfilesystem", new_name="filesystem")
-        self.assertEqual(new_name, new_filesystem.get_file_system_properties().name)
+        assert new_name == new_filesystem.get_file_system_properties().name
 
-    @pytest.mark.skip(reason="Feature not yet enabled. Record when enabled.")
+    @pytest.mark.skip(reason="Feature not yet enabled. Make sure to record this test once enabled.")
     @DataLakePreparer()
-    def test_rename_file_system_with_file_system_client(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_rename_file_system_with_file_system_client(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         old_name1 = self._get_file_system_reference(prefix="oldcontainer1")
         old_name2 = self._get_file_system_reference(prefix="oldcontainer2")
         new_name = self._get_file_system_reference(prefix="newcontainer")
         bad_name = self._get_file_system_reference(prefix="badcontainer")
         filesystem1 = self.dsc.create_file_system(old_name1)
         file_system2 = self.dsc.create_file_system(old_name2)
         bad_file_system = self.dsc.get_file_system_client(bad_name)
 
         new_filesystem = filesystem1._rename_file_system(new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             file_system2._rename_file_system(new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             filesystem1.get_file_system_properties()
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             bad_file_system._rename_file_system(new_name="filesystem")
-        self.assertEqual(new_name, new_filesystem.get_file_system_properties().name)
+        assert new_name == new_filesystem.get_file_system_properties().name
 
     @DataLakePreparer()
-    def test_list_system_filesystems(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_system_filesystems(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         dsc = DataLakeServiceClient(self.dsc.url, credential=datalake_storage_account_key)
         # Act
         filesystems = list(dsc.list_file_systems(include_system=True))
 
         # Assert
         found = False
         for fs in filesystems:
             if fs.name == "$logs":
                 found = True
-        self.assertEqual(found, True)
+        assert found == True
 
+    @pytest.mark.skip(reason="Feature not yet enabled. Make sure to record this test once enabled.")
     @DataLakePreparer()
-    def test_rename_file_system_with_source_lease(self, datalake_storage_account_name, datalake_storage_account_key):
-        if not self.is_playback():
-            return
+    @recorded_by_proxy
+    def test_rename_file_system_with_source_lease(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         old_name = self._get_file_system_reference(prefix="old")
         new_name = self._get_file_system_reference(prefix="new")
         filesystem = self.dsc.create_file_system(old_name)
-        filesystem_lease_id = filesystem.acquire_lease()
-        with self.assertRaises(HttpResponseError):
+        filesystem_lease_id = filesystem.acquire_lease(lease_id='00000000-1111-2222-3333-444444444444')
+        with pytest.raises(HttpResponseError):
             self.dsc._rename_file_system(name=old_name, new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             self.dsc._rename_file_system(name=old_name, new_name=new_name, lease="bad_id")
         new_filesystem = self.dsc._rename_file_system(name=old_name, new_name=new_name, lease=filesystem_lease_id)
-        self.assertEqual(new_name, new_filesystem.get_file_system_properties().name)
+        assert new_name == new_filesystem.get_file_system_properties().name
 
     @DataLakePreparer()
-    def test_undelete_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
-        # TODO: Needs soft delete enabled account in ARM template.
-        if not self.is_playback():
-            return
+    @recorded_by_proxy
+    def test_undelete_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        name = self._get_file_system_reference("testfs")
+        name = self._get_file_system_reference("testfs5")
         filesystem_client = self.dsc.create_file_system(name)
 
         # Act
         filesystem_client.delete_file_system()
+        if self.is_live:
+            sleep(30)
         # to make sure the filesystem deleted
-        with self.assertRaises(ResourceNotFoundError):
+        with pytest.raises(ResourceNotFoundError):
             filesystem_client.get_file_system_properties()
 
         filesystem_list = list(self.dsc.list_file_systems(include_deleted=True))
-        self.assertTrue(len(filesystem_list) >= 1)
+        assert len(filesystem_list) >= 1
 
         for filesystem in filesystem_list:
             # find the deleted filesystem and restore it
             if filesystem.deleted and filesystem.name == filesystem_client.file_system_name:
                 restored_fs_client = self.dsc.undelete_file_system(filesystem.name, filesystem.deleted_version)
 
                 # to make sure the deleted filesystem is restored
                 props = restored_fs_client.get_file_system_properties()
-                self.assertIsNotNone(props)
+                assert props is not None
 
-    @pytest.mark.skip(reason="We are generating a SAS token therefore play only live but we also need a soft delete enabled account.")
     @DataLakePreparer()
-    def test_restore_file_system_with_sas(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_restore_file_system_with_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        token = generate_account_sas(
+        token = self.generate_sas(
+            generate_account_sas,
             self.dsc.account_name,
             self.dsc.credential.account_key,
             ResourceTypes(service=True, file_system=True),
             AccountSasPermissions(read=True, write=True, list=True, delete=True),
             datetime.utcnow() + timedelta(hours=1),
         )
         dsc = DataLakeServiceClient(self.dsc.url, token)
-        name = self._get_file_system_reference(prefix="filesystem")
+        name = self._get_file_system_reference(prefix="filesystem9")
         filesystem_client = dsc.create_file_system(name)
         filesystem_client.delete_file_system()
+        if self.is_live:
+            sleep(30)
         # to make sure the filesystem is deleted
-        with self.assertRaises(ResourceNotFoundError):
+        with pytest.raises(ResourceNotFoundError):
             filesystem_client.get_file_system_properties()
 
         filesystem_list = list(dsc.list_file_systems(include_deleted=True))
-        self.assertTrue(len(filesystem_list) >= 1)
+        assert len(filesystem_list) >= 1
 
         restored_version = 0
         for filesystem in filesystem_list:
             # find the deleted filesystem and restore it
             if filesystem.deleted and filesystem.name == filesystem_client.file_system_name:
                 restored_fs_client = dsc.undelete_file_system(filesystem.name, filesystem.deleted_version)
 
                 # to make sure the deleted filesystem is restored
                 props = restored_fs_client.get_file_system_properties()
-                self.assertIsNotNone(props)
+                assert props is not None
 
     @DataLakePreparer()
-    def test_delete_file_system_with_existing_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_delete_file_system_with_existing_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
 
         # Act
         deleted = file_system.delete_file_system()
 
         # Assert
-        self.assertIsNone(deleted)
+        assert deleted is None
 
     @DataLakePreparer()
-    def test_delete_none_existing_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_delete_none_existing_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         fake_file_system_client = self.dsc.get_file_system_client("fakeclient")
 
         # Act
-        with self.assertRaises(ResourceNotFoundError):
+        with pytest.raises(ResourceNotFoundError):
             fake_file_system_client.delete_file_system(match_condition=MatchConditions.IfMissing)
 
     @DataLakePreparer()
-    def test_list_file_systems_with_include_metadata(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_file_systems_with_include_metadata(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         metadata = {'hello': 'world', 'number': '42'}
         resp = file_system.set_file_system_metadata(metadata)
 
         # Act
         file_systems = list(self.dsc.list_file_systems(
             name_starts_with=file_system.file_system_name,
             include_metadata=True))
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 1)
-        self.assertIsNotNone(file_systems[0])
+        assert file_systems is not None
+        assert len(file_systems) >= 1
+        assert file_systems[0] is not None
         self.assertNamedItemInContainer(file_systems, file_system.file_system_name)
-        self.assertDictEqual(file_systems[0].metadata, metadata)
+        assert file_systems[0].metadata == metadata
 
     @DataLakePreparer()
-    def test_list_file_systems_by_page(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_file_systems_by_page(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         for i in range(0, 6):
             self._create_file_system(file_system_prefix="filesystem{}".format(i))
 
         # Act
         file_systems = list(next(self.dsc.list_file_systems(
             results_per_page=3,
             name_starts_with="file",
             include_metadata=True).by_page()))
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 3)
+        assert file_systems is not None
+        assert len(file_systems) >= 3
 
     @DataLakePreparer()
-    def test_list_file_systems_with_public_access(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_file_systems_with_public_access(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
         file_system = self.dsc.get_file_system_client(file_system_name)
         file_system.create_file_system(public_access="blob")
         metadata = {'hello': 'world', 'number': '42'}
         resp = file_system.set_file_system_metadata(metadata)
 
         # Act
         file_systems = list(self.dsc.list_file_systems(
             name_starts_with=file_system.file_system_name,
             include_metadata=True))
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 1)
-        self.assertIsNotNone(file_systems[0])
+        assert file_systems is not None
+        assert len(file_systems) >= 1
+        assert file_systems[0] is not None
         self.assertNamedItemInContainer(file_systems, file_system.file_system_name)
-        self.assertDictEqual(file_systems[0].metadata, metadata)
-        self.assertTrue(file_systems[0].public_access is PublicAccess.File)
+        assert file_systems[0].metadata == metadata
+        assert file_systems[0].public_access is PublicAccess.File
 
     @DataLakePreparer()
-    def test_get_file_system_properties(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_get_file_system_properties(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         metadata = {'hello': 'world', 'number': '42'}
         file_system = self._create_file_system()
         file_system.set_file_system_metadata(metadata)
 
         # Act
         props = file_system.get_file_system_properties()
 
         # Assert
-        self.assertIsNotNone(props)
-        self.assertDictEqual(props.metadata, metadata)
-        self.assertIsNotNone(props.has_immutability_policy)
-        self.assertIsNotNone(props.has_legal_hold)
+        assert props is not None
+        assert props.metadata == metadata
+        assert props.has_immutability_policy is not None
+        assert props.has_legal_hold is not None
 
     @DataLakePreparer()
-    def test_service_client_session_closes_after_filesystem_creation(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_service_client_session_closes_after_filesystem_creation(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         dsc2 = DataLakeServiceClient(self.dsc.url, credential=datalake_storage_account_key)
         with DataLakeServiceClient(self.dsc.url, credential=datalake_storage_account_key) as ds_client:
             fs1 = ds_client.create_file_system(self._get_file_system_reference(prefix="fs1"))
             fs1.delete_file_system()
         dsc2.create_file_system(self._get_file_system_reference(prefix="fs2"))
         dsc2.close()
 
     @DataLakePreparer()
-    def test_list_paths(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_paths(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         for i in range(0, 6):
             file_system.create_directory("dir1{}".format(i))
 
         paths = list(file_system.get_paths(upn=True))
 
-        self.assertEqual(len(paths), 6)
-        self.assertTrue(isinstance(paths[0].last_modified, datetime))
+        assert len(paths) == 6
+        assert isinstance(paths[0].last_modified, datetime)
 
     @DataLakePreparer()
-    def test_list_paths_create_expiry(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_paths_create_expiry(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+        variables = kwargs.pop('variables', {})
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         file_client = file_system.create_file('file1')
 
-        expires_on = datetime.utcnow() + timedelta(days=1)
-        file_client.set_file_expiry("Absolute", expires_on=expires_on)
+        expiry_time = self.get_datetime_variable(variables, 'expiry_time', datetime.utcnow() + timedelta(days=1))
+        file_client.set_file_expiry("Absolute", expires_on=expiry_time)
 
         # Act
         paths = list(file_system.get_paths(upn=True))
 
         # Assert
-        self.assertEqual(1, len(paths))
+        assert 1 == len(paths)
         props = file_client.get_file_properties()
         # Properties do not include microseconds so let them vary by 1 second
-        self.assertAlmostEqual(props.creation_time, paths[0].creation_time, delta=timedelta(seconds=1))
-        self.assertAlmostEqual(props.expiry_time, paths[0].expiry_time, delta=timedelta(seconds=1))
+        self._is_almost_equal(props.creation_time, paths[0].creation_time, delta=timedelta(seconds=1))
+        self._is_almost_equal(props.expiry_time, paths[0].expiry_time, delta=timedelta(seconds=1))
+
+        return variables
 
     @DataLakePreparer()
-    def test_list_paths_no_expiry(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_paths_no_expiry(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         file_system.create_file('file1')
 
         # Act
         paths = list(file_system.get_paths(upn=True))
 
         # Assert
-        self.assertEqual(1, len(paths))
-        self.assertIsNone(paths[0].expiry_time)
+        assert 1 == len(paths)
+        assert paths[0].expiry_time is None
 
     @DataLakePreparer()
-    def test_get_deleted_paths(self, datalake_storage_account_name, datalake_storage_account_key):
-        # TODO: Needs soft delete enabled account in ARM template.
-        if not self.is_playback():
-            return
+    @recorded_by_proxy
+    def test_get_deleted_paths(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         file0 = file_system.create_file("file0")
         file1 = file_system.create_file("file1")
 
         dir1 = file_system.create_directory("dir1")
@@ -672,88 +802,104 @@
         dir2.delete_directory()
         file_in_dir3.delete_file()
         file_in_subdir.delete_file()
         deleted_paths = list(file_system.list_deleted_paths())
         dir3_paths = list(file_system.list_deleted_paths(path_prefix="dir3/"))
 
         # Assert
-        self.assertEqual(len(deleted_paths), 6)
-        self.assertEqual(len(dir3_paths), 2)
-        self.assertIsNotNone(dir3_paths[0].deletion_id)
-        self.assertIsNotNone(dir3_paths[1].deletion_id)
-        self.assertEqual(dir3_paths[0].name, 'dir3/file_in_dir3')
-        self.assertEqual(dir3_paths[1].name, 'dir3/subdir/file_in_subdir')
+        assert len(deleted_paths) == 6
+        assert len(dir3_paths) == 2
+        assert dir3_paths[0].deletion_id is not None
+        assert dir3_paths[1].deletion_id is not None
+        assert dir3_paths[0].name == 'dir3/file_in_dir3'
+        assert dir3_paths[1].name == 'dir3/subdir/file_in_subdir'
 
         paths_generator1 = file_system.list_deleted_paths(results_per_page=2).by_page()
         paths1 = list(next(paths_generator1))
 
         paths_generator2 = file_system.list_deleted_paths(results_per_page=4).by_page(
             continuation_token=paths_generator1.continuation_token)
         paths2 = list(next(paths_generator2))
 
         # Assert
-        self.assertEqual(len(paths1), 2)
-        self.assertEqual(len(paths2), 4)
+        assert len(paths1) == 2
+        assert len(paths2) == 4
 
     @DataLakePreparer()
-    def test_list_paths_which_are_all_files(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_paths_which_are_all_files(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         for i in range(0, 6):
             file_system.create_file("file{}".format(i))
 
         paths = list(file_system.get_paths(upn=True))
 
-        self.assertEqual(len(paths), 6)
+        assert len(paths) == 6
 
     @DataLakePreparer()
-    def test_list_paths_with_max_per_page(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_paths_with_max_per_page(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         for i in range(0, 6):
             file_system.create_directory("dir1{}".format(i))
 
         generator1 = file_system.get_paths(max_results=2, upn=True).by_page()
         paths1 = list(next(generator1))
 
         generator2 = file_system.get_paths(max_results=4, upn=True)\
             .by_page(continuation_token=generator1.continuation_token)
         paths2 = list(next(generator2))
 
-        self.assertEqual(len(paths1), 2)
-        self.assertEqual(len(paths2), 4)
-        self.assertEqual(paths2[0].name, "dir12")
+        assert len(paths1) == 2
+        assert len(paths2) == 4
+        assert paths2[0].name == "dir12"
 
     @DataLakePreparer()
-    def test_list_paths_under_specific_path(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_paths_under_specific_path(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         for i in range(0, 6):
             file_system.create_directory("dir1{}".format(i))
 
             # create a subdirectory under the current directory
             subdir = file_system.get_directory_client("dir1{}".format(i)).create_sub_directory("subdir")
             subdir.create_sub_directory("subsub")
 
             # create a file under the current directory
             file_client = subdir.create_file("file")
-            file_client.append_data(b"abced", 0, 5)
+            file_client.append_data(b"abced", 0, 5) # cspell:disable-line
             file_client.flush_data(5)
 
         generator1 = file_system.get_paths(path="dir10/subdir", max_results=2, upn=True).by_page()
         paths = list(next(generator1))
 
-        self.assertEqual(len(paths), 2)
-        self.assertEqual(paths[0].content_length, 5)
+        assert len(paths) == 2
+        assert paths[0].content_length == 5
 
     @DataLakePreparer()
-    def test_list_paths_recursively(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_paths_recursively(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         for i in range(0, 6):
             file_system.create_directory("dir1{}".format(i))
 
             # create a subdirectory under the current directory
@@ -762,37 +908,45 @@
 
             # create a file under the current directory
             subdir.create_file("file")
 
         paths = list(file_system.get_paths(recursive=True, upn=True))
 
         # there are 24 subpaths in total
-        self.assertEqual(len(paths), 24)
+        assert len(paths) == 24
 
     @DataLakePreparer()
-    def test_list_paths_pages_correctly(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_list_paths_pages_correctly(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system(file_system_prefix="fs1")
         for i in range(0, 6):
             file_system.create_directory("dir1{}".format(i))
         for i in range(0, 6):
             file_system.create_file("file{}".format(i))
 
         generator = file_system.get_paths(max_results=6, upn=True).by_page()
         paths1 = list(next(generator))
         paths2 = list(next(generator))
-        with self.assertRaises(StopIteration):
+        with pytest.raises(StopIteration):
             list(next(generator))
 
-        self.assertEqual(len(paths1), 6)
-        self.assertEqual(len(paths2), 6)
+        assert len(paths1) == 6
+        assert len(paths2) == 6
 
     @DataLakePreparer()
-    def test_path_properties_encryption_scope(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_path_properties_encryption_scope(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         encryption_scope = EncryptionScopeOptions(default_encryption_scope="hnstestscope1")
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name)
@@ -801,100 +955,118 @@
         file_system_client.create_file('dir1/file1')
 
         dir_props = file_system_client.get_directory_client('dir1').get_directory_properties()
         file_props = file_system_client.get_file_client('dir1/file1').get_file_properties()
         paths = list(file_system_client.get_paths(recursive=False, upn=True))
 
         # Assert
-        self.assertEqual(dir_props.encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertEqual(file_props.encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertTrue(paths)
-        self.assertIsNotNone(paths[0].encryption_scope)
-        self.assertEqual(paths[0].encryption_scope, encryption_scope.default_encryption_scope)
+        assert dir_props.encryption_scope == encryption_scope.default_encryption_scope
+        assert file_props.encryption_scope == encryption_scope.default_encryption_scope
+        assert paths
+        assert paths[0].encryption_scope is not None
+        assert paths[0].encryption_scope == encryption_scope.default_encryption_scope
 
     @DataLakePreparer()
-    def test_create_directory_from_file_system_client(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_directory_from_file_system_client(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         file_system.create_directory("dir1/dir2")
 
         paths = list(file_system.get_paths(recursive=False, upn=True))
 
-        self.assertEqual(len(paths), 1)
-        self.assertEqual(paths[0].name, "dir1")
+        assert len(paths) == 1
+        assert paths[0].name == "dir1"
 
     @DataLakePreparer()
-    def test_create_file_from_file_system_client(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_from_file_system_client(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = self._create_file_system()
         file_system.create_file("dir1/dir2/file")
 
         paths = list(file_system.get_paths(recursive=True, upn=True))
 
-        self.assertEqual(len(paths), 3)
-        self.assertEqual(paths[0].name, "dir1")
-        self.assertEqual(paths[2].is_directory, False)
+        assert len(paths) == 3
+        assert paths[0].name == "dir1"
+        assert paths[2].is_directory == False
 
     @DataLakePreparer()
-    def test_get_root_directory_client(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_get_root_directory_client(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         file_system = self._create_file_system()
         directory_client = file_system._get_root_directory_client()
 
         acl = 'user::rwx,group::r-x,other::rwx'
         directory_client.set_access_control(acl=acl)
         access_control = directory_client.get_access_control()
 
-        self.assertEqual(acl, access_control['acl'])
+        assert acl == access_control['acl']
 
     @DataLakePreparer()
-    def test_file_system_sessions_closes_properly(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_file_system_sessions_closes_properly(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
-        file_system_client = self._create_file_system("fenrhxsbfvsdvdsvdsadb")
+        file_system_client = self._create_file_system("fenrhxsbfvsdvdsvdsadb") # cspell:disable-line
         with file_system_client as fs_client:
             with fs_client.get_file_client("file1.txt") as f_client:
                 f_client.create_file()
             with fs_client.get_file_client("file2.txt") as f_client:
                 f_client.create_file()
             with fs_client.get_directory_client("file1") as f_client:
                 f_client.create_directory()
             with fs_client.get_directory_client("file2") as f_client:
                 f_client.create_directory()
 
     @DataLakePreparer()
-    def test_undelete_dir_with_version_id(self, datalake_storage_account_name, datalake_storage_account_key):
-        # TODO: Needs soft delete enabled account in ARM template.
-        if not self.is_playback():
-            return
+    @recorded_by_proxy
+    def test_undelete_dir_with_version_id(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        file_system_client = self._create_file_system("fs")
+        file_system_client = self._create_file_system("fs2")
         dir_path = 'dir10'
         dir_client = file_system_client.create_directory(dir_path)
         resp = dir_client.delete_directory()
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             file_system_client.get_file_client(dir_path).get_file_properties()
         restored_dir_client = file_system_client._undelete_path(dir_path, resp['deletion_id'])
         resp = restored_dir_client.get_directory_properties()
-        self.assertIsNotNone(resp)
+        assert resp is not None
 
     @DataLakePreparer()
-    def test_undelete_file_with_version_id(self, datalake_storage_account_name, datalake_storage_account_key):
-        # TODO: Needs soft delete enabled account in ARM template.
-        if not self.is_playback():
-            return
+    @recorded_by_proxy
+    def test_undelete_file_with_version_id(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        file_system_client = self._create_file_system("fs1")
+        file_system_client = self._create_file_system("fs3")
         file_path = 'dir10/fileŇ'
         dir_client = file_system_client.create_file(file_path)
         resp = dir_client.delete_file()
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             file_system_client.get_file_client(file_path).get_file_properties()
         restored_file_client = file_system_client._undelete_path(file_path, resp['deletion_id'])
         resp = restored_file_client.get_file_properties()
-        self.assertIsNotNone(resp)
+        assert resp is not None
 
 # ------------------------------------------------------------------------------
 if __name__ == '__main__':
     unittest.main()
```

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_file_async.py` & `azure-storage-file-datalake-12.9.1/tests/test_file_async.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_directory.py` & `azure-storage-file-datalake-12.9.1/tests/test_file.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,54 +1,56 @@
-# coding: utf-8
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 import unittest
 from datetime import datetime, timedelta
+from math import ceil
 
 import pytest
-
 from azure.core import MatchConditions
+from azure.core.credentials import AzureSasCredential
 from azure.core.exceptions import (
-    AzureError,
+    ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceModifiedError,
-    ResourceNotFoundError,
-    ServiceRequestError
+    ResourceNotFoundError
 )
 from azure.storage.filedatalake import (
+    AccountSasPermissions,
     ContentSettings,
     DataLakeDirectoryClient,
+    DataLakeFileClient,
     DataLakeServiceClient,
-    DirectorySasPermissions,
     EncryptionScopeOptions,
+    FileSasPermissions,
+    FileSystemClient,
     FileSystemSasPermissions,
-    generate_directory_sas,
-    generate_file_system_sas
+    generate_account_sas,
+    generate_file_sas,
+    generate_file_system_sas,
+    ResourceTypes
 )
-from azure.storage.filedatalake._models import AccessControlChangeCounters, AccessControlChangeResult
-from azure.storage.filedatalake._serialize import _SUPPORTED_API_VERSIONS
 
+from devtools_testutils import recorded_by_proxy
+from devtools_testutils.storage import StorageRecordedTestCase
 from settings.testcase import DataLakePreparer
-from devtools_testutils.storage import StorageTestCase
 
 # ------------------------------------------------------------------------------
-TEST_DIRECTORY_PREFIX = 'directory'
-REMOVE_ACL = "mask," + "default:user,default:group," + \
-             "user:ec3595d6-2c17-4696-8caa-7e139758d24a,group:ec3595d6-2c17-4696-8caa-7e139758d24a," + \
-             "default:user:ec3595d6-2c17-4696-8caa-7e139758d24a,default:group:ec3595d6-2c17-4696-8caa-7e139758d24a"
 
+TEST_DIRECTORY_PREFIX = 'directory'
+TEST_FILE_PREFIX = 'file'
+FILE_PATH = 'file_output.temp.dat'
 
 # ------------------------------------------------------------------------------
 
 
-class DirectoryTest(StorageTestCase):
+class TestFile(StorageRecordedTestCase):
     def _setUp(self, account_name, account_key):
         url = self.account_url(account_name, 'dfs')
         self.dsc = DataLakeServiceClient(url, credential=account_key, logging_enable=True)
         self.config = self.dsc._config
 
         self.file_system_name = self.get_resource_name('filesystem')
 
@@ -59,1172 +61,1245 @@
             except ResourceExistsError:
                 pass
 
     def tearDown(self):
         if not self.is_playback():
             try:
                 self.dsc.delete_file_system(self.file_system_name)
-                for file_system in self.dsc.list_file_systems():
-                    self.dsc.delete_file_system(file_system.name)
             except:
                 pass
 
-        return super(DirectoryTest, self).tearDown()
-
     # --Helpers-----------------------------------------------------------------
     def _get_directory_reference(self, prefix=TEST_DIRECTORY_PREFIX):
         directory_name = self.get_resource_name(prefix)
         return directory_name
 
-    def _create_directory_and_get_directory_client(self, directory_name=None):
-        directory_name = directory_name if directory_name else self._get_directory_reference()
+    def _get_file_reference(self, prefix=TEST_FILE_PREFIX):
+        file_name = self.get_resource_name(prefix)
+        return file_name
+
+    def _create_file_system(self):
+        return self.dsc.create_file_system(self._get_file_system_reference())
+
+    def _create_directory_and_return_client(self, directory=None):
+        directory_name = directory if directory else self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client.create_directory()
         return directory_client
 
-    def _create_sub_directory_and_files(self, directory_client, num_of_dirs, num_of_files_per_dir):
-        # the name suffix matter since we need to avoid creating the same directories/files in record mode
-        for i in range(0, num_of_dirs):
-            sub_dir = directory_client.create_sub_directory(self.get_resource_name('subdir' + str(i)))
-            for j in range(0, num_of_files_per_dir):
-                sub_dir.create_file(self.get_resource_name('subfile' + str(j)))
-
-    def _create_file_system(self):
-        return self.dsc.create_file_system(self._get_file_system_reference())
+    def _create_file_and_return_client(self, directory="", file=None):
+        if directory:
+            self._create_directory_and_return_client(directory)
+        if not file:
+            file = self._get_file_reference()
+        file_client = self.dsc.get_file_client(self.file_system_name, directory + '/' + file)
+        file_client.create_file()
+        return file_client
+
+    def _is_almost_equal(self, first, second, delta):
+        if first == second:
+            return True
+        diff = abs(first - second)
+        if delta is not None:
+            if diff <= delta:
+                return True
+        return False
 
     # --Helpers-----------------------------------------------------------------
 
     @DataLakePreparer()
-    def test_create_directory(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
-        content_settings = ContentSettings(
-            content_language='spanish',
-            content_disposition='inline')
-        # Act
+
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        created = directory_client.create_directory(content_settings=content_settings)
+        directory_client.create_directory()
+
+        file_client = directory_client.get_file_client('filename')
+        response = file_client.create_file()
 
         # Assert
-        self.assertTrue(created)
+        assert response is not None
 
     @DataLakePreparer()
-    def test_create_directory_owner_group_acl(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_owner_group_acl(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         test_string = '4cf4e284-f6a8-4540-b53e-c3469af032dc'
         test_string_acl = 'user::rwx,group::r-x,other::rwx'
         # Arrange
         directory_name = self._get_directory_reference()
 
-        # Create a directory
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory(owner=test_string, group=test_string, acl=test_string_acl)
-
-        # Assert
-        acl_properties = directory_client.get_access_control()
-        self.assertIsNotNone(acl_properties)
-        self.assertEqual(acl_properties['owner'], test_string)
-        self.assertEqual(acl_properties['group'], test_string)
-        self.assertEqual(acl_properties['acl'], test_string_acl)
+        directory_client.create_directory()
 
-    @DataLakePreparer()
-    def test_create_directory_proposed_lease_id(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        test_string = '4cf4e284-f6a8-4540-b53e-c3469af032dc'
-        test_duration = 15
-        # Arrange
-        directory_name = self._get_directory_reference()
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory(lease_id=test_string, lease_duration=test_duration)
+        file_client = directory_client.get_file_client('filename')
+        file_client.create_file(owner=test_string, group=test_string, acl=test_string_acl)
 
         # Assert
-        properties = directory_client.get_directory_properties()
-        self.assertIsNotNone(properties)
-        self.assertEqual(properties.lease['status'], 'locked')
-        self.assertEqual(properties.lease['state'], 'leased')
-        self.assertEqual(properties.lease['duration'], 'fixed')
+        acl_properties = file_client.get_access_control()
+        assert acl_properties is not None
+        assert acl_properties['owner'] == test_string
+        assert acl_properties['group'] == test_string
+        assert acl_properties['acl'] == test_string_acl
 
     @DataLakePreparer()
-    def test_create_sub_directory_proposed_lease_id(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_proposed_lease_id(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         test_string = '4cf4e284-f6a8-4540-b53e-c3469af032dc'
         test_duration = 15
         # Arrange
         directory_name = self._get_directory_reference()
+
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client = directory_client.create_sub_directory(sub_directory='sub1',
-                                                                 lease_id=test_string,
-                                                                 lease_duration=test_duration)
+        directory_client.create_directory()
+
+        file_client = directory_client.get_file_client('filename')
+        file_client.create_file(lease_id=test_string, lease_duration=test_duration)
 
         # Assert
-        properties = directory_client.get_directory_properties()
-        self.assertIsNotNone(properties)
-        self.assertEqual(properties.lease['status'], 'locked')
-        self.assertEqual(properties.lease['state'], 'leased')
-        self.assertEqual(properties.lease['duration'], 'fixed')
+        properties = file_client.get_file_properties()
+        assert properties is not None
+        assert properties.lease['status'] == 'locked'
+        assert properties.lease['state'] == 'leased'
+        assert properties.lease['duration'] == 'fixed'
 
     @DataLakePreparer()
-    def test_directory_exists(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_relative_expiry(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        test_expiry_time = 86400000  # 1 day in milliseconds
         # Arrange
         directory_name = self._get_directory_reference()
 
-        directory_client1 = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client2 = self.dsc.get_directory_client(self.file_system_name, "nonexistentdir")
-        directory_client1.create_directory()
+        # Create a directory to put the file under that
+        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
+        directory_client.create_directory()
 
-        self.assertTrue(directory_client1.exists())
-        self.assertFalse(directory_client2.exists())
+        file_client = directory_client.get_file_client('filename')
+        file_client.create_file(expires_on=test_expiry_time)
 
-    @DataLakePreparer()
-    def test_using_oauth_token_credential_to_create_directory(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        # generate a token with directory level create permission
-        directory_name = self._get_directory_reference()
-        token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        response = directory_client.create_directory()
-        self.assertIsNotNone(response)
+        # Assert
+        file_properties = file_client.get_file_properties()
+        expiry_time = file_properties['expiry_time']
+        expiry_time = expiry_time.replace(tzinfo=None)  # Strip timezone info to be able to compare
+        creation_time = file_properties['creation_time']
+        creation_time = creation_time.replace(tzinfo=None)  # Strip timezone info to be able to compare
+        assert file_properties is not None
+        assert self._is_almost_equal(expiry_time, creation_time + timedelta(days=1), timedelta(seconds=60)) is True
 
     @DataLakePreparer()
-    def test_create_directory_with_match_conditions(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_absolute_expiry(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        test_expiry_time = datetime(2075, 4, 4)
         # Arrange
         directory_name = self._get_directory_reference()
 
-        # Act
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        created = directory_client.create_directory(match_condition=MatchConditions.IfMissing)
+        directory_client.create_directory()
+
+        file_client = directory_client.get_file_client('filename')
+        file_client.create_file(expires_on=test_expiry_time)
 
         # Assert
-        self.assertTrue(created)
+        file_properties = file_client.get_file_properties()
+        expiry_time = file_properties['expiry_time']
+        expiry_time = expiry_time.replace(tzinfo=None)  # Strip timezone info to be able to compare
+        assert file_properties is not None
+        assert self._is_almost_equal(expiry_time, test_expiry_time, timedelta(seconds=1)) is True
 
     @DataLakePreparer()
-    def test_create_directory_with_permission(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_extra_backslashes(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
-        directory_name = self._get_directory_reference()
-
-        # Act
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        created = directory_client.create_directory(permissions="rwxr--r--", umask="0000")
+        file_client = self._create_file_and_return_client()
 
-        prop = directory_client.get_access_control()
+        new_file_client = DataLakeFileClient(self.account_url(datalake_storage_account_name, 'dfs'),
+                                             file_client.file_system_name + '/',
+                                             '/' + file_client.path_name,
+                                             credential=datalake_storage_account_key, logging_enable=True)
+        response = new_file_client.create_file()
 
         # Assert
-        self.assertTrue(created)
-        self.assertEqual(prop['permissions'], 'rwxr--r--')
+        assert response is not None
 
     @DataLakePreparer()
-    def test_create_directory_with_content_settings(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_file_exists(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
-        content_settings = ContentSettings(
-            content_language='spanish',
-            content_disposition='inline')
-        # Act
+
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        created = directory_client.create_directory(content_settings=content_settings)
+        directory_client.create_directory()
 
-        # Assert
-        self.assertTrue(created)
+        file_client1 = directory_client.get_file_client('filename')
+        file_client2 = directory_client.get_file_client('nonexistentfile')
+        file_client1.create_file()
+
+        assert file_client1.exists()
+        assert not file_client2.exists()
 
     @DataLakePreparer()
-    def test_create_directory_with_metadata(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_using_oauth_token_credential(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
-        directory_name = self._get_directory_reference()
-        metadata = {'hello': 'world', 'number': '42'}
-        # Act
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        created = directory_client.create_directory(metadata=metadata)
+        file_name = self._get_file_reference()
+        token_credential = self.generate_oauth_token()
+
+        # Create a directory to put the file under that
+        file_client = DataLakeFileClient(self.dsc.url, self.file_system_name, file_name,
+                                         credential=token_credential)
 
-        properties = directory_client.get_directory_properties()
+        response = file_client.create_file()
 
         # Assert
-        self.assertTrue(created)
+        assert response is not None
 
     @DataLakePreparer()
-    def test_delete_directory(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_with_existing_name(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
-        directory_name = self._get_directory_reference()
-        metadata = {'hello': 'world', 'number': '42'}
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory(metadata=metadata)
+        file_client = self._create_file_and_return_client()
 
-        directory_client.delete_directory()
+        with pytest.raises(ResourceExistsError):
+            # if the file exists then throw error
+            # if_none_match='*' is to make sure no existing file
+            file_client.create_file(match_condition=MatchConditions.IfMissing)
 
     @DataLakePreparer()
-    def test_delete_directory_with_if_modified_since(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_with_lease_id(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
-
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client.create_directory()
-        prop = directory_client.get_directory_properties()
 
-        with self.assertRaises(ResourceModifiedError):
-            directory_client.delete_directory(if_modified_since=prop['last_modified'])
+        file_client = directory_client.get_file_client('filename')
+        # Act
+        file_client.create_file()
+        lease = file_client.acquire_lease(lease_id='00000000-1111-2222-3333-444444444444')
+        create_resp = file_client.create_file(lease=lease)
+
+        # Assert
+        file_properties = file_client.get_file_properties()
+        assert file_properties is not None
+        assert file_properties.etag == create_resp.get('etag')
+        assert file_properties.last_modified == create_resp.get('last_modified')
 
     @DataLakePreparer()
-    def test_create_sub_directory_and_delete_sub_directory(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_create_file_under_root_directory(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
-        directory_name = self._get_directory_reference()
-        metadata = {'hello': 'world', 'number': '42'}
-
-        # Create a directory first, to prepare for creating sub directory
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory(metadata=metadata)
-
-        # Create sub directory from the current directory
-        sub_directory_name = 'subdir'
-        sub_directory_created = directory_client.create_sub_directory(sub_directory_name)
+        # get a file client to interact with the file under root directory
+        file_client = self.dsc.get_file_client(self.file_system_name, "filename")
 
-        # to make sure the sub directory was indeed created by get sub_directory properties from sub directory client
-        sub_directory_client = self.dsc.get_directory_client(self.file_system_name,
-                                                             directory_name + '/' + sub_directory_name)
-        sub_properties = sub_directory_client.get_directory_properties()
+        response = file_client.create_file()
 
         # Assert
-        self.assertTrue(sub_directory_created)
-        self.assertTrue(sub_properties)
-
-        # Act
-        directory_client.delete_sub_directory(sub_directory_name)
-        with self.assertRaises(ResourceNotFoundError):
-            sub_directory_client.get_directory_properties()
+        assert response is not None
 
     @DataLakePreparer()
-    def test_set_access_control(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        metadata = {'hello': 'world', 'number': '42'}
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory(metadata=metadata)
+    @recorded_by_proxy
+    def test_append_data(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        response = directory_client.set_access_control(permissions='0777')
-        # Assert
-        self.assertIsNotNone(response)
-
-    @DataLakePreparer()
-    def test_set_access_control_with_acl(self, datalake_storage_account_name, datalake_storage_account_key):
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
-        metadata = {'hello': 'world', 'number': '42'}
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory(metadata=metadata)
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        directory_client.set_access_control(acl=acl)
-        access_control = directory_client.get_access_control()
+        # Create a directory to put the file under that
+        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
+        directory_client.create_directory()
 
-        # Assert
+        file_client = directory_client.get_file_client('filename')
+        file_client.create_file()
 
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        # Act
+        response = file_client.append_data(b'abc', 0, 3)
+        assert response is not None
 
     @DataLakePreparer()
-    def test_set_access_control_if_none_modified(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        resp = directory_client.create_directory()
-
-        response = directory_client.set_access_control(permissions='0777', etag=resp['etag'],
-                                                       match_condition=MatchConditions.IfNotModified)
-        # Assert
-        self.assertIsNotNone(response)
+    @recorded_by_proxy
+    def test_append_empty_data(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-    @DataLakePreparer()
-    def test_get_access_control(self, datalake_storage_account_name, datalake_storage_account_key):
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        metadata = {'hello': 'world', 'number': '42'}
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory(metadata=metadata, permissions='0777')
+        file_client = self._create_file_and_return_client()
 
         # Act
-        response = directory_client.get_access_control()
-        # Assert
-        self.assertIsNotNone(response)
+        file_client.flush_data(0)
+        file_props = file_client.get_file_properties()
+
+        assert file_props['size'] == 0
 
     @DataLakePreparer()
-    def test_get_access_control_with_match_conditions(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_flush_data(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
+
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        resp = directory_client.create_directory(permissions='0777', umask='0000')
+        directory_client.create_directory()
+
+        file_client = directory_client.get_file_client('filename')
+        file_client.create_file()
 
         # Act
-        response = directory_client.get_access_control(etag=resp['etag'], match_condition=MatchConditions.IfNotModified)
+        file_client.append_data(b'abc', 0, 3)
+        response = file_client.flush_data(3)
+
         # Assert
-        self.assertIsNotNone(response)
-        self.assertEqual(response['permissions'], 'rwxrwxrwx')
+        prop = file_client.get_file_properties()
+        assert response is not None
+        assert prop['size'] == 3
 
     @DataLakePreparer()
-    def test_set_access_control_recursive(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_flush_data_with_bool(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
+
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        summary = directory_client.set_access_control_recursive(acl=acl)
+        file_client = directory_client.get_file_client('filename')
+        file_client.create_file()
+
+        # Act
+        response = file_client.append_data(b'abc', 0, 3, flush=True)
 
         # Assert
-        # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.directories_successful, num_sub_dirs + 1)
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertIsNone(summary.continuation)
-        access_control = directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        prop = file_client.get_file_properties()
+        assert response is not None
+        assert prop['size'] == 3
 
     @DataLakePreparer()
-    def test_set_access_control_recursive_throws_exception_containing_continuation_token(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_flush_data_with_match_condition(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
+
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
-
-        response_list = list()
-
-        def callback(response):
-            response_list.append(response)
-            if len(response_list) == 2:
-                raise ServiceRequestError("network problem")
-        acl = 'user::rwx,group::r-x,other::rwx'
 
-        with self.assertRaises(AzureError) as acl_error:
-            directory_client.set_access_control_recursive(acl=acl, batch_size=2, max_batches=2,
-                                                          raw_response_hook=callback, retry_total=0)
-        self.assertIsNotNone(acl_error.exception.continuation_token)
-        self.assertEqual(acl_error.exception.message, "network problem")
-        self.assertIsInstance(acl_error.exception, ServiceRequestError)
+        file_client = directory_client.get_file_client('filename')
+        resp = file_client.create_file()
+
+        # Act
+        file_client.append_data(b'abc', 0, 3)
+
+        # flush is successful because it isn't touched
+        response = file_client.flush_data(3, etag=resp['etag'], match_condition=MatchConditions.IfNotModified)
+
+        file_client.append_data(b'abc', 3, 3)
+        with pytest.raises(ResourceModifiedError):
+            # flush is unsuccessful because extra data were appended.
+            file_client.flush_data(6, etag=resp['etag'], match_condition=MatchConditions.IfNotModified)
 
     @DataLakePreparer()
-    def test_set_access_control_recursive_in_batches(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_upload_data_to_none_existing_file(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+
         directory_name = self._get_directory_reference()
+
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        summary = directory_client.set_access_control_recursive(acl=acl, batch_size=2)
+        file_client = directory_client.get_file_client('filename')
+        data = self.get_random_bytes(200*1024)
+        file_client.upload_data(data, overwrite=True, max_concurrency=3)
 
-        # Assert
-        # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.directories_successful, num_sub_dirs + 1)
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertIsNone(summary.continuation)
-        access_control = directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        downloaded_data = file_client.download_file().readall()
+        assert data == downloaded_data
 
+    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_set_access_control_recursive_in_batches_with_progress_callback(self, datalake_storage_account_name, datalake_storage_account_key):
+    def test_upload_data_in_substreams(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
+
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        last_response = AccessControlChangeResult(None, "")
+        file_client = directory_client.get_file_client('filename')
+        # Get 16MB data
+        data = self.get_random_bytes(16*1024*1024)
+        # Ensure chunk size is greater than threshold (8MB > 4MB) - for optimized upload
+        file_client.upload_data(data, chunk_size=8*1024*1024, overwrite=True, max_concurrency=3)
+        downloaded_data = file_client.download_file().readall()
+        assert data == downloaded_data
 
-        def progress_callback(resp):
-            running_tally.directories_successful += resp.batch_counters.directories_successful
-            running_tally.files_successful += resp.batch_counters.files_successful
-            running_tally.failure_count += resp.batch_counters.failure_count
-
-            last_response.counters = resp.aggregate_counters
-
-        summary = directory_client.set_access_control_recursive(acl=acl, progress_hook=progress_callback,
-                                                                batch_size=2)
-
-        # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertIsNone(summary.continuation)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(summary.counters.directories_successful, last_response.counters.directories_successful)
-        self.assertEqual(summary.counters.files_successful, last_response.counters.files_successful)
-        self.assertEqual(summary.counters.failure_count, last_response.counters.failure_count)
-        access_control = directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        # Run on single thread
+        file_client.upload_data(data, chunk_size=8*1024*1024, overwrite=True)
+        downloaded_data = file_client.download_file().readall()
+        assert data == downloaded_data
 
     @DataLakePreparer()
-    def test_set_access_control_recursive_with_failures(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        if not self.is_playback():
-            return
-        root_directory_client = self.dsc.get_file_system_client(self.file_system_name)._get_root_directory_client()
-        root_directory_client.set_access_control(acl="user::--x,group::--x,other::--x")
+    @recorded_by_proxy
+    def test_upload_data_to_existing_file(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        # Using an AAD identity, create a directory to put files under that
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
-        token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
-
-        # Create a file as super user
-        self.dsc.get_directory_client(self.file_system_name, directory_name).get_file_client("cannottouchthis") \
-            .create_file()
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        failed_entries = []
+        # Create a directory to put the file under that
+        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
+        directory_client.create_directory()
 
-        def progress_callback(resp):
-            running_tally.directories_successful += resp.batch_counters.directories_successful
-            running_tally.files_successful += resp.batch_counters.files_successful
-            running_tally.failure_count += resp.batch_counters.failure_count
-            failed_entries.append(resp.batch_failures)
+        # create an existing file
+        file_client = directory_client.get_file_client('filename')
+        file_client.create_file()
+        file_client.append_data(b"abc", 0)
+        file_client.flush_data(3)
 
-        summary = directory_client.set_access_control_recursive(acl=acl, progress_hook=progress_callback,
-                                                                batch_size=2)
+        # to override the existing file
+        data = self.get_random_bytes(100)
+        with pytest.raises(HttpResponseError):
+            file_client.upload_data(data)
+        file_client.upload_data(data, overwrite=True)
 
-        # Assert
-        self.assertEqual(summary.counters.failure_count, 1)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 1)
+        downloaded_data = file_client.download_file().readall()
+        assert data == downloaded_data
 
     @DataLakePreparer()
-    def test_set_access_control_recursive_stop_on_failures(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        if not self.is_playback():
-            return
-        root_directory_client = self.dsc.get_file_system_client(self.file_system_name)._get_root_directory_client()
-        root_directory_client.set_access_control(acl="user::--x,group::--x,other::--x")
+    @recorded_by_proxy
+    def test_upload_data_to_existing_file_with_content_settings(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        # Using an AAD identity, create a directory to put files under that
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
-        token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
-
-        # Create a file as super user
-        self.dsc.get_directory_client(self.file_system_name, directory_name).get_file_client("cannottouchthis") \
-            .create_file()
-
-        acl = 'user::rwx,group::r-x,other::rwx'
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        failed_entries = []
-
-        def progress_callback(resp):
-            running_tally.directories_successful += resp.batch_counters.directories_successful
-            running_tally.files_successful += resp.batch_counters.files_successful
-            running_tally.failure_count += resp.batch_counters.failure_count
-            if resp.batch_failures:
-                failed_entries.extend(resp.batch_failures)
 
-        summary = directory_client.set_access_control_recursive(acl=acl, progress_hook=progress_callback,
-                                                                batch_size=6)
+        # Create a directory to put the file under that
+        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
+        directory_client.create_directory()
 
-        # Assert
-        self.assertEqual(summary.counters.failure_count, 1)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 1)
+        # create an existing file
+        file_client = directory_client.get_file_client('filename')
+        etag = file_client.create_file()['etag']
 
-    @DataLakePreparer()
-    def test_set_access_control_recursive_continue_on_failures(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        if not self.is_playback():
-            return
-        root_directory_client = self.dsc.get_file_system_client(self.file_system_name)._get_root_directory_client()
-        root_directory_client.set_access_control(acl="user::--x,group::--x,other::--x")
+        # to override the existing file
+        data = self.get_random_bytes(100)
+        content_settings = ContentSettings(
+            content_language='spanish',
+            content_disposition='inline')
 
-        # Using an AAD identity, create a directory to put files under that
-        directory_name = self._get_directory_reference()
-        token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
-
-        # Create a file as super user
-        self.dsc.get_directory_client(self.file_system_name, directory_name).get_file_client("cannottouchthis") \
-            .create_file()
-        self.dsc.get_directory_client(self.file_system_name, directory_name).get_sub_directory_client("cannottouchthisdir") \
-            .create_directory()
+        file_client.upload_data(data, content_settings=content_settings, etag=etag, match_condition=MatchConditions.IfNotModified)
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        failed_entries = []
+        downloaded_data = file_client.download_file().readall()
+        properties = file_client.get_file_properties()
 
-        def progress_callback(resp):
-            running_tally.directories_successful += resp.batch_counters.directories_successful
-            running_tally.files_successful += resp.batch_counters.files_successful
-            running_tally.failure_count += resp.batch_counters.failure_count
-            if resp.batch_failures:
-                failed_entries.extend(resp.batch_failures)
-
-        # set acl for all directories
-        summary = directory_client.set_access_control_recursive(acl=acl, progress_hook=progress_callback,
-                                                                batch_size=6,
-                                                                continue_on_failure=True)
-
-        # Assert
-        self.assertEqual(summary.counters.failure_count, 2)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 2)
-        self.assertIsNone(summary.continuation)
-
-        # reset the counter, set acl for part of the directories
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        failed_entries = []
-        summary2 = directory_client.set_access_control_recursive(acl=acl, progress_hook=progress_callback,
-                                                                 batch_size=6, max_batches=3,
-                                                                 continue_on_failure=True)
-        self.assertEqual(summary2.counters.failure_count, 2)
-        self.assertEqual(summary2.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary2.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary2.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 2)
-        self.assertIsNotNone(summary2.continuation)
+        assert data == downloaded_data
+        assert properties.content_settings.content_language == content_settings.content_language
 
     @DataLakePreparer()
-    def test_set_access_control_recursive_in_batches_with_explicit_iteration(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_upload_data_to_existing_file_with_permission_and_umask(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
+
+        # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        result = AccessControlChangeResult(None, "")
-        iteration_count = 0
-        max_batches = 2
-        batch_size = 2
-
-        while result.continuation is not None:
-            result = directory_client.set_access_control_recursive(acl=acl, batch_size=batch_size, max_batches=max_batches,
-                                                                   continuation=result.continuation)
-
-            running_tally.directories_successful += result.counters.directories_successful
-            running_tally.files_successful += result.counters.files_successful
-            running_tally.failure_count += result.counters.failure_count
-            iteration_count += 1
-
-        # Assert
-        self.assertEqual(running_tally.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(running_tally.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(running_tally.failure_count, 0)
-        access_control = directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        # create an existing file
+        file_client = directory_client.get_file_client('filename')
+        etag = file_client.create_file()['etag']
 
-    @DataLakePreparer()
-    def test_update_access_control_recursive(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        # to override the existing file
+        data = self.get_random_bytes(100)
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        summary = directory_client.update_access_control_recursive(acl=acl)
+        file_client.upload_data(data, overwrite=True, permissions='0777', umask="0000", etag=etag, match_condition=MatchConditions.IfNotModified)
+
+        downloaded_data = file_client.download_file().readall()
+        prop = file_client.get_access_control()
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        access_control = directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        assert data == downloaded_data
+        assert prop['permissions'] == 'rwxrwxrwx'
 
     @DataLakePreparer()
-    def test_update_access_control_recursive_in_batches(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_read_file(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        file_client = self._create_file_and_return_client()
+        data = self.get_random_bytes(1024)
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        summary = directory_client.update_access_control_recursive(acl=acl, batch_size=2)
+        # upload data to file
+        file_client.append_data(data, 0, len(data))
+        file_client.flush_data(len(data))
 
-        # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        access_control = directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        # download the data and make sure it is the same as uploaded data
+        downloaded_data = file_client.download_file().readall()
+        assert data == downloaded_data
 
+    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_update_access_control_recursive_in_batches_with_progress_callback(self, datalake_storage_account_name, datalake_storage_account_key):
+    def test_read_file_with_user_delegation_key(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        # SAS URL is calculated from storage key, so this test runs live only
 
-        acl = 'user::rwx,group::r-x,other::rwx'
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        last_response = AccessControlChangeResult(None, "")
+        # Create file
+        file_client = self._create_file_and_return_client()
+        data = self.get_random_bytes(1024)
+        # Upload data to file
+        file_client.append_data(data, 0, len(data))
+        file_client.flush_data(len(data))
 
-        def progress_callback(resp):
-            running_tally.directories_successful += resp.batch_counters.directories_successful
-            running_tally.files_successful += resp.batch_counters.files_successful
-            running_tally.failure_count += resp.batch_counters.failure_count
-
-            last_response.counters = resp.aggregate_counters
-
-        summary = directory_client.update_access_control_recursive(acl=acl, progress_hook=progress_callback,
-                                                                   batch_size=2)
-
-        # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertIsNone(summary.continuation)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(summary.counters.directories_successful, last_response.counters.directories_successful)
-        self.assertEqual(summary.counters.files_successful, last_response.counters.files_successful)
-        self.assertEqual(summary.counters.failure_count, last_response.counters.failure_count)
-        access_control = directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        # Get user delegation key
+        token_credential = self.generate_oauth_token()
+        service_client = DataLakeServiceClient(self.account_url(datalake_storage_account_name, 'dfs'), credential=token_credential, logging_enable=True)
+        user_delegation_key = service_client.get_user_delegation_key(datetime.utcnow(),
+                                                                     datetime.utcnow() + timedelta(hours=1))
+
+        sas_token = generate_file_sas(file_client.account_name,
+                                      file_client.file_system_name,
+                                      None,
+                                      file_client.path_name,
+                                      user_delegation_key,
+                                      permission=FileSasPermissions(read=True, create=True, write=True, delete=True),
+                                      expiry=datetime.utcnow() + timedelta(hours=1),
+                                      )
+
+        # download the data and make sure it is the same as uploaded data
+        new_file_client = DataLakeFileClient(self.account_url(datalake_storage_account_name, 'dfs'),
+                                             file_client.file_system_name,
+                                             file_client.path_name,
+                                             credential=sas_token, logging_enable=True)
+        downloaded_data = new_file_client.download_file().readall()
+        assert data == downloaded_data
 
+    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_update_access_control_recursive_with_failures(self, datalake_storage_account_name, datalake_storage_account_key):
+    def test_set_acl_with_user_delegation_key(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        if not self.is_playback():
-            return
-        root_directory_client = self.dsc.get_file_system_client(self.file_system_name)._get_root_directory_client()
-        root_directory_client.set_access_control(acl="user::--x,group::--x,other::--x")
+        # SAS URL is calculated from storage key, so this test runs live only
 
-        # Using an AAD identity, create a directory to put files under that
-        directory_name = self._get_directory_reference()
-        token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
-
-        # Create a file as super user
-        self.dsc.get_directory_client(self.file_system_name, directory_name).get_file_client("cannottouchthis") \
-            .create_file()
+        # Create file
+        file_client = self._create_file_and_return_client()
+        data = self.get_random_bytes(1024)
+        # Upload data to file
+        file_client.append_data(data, 0, len(data))
+        file_client.flush_data(len(data))
 
+        # Get user delegation key
+        token_credential = self.generate_oauth_token()
+        service_client = DataLakeServiceClient(self.account_url(datalake_storage_account_name, 'dfs'), credential=token_credential)
+        user_delegation_key = service_client.get_user_delegation_key(datetime.utcnow(),
+                                                                     datetime.utcnow() + timedelta(hours=1))
+
+        sas_token = generate_file_sas(file_client.account_name,
+                                      file_client.file_system_name,
+                                      None,
+                                      file_client.path_name,
+                                      user_delegation_key,
+                                      permission=FileSasPermissions(execute=True, manage_access_control=True,
+                                                                    manage_ownership=True),
+                                      expiry=datetime.utcnow() + timedelta(hours=1),
+                                      )
+
+        # download the data and make sure it is the same as uploaded data
+        new_file_client = DataLakeFileClient(self.account_url(datalake_storage_account_name, 'dfs'),
+                                             file_client.file_system_name,
+                                             file_client.path_name,
+                                             credential=sas_token)
         acl = 'user::rwx,group::r-x,other::rwx'
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        failed_entries = []
+        owner = "dc140949-53b7-44af-b1e9-cd994951fb86"
+        new_file_client.set_access_control(acl=acl, owner=owner)
+        access_control = new_file_client.get_access_control()
+        assert acl == access_control['acl']
+        assert owner == access_control['owner']
+
+    @pytest.mark.live_test_only
+    @DataLakePreparer()
+    def test_preauthorize_user_with_user_delegation_key(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        def progress_callback(resp):
-            running_tally.directories_successful += resp.batch_counters.directories_successful
-            running_tally.files_successful += resp.batch_counters.files_successful
-            running_tally.failure_count += resp.batch_counters.failure_count
-            failed_entries.append(resp.batch_failures)
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        # SAS URL is calculated from storage key, so this test runs live only
 
-        summary = directory_client.update_access_control_recursive(acl=acl, progress_hook=progress_callback,
-                                                                   batch_size=2)
+        # Create file
+        file_client = self._create_file_and_return_client()
+        data = self.get_random_bytes(1024)
+        # Upload data to file
+        file_client.append_data(data, 0, len(data))
+        file_client.flush_data(len(data))
+        file_client.set_access_control(owner="68390a19-a643-458b-b726-408abf67b4fc", permissions='0777')
+        acl = file_client.get_access_control()
 
-        # Assert
-        self.assertEqual(summary.counters.failure_count, 1)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 1)
+        # Get user delegation key
+        token_credential = self.generate_oauth_token()
+        service_client = DataLakeServiceClient(self.account_url(datalake_storage_account_name, 'dfs'), credential=token_credential)
+        user_delegation_key = service_client.get_user_delegation_key(datetime.utcnow(),
+                                                                     datetime.utcnow() + timedelta(hours=1))
 
-    @DataLakePreparer()
-    def test_remove_access_control_recursive(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        sas_token = generate_file_sas(file_client.account_name,
+                                      file_client.file_system_name,
+                                      None,
+                                      file_client.path_name,
+                                      user_delegation_key,
+                                      permission=FileSasPermissions(read=True, write=True, manage_access_control=True,
+                                                                    manage_ownership=True),
+                                      expiry=datetime.utcnow() + timedelta(hours=1),
+                                      preauthorized_agent_object_id="68390a19-a643-458b-b726-408abf67b4fc"
+                                      )
 
-        summary = directory_client.remove_access_control_recursive(acl=REMOVE_ACL)
+        # download the data and make sure it is the same as uploaded data
+        new_file_client = DataLakeFileClient(self.account_url(datalake_storage_account_name, 'dfs'),
+                                             file_client.file_system_name,
+                                             file_client.path_name,
+                                             credential=sas_token)
 
-        # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
+        acl = new_file_client.set_access_control(permissions='0777')
+        assert acl is not None
 
     @DataLakePreparer()
-    def test_remove_access_control_recursive_in_batches(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_read_file_into_file(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        file_client = self._create_file_and_return_client()
+        data = self.get_random_bytes(1024)
+
+        # upload data to file
+        file_client.append_data(data, 0, len(data))
+        file_client.flush_data(len(data))
 
-        summary = directory_client.remove_access_control_recursive(acl=REMOVE_ACL, batch_size=2)
+        # download the data into a file and make sure it is the same as uploaded data
+        with open(FILE_PATH, 'wb') as stream:
+            download = file_client.download_file()
+            download.readinto(stream)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
+        with open(FILE_PATH, 'rb') as stream:
+            actual = stream.read()
+            assert data == actual
 
     @DataLakePreparer()
-    def test_remove_access_control_recursive_in_batches_with_progress_callback(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+    @recorded_by_proxy
+    def test_read_file_to_text(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        last_response = AccessControlChangeResult(None, "")
-
-        def progress_callback(resp):
-            running_tally.directories_successful += resp.batch_counters.directories_successful
-            running_tally.files_successful += resp.batch_counters.files_successful
-            running_tally.failure_count += resp.batch_counters.failure_count
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        file_client = self._create_file_and_return_client()
+        data = self.get_random_text_data(1024)
 
-            last_response.counters = resp.aggregate_counters
+        # upload data to file
+        file_client.append_data(data, 0, len(data))
+        file_client.flush_data(len(data))
 
-        summary = directory_client.remove_access_control_recursive(acl=REMOVE_ACL, progress_hook=progress_callback,
-                                                                   batch_size=2)
+        # download the text data and make sure it is the same as uploaded data
+        downloaded_data = file_client.download_file(encoding="utf-8").readall()
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(summary.counters.directories_successful, last_response.counters.directories_successful)
-        self.assertEqual(summary.counters.files_successful, last_response.counters.files_successful)
-        self.assertEqual(summary.counters.failure_count, last_response.counters.failure_count)
+        assert data == downloaded_data
+
 
+    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_remove_access_control_recursive_with_failures(self, datalake_storage_account_name, datalake_storage_account_key):
+    def test_account_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        if not self.is_playback():
-            return
-        root_directory_client = self.dsc.get_file_system_client(self.file_system_name)._get_root_directory_client()
-        root_directory_client.set_access_control(acl="user::--x,group::--x,other::--x")
+        # SAS URL is calculated from storage key, so this test runs live only
 
-        # Using an AAD identity, create a directory to put files under that
-        directory_name = self._get_directory_reference()
-        token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        file_name = self._get_file_reference()
+        # create a file under root directory
+        self._create_file_and_return_client(file=file_name)
+
+        # generate a token with file level read permission
+        token = generate_account_sas(
+            self.dsc.account_name,
+            self.dsc.credential.account_key,
+            ResourceTypes(file_system=True, object=True),
+            AccountSasPermissions(read=True),
+            datetime.utcnow() + timedelta(hours=1),
+        )
 
-        # Create a file as super user
-        self.dsc.get_directory_client(self.file_system_name, directory_name).get_file_client("cannottouchthis") \
-            .create_file()
+        for credential in [token, AzureSasCredential(token)]:
+            # read the created file which is under root directory
+            file_client = DataLakeFileClient(self.dsc.url, self.file_system_name, file_name, credential=credential)
+            properties = file_client.get_file_properties()
 
-        running_tally = AccessControlChangeCounters(0, 0, 0)
-        failed_entries = []
+            # make sure we can read the file properties
+            assert properties is not None
 
-        def progress_callback(resp):
-            running_tally.directories_successful += resp.batch_counters.directories_successful
-            running_tally.files_successful += resp.batch_counters.files_successful
-            running_tally.failure_count += resp.batch_counters.failure_count
-            failed_entries.append(resp.batch_failures)
+            # try to write to the created file with the token
+            with pytest.raises(HttpResponseError):
+                file_client.append_data(b"abcd", 0, 4)
 
-        summary = directory_client.remove_access_control_recursive(acl=REMOVE_ACL, progress_hook=progress_callback,
-                                                                   batch_size=2)
+    @DataLakePreparer()
+    @recorded_by_proxy
+    def test_account_sas_raises_if_sas_already_in_uri(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        # Assert
-        self.assertEqual(summary.counters.failure_count, 1)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 1)
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        with pytest.raises(ValueError):
+            DataLakeFileClient(self.dsc.url + "?sig=foo", self.file_system_name, "foo", credential=AzureSasCredential("?foo=bar"))
 
+    @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_rename_from(self, datalake_storage_account_name, datalake_storage_account_key):
+    def test_file_sas_only_applies_to_file_level(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        content_settings = ContentSettings(
-            content_language='spanish',
-            content_disposition='inline')
+        # SAS URL is calculated from storage key, so this test runs live only
+        file_name = self._get_file_reference()
         directory_name = self._get_directory_reference()
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory()
+        self._create_file_and_return_client(directory=directory_name, file=file_name)
 
-        new_name = "newname"
-
-        new_directory_client = self.dsc.get_directory_client(self.file_system_name, new_name)
+        # generate a token with file level read and write permissions
+        token = generate_file_sas(
+            self.dsc.account_name,
+            self.file_system_name,
+            directory_name,
+            file_name,
+            self.dsc.credential.account_key,
+            permission=FileSasPermissions(read=True, write=True),
+            expiry=datetime.utcnow() + timedelta(hours=1),
+        )
 
-        new_directory_client._rename_path('/' + self.file_system_name + '/' + directory_name,
-                                          content_settings=content_settings)
-        properties = new_directory_client.get_directory_properties()
+        # read the created file which is under root directory
+        file_client = DataLakeFileClient(self.dsc.url, self.file_system_name, directory_name+'/'+file_name,
+                                         credential=token)
+        properties = file_client.get_file_properties()
+
+        # make sure we can read the file properties
+        assert properties is not None
+
+        # try to write to the created file with the token
+        response = file_client.append_data(b"abcd", 0, 4, validate_content=True)
+        assert response is not None
+
+        # the token is for file level, so users are not supposed to have access to file system level operations
+        file_system_client = FileSystemClient(self.dsc.url, self.file_system_name, credential=token)
+        with pytest.raises(ClientAuthenticationError):
+            file_system_client.get_file_system_properties()
 
-        self.assertIsNotNone(properties)
-        self.assertIsNone(properties.get('content_settings'))
+        # the token is for file level, so users are not supposed to have access to directory level operations
+        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
+                                                   credential=token)
+        with pytest.raises(ClientAuthenticationError):
+            directory_client.get_directory_properties()
 
-    @pytest.mark.skip(reason="Investigate why renaming from shorter path to longer path does not work")
     @DataLakePreparer()
-    def test_rename_from_a_shorter_directory_to_longer_directory(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        directory_name = self._get_directory_reference()
-        self._create_directory_and_get_directory_client(directory_name="old")
+    @recorded_by_proxy
+    def test_delete_file(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        new_name = "newname"
-        new_directory_client = self._create_directory_and_get_directory_client(directory_name=new_name)
-        new_directory_client = new_directory_client.create_sub_directory("newsub")
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        # Arrange
+        file_client = self._create_file_and_return_client()
 
-        new_directory_client._rename_path('/' + self.file_system_name + '/' + directory_name)
-        properties = new_directory_client.get_directory_properties()
+        file_client.delete_file()
 
-        self.assertIsNotNone(properties)
+        with pytest.raises(ResourceNotFoundError):
+            file_client.get_file_properties()
 
     @DataLakePreparer()
-    def test_rename_from_a_directory_in_another_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        # create a file dir1 under filesystem1
-        old_file_system_name = self._get_directory_reference("oldfilesystem")
-        old_dir_name = "olddir"
-        old_client = self.dsc.get_file_system_client(old_file_system_name)
-        old_client.create_file_system()
-        old_client.create_directory(old_dir_name)
+    @recorded_by_proxy
+    def test_delete_file_with_if_unmodified_since(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        # create a dir2 under filesystem2
-        new_name = "newname"
-        new_directory_client = self._create_directory_and_get_directory_client(directory_name=new_name)
-        new_directory_client = new_directory_client.create_sub_directory("newsub")
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        # Arrange
+        file_client = self._create_file_and_return_client()
 
-        # rename dir1 under filesystem1 to dir2 under filesystem2
-        new_directory_client._rename_path('/' + old_file_system_name + '/' + old_dir_name)
-        properties = new_directory_client.get_directory_properties()
+        prop = file_client.get_file_properties()
+        file_client.delete_file(if_unmodified_since=prop['last_modified'])
 
-        self.assertIsNotNone(properties)
+        # Make sure the file was deleted
+        with pytest.raises(ResourceNotFoundError):
+            file_client.get_file_properties()
 
     @DataLakePreparer()
-    def test_rename_from_an_unencoded_directory_in_another_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        # create a directory under filesystem1
-        old_file_system_name = self._get_directory_reference("oldfilesystem")
-        old_dir_name = "old dir"
-        old_client = self.dsc.get_file_system_client(old_file_system_name)
-        old_client.create_file_system()
-        old_dir_client = old_client.create_directory(old_dir_name)
-        file_name = "oldfile"
-        old_dir_client.create_file(file_name)
+    @recorded_by_proxy
+    def test_set_access_control(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        # move the directory under filesystem2
-        new_name = "new name/sub dir"
-        new_file_system_name = self._get_directory_reference("newfilesystem")
-        new_file_system_client = self.dsc.get_file_system_client(new_file_system_name)
-        new_file_system_client.create_file_system()
-        new_file_system_client.create_directory(new_name)
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        file_client = self._create_file_and_return_client()
 
-        # rename dir1 under filesystem1 to dir2 under filesystem2
-        new_directory_client = old_dir_client.rename_directory('/' + new_file_system_name + '/' + new_name)
-        properties = new_directory_client.get_directory_properties()
-        file_properties = new_directory_client.get_file_client(file_name).get_file_properties()
+        response = file_client.set_access_control(permissions='0777')
 
-        self.assertIsNotNone(properties)
-        self.assertIsNotNone(file_properties)
-        old_client.delete_file_system()
+        # Assert
+        assert response is not None
 
     @DataLakePreparer()
-    def test_rename_to_an_existing_directory_in_another_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_set_access_control_with_match_conditions(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        # create a file dir1 under filesystem1
-        destination_file_system_name = self._get_directory_reference("destfilesystem")
-        destination_dir_name = "destdir"
-        fs_client = self.dsc.get_file_system_client(destination_file_system_name)
-        fs_client.create_file_system()
-        destination_directory_client = fs_client.create_directory(destination_dir_name)
+        file_client = self._create_file_and_return_client()
 
-        # create a dir2 under filesystem2
-        source_name = "source"
-        source_directory_client = self._create_directory_and_get_directory_client(directory_name=source_name)
-        source_directory_client = source_directory_client.create_sub_directory("subdir")
+        with pytest.raises(ResourceModifiedError):
+            file_client.set_access_control(permissions='0777', match_condition=MatchConditions.IfMissing)
 
-        # rename dir2 under filesystem2 to dir1 under filesystem1
-        res = source_directory_client.rename_directory('/' + destination_file_system_name + '/' + destination_dir_name)
+    @DataLakePreparer()
+    @recorded_by_proxy
+    def test_get_access_control(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        file_client = self._create_file_and_return_client()
+        file_client.set_access_control(permissions='0777')
 
-        # the source directory has been renamed to destination directory, so it cannot be found
-        with self.assertRaises(HttpResponseError):
-            source_directory_client.get_directory_properties()
+        # Act
+        response = file_client.get_access_control()
 
-        self.assertEqual(res.url, destination_directory_client.url)
+        # Assert
+        assert response is not None
 
     @DataLakePreparer()
-    def test_rename_with_none_existing_destination_condition_and_source_unmodified_condition(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_get_access_control_with_if_modified_since(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        non_existing_dir_name = "nonexistingdir"
+        file_client = self._create_file_and_return_client()
+        file_client.set_access_control(permissions='0777')
+
+        prop = file_client.get_file_properties()
+
+        # Act
+        response = file_client.get_access_control(if_modified_since=prop['last_modified']-timedelta(minutes=15))
 
-        # create a filesystem1
-        destination_file_system_name = self._get_directory_reference("destfilesystem")
-        fs_client = self.dsc.get_file_system_client(destination_file_system_name)
-        fs_client.create_file_system()
+        # Assert
+        assert response is not None
 
-        # create a dir2 under filesystem2
-        source_name = "source"
-        source_directory_client = self._create_directory_and_get_directory_client(directory_name=source_name)
-        source_directory_client = source_directory_client.create_sub_directory("subdir")
+    @DataLakePreparer()
+    @recorded_by_proxy
+    def test_set_access_control_recursive(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        # rename dir2 under filesystem2 to a non existing directory under filesystem1,
-        # when dir1 does not exist and dir2 wasn't modified
-        etag = source_directory_client.get_directory_properties()['etag']
-        res = source_directory_client.rename_directory('/' + destination_file_system_name + '/' + non_existing_dir_name,
-                                                       match_condition=MatchConditions.IfMissing,
-                                                       source_etag=etag,
-                                                       source_match_condition=MatchConditions.IfNotModified)
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        acl = 'user::rwx,group::r-x,other::rwx'
+        file_client = self._create_file_and_return_client()
 
-        # the source directory has been renamed to destination directory, so it cannot be found
-        with self.assertRaises(HttpResponseError):
-            source_directory_client.get_directory_properties()
+        summary = file_client.set_access_control_recursive(acl=acl)
 
-        self.assertEqual(non_existing_dir_name, res.path_name)
+        # Assert
+        assert summary.counters.directories_successful == 0
+        assert summary.counters.files_successful == 1
+        assert summary.counters.failure_count == 0
+        access_control = file_client.get_access_control()
+        assert access_control is not None
+        assert acl == access_control['acl']
 
     @DataLakePreparer()
-    def test_rename_to_an_non_existing_directory_in_another_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_update_access_control_recursive(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        # create a file dir1 under filesystem1
-        destination_file_system_name = self._get_directory_reference("destfilesystem")
-        non_existing_dir_name = "nonexistingdir"
-        fs_client = self.dsc.get_file_system_client(destination_file_system_name)
-        fs_client.create_file_system()
+        acl = 'user::rwx,group::r-x,other::rwx'
+        file_client = self._create_file_and_return_client()
 
-        # create a dir2 under filesystem2
-        source_name = "source"
-        source_directory_client = self._create_directory_and_get_directory_client(directory_name=source_name)
-        source_directory_client = source_directory_client.create_sub_directory("subdir")
+        summary = file_client.update_access_control_recursive(acl=acl)
 
-        # rename dir2 under filesystem2 to dir1 under filesystem1
-        res = source_directory_client.rename_directory('/' + destination_file_system_name + '/' + non_existing_dir_name)
+        # Assert
+        assert summary.counters.directories_successful == 0
+        assert summary.counters.files_successful == 1
+        assert summary.counters.failure_count == 0
+        access_control = file_client.get_access_control()
+        assert access_control is not None
+        assert acl == access_control['acl']
 
+    @DataLakePreparer()
+    @recorded_by_proxy
+    def test_remove_access_control_recursive(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        # the source directory has been renamed to destination directory, so it cannot be found
-        with self.assertRaises(HttpResponseError):
-            source_directory_client.get_directory_properties()
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        acl = "mask," + "default:user,default:group," + \
+             "user:ec3595d6-2c17-4696-8caa-7e139758d24a,group:ec3595d6-2c17-4696-8caa-7e139758d24a," + \
+             "default:user:ec3595d6-2c17-4696-8caa-7e139758d24a,default:group:ec3595d6-2c17-4696-8caa-7e139758d24a"
+        file_client = self._create_file_and_return_client()
+        summary = file_client.remove_access_control_recursive(acl=acl)
 
-        self.assertEqual(non_existing_dir_name, res.path_name)
+        # Assert
+        assert summary.counters.directories_successful == 0
+        assert summary.counters.files_successful == 1
+        assert summary.counters.failure_count == 0
 
-    @pytest.mark.skip(reason="Investigate why renaming non-empty directory doesn't work")
     @DataLakePreparer()
-    def test_rename_directory_to_non_empty_directory(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        dir1 = self._create_directory_and_get_directory_client("dir1")
-        dir1.create_sub_directory("subdir")
+    @recorded_by_proxy
+    def test_get_properties(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        dir2 = self._create_directory_and_get_directory_client("dir2")
-        dir2.rename_directory(dir1.file_system_name + '/' + dir1.path_name)
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        # Arrange
+        directory_client = self._create_directory_and_return_client()
 
-        with self.assertRaises(HttpResponseError):
-            dir2.get_directory_properties()
+        metadata = {'hello': 'world', 'number': '42'}
+        content_settings = ContentSettings(
+            content_language='spanish',
+            content_disposition='inline')
+        file_client = directory_client.create_file("newfile", metadata=metadata, content_settings=content_settings)
+        file_client.append_data(b"abc", 0, 3)
+        file_client.flush_data(3)
+        properties = file_client.get_file_properties()
+
+        # Assert
+        assert properties
+        assert properties.size == 3
+        assert properties.metadata['hello'] == metadata['hello']
+        assert properties.content_settings.content_language == content_settings.content_language
+
+    @DataLakePreparer()
+    @recorded_by_proxy
+    def test_set_expiry(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+        variables = kwargs.pop('variables', {})
 
-    @pytest.mark.live_test_only
-    @DataLakePreparer()
-    def test_rename_dir_with_file_system_sas(self, datalake_storage_account_name, datalake_storage_account_key):
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        # Arrange
+        directory_client = self._create_directory_and_return_client()
 
-        token = generate_file_system_sas(
-            self.dsc.account_name,
-            self.file_system_name,
-            self.dsc.credential.account_key,
-            FileSystemSasPermissions(write=True, read=True, delete=True),
-            datetime.utcnow() + timedelta(hours=1),
-        )
+        metadata = {'hello': 'world', 'number': '42'}
+        content_settings = ContentSettings(
+            content_language='spanish',
+            content_disposition='inline')
+        expiry_time = self.get_datetime_variable(variables, 'expiry_time', datetime.utcnow() + timedelta(hours=1))
+        file_client = directory_client.create_file("newfile", metadata=metadata, content_settings=content_settings)
+        file_client.set_file_expiry("Absolute", expires_on=expiry_time)
+        properties = file_client.get_file_properties()
 
-        # read the created file which is under root directory
-        dir_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, "olddirectory", credential=token)
-        dir_client.create_directory()
-        new_client = dir_client.rename_directory(dir_client.file_system_name+'/'+'newdirectory')
+        # Assert
+        assert properties
+        assert properties.expiry_time is not None
 
-        new_client.get_directory_properties()
-        self.assertEqual(new_client.path_name, "newdirectory")
+        return variables
 
     @DataLakePreparer()
-    def test_get_properties(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_rename_file_with_non_used_name(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        # Arrange
-        directory_name = self._get_directory_reference()
-        metadata = {'hello': 'world', 'number': '42'}
-        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
-        directory_client.create_directory(metadata=metadata)
+        file_client = self._create_file_and_return_client()
+        data_bytes = b"abc"
+        file_client.append_data(data_bytes, 0, 3)
+        file_client.flush_data(3)
+        new_client = file_client.rename_file(file_client.file_system_name+'/'+'newname')
 
-        properties = directory_client.get_directory_properties()
-        # Assert
-        self.assertTrue(properties)
-        self.assertIsNotNone(properties.metadata)
-        self.assertEqual(properties.metadata['hello'], metadata['hello'])
+        data = new_client.download_file().readall()
+        assert data == data_bytes
+        assert new_client.path_name == "newname"
 
     @DataLakePreparer()
-    def test_directory_encryption_scope_from_file_system(self, datalake_storage_account_name,
-                                                         datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_file_encryption_scope_from_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
         self.dsc = DataLakeServiceClient(url, credential=datalake_storage_account_key, logging_enable=True)
-        self.config = self.dsc._config
         self.file_system_name = self.get_resource_name('filesystem')
-        dir_name = 'testdir'
-        file_system = self.dsc.get_file_system_client(self.file_system_name)
+        file_name = 'testfile'
         encryption_scope = EncryptionScopeOptions(default_encryption_scope="hnstestscope1")
 
+        file_system = self.dsc.get_file_system_client(self.file_system_name)
         file_system.create_file_system(encryption_scope_options=encryption_scope)
-        file_system.create_directory(dir_name)
 
-        directory_client = file_system.get_directory_client(dir_name)
-        props = directory_client.get_directory_properties()
+        file_client = file_system.create_file(file_name)
+        props = file_client.get_file_properties()
 
         # Assert
-        self.assertTrue(props)
-        self.assertIsNotNone(props['encryption_scope'])
-        self.assertEqual(props['encryption_scope'], encryption_scope.default_encryption_scope)
+        assert props
+        assert props['encryption_scope'] is not None
+        assert props['encryption_scope'] == encryption_scope.default_encryption_scope
 
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_using_directory_sas_to_read(self, datalake_storage_account_name, datalake_storage_account_key):
-        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        # SAS URL is calculated from storage key, so this test runs live only
+    def test_rename_file_with_file_system_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
 
-        client = self._create_directory_and_get_directory_client()
-        directory_name = client.path_name
-
-        # generate a token with directory level read permission
-        token = generate_directory_sas(
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        # sas token is calculated from storage key, so live only
+        token = generate_file_system_sas(
             self.dsc.account_name,
             self.file_system_name,
-            directory_name,
             self.dsc.credential.account_key,
-            permission=DirectorySasPermissions(read=True),
-            expiry=datetime.utcnow() + timedelta(hours=1),
+            FileSystemSasPermissions(write=True, read=True, delete=True),
+            datetime.utcnow() + timedelta(hours=1),
         )
 
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token)
-        access_control = directory_client.get_access_control()
-
-        self.assertIsNotNone(access_control)
+        # read the created file which is under root directory
+        file_client = DataLakeFileClient(self.dsc.url, self.file_system_name, "oldfile", credential=token)
+        file_client.create_file()
+        data_bytes = b"abc"
+        file_client.append_data(data_bytes, 0, 3)
+        file_client.flush_data(3)
+        new_client = file_client.rename_file(file_client.file_system_name+'/'+'newname')
+
+        data = new_client.download_file().readall()
+        assert data == data_bytes
+        assert new_client.path_name == "newname"
 
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_using_directory_sas_to_create(self, datalake_storage_account_name, datalake_storage_account_key):
+    def test_rename_file_with_file_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # SAS URL is calculated from storage key, so this test runs live only
+        token = generate_file_sas(self.dsc.account_name,
+                                  self.file_system_name,
+                                  None,
+                                  "oldfile",
+                                  datalake_storage_account_key,
+                                  permission=FileSasPermissions(read=True, create=True, write=True, delete=True),
+                                  expiry=datetime.utcnow() + timedelta(hours=1),
+                                  )
+
+        new_token = generate_file_sas(self.dsc.account_name,
+                                      self.file_system_name,
+                                      None,
+                                      "newname",
+                                      datalake_storage_account_key,
+                                      permission=FileSasPermissions(read=True, create=True, write=True, delete=True),
+                                      expiry=datetime.utcnow() + timedelta(hours=1),
+                                      )
 
-        # generate a token with directory level create permission
-        directory_name = self._get_directory_reference()
-        token = generate_directory_sas(
+        # read the created file which is under root directory
+        file_client = DataLakeFileClient(self.dsc.url, self.file_system_name, "oldfile", credential=token)
+        file_client.create_file()
+        data_bytes = b"abc"
+        file_client.append_data(data_bytes, 0, 3)
+        file_client.flush_data(3)
+        new_client = file_client.rename_file(file_client.file_system_name+'/'+'newname'+'?'+new_token)
+
+        data = new_client.download_file().readall()
+        assert data == data_bytes
+        assert new_client.path_name == "newname"
+
+    @pytest.mark.live_test_only
+    @DataLakePreparer()
+    def test_rename_file_with_account_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        token = generate_account_sas(
             self.dsc.account_name,
-            self.file_system_name,
-            directory_name,
             self.dsc.credential.account_key,
-            permission=DirectorySasPermissions(create=True),
-            expiry=datetime.utcnow() + timedelta(hours=1),
+            ResourceTypes(object=True),
+            AccountSasPermissions(write=True, read=True, create=True, delete=True),
+            datetime.utcnow() + timedelta(hours=5),
         )
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token)
-        response = directory_client.create_directory()
-        self.assertIsNotNone(response)
 
-    @pytest.mark.live_test_only
+        # read the created file which is under root directory
+        file_client = DataLakeFileClient(self.dsc.url, self.file_system_name, "oldfile", credential=token)
+        file_client.create_file()
+        data_bytes = b"abc"
+        file_client.append_data(data_bytes, 0, 3)
+        file_client.flush_data(3)
+        new_client = file_client.rename_file(file_client.file_system_name+'/'+'newname')
+
+        data = new_client.download_file().readall()
+        assert data == data_bytes
+        assert new_client.path_name == "newname"
+
     @DataLakePreparer()
-    def test_using_directory_sas_to_create_file(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_rename_file_to_existing_file(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        # SAS URL is calculated from storage key, so this test runs live only
+        # create the existing file
+        existing_file_client = self._create_file_and_return_client(file="existingfile")
+        existing_file_client.append_data(b"a", 0, 1)
+        existing_file_client.flush_data(1)
+        old_url = existing_file_client.url
 
-        client = self._create_directory_and_get_directory_client()
-        directory_name = client.path_name
+        # prepare to rename the file to the existing file
+        file_client = self._create_file_and_return_client()
+        data_bytes = b"abc"
+        file_client.append_data(data_bytes, 0, 3)
+        file_client.flush_data(3)
+        new_client = file_client.rename_file(file_client.file_system_name+'/'+existing_file_client.path_name)
+        new_url = file_client.url
 
-        # generate a token with directory level read permission
-        token = generate_directory_sas(
-            self.dsc.account_name,
-            self.file_system_name,
-            directory_name,
-            self.dsc.credential.account_key,
-            permission=DirectorySasPermissions(create=True),
-            expiry=datetime.utcnow() + timedelta(hours=1),
-        )
+        data = new_client.download_file().readall()
+        # the existing file was overridden
+        assert data == data_bytes
 
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token)
-        directory_client.create_sub_directory("subdir")
+    @DataLakePreparer()
+    @recorded_by_proxy
+    def test_rename_file_will_not_change_existing_directory(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        # create none empty directory(with 2 files)
+        dir1 = self._create_directory_and_return_client(directory="dir1")
+        f1 = dir1.create_file("file1")
+        f1.append_data(b"file1", 0, 5)
+        f1.flush_data(5)
+        f2 = dir1.create_file("file2")
+        f2.append_data(b"file2", 0, 5)
+        f2.flush_data(5)
+
+        # create another none empty directory(with 2 files)
+        dir2 = self._create_directory_and_return_client(directory="dir2")
+        f3 = dir2.create_file("file3")
+        f3.append_data(b"file3", 0, 5)
+        f3.flush_data(5)
+        f4 = dir2.create_file("file4")
+        f4.append_data(b"file4", 0, 5)
+        f4.flush_data(5)
 
-        with self.assertRaises(HttpResponseError):
-            directory_client.delete_directory()
+        new_client = f3.rename_file(f1.file_system_name+'/'+f1.path_name)
+
+        assert new_client.download_file().readall() == b"file3"
+
+        # make sure the data in file2 and file4 weren't touched
+        f2_data = f2.download_file().readall()
+        assert f2_data == b"file2"
+
+        f4_data = f4.download_file().readall()
+        assert f4_data == b"file4"
+
+        with pytest.raises(HttpResponseError):
+            f3.download_file().readall()
 
     @DataLakePreparer()
-    def test_using_directory_sas_to_create_file(self, datalake_storage_account_name, datalake_storage_account_key):
-        newest_api_version = _SUPPORTED_API_VERSIONS[-1]
+    @recorded_by_proxy
+    def test_read_file_read(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
+        self._setUp(datalake_storage_account_name, datalake_storage_account_key)
+        self.dsc._config.max_single_get_size = 1024
+        self.dsc._config.max_chunk_get_size = 1024
+
+        file_client = self._create_file_and_return_client()
+        data = b'12345' * 205 * 5  # 5125 bytes
+
+        file_client.append_data(data, 0, len(data), flush=True)
+        stream = file_client.download_file()
+
+        # Act
+        result = bytearray()
+        read_size = 512
+        num_chunks = int(ceil(len(data) / read_size))
+        for i in range(num_chunks):
+            content = stream.read(read_size)
+            start = i * read_size
+            end = start + read_size
+            assert data[start:end] == content
+            result.extend(content)
+
+        # Assert
+        assert result == data
 
-        service_client = DataLakeServiceClient("https://abc.dfs.core.windows.net", credential='fake')
-        filesys_client = service_client.get_file_system_client("filesys")
-        dir_client = DataLakeDirectoryClient("https://abc.dfs.core.windows.net", "filesys", "dir", credential='fake')
-        file_client = dir_client.get_file_client("file")
-        self.assertEqual(service_client.api_version, newest_api_version)
-        self.assertEqual(filesys_client.api_version, newest_api_version)
-        self.assertEqual(dir_client.api_version, newest_api_version)
-        self.assertEqual(file_client.api_version, newest_api_version)
-        
-        service_client2 = DataLakeServiceClient("https://abc.dfs.core.windows.net", credential='fake', api_version="2019-02-02")
-        filesys_client2 = service_client2.get_file_system_client("filesys")
-        dir_client2 = DataLakeDirectoryClient("https://abc.dfs.core.windows.net", "filesys", "dir", credential='fake', api_version="2019-02-02")
-        file_client2 = dir_client2.get_file_client("file")
-        self.assertEqual(service_client2.api_version, "2019-02-02")
-        self.assertEqual(filesys_client2.api_version, "2019-02-02")
-        self.assertEqual(dir_client2.api_version, "2019-02-02")
-        self.assertEqual(file_client2.api_version, "2019-02-02")
 
 # ------------------------------------------------------------------------------
 if __name__ == '__main__':
     unittest.main()
```

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_large_file.py` & `azure-storage-file-datalake-12.9.1/tests/test_large_file.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,36 +1,35 @@
-# coding: utf-8
-
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
+import platform
+import re
 import unittest
 from os import urandom
 
 import pytest
-import re
-from azure.core.pipeline.policies import HTTPPolicy
-
 from azure.core.exceptions import ResourceExistsError
+from azure.core.pipeline.policies import HTTPPolicy
 from azure.storage.blob._shared.base_client import _format_shared_key_credential
 from azure.storage.filedatalake import DataLakeServiceClient
+
+from devtools_testutils.storage import StorageRecordedTestCase
 from settings.testcase import DataLakePreparer
-from devtools_testutils.storage import StorageTestCase
 
 # ------------------------------------------------------------------------------
 TEST_DIRECTORY_PREFIX = 'directory'
 TEST_FILE_PREFIX = 'file'
 FILE_PATH = 'file_output.temp.dat'
 LARGEST_BLOCK_SIZE = 4000 * 1024 * 1024
 # ------------------------------------------------------------------------------
 
 
-class LargeFileTest(StorageTestCase):
+class TestLargeFile(StorageRecordedTestCase):
     def _setUp(self, account_name, account_key):
         url = self.account_url(account_name, 'dfs')
         self.payload_dropping_policy = PayloadDroppingPolicy()
         credential_policy = _format_shared_key_credential(account_name,
                                                           account_key)
         self.dsc = DataLakeServiceClient(url,
                                          credential=account_key,
@@ -50,19 +49,22 @@
     def tearDown(self):
         if not self.is_playback():
             try:
                 self.dsc.delete_file_system(self.file_system_name)
             except:
                 pass
 
-        return super(LargeFileTest, self).tearDown()
+        return super(TestLargeFile, self).tearDown()
 
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_append_large_stream_without_network(self, datalake_storage_account_name, datalake_storage_account_key):
+    def test_append_large_stream_without_network(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self.get_resource_name(TEST_DIRECTORY_PREFIX)
 
         # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client.create_directory()
 
@@ -70,22 +72,25 @@
         file_client.create_file()
 
         data = LargeStream(LARGEST_BLOCK_SIZE)
 
         # Act
         response = file_client.append_data(data, 0, LARGEST_BLOCK_SIZE)
 
-        self.assertIsNotNone(response)
-        self.assertEqual(self.payload_dropping_policy.append_counter, 1)
-        self.assertEqual(self.payload_dropping_policy.append_sizes[0], LARGEST_BLOCK_SIZE)
+        assert response is not None
+        assert self.payload_dropping_policy.append_counter == 1
+        assert self.payload_dropping_policy.append_sizes[0] == LARGEST_BLOCK_SIZE
 
-    @pytest.mark.skip(reason="Pypy3 on Linux failed somehow, skip for now to investigate")
+    @pytest.mark.skipif(platform.python_implementation() == "PyPy", reason="Test failing on Pypy3 Linux, skip to investigate")
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    def test_upload_large_stream_without_network(self, datalake_storage_account_name, datalake_storage_account_key):
+    def test_upload_large_stream_without_network(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
 
         directory_name = self.get_resource_name(TEST_DIRECTORY_PREFIX)
 
         # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client.create_directory()
@@ -95,18 +100,18 @@
 
         length = 2*LARGEST_BLOCK_SIZE
         data = LargeStream(length)
 
         # Act
         response = file_client.upload_data(data, length, overwrite=True, chunk_size=LARGEST_BLOCK_SIZE)
 
-        self.assertIsNotNone(response)
-        self.assertEqual(self.payload_dropping_policy.append_counter, 2)
-        self.assertEqual(self.payload_dropping_policy.append_sizes[0], LARGEST_BLOCK_SIZE)
-        self.assertEqual(self.payload_dropping_policy.append_sizes[1], LARGEST_BLOCK_SIZE)
+        assert response is not None
+        assert self.payload_dropping_policy.append_counter == 2
+        assert self.payload_dropping_policy.append_sizes[0] == LARGEST_BLOCK_SIZE
+        assert self.payload_dropping_policy.append_sizes[1] == LARGEST_BLOCK_SIZE
 
 
 class LargeStream:
     def __init__(self, length, initial_buffer_length=1024*1024):
         self._base_data = urandom(initial_buffer_length)
         self._base_data_length = initial_buffer_length
         self._position = 0
```

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_directory_async.py` & `azure-storage-file-datalake-12.9.1/tests/test_directory_async.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,23 +1,19 @@
-# coding: utf-8
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
-import pytest
-import unittest
 import asyncio
 import time
+import unittest
 from datetime import datetime, timedelta
 
+import pytest
 from azure.core import MatchConditions
-from azure.core.pipeline.transport import AioHttpTransport
-from multidict import CIMultiDict, CIMultiDictProxy
-
 from azure.core.exceptions import (
     AzureError,
     HttpResponseError,
     ResourceExistsError,
     ResourceModifiedError,
     ResourceNotFoundError,
     ServiceRequestError
@@ -31,43 +27,31 @@
     FileSystemSasPermissions,
     generate_directory_sas,
     generate_file_system_sas
 )
 from azure.storage.filedatalake.aio import DataLakeDirectoryClient, DataLakeServiceClient
 from azure.storage.filedatalake._serialize import _SUPPORTED_API_VERSIONS
 
-from devtools_testutils.storage.aio import AsyncStorageTestCase as StorageTestCase
+from devtools_testutils.aio import recorded_by_proxy_async
+from devtools_testutils.storage.aio import AsyncStorageRecordedTestCase
 from settings.testcase import DataLakePreparer
 # ------------------------------------------------------------------------------
 TEST_DIRECTORY_PREFIX = 'directory'
 REMOVE_ACL = "mask," + "default:user,default:group," + \
              "user:ec3595d6-2c17-4696-8caa-7e139758d24a,group:ec3595d6-2c17-4696-8caa-7e139758d24a," + \
              "default:user:ec3595d6-2c17-4696-8caa-7e139758d24a,default:group:ec3595d6-2c17-4696-8caa-7e139758d24a"
 
 
 # ------------------------------------------------------------------------------
 
 
-class AiohttpTestTransport(AioHttpTransport):
-    """Workaround to vcrpy bug: https://github.com/kevin1024/vcrpy/pull/461
-    """
-
-    async def send(self, request, **config):
-        response = await super(AiohttpTestTransport, self).send(request, **config)
-        if not isinstance(response.headers, CIMultiDictProxy):
-            response.headers = CIMultiDictProxy(CIMultiDict(response.internal_response.headers))
-            response.content_type = response.headers.get("content-type")
-        return response
-
-
-class DirectoryTest(StorageTestCase):
+class TestDirectoryAsync(AsyncStorageRecordedTestCase):
     async def _setUp(self, account_name, account_key):
         url = self.account_url(account_name, 'dfs')
-        self.dsc = DataLakeServiceClient(url, credential=account_key,
-                                         transport=AiohttpTestTransport())
+        self.dsc = DataLakeServiceClient(url, credential=account_key)
         self.config = self.dsc._config
 
         self.file_system_name = self.get_resource_name('filesystem')
 
         if not self.is_playback():
             file_system = self.dsc.get_file_system_client(self.file_system_name)
             try:
@@ -80,16 +64,14 @@
             try:
                 loop = asyncio.get_event_loop()
                 loop.run_until_complete(self.dsc.delete_file_system(self.file_system_name))
                 loop.run_until_complete(self.dsc.__aexit__())
             except:
                 pass
 
-        return super(DirectoryTest, self).tearDown()
-
     # --Helpers-----------------------------------------------------------------
     def _get_directory_reference(self, prefix=TEST_DIRECTORY_PREFIX):
         directory_name = self.get_resource_name(prefix)
         return directory_name
 
     async def _create_directory_and_get_directory_client(self, directory_name=None):
         directory_name = directory_name if directory_name else self._get_directory_reference()
@@ -106,196 +88,245 @@
 
     async def _create_file_system(self):
         return await self.dsc.create_file_system(self._get_file_system_reference())
 
     # --Helpers-----------------------------------------------------------------
 
     @DataLakePreparer()
-    async def test_create_directory_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_directory(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
         content_settings = ContentSettings(
             content_language='spanish',
             content_disposition='inline')
         # Act
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         created = await directory_client.create_directory(content_settings=content_settings)
 
         # Assert
-        self.assertTrue(created)
+        assert created
 
     @DataLakePreparer()
-    async def test_create_directory_owner_group_acl_async(self, datalake_storage_account_name,
-                                                          datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_directory_owner_group_acl_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         test_string = '4cf4e284-f6a8-4540-b53e-c3469af032dc'
         test_string_acl = 'user::rwx,group::r-x,other::rwx'
         # Arrange
         directory_name = self._get_directory_reference()
 
         # Create a directory
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory(owner=test_string, group=test_string, acl=test_string_acl)
 
         # Assert
         acl_properties = await directory_client.get_access_control()
-        self.assertIsNotNone(acl_properties)
-        self.assertEqual(acl_properties['owner'], test_string)
-        self.assertEqual(acl_properties['group'], test_string)
-        self.assertEqual(acl_properties['acl'], test_string_acl)
+        assert acl_properties is not None
+        assert acl_properties['owner'] == test_string
+        assert acl_properties['group'] == test_string
+        assert acl_properties['acl'] == test_string_acl
 
     @DataLakePreparer()
-    async def test_create_directory_proposed_lease_id_async(self, datalake_storage_account_name,
-                                                            datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_directory_proposed_lease_id_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         test_string = '4cf4e284-f6a8-4540-b53e-c3469af032dc'
         test_duration = 15
         # Arrange
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory(lease_id=test_string, lease_duration=test_duration)
 
         # Assert
         properties = await directory_client.get_directory_properties()
-        self.assertIsNotNone(properties)
-        self.assertEqual(properties.lease['status'], 'locked')
-        self.assertEqual(properties.lease['state'], 'leased')
-        self.assertEqual(properties.lease['duration'], 'fixed')
+        assert properties is not None
+        assert properties.lease['status'] == 'locked'
+        assert properties.lease['state'] == 'leased'
+        assert properties.lease['duration'] == 'fixed'
 
     @DataLakePreparer()
-    async def test_create_sub_directory_proposed_lease_id_async(self, datalake_storage_account_name,
-                                                                datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_sub_directory_proposed_lease_id_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         test_string = '4cf4e284-f6a8-4540-b53e-c3469af032dc'
         test_duration = 15
         # Arrange
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client = await directory_client.create_sub_directory(sub_directory='sub1',
                                                                        lease_id=test_string,
                                                                        lease_duration=test_duration)
 
         # Assert
         properties = await directory_client.get_directory_properties()
-        self.assertIsNotNone(properties)
-        self.assertEqual(properties.lease['status'], 'locked')
-        self.assertEqual(properties.lease['state'], 'leased')
-        self.assertEqual(properties.lease['duration'], 'fixed')
+        assert properties is not None
+        assert properties.lease['status'] == 'locked'
+        assert properties.lease['state'] == 'leased'
+        assert properties.lease['duration'] == 'fixed'
 
     @DataLakePreparer()
-    async def test_directory_exists(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_directory_exists(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
 
         directory_client1 = self.dsc.get_directory_client(self.file_system_name, directory_name)
         directory_client2 = self.dsc.get_directory_client(self.file_system_name, "nonexistentdir")
         await directory_client1.create_directory()
 
-        self.assertTrue(await directory_client1.exists())
-        self.assertFalse(await directory_client2.exists())
+        assert await directory_client1.exists()
+        assert not await directory_client2.exists()
 
     @DataLakePreparer()
-    async def test_using_oauth_token_credential_to_create_directory_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_using_oauth_token_credential_to_create_directory(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # generate a token with directory level create permission
         directory_name = self._get_directory_reference()
 
         token_credential = self.generate_oauth_token()
         directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
                                                    credential=token_credential)
         response = await directory_client.create_directory()
-        self.assertIsNotNone(response)
+        assert response is not None
 
     @DataLakePreparer()
-    async def test_create_directory_with_match_conditions_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_directory_with_match_conditions(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
 
         # Act
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         created = await directory_client.create_directory(match_condition=MatchConditions.IfMissing)
 
         # Assert
-        self.assertTrue(created)
+        assert created
 
     @DataLakePreparer()
-    async def test_create_directory_with_permission_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_directory_with_permission(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
 
         # Act
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         created = await directory_client.create_directory(permissions="rwxr--r--", umask="0000")
 
         prop = await directory_client.get_access_control()
 
         # Assert
-        self.assertTrue(created)
-        self.assertEqual(prop['permissions'], 'rwxr--r--')
+        assert created
+        assert prop['permissions'] == 'rwxr--r--'
 
     @DataLakePreparer()
-    async def test_create_directory_with_content_settings_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_directory_with_content_settings(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
         content_settings = ContentSettings(
             content_language='spanish',
             content_disposition='inline')
         # Act
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         created = await directory_client.create_directory(content_settings=content_settings)
 
         # Assert
-        self.assertTrue(created)
+        assert created
 
     @DataLakePreparer()
-    async def test_create_directory_with_metadata_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_directory_with_metadata(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
         metadata = {'hello': 'world', 'number': '42'}
         # Act
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         created = await directory_client.create_directory(metadata=metadata)
 
         properties = await directory_client.get_directory_properties()
 
         # Assert
-        self.assertTrue(created)
+        assert created
 
     @DataLakePreparer()
-    async def test_delete_directory_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_delete_directory(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
         metadata = {'hello': 'world', 'number': '42'}
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory(metadata=metadata)
 
         await directory_client.delete_directory()
 
     @DataLakePreparer()
-    async def test_delete_directory_with_if_modified_since_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_delete_directory_with_if_modified_since(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
 
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         prop = await directory_client.get_directory_properties()
 
-        with self.assertRaises(ResourceModifiedError):
+        with pytest.raises(ResourceModifiedError):
             await directory_client.delete_directory(if_modified_since=prop['last_modified'])
 
     @DataLakePreparer()
-    async def test_create_sub_directory_and_delete_sub_directory_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_sub_directory_and_delete_sub_directory(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
         metadata = {'hello': 'world', 'number': '42'}
 
         # Create a directory first, to prepare for creating sub directory
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
@@ -307,115 +338,142 @@
 
         # to make sure the sub directory was indeed created by get sub_directory properties from sub directory client
         sub_directory_client = self.dsc.get_directory_client(self.file_system_name,
                                                              directory_name + '/' + sub_directory_name)
         sub_properties = await sub_directory_client.get_directory_properties()
 
         # Assert
-        self.assertTrue(sub_directory_created)
-        self.assertTrue(sub_properties)
+        assert sub_directory_created
+        assert sub_properties
 
         # Act
         await directory_client.delete_sub_directory(sub_directory_name)
-        with self.assertRaises(ResourceNotFoundError):
+        with pytest.raises(ResourceNotFoundError):
             await sub_directory_client.get_directory_properties()
 
     @DataLakePreparer()
-    async def test_set_access_control_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_set_access_control(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         metadata = {'hello': 'world', 'number': '42'}
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory(metadata=metadata)
 
         response = await directory_client.set_access_control(permissions='0777')
         # Assert
-        self.assertIsNotNone(response)
+        assert response is not None
 
     @DataLakePreparer()
-    async def test_set_access_control_with_acl_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_set_access_control_with_acl(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         metadata = {'hello': 'world', 'number': '42'}
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory(metadata=metadata)
 
         acl = 'user::rwx,group::r-x,other::rwx'
         await directory_client.set_access_control(acl=acl)
         access_control = await directory_client.get_access_control()
 
         # Assert
 
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        assert access_control is not None
+        assert acl == access_control['acl']
 
     @DataLakePreparer()
-    async def test_set_access_control_if_none_modified_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_set_access_control_if_none_modified(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         resp = await directory_client.create_directory()
 
         response = await directory_client.set_access_control(permissions='0777', etag=resp['etag'],
                                                              match_condition=MatchConditions.IfNotModified)
         # Assert
-        self.assertIsNotNone(response)
+        assert response is not None
 
     @DataLakePreparer()
-    async def test_get_access_control_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_get_access_control(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         metadata = {'hello': 'world', 'number': '42'}
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory(metadata=metadata, permissions='0777')
 
         # Act
         response = await directory_client.get_access_control()
         # Assert
-        self.assertIsNotNone(response)
+        assert response is not None
 
     @DataLakePreparer()
-    async def test_get_access_control_with_match_conditions_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_get_access_control_with_match_conditions(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         resp = await directory_client.create_directory(permissions='0777', umask='0000')
 
         # Act
         response = await directory_client.get_access_control(etag=resp['etag'],
                                                              match_condition=MatchConditions.IfNotModified)
         # Assert
-        self.assertIsNotNone(response)
-        self.assertEqual(response['permissions'], 'rwxrwxrwx')
+        assert response is not None
+        assert response['permissions'] == 'rwxrwxrwx'
 
     @DataLakePreparer()
-    async def test_set_access_control_recursive_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_set_access_control_recursive(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
         acl = 'user::rwx,group::r-x,other::rwx'
         summary = await directory_client.set_access_control_recursive(acl=acl)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertIsNone(summary.continuation)
+        assert summary.counters.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert summary.counters.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert summary.counters.failure_count == 0
+        assert summary.continuation is None
         access_control = await directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        assert access_control is not None
+        assert acl == access_control['acl']
 
     @DataLakePreparer()
-    async def test_set_access_control_recursive_throws_exception_containing_continuation_token_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_set_access_control_recursive_throws_exception_containing_continuation_token(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
@@ -424,46 +482,53 @@
 
         def callback(response):
             response_list.append(response)
             if len(response_list) == 2:
                 raise ServiceRequestError("network problem")
         acl = 'user::rwx,group::r-x,other::rwx'
 
-        with self.assertRaises(AzureError) as acl_error:
+        with pytest.raises(AzureError) as acl_error:
             await directory_client.set_access_control_recursive(acl=acl, batch_size=2, max_batches=2,
                                                                 raw_response_hook=callback, retry_total=0)
-        self.assertIsNotNone(acl_error.exception.continuation_token)
-        self.assertEqual(acl_error.exception.message, "network problem")
-        self.assertIsInstance(acl_error.exception, ServiceRequestError)
+        assert acl_error.value.continuation_token is not None
+        assert acl_error.value.message == "network problem"
+        assert acl_error.typename == "ServiceRequestError"
 
     @DataLakePreparer()
-    async def test_set_access_control_recursive_in_batches_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_set_access_control_recursive_in_batches(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
         acl = 'user::rwx,group::r-x,other::rwx'
         summary = await directory_client.set_access_control_recursive(acl=acl, batch_size=2)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertIsNone(summary.continuation)
+        assert summary.counters.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert summary.counters.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert summary.counters.failure_count == 0
+        assert summary.continuation is None
         access_control = await directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        assert access_control is not None
+        assert acl == access_control['acl']
 
     @DataLakePreparer()
-    async def test_set_access_control_recursive_in_batches_with_progress_callback_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_set_access_control_recursive_in_batches_with_progress_callback(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
@@ -479,73 +544,102 @@
 
             last_response.counters = resp.aggregate_counters
 
         summary = await directory_client.set_access_control_recursive(acl=acl, progress_hook=progress_callback,
                                                                       batch_size=2)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertIsNone(summary.continuation)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(summary.counters.directories_successful, last_response.counters.directories_successful)
-        self.assertEqual(summary.counters.files_successful, last_response.counters.files_successful)
-        self.assertEqual(summary.counters.failure_count, last_response.counters.failure_count)
+        assert summary.counters.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert summary.counters.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert summary.counters.failure_count == 0
+        assert summary.continuation is None
+        assert summary.counters.directories_successful == running_tally.directories_successful
+        assert summary.counters.files_successful == running_tally.files_successful
+        assert summary.counters.failure_count == running_tally.failure_count
+        assert summary.counters.directories_successful == last_response.counters.directories_successful
+        assert summary.counters.files_successful == last_response.counters.files_successful
+        assert summary.counters.failure_count == last_response.counters.failure_count
         access_control = await directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        assert access_control is not None
+        assert acl == access_control['acl']
 
+    @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_set_access_control_recursive_with_failures_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_set_access_control_recursive_with_failures(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+        url = self.account_url(datalake_storage_account_name, 'dfs')
+        variables = kwargs.pop('variables', {})
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        if not self.is_playback():
-            return
+
         root_directory_client = self.dsc.get_file_system_client(self.file_system_name)._get_root_directory_client()
         await root_directory_client.set_access_control(acl="user::--x,group::--x,other::--x")
 
-        # Using an AAD identity, create a directory to put files under that
+        # Create files and directories with provided owner except file3
+        test_guid = "5d56d308-df82-4266-ba63-ef1da3945873"
         directory_name = self._get_directory_reference()
+        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
+        await directory_client.create_directory(owner=test_guid)
+        await self.dsc.get_directory_client(self.file_system_name, directory_name + '/subdir1').create_directory(owner=test_guid, permissions='0777')
+        await self.dsc.get_directory_client(self.file_system_name, directory_name + '/subdir2').create_directory(owner=test_guid, permissions='0777')
+        await self.dsc.get_file_client(self.file_system_name, directory_name + '/subdir1/file1').create_file(owner=test_guid, permissions='0777')
+        await self.dsc.get_file_client(self.file_system_name, directory_name + '/subdir2/file2').create_file(owner=test_guid, permissions='0777')
+        await directory_client.get_file_client('file3').create_file()
+
+        # User delegation SAS with provided owner permissions
         token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        await directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        start_time = self.get_datetime_variable(variables, 'start_time', datetime.utcnow())
+        expiry_time = self.get_datetime_variable(variables, 'expiry_time', datetime.utcnow() + timedelta(hours=1))
+        owner_dsc = DataLakeServiceClient(url, credential=token_credential)
+        user_delegation_key = await owner_dsc.get_user_delegation_key(start_time, expiry_time)
+        sas_token = self.generate_sas(
+            generate_directory_sas,
+            datalake_storage_account_name,
+            self.file_system_name,
+            directory_name,
+            user_delegation_key,
+            permission='racwdlmeop',
+            expiry=expiry_time,
+            agent_object_id=test_guid
+        )
 
-        # Create a file as super user
-        await self.dsc.get_directory_client(self.file_system_name, directory_name).get_file_client("cannottouchthis") \
-            .create_file()
+        if self.is_live:
+            time.sleep(10)
+
+        owner_dir_client = DataLakeDirectoryClient(url, self.file_system_name, directory_name, sas_token)
 
         acl = 'user::rwx,group::r-x,other::rwx'
         running_tally = AccessControlChangeCounters(0, 0, 0)
         failed_entries = []
 
         async def progress_callback(resp):
             running_tally.directories_successful += resp.batch_counters.directories_successful
             running_tally.files_successful += resp.batch_counters.files_successful
             running_tally.failure_count += resp.batch_counters.failure_count
             failed_entries.append(resp.batch_failures)
 
-        summary = await directory_client.set_access_control_recursive(acl=acl, progress_hook=progress_callback,
+        summary = await owner_dir_client.set_access_control_recursive(acl=acl, progress_hook=progress_callback,
                                                                       batch_size=2)
 
         # Assert
-        self.assertEqual(summary.counters.failure_count, 1)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 1)
+        assert summary.counters.failure_count == 1
+        assert summary.counters.directories_successful == running_tally.directories_successful
+        assert summary.counters.files_successful == running_tally.files_successful
+        assert summary.counters.failure_count == running_tally.failure_count
+        assert len(failed_entries) == 1
+
+        return variables
 
     @DataLakePreparer()
-    async def test_set_access_control_recursive_in_batches_with_explicit_iteration_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_set_access_control_recursive_in_batches_with_explicit_iteration(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
@@ -564,68 +658,77 @@
 
             running_tally.directories_successful += result.counters.directories_successful
             running_tally.files_successful += result.counters.files_successful
             running_tally.failure_count += result.counters.failure_count
             iteration_count += 1
 
         # Assert
-        self.assertEqual(running_tally.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(running_tally.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(running_tally.failure_count, 0)
+        assert running_tally.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert running_tally.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert running_tally.failure_count == 0
         access_control = await directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        assert access_control is not None
+        assert acl == access_control['acl']
 
     @DataLakePreparer()
-    async def test_update_access_control_recursive_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_update_access_control_recursive(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
         acl = 'user::rwx,group::r-x,other::rwx'
         summary = await directory_client.update_access_control_recursive(acl=acl)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
+        assert summary.counters.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert summary.counters.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert summary.counters.failure_count == 0
         access_control = await directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        assert access_control is not None
+        assert acl == access_control['acl']
 
     @DataLakePreparer()
-    async def test_update_access_control_recursive_in_batches_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_update_access_control_recursive_in_batches(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
         acl = 'user::rwx,group::r-x,other::rwx'
         summary = await directory_client.update_access_control_recursive(acl=acl, batch_size=2)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
+        assert summary.counters.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert summary.counters.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert summary.counters.failure_count == 0
         access_control = await directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        assert access_control is not None
+        assert acl == access_control['acl']
 
     @DataLakePreparer()
-    async def test_update_access_control_recursive_in_batches_with_progress_callback_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_update_access_control_recursive_in_batches_with_progress_callback(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
@@ -641,149 +744,216 @@
 
             last_response.counters = resp.aggregate_counters
 
         summary = await directory_client.update_access_control_recursive(acl=acl, progress_hook=progress_callback,
                                                                          batch_size=2)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
+        assert summary.counters.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert summary.counters.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert summary.counters.failure_count == 0
+        assert summary.counters.directories_successful == running_tally.directories_successful
+        assert summary.counters.files_successful == running_tally.files_successful
+        assert summary.counters.failure_count == running_tally.failure_count
         access_control = await directory_client.get_access_control()
-        self.assertIsNotNone(access_control)
-        self.assertEqual(acl, access_control['acl'])
+        assert access_control is not None
+        assert acl == access_control['acl']
 
+    @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_update_access_control_recursive_with_failures_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_update_access_control_recursive_with_failures(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+        url = self.account_url(datalake_storage_account_name, 'dfs')
+        variables = kwargs.pop('variables', {})
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        if not self.is_playback():
-            return
+
         root_directory_client = self.dsc.get_file_system_client(self.file_system_name)._get_root_directory_client()
         await root_directory_client.set_access_control(acl="user::--x,group::--x,other::--x")
 
-        # Using an AAD identity, create a directory to put files under that
+        # Create files and directories with provided owner except file3
+        test_guid = "5d56d308-df82-4266-ba63-ef1da3945873"
         directory_name = self._get_directory_reference()
+        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
+        await directory_client.create_directory(owner=test_guid)
+        await self.dsc.get_directory_client(self.file_system_name, directory_name + '/subdir1').create_directory(owner=test_guid, permissions='0777')
+        await self.dsc.get_directory_client(self.file_system_name, directory_name + '/subdir2').create_directory(owner=test_guid, permissions='0777')
+        await self.dsc.get_file_client(self.file_system_name, directory_name + '/subdir1/file1').create_file(owner=test_guid, permissions='0777')
+        await self.dsc.get_file_client(self.file_system_name, directory_name + '/subdir2/file2').create_file(owner=test_guid, permissions='0777')
+        await directory_client.get_file_client('file3').create_file()
+
+        # User delegation SAS with provided owner permissions
         token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        await directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        start_time = self.get_datetime_variable(variables, 'start_time', datetime.utcnow())
+        expiry_time = self.get_datetime_variable(variables, 'expiry_time', datetime.utcnow() + timedelta(hours=1))
+        owner_dsc = DataLakeServiceClient(url, credential=token_credential)
+        user_delegation_key = await owner_dsc.get_user_delegation_key(start_time, expiry_time)
+        sas_token = self.generate_sas(
+            generate_directory_sas,
+            datalake_storage_account_name,
+            self.file_system_name,
+            directory_name,
+            user_delegation_key,
+            permission='racwdlmeop',
+            expiry=expiry_time,
+            agent_object_id=test_guid
+        )
+
+        if self.is_live:
+            time.sleep(10)
 
-        # Create a file as super user
-        await self.dsc.get_directory_client(self.file_system_name, directory_name).get_file_client("cannottouchthis") \
-            .create_file()
+        owner_dir_client = DataLakeDirectoryClient(url, self.file_system_name, directory_name, sas_token)
 
         acl = 'user::rwx,group::r-x,other::rwx'
         running_tally = AccessControlChangeCounters(0, 0, 0)
         failed_entries = []
 
         async def progress_callback(resp):
             running_tally.directories_successful += resp.batch_counters.directories_successful
             running_tally.files_successful += resp.batch_counters.files_successful
             running_tally.failure_count += resp.batch_counters.failure_count
-            failed_entries.append(resp.batch_failures)
+            if resp.batch_failures:
+                failed_entries.append(resp.batch_failures)
 
-        summary = await directory_client.update_access_control_recursive(acl=acl, progress_hook=progress_callback,
+        summary = await owner_dir_client.update_access_control_recursive(acl=acl, progress_hook=progress_callback,
                                                                          batch_size=2)
 
         # Assert
-        self.assertEqual(summary.counters.failure_count, 1)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 1)
+        assert summary.counters.failure_count == 1
+        assert summary.counters.directories_successful == running_tally.directories_successful
+        assert summary.counters.files_successful == running_tally.files_successful
+        assert summary.counters.failure_count == running_tally.failure_count
+        assert len(failed_entries) == 1
+
+        return variables
 
+    @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_update_access_control_recursive_continue_on_failures_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_update_access_control_recursive_continue_on_failures(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+        url = self.account_url(datalake_storage_account_name, 'dfs')
+        variables = kwargs.pop('variables', {})
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        if not self.is_playback():
-            return
+
         root_directory_client = self.dsc.get_file_system_client(self.file_system_name)._get_root_directory_client()
         await root_directory_client.set_access_control(acl="user::--x,group::--x,other::--x")
 
-        # Using an AAD identity, create a directory to put files under that
+        # Create files and directories with provided owner except file3, dir3
+        test_guid = "5d56d308-df82-4266-ba63-ef1da3945873"
         directory_name = self._get_directory_reference()
+        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
+        await directory_client.create_directory(owner=test_guid)
+        await self.dsc.get_directory_client(self.file_system_name, directory_name + '/subdir1').create_directory(
+            owner=test_guid, permissions='0777')
+        await self.dsc.get_directory_client(self.file_system_name, directory_name + '/subdir2').create_directory(
+            owner=test_guid, permissions='0777')
+        await self.dsc.get_file_client(self.file_system_name, directory_name + '/subdir1/file1').create_file(
+            owner=test_guid, permissions='0777')
+        await self.dsc.get_file_client(self.file_system_name, directory_name + '/subdir2/file2').create_file(
+            owner=test_guid, permissions='0777')
+
+        await directory_client.get_file_client('file3').create_file()
+        await self.dsc.get_directory_client(self.file_system_name, directory_name + '/dir3').create_directory()
+
+        # User delegation SAS with provided owner permissions
         token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        await directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        start_time = self.get_datetime_variable(variables, 'start_time', datetime.utcnow())
+        expiry_time = self.get_datetime_variable(variables, 'expiry_time', datetime.utcnow() + timedelta(hours=1))
+        owner_dsc = DataLakeServiceClient(url, credential=token_credential)
+        user_delegation_key = await owner_dsc.get_user_delegation_key(start_time, expiry_time)
+        sas_token = self.generate_sas(
+            generate_directory_sas,
+            datalake_storage_account_name,
+            self.file_system_name,
+            directory_name,
+            user_delegation_key,
+            permission='racwdlmeop',
+            expiry=expiry_time,
+            agent_object_id=test_guid
+        )
+
+        if self.is_live:
+            time.sleep(10)
 
-        # Create a file as super user
-        await self.dsc.get_directory_client(self.file_system_name, directory_name).get_file_client("cannottouchthis") \
-            .create_file()
+        owner_dir_client = DataLakeDirectoryClient(url, self.file_system_name, directory_name, sas_token)
 
         acl = 'user::rwx,group::r-x,other::rwx'
         running_tally = AccessControlChangeCounters(0, 0, 0)
         failed_entries = []
 
         async def progress_callback(resp):
             running_tally.directories_successful += resp.batch_counters.directories_successful
             running_tally.files_successful += resp.batch_counters.files_successful
             running_tally.failure_count += resp.batch_counters.failure_count
             if resp.batch_failures:
-                failed_entries.extend(resp.batch_failures)
+                failed_entries.append(resp.batch_failures)
 
-        summary = await directory_client.update_access_control_recursive(acl=acl, progress_hook=progress_callback,
+        summary = await owner_dir_client.update_access_control_recursive(acl=acl, progress_hook=progress_callback,
                                                                          batch_size=2, continue_on_failure=True)
 
         # Assert
-        self.assertEqual(summary.counters.failure_count, 1)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 1)
-        self.assertIsNone(summary.continuation)
+        assert summary.counters.failure_count == 2
+        assert summary.counters.directories_successful == running_tally.directories_successful
+        assert summary.counters.files_successful == running_tally.files_successful
+        assert summary.counters.failure_count == running_tally.failure_count
+        assert len(failed_entries) == 2
+
+        return variables
 
     @DataLakePreparer()
-    async def test_remove_access_control_recursive_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_remove_access_control_recursive(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
         summary = await directory_client.remove_access_control_recursive(acl=REMOVE_ACL)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
+        assert summary.counters.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert summary.counters.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert summary.counters.failure_count == 0
 
     @DataLakePreparer()
-    async def test_remove_access_control_recursive_in_batches_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_remove_access_control_recursive_in_batches(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
 
         summary = await directory_client.remove_access_control_recursive(acl=REMOVE_ACL, batch_size=2)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
+        assert summary.counters.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert summary.counters.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert summary.counters.failure_count == 0
 
     @DataLakePreparer()
-    async def test_remove_access_control_recursive_in_batches_with_progress_callback_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_remove_access_control_recursive_in_batches_with_progress_callback(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
         num_sub_dirs = 5
         num_file_per_sub_dir = 5
         await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
@@ -799,66 +969,95 @@
             last_response.counters = resp.aggregate_counters
 
         summary = await directory_client.remove_access_control_recursive(acl=REMOVE_ACL,
                                                                          progress_hook=progress_callback,
                                                                          batch_size=2)
 
         # Assert
-        self.assertEqual(summary.counters.directories_successful,
-                         num_sub_dirs + 1)  # +1 as the dir itself was also included
-        self.assertEqual(summary.counters.files_successful, num_sub_dirs * num_file_per_sub_dir)
-        self.assertEqual(summary.counters.failure_count, 0)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
+        assert summary.counters.directories_successful == num_sub_dirs + 1  # +1 as the dir itself was also included
+        assert summary.counters.files_successful == num_sub_dirs * num_file_per_sub_dir
+        assert summary.counters.failure_count == 0
+        assert summary.counters.directories_successful == running_tally.directories_successful
+        assert summary.counters.files_successful == running_tally.files_successful
+        assert summary.counters.failure_count == running_tally.failure_count
 
+    @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_remove_access_control_recursive_with_failures_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_remove_access_control_recursive_with_failures(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+        url = self.account_url(datalake_storage_account_name, 'dfs')
+        variables = kwargs.pop('variables', {})
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        if not self.is_playback():
-            return
+
         root_directory_client = self.dsc.get_file_system_client(self.file_system_name)._get_root_directory_client()
         await root_directory_client.set_access_control(acl="user::--x,group::--x,other::--x")
 
-        # Using an AAD identity, create a directory to put files under that
+        # Create files and directories with provided owner except file3
+        test_guid = "5d56d308-df82-4266-ba63-ef1da3945873"
         directory_name = self._get_directory_reference()
+        directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
+        await directory_client.create_directory(owner=test_guid)
+        await self.dsc.get_directory_client(self.file_system_name, directory_name + '/subdir1').create_directory(owner=test_guid, permissions='0777')
+        await self.dsc.get_directory_client(self.file_system_name, directory_name + '/subdir2').create_directory(owner=test_guid, permissions='0777')
+        await self.dsc.get_file_client(self.file_system_name, directory_name + '/subdir1/file1').create_file(owner=test_guid, permissions='0777')
+        await self.dsc.get_file_client(self.file_system_name, directory_name + '/subdir2/file2').create_file(owner=test_guid, permissions='0777')
+        await directory_client.get_file_client('file3').create_file()
+
+        # User delegation SAS with provided owner permissions
         token_credential = self.generate_oauth_token()
-        directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
-                                                   credential=token_credential)
-        await directory_client.create_directory()
-        num_sub_dirs = 5
-        num_file_per_sub_dir = 5
-        await self._create_sub_directory_and_files(directory_client, num_sub_dirs, num_file_per_sub_dir)
+        start_time = self.get_datetime_variable(variables, 'start_time', datetime.utcnow())
+        expiry_time = self.get_datetime_variable(variables, 'expiry_time', datetime.utcnow() + timedelta(hours=1))
+        owner_dsc = DataLakeServiceClient(url, credential=token_credential)
+        user_delegation_key = await owner_dsc.get_user_delegation_key(start_time, expiry_time)
+        sas_token = self.generate_sas(
+            generate_directory_sas,
+            datalake_storage_account_name,
+            self.file_system_name,
+            directory_name,
+            user_delegation_key,
+            permission='racwdlmeop',
+            expiry=expiry_time,
+            agent_object_id=test_guid
+        )
 
-        # Create a file as super user
-        await self.dsc.get_directory_client(self.file_system_name, directory_name).get_file_client("cannottouchthis") \
-            .create_file()
+        if self.is_live:
+            time.sleep(10)
+
+        owner_dir_client = DataLakeDirectoryClient(url, self.file_system_name, directory_name, sas_token)
 
         running_tally = AccessControlChangeCounters(0, 0, 0)
         failed_entries = []
 
         async def progress_callback(resp):
             running_tally.directories_successful += resp.batch_counters.directories_successful
             running_tally.files_successful += resp.batch_counters.files_successful
             running_tally.failure_count += resp.batch_counters.failure_count
-            failed_entries.append(resp.batch_failures)
+            if resp.batch_failures:
+                failed_entries.append(resp.batch_failures)
 
-        summary = await directory_client.remove_access_control_recursive(acl=REMOVE_ACL,
-                                                                         progress_hook=progress_callback,
+        summary = await owner_dir_client.remove_access_control_recursive(acl=REMOVE_ACL, progress_hook=progress_callback,
                                                                          batch_size=2)
 
         # Assert
-        self.assertEqual(summary.counters.failure_count, 1)
-        self.assertEqual(summary.counters.directories_successful, running_tally.directories_successful)
-        self.assertEqual(summary.counters.files_successful, running_tally.files_successful)
-        self.assertEqual(summary.counters.failure_count, running_tally.failure_count)
-        self.assertEqual(len(failed_entries), 1)
+        assert summary.counters.failure_count == 1
+        assert summary.counters.directories_successful == running_tally.directories_successful
+        assert summary.counters.files_successful == running_tally.files_successful
+        assert summary.counters.failure_count == running_tally.failure_count
+        assert len(failed_entries) == 1
+
+        return variables
 
     @DataLakePreparer()
-    async def test_rename_from_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_rename_from(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         content_settings = ContentSettings(
             content_language='spanish',
             content_disposition='inline')
         directory_name = self._get_directory_reference()
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
@@ -867,35 +1066,43 @@
 
         new_directory_client = self.dsc.get_directory_client(self.file_system_name, new_name)
 
         await new_directory_client._rename_path('/' + self.file_system_name + '/' + directory_name,
                                                 content_settings=content_settings)
         properties = await new_directory_client.get_directory_properties()
 
-        self.assertIsNotNone(properties)
-        self.assertIsNone(properties.get('content_settings'))
+        assert properties is not None
+        assert properties.get('content_settings') is None
 
     @pytest.mark.skip(reason="Investigate why renaming from shorter path to longer path does not work")
     @DataLakePreparer()
-    async def test_rename_from_a_shorter_directory_to_longer_directory_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_rename_from_a_shorter_directory_to_longer_directory(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
         await self._create_directory_and_get_directory_client(directory_name="old")
 
         new_name = "newname"
         new_directory_client = await self._create_directory_and_get_directory_client(directory_name=new_name)
         new_directory_client = await new_directory_client.create_sub_directory("newsub")
 
         await new_directory_client._rename_path('/' + self.file_system_name + '/' + directory_name)
         properties = await new_directory_client.get_directory_properties()
 
-        self.assertIsNotNone(properties)
+        assert properties is not None
 
     @DataLakePreparer()
-    async def test_rename_from_a_directory_in_another_file_system_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_rename_from_a_directory_in_another_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # create a file dir1 under filesystem1
         old_file_system_name = self._get_directory_reference("oldfilesystem")
         old_dir_name = "olddir"
         old_client = self.dsc.get_file_system_client(old_file_system_name)
         if not self.is_playback():
             time.sleep(30)
@@ -909,18 +1116,22 @@
         new_directory_client = await self._create_directory_and_get_directory_client(directory_name=new_name)
         new_directory_client = await new_directory_client.create_sub_directory("newsub")
 
         # rename dir1 under filesystem1 to dir2 under filesystem2
         await new_directory_client._rename_path('/' + old_file_system_name + '/' + old_dir_name)
         properties = await new_directory_client.get_directory_properties()
 
-        self.assertIsNotNone(properties)
+        assert properties is not None
 
     @DataLakePreparer()
-    async def test_rename_from_an_unencoded_directory_in_another_file_system_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_rename_from_an_unencoded_directory_in_another_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # create a directory under filesystem1
         old_file_system_name = self._get_directory_reference("oldfilesystem")
         old_dir_name = "old dir"
         old_client = self.dsc.get_file_system_client(old_file_system_name)
         await old_client.create_file_system()
         old_dir_client =await old_client.create_directory(old_dir_name)
@@ -936,20 +1147,24 @@
         await new_file_system_client.create_directory(new_name)
 
         # rename dir1 under filesystem1 to dir2 under filesystem2
         new_directory_client = await old_dir_client.rename_directory('/' + new_file_system_name + '/' + new_name)
         properties = await new_directory_client.get_directory_properties()
         file_properties = await new_directory_client.get_file_client(file_name).get_file_properties()
 
-        self.assertIsNotNone(properties)
-        self.assertIsNotNone(file_properties)
+        assert properties is not None
+        assert file_properties is not None
         await old_client.delete_file_system()
 
     @DataLakePreparer()
-    async def test_rename_to_an_existing_directory_in_another_file_system_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_rename_to_an_existing_directory_in_another_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # create a file dir1 under filesystem1
         destination_file_system_name = self._get_directory_reference("destfilesystem")
         destination_dir_name = "destdir"
         fs_client = self.dsc.get_file_system_client(destination_file_system_name)
         if not self.is_playback():
             time.sleep(30)
@@ -962,21 +1177,25 @@
         source_directory_client = await source_directory_client.create_sub_directory("subdir")
 
         # rename dir2 under filesystem2 to dir1 under filesystem1
         res = await source_directory_client.rename_directory(
             '/' + destination_file_system_name + '/' + destination_dir_name)
 
         # the source directory has been renamed to destination directory, so it cannot be found
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await source_directory_client.get_directory_properties()
 
-        self.assertEqual(res.url, destination_directory_client.url)
+        assert res.url == destination_directory_client.url
 
     @DataLakePreparer()
-    async def test_rename_with_none_existing_destination_condition_and_source_unmodified_condition_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_rename_with_none_existing_destination_condition_and_source_unmodified_condition(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         non_existing_dir_name = "nonexistingdir"
 
         # create a filesystem1
         destination_file_system_name = self._get_directory_reference("destfilesystem")
         fs_client = self.dsc.get_file_system_client(destination_file_system_name)
         await fs_client.create_file_system()
@@ -993,21 +1212,25 @@
         res = await source_directory_client.rename_directory(
             '/' + destination_file_system_name + '/' + non_existing_dir_name,
             match_condition=MatchConditions.IfMissing,
             source_etag=etag,
             source_match_condition=MatchConditions.IfNotModified)
 
         # the source directory has been renamed to destination directory, so it cannot be found
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await source_directory_client.get_directory_properties()
 
-        self.assertEqual(non_existing_dir_name, res.path_name)
+        assert non_existing_dir_name == res.path_name
 
     @DataLakePreparer()
-    async def test_rename_to_an_non_existing_directory_in_another_file_system_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_rename_to_an_non_existing_directory_in_another_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # create a file dir1 under filesystem1
         destination_file_system_name = self._get_directory_reference("destfilesystem")
         non_existing_dir_name = "nonexistingdir"
         fs_client = self.dsc.get_file_system_client(destination_file_system_name)
         await fs_client.create_file_system()
 
@@ -1017,35 +1240,42 @@
         source_directory_client = await source_directory_client.create_sub_directory("subdir")
 
         # rename dir2 under filesystem2 to dir1 under filesystem1
         res = await source_directory_client.rename_directory(
             '/' + destination_file_system_name + '/' + non_existing_dir_name)
 
         # the source directory has been renamed to destination directory, so it cannot be found
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await source_directory_client.get_directory_properties()
 
-        self.assertEqual(non_existing_dir_name, res.path_name)
+        assert non_existing_dir_name == res.path_name
 
     @pytest.mark.skip(reason="Investigate why renaming non-empty directory doesn't work")
     @DataLakePreparer()
-    async def test_rename_directory_to_non_empty_directory_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_rename_directory_to_non_empty_directory(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         dir1 = await self._create_directory_and_get_directory_client("dir1")
         await dir1.create_sub_directory("subdir")
 
         dir2 = await self._create_directory_and_get_directory_client("dir2")
         await dir2.rename_directory(dir1.file_system_name + '/' + dir1.path_name)
 
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await dir2.get_directory_properties()
 
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_rename_dir_with_file_system_sas_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_rename_dir_with_file_system_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
 
         token = generate_file_system_sas(
             self.dsc.account_name,
             self.file_system_name,
             self.dsc.credential.account_key,
             FileSystemSasPermissions(write=True, read=True, delete=True, move=True),
@@ -1054,19 +1284,22 @@
 
         # read the created file which is under root directory
         dir_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, "olddir", credential=token)
         await dir_client.create_directory()
         new_client = await dir_client.rename_directory(dir_client.file_system_name+'/'+'newdir'+'?')
 
         properties = await new_client.get_directory_properties()
-        self.assertEqual(properties.name, "newdir")
+        assert properties.name == "newdir"
 
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_rename_dir_with_file_sas_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_rename_dir_with_file_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         token = generate_directory_sas(self.dsc.account_name,
                                        self.file_system_name,
                                        "olddir",
                                        datalake_storage_account_key,
                                        permission=DirectorySasPermissions(read=True, create=True, write=True,
                                                                           delete=True, move=True),
@@ -1084,34 +1317,41 @@
 
         # read the created file which is under root directory
         dir_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, "olddir", credential=token)
         await dir_client.create_directory()
         new_client = await dir_client.rename_directory(dir_client.file_system_name+'/'+'newdir'+'?'+new_token)
 
         properties = await new_client.get_directory_properties()
-        self.assertEqual(properties.name, "newdir")
+        assert properties.name == "newdir"
 
     @DataLakePreparer()
-    async def test_get_properties_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_get_properties(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         directory_name = self._get_directory_reference()
         metadata = {'hello': 'world', 'number': '42'}
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory(metadata=metadata)
 
         properties = await directory_client.get_directory_properties()
         # Assert
-        self.assertTrue(properties)
-        self.assertIsNotNone(properties.metadata)
-        self.assertEqual(properties.metadata['hello'], metadata['hello'])
+        assert properties
+        assert properties.metadata is not None
+        assert properties.metadata['hello'] == metadata['hello']
 
     @DataLakePreparer()
-    async def test_directory_encryption_scope_from_file_system_async(self, datalake_storage_account_name,
-                                                                     datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_directory_encryption_scope_from_file_system_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
         self.dsc = DataLakeServiceClient(url, credential=datalake_storage_account_key, logging_enable=True)
         self.config = self.dsc._config
         self.file_system_name = self.get_resource_name('filesystem')
         dir_name = 'testdir'
         file_system = self.dsc.get_file_system_client(self.file_system_name)
@@ -1120,21 +1360,24 @@
         await file_system.create_file_system(encryption_scope_options=encryption_scope)
         await file_system.create_directory(dir_name)
 
         directory_client = file_system.get_directory_client(dir_name)
         props = await directory_client.get_directory_properties()
 
         # Assert
-        self.assertTrue(props)
-        self.assertIsNotNone(props['encryption_scope'])
-        self.assertEqual(props['encryption_scope'], encryption_scope.default_encryption_scope)
+        assert props
+        assert props['encryption_scope'] is not None
+        assert props['encryption_scope'] == encryption_scope.default_encryption_scope
 
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_using_directory_sas_to_read_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_using_directory_sas_to_read(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # SAS URL is calculated from storage key, so this test runs live only
 
         client = await self._create_directory_and_get_directory_client()
         directory_name = client.path_name
 
         # generate a token with directory level read permission
@@ -1147,19 +1390,22 @@
             expiry=datetime.utcnow() + timedelta(hours=1),
         )
 
         directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
                                                    credential=token)
         access_control = await directory_client.get_access_control()
 
-        self.assertIsNotNone(access_control)
+        assert access_control is not None
 
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_using_directory_sas_to_create_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_using_directory_sas_to_create(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # SAS URL is calculated from storage key, so this test runs live only
 
         # generate a token with directory level create permission
         directory_name = self._get_directory_reference()
         token = generate_directory_sas(
             self.dsc.account_name,
@@ -1168,36 +1414,40 @@
             self.dsc.credential.account_key,
             permission=DirectorySasPermissions(create=True),
             expiry=datetime.utcnow() + timedelta(hours=1),
         )
         directory_client = DataLakeDirectoryClient(self.dsc.url, self.file_system_name, directory_name,
                                                    credential=token)
         response = await directory_client.create_directory()
-        self.assertIsNotNone(response)
+        assert response is not None
 
     @DataLakePreparer()
-    def test_using_directory_sas_to_create_file(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_using_directory_sas_to_create_file(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         newest_api_version = _SUPPORTED_API_VERSIONS[-1]
 
         service_client = DataLakeServiceClient("https://abc.dfs.core.windows.net", credential='fake')
         filesys_client = service_client.get_file_system_client("filesys")
         dir_client = DataLakeDirectoryClient("https://abc.dfs.core.windows.net", "filesys", "dir", credential='fake')
         file_client = dir_client.get_file_client("file")
-        self.assertEqual(service_client.api_version, newest_api_version)
-        self.assertEqual(filesys_client.api_version, newest_api_version)
-        self.assertEqual(dir_client.api_version, newest_api_version)
-        self.assertEqual(file_client.api_version, newest_api_version)
+        assert service_client.api_version == newest_api_version
+        assert filesys_client.api_version == newest_api_version
+        assert dir_client.api_version == newest_api_version
+        assert file_client.api_version == newest_api_version
 
         service_client2 = DataLakeServiceClient("https://abc.dfs.core.windows.net", credential='fake',
                                                 api_version="2019-02-02")
         filesys_client2 = service_client2.get_file_system_client("filesys")
         dir_client2 = DataLakeDirectoryClient("https://abc.dfs.core.windows.net", "filesys", "dir", credential='fake',
                                               api_version="2019-02-02")
         file_client2 = dir_client2.get_file_client("file")
-        self.assertEqual(service_client2.api_version, "2019-02-02")
-        self.assertEqual(filesys_client2.api_version, "2019-02-02")
-        self.assertEqual(dir_client2.api_version, "2019-02-02")
-        self.assertEqual(file_client2.api_version, "2019-02-02")
+        assert service_client2.api_version == "2019-02-02"
+        assert filesys_client2.api_version == "2019-02-02"
+        assert dir_client2.api_version == "2019-02-02"
+        assert file_client2.api_version == "2019-02-02"
 
 # ------------------------------------------------------------------------------
 if __name__ == '__main__':
     unittest.main()
```

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_file_system_async.py` & `azure-storage-file-datalake-12.9.1/tests/test_file_system_async.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,24 +1,21 @@
-# coding: utf-8
-
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 import asyncio
-import pytest
 import unittest
 import uuid
 from datetime import datetime, timedelta
+from time import sleep
 
+import pytest
 from azure.core import MatchConditions
 from azure.core.exceptions import HttpResponseError, ResourceNotFoundError
-from azure.core.pipeline.transport import AioHttpTransport
-
 from azure.storage.filedatalake import (
     AccessPolicy,
     AccountSasPermissions,
     DirectorySasPermissions,
     EncryptionScopeOptions,
     FileSystemSasPermissions,
     generate_account_sas,
@@ -26,56 +23,41 @@
     generate_file_sas,
     generate_file_system_sas,
     PublicAccess,
     ResourceTypes
 )
 from azure.storage.filedatalake.aio import DataLakeDirectoryClient, DataLakeFileClient, DataLakeServiceClient, FileSystemClient
 from azure.storage.filedatalake._models import FileSasPermissions
-from multidict import CIMultiDict, CIMultiDictProxy
-
-from devtools_testutils.storage.aio import AsyncStorageTestCase as StorageTestCase
 
+from devtools_testutils.aio import recorded_by_proxy_async
+from devtools_testutils.storage.aio import AsyncStorageRecordedTestCase
 from settings.testcase import DataLakePreparer
 
 # ------------------------------------------------------------------------------
 TEST_FILE_SYSTEM_PREFIX = 'filesystem'
 # ------------------------------------------------------------------------------
 
-class AiohttpTestTransport(AioHttpTransport):
-    """Workaround to vcrpy bug: https://github.com/kevin1024/vcrpy/pull/461
-    """
-
-    async def send(self, request, **config):
-        response = await super(AiohttpTestTransport, self).send(request, **config)
-        if not isinstance(response.headers, CIMultiDictProxy):
-            response.headers = CIMultiDictProxy(CIMultiDict(response.internal_response.headers))
-            response.content_type = response.headers.get("content-type")
-        return response
 
-
-class FileSystemTest(StorageTestCase):
+class TestFileSystemAsync(AsyncStorageRecordedTestCase):
     def _setUp(self, account_name, account_key):
         url = self.account_url(account_name, 'dfs')
-        self.dsc = DataLakeServiceClient(url, credential=account_key,
-                                         transport=AiohttpTestTransport(), logging_enable=True)
+        self.dsc = DataLakeServiceClient(url, credential=account_key, logging_enable=True)
         self.config = self.dsc._config
         self.test_file_systems = []
 
     def tearDown(self):
         if not self.is_playback():
             loop = asyncio.get_event_loop()
             try:
                 for file_system in self.test_file_systems:
                     loop.run_until_complete(self.dsc.delete_file_system(file_system))
                 loop.run_until_complete(self.fsc.__aexit__())
             except:
                 pass
 
-        return super(FileSystemTest, self).tearDown()
-
     # --Helpers-----------------------------------------------------------------
     def _get_file_system_reference(self, prefix=TEST_FILE_SYSTEM_PREFIX):
         file_system_name = self.get_resource_name(prefix)
         self.test_file_systems.append(file_system_name)
         return file_system_name
 
     async def _create_file_system(self, file_system_prefix=TEST_FILE_SYSTEM_PREFIX):
@@ -83,65 +65,92 @@
 
     async def _to_list(self, async_iterator):
         result = []
         async for item in async_iterator:
             result.append(item)
         return result
 
+    def _is_almost_equal(self, first, second, delta):
+        if first == second:
+            return True
+        diff = abs(first - second)
+        if delta is not None:
+            if diff <= delta:
+                return True
+        return False
+
+
     # --Test cases for file system ---------------------------------------------
+
     @DataLakePreparer()
-    async def test_create_file_system_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name)
         created = await file_system_client.create_file_system()
 
         # Assert
-        self.assertTrue(created)
+        assert created
 
     @DataLakePreparer()
-    async def test_create_file_system_async_extra_backslash(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_file_system_async_extra_backslash(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name + '/')
         created = await file_system_client.create_file_system()
 
         # Assert
-        self.assertTrue(created)
+        assert created
 
     @DataLakePreparer()
-    async def test_create_file_system_encryption_scope(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_file_system_encryption_scope(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
         encryption_scope = EncryptionScopeOptions(default_encryption_scope="hnstestscope1")
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name)
         await file_system_client.create_file_system(encryption_scope_options=encryption_scope)
         props = await file_system_client.get_file_system_properties()
 
         # Assert
-        self.assertTrue(props)
-        self.assertIsNotNone(props['encryption_scope'])
-        self.assertEqual(props['encryption_scope'].default_encryption_scope, encryption_scope.default_encryption_scope)
+        assert props
+        assert props['encryption_scope'] is not None
+        assert props['encryption_scope'].default_encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_create_file_system_encryption_scope_account_sas_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_file_system_encryption_scope_account_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
-        token = generate_account_sas(
+        token = self.generate_sas(
+            generate_account_sas,
             self.dsc.account_name,
             self.dsc.credential.account_key,
             ResourceTypes(service=True, file_system=True, object=True),
             permission=AccountSasPermissions(write=True, read=True, create=True, delete=True),
             expiry=datetime.utcnow() + timedelta(hours=5),
             encryption_scope="hnstestscope1")
         file_system_name = self._get_file_system_reference()
@@ -154,29 +163,33 @@
         fsc_sas = FileSystemClient(url, file_system_name, token)
         await fsc_sas.create_file('file1')
         await fsc_sas.create_directory('dir1')
         dir_props = await fsc_sas.get_directory_client('dir1').get_directory_properties()
         file_props = await fsc_sas.get_file_client('file1').get_file_properties()
 
         # Assert
-        self.assertTrue(dir_props)
-        self.assertIsNotNone(dir_props.encryption_scope)
-        self.assertEqual(dir_props.encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertTrue(file_props)
-        self.assertIsNotNone(file_props.encryption_scope)
-        self.assertEqual(file_props.encryption_scope, encryption_scope.default_encryption_scope)
+        assert dir_props
+        assert dir_props.encryption_scope is not None
+        assert dir_props.encryption_scope == encryption_scope.default_encryption_scope
+        assert file_props
+        assert file_props.encryption_scope is not None
+        assert file_props.encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_create_file_system_encryption_scope_file_system_sas_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_file_system_encryption_scope_file_system_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
         file_system_name = self._get_file_system_reference()
-        token = generate_file_system_sas(
+        token = self.generate_sas(
+            generate_file_system_sas,
             self.dsc.account_name,
             file_system_name,
             self.dsc.credential.account_key,
             permission=FileSystemSasPermissions(write=True, read=True, delete=True),
             expiry=datetime.utcnow() + timedelta(hours=5),
             encryption_scope="hnstestscope1")
         encryption_scope = EncryptionScopeOptions(default_encryption_scope="hnstestscope1")
@@ -188,29 +201,33 @@
         fsc_sas = FileSystemClient(url, file_system_name, token)
         await fsc_sas.create_file('file1')
         await fsc_sas.create_directory('dir1')
         dir_props = await fsc_sas.get_directory_client('dir1').get_directory_properties()
         file_props = await fsc_sas.get_file_client('file1').get_file_properties()
 
         # Assert
-        self.assertTrue(dir_props)
-        self.assertIsNotNone(dir_props.encryption_scope)
-        self.assertEqual(dir_props.encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertTrue(file_props)
-        self.assertIsNotNone(file_props.encryption_scope)
-        self.assertEqual(file_props.encryption_scope, encryption_scope.default_encryption_scope)
+        assert dir_props
+        assert dir_props.encryption_scope is not None
+        assert dir_props.encryption_scope == encryption_scope.default_encryption_scope
+        assert file_props
+        assert file_props.encryption_scope is not None
+        assert file_props.encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_create_file_system_encryption_scope_directory_sas_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_file_system_encryption_scope_directory_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
         file_system_name = self._get_file_system_reference()
-        token = generate_directory_sas(
+        token = self.generate_sas(
+            generate_directory_sas,
             self.dsc.account_name,
             file_system_name,
             'dir1',
             self.dsc.credential.account_key,
             permission=FileSasPermissions(write=True, read=True, delete=True),
             expiry=datetime.utcnow() + timedelta(hours=5),
             encryption_scope="hnstestscope1")
@@ -221,26 +238,30 @@
         await file_system_client.create_file_system(encryption_scope_options=encryption_scope)
 
         dir_client = DataLakeDirectoryClient(url, file_system_name, 'dir1', credential=token)
         await dir_client.create_directory()
         dir_props = await dir_client.get_directory_properties()
 
         # Assert
-        self.assertTrue(dir_props)
-        self.assertIsNotNone(dir_props.encryption_scope)
-        self.assertEqual(dir_props.encryption_scope, encryption_scope.default_encryption_scope)
+        assert dir_props
+        assert dir_props.encryption_scope is not None
+        assert dir_props.encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_create_file_system_encryption_scope_file_sas(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_file_system_encryption_scope_file_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         url = self.account_url(datalake_storage_account_name, 'dfs')
         file_system_name = self._get_file_system_reference()
-        token = generate_file_sas(
+        token = self.generate_sas(
+            generate_file_sas,
             self.dsc.account_name,
             file_system_name,
             'dir1',
             'file1',
             self.dsc.credential.account_key,
             permission=FileSasPermissions(write=True, read=True, delete=True),
             expiry=datetime.utcnow() + timedelta(hours=5),
@@ -253,71 +274,86 @@
         await file_system_client.create_directory('dir1')
 
         file_client = DataLakeFileClient(url, file_system_name, 'dir1/file1', token)
         await file_client.create_file()
         file_props = await file_client.get_file_properties()
 
         # Assert
-        self.assertTrue(file_props)
-        self.assertIsNotNone(file_props.encryption_scope)
-        self.assertEqual(file_props.encryption_scope, encryption_scope.default_encryption_scope)
+        assert file_props
+        assert file_props.encryption_scope is not None
+        assert file_props.encryption_scope == encryption_scope.default_encryption_scope
 
     @DataLakePreparer()
-    async def test_file_system_exists(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_file_system_exists(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client1 = self.dsc.get_file_system_client(file_system_name)
         file_system_client2 = self.dsc.get_file_system_client("nonexistentfs")
         await file_system_client1.create_file_system()
 
-        self.assertTrue(await file_system_client1.exists())
-        self.assertFalse(await file_system_client2.exists())
+        assert await file_system_client1.exists()
+        assert not await file_system_client2.exists()
 
     @DataLakePreparer()
-    async def test_create_file_system_with_metadata_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_file_system_with_metadata_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         metadata = {'hello': 'world', 'number': '42'}
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name)
         created = await file_system_client.create_file_system(metadata=metadata)
 
         # Assert
         properties = await file_system_client.get_file_system_properties()
-        self.assertTrue(created)
-        self.assertDictEqual(properties.metadata, metadata)
+        assert created
+        assert properties.metadata == metadata
 
     @DataLakePreparer()
-    async def test_list_file_systems_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_file_systems(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
         file_system = await self.dsc.create_file_system(file_system_name)
 
         # Act
         file_systems = []
         async for filesystem in self.dsc.list_file_systems():
             file_systems.append(filesystem)
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 1)
-        self.assertIsNotNone(file_systems[0])
+        assert file_systems is not None
+        assert len(file_systems) >= 1
+        assert file_systems[0] is not None
         self.assertNamedItemInContainer(file_systems, file_system.file_system_name)
-        self.assertIsNotNone(file_systems[0].has_immutability_policy)
-        self.assertIsNotNone(file_systems[0].has_legal_hold)
+        assert file_systems[0].has_immutability_policy is not None
+        assert file_systems[0].has_legal_hold is not None
 
     @DataLakePreparer()
-    async def test_list_file_systems_encryption_scope_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_file_systems_encryption_scope(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name1 = self._get_file_system_reference(prefix='es')
         file_system_name2 = self._get_file_system_reference(prefix='es2')
         encryption_scope = EncryptionScopeOptions(default_encryption_scope="hnstestscope1")
         await self.dsc.create_file_system(file_system_name1, encryption_scope_options=encryption_scope)
         await self.dsc.create_file_system(file_system_name2, encryption_scope_options=encryption_scope)
@@ -325,262 +361,308 @@
         # Act
         file_systems = []
         async for filesystem in self.dsc.list_file_systems():
             if filesystem['name'] in [file_system_name1, file_system_name2]:
                 file_systems.append(filesystem)
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertEqual(len(file_systems), 2)
-        self.assertEqual(file_systems[0].encryption_scope.default_encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertEqual(file_systems[1].encryption_scope.default_encryption_scope, encryption_scope.default_encryption_scope)
+        assert file_systems is not None
+        assert len(file_systems) == 2
+        assert file_systems[0].encryption_scope.default_encryption_scope == encryption_scope.default_encryption_scope
+        assert file_systems[1].encryption_scope.default_encryption_scope == encryption_scope.default_encryption_scope
 
-    @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_list_file_systems_account_sas(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_file_systems_account_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
         file_system = await self.dsc.create_file_system(file_system_name)
-        sas_token = generate_account_sas(
+        sas_token = self.generate_sas(
+            generate_account_sas,
             datalake_storage_account_name,
             datalake_storage_account_key,
             ResourceTypes(service=True),
             AccountSasPermissions(list=True),
             datetime.utcnow() + timedelta(hours=1),
         )
 
         # Act
         dsc = DataLakeServiceClient(self.account_url(datalake_storage_account_name, 'dfs'), credential=sas_token)
         file_systems = []
         async for filesystem in dsc.list_file_systems():
             file_systems.append(filesystem)
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 1)
-        self.assertIsNotNone(file_systems[0])
+        assert file_systems is not None
+        assert len(file_systems) >= 1
+        assert file_systems[0] is not None
         self.assertNamedItemInContainer(file_systems, file_system.file_system_name)
 
     @DataLakePreparer()
-    async def test_delete_file_system_with_existing_file_system_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_delete_file_system_with_existing_file_system_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
 
         # Act
         deleted = await file_system.delete_file_system()
 
         # Assert
-        self.assertIsNone(deleted)
+        assert deleted is None
 
+    @pytest.mark.skip(reason="Feature not yet enabled. Make sure to record this test once enabled.")
     @DataLakePreparer()
-    async def test_rename_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
-        if not self.is_playback():
-            return
+    @recorded_by_proxy_async
+    async def test_rename_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         old_name1 = self._get_file_system_reference(prefix="oldcontainer1")
         old_name2 = self._get_file_system_reference(prefix="oldcontainer2")
         new_name = self._get_file_system_reference(prefix="newcontainer")
         filesystem1 = await self.dsc.create_file_system(old_name1)
         await self.dsc.create_file_system(old_name2)
 
         new_filesystem = await self.dsc._rename_file_system(name=old_name1, new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await self.dsc._rename_file_system(name=old_name2, new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await filesystem1.get_file_system_properties()
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await self.dsc._rename_file_system(name="badfilesystem", new_name="filesystem")
         props = await new_filesystem.get_file_system_properties()
-        self.assertEqual(new_name, props.name)
+        assert new_name == props.name
 
-    @pytest.mark.skip(reason="Feature not yet enabled. Record when enabled.")
+    @pytest.mark.skip(reason="Feature not yet enabled. Make sure to record this test once enabled.")
     @DataLakePreparer()
-    async def test_rename_file_system_with_file_system_client(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_rename_file_system_with_file_system_client(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         old_name1 = self._get_file_system_reference(prefix="oldcontainer1")
         old_name2 = self._get_file_system_reference(prefix="oldcontainer2")
         new_name = self._get_file_system_reference(prefix="newcontainer")
         bad_name = self._get_file_system_reference(prefix="badcontainer")
         filesystem1 = await self.dsc.create_file_system(old_name1)
         file_system2 = await self.dsc.create_file_system(old_name2)
         bad_file_system = self.dsc.get_file_system_client(bad_name)
 
         new_filesystem = await filesystem1._rename_file_system(new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await file_system2._rename_file_system(new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await filesystem1.get_file_system_properties()
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await bad_file_system._rename_file_system(new_name="filesystem")
         new_file_system_props = await new_filesystem.get_file_system_properties()
-        self.assertEqual(new_name, new_file_system_props.name)
+        assert new_name == new_file_system_props.name
 
+    @pytest.mark.skip(reason="Feature not yet enabled. Make sure to record this test once enabled.")
     @DataLakePreparer()
-    async def test_rename_file_system_with_source_lease(
-            self, datalake_storage_account_name, datalake_storage_account_key):
-        if not self.is_playback():
-            return
+    @recorded_by_proxy_async
+    async def test_rename_file_system_with_source_lease(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         old_name = self._get_file_system_reference(prefix="old")
         new_name = self._get_file_system_reference(prefix="new")
         filesystem = await self.dsc.create_file_system(old_name)
         filesystem_lease_id = await filesystem.acquire_lease()
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await self.dsc._rename_file_system(name=old_name, new_name=new_name)
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await self.dsc._rename_file_system(name=old_name, new_name=new_name, lease="bad_id")
         new_filesystem = await self.dsc._rename_file_system(name=old_name, new_name=new_name, lease=filesystem_lease_id)
         props = await new_filesystem.get_file_system_properties()
-        self.assertEqual(new_name, props.name)
+        assert new_name == props.name
 
     @DataLakePreparer()
-    async def test_undelete_file_system(self, datalake_storage_account_name, datalake_storage_account_key):
-        # TODO: Needs soft delete enabled account in ARM template.
-        if not self.is_playback():
-            return
+    @recorded_by_proxy_async
+    async def test_undelete_file_system(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        name = self._get_file_system_reference(prefix="filesystem")
+        name = self._get_file_system_reference(prefix="filesystem2")
         filesystem_client = await self.dsc.create_file_system(name)
 
         await filesystem_client.delete_file_system()
+        if self.is_live:
+            sleep(30)
         # to make sure the filesystem deleted
-        with self.assertRaises(ResourceNotFoundError):
+        with pytest.raises(ResourceNotFoundError):
             await filesystem_client.get_file_system_properties()
 
         filesystem_list = []
         async for fs in self.dsc.list_file_systems(include_deleted=True):
             filesystem_list.append(fs)
-        self.assertTrue(len(filesystem_list) >= 1)
+        assert len(filesystem_list) >= 1
 
         for filesystem in filesystem_list:
             # find the deleted filesystem and restore it
             if filesystem.deleted and filesystem.name == filesystem_client.file_system_name:
                 restored_fs_client = await self.dsc.undelete_file_system(filesystem.name, filesystem.deleted_version)
 
                 # to make sure the deleted filesystem is restored
                 props = await restored_fs_client.get_file_system_properties()
-                self.assertIsNotNone(props)
+                assert props is not None
 
-    @pytest.mark.skip(reason="We are generating a SAS token therefore play only live but we also need a soft delete enabled account.")
     @DataLakePreparer()
-    async def test_restore_file_system_with_sas(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_restore_file_system_with_sas(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        token = generate_account_sas(
+        token = self.generate_sas(
+            generate_account_sas,
             self.dsc.account_name,
             self.dsc.credential.account_key,
             ResourceTypes(service=True, file_system=True),
             AccountSasPermissions(read=True, write=True, list=True, delete=True),
             datetime.utcnow() + timedelta(hours=1),
         )
         dsc = DataLakeServiceClient(self.dsc.url, token)
-        name = self._get_file_system_reference(prefix="filesystem")
+        name = self._get_file_system_reference(prefix="filesystem2")
         filesystem_client = await dsc.create_file_system(name)
         await filesystem_client.delete_file_system()
+        if self.is_live:
+            sleep(30)
         # to make sure the filesystem is deleted
-        with self.assertRaises(ResourceNotFoundError):
+        with pytest.raises(ResourceNotFoundError):
             await filesystem_client.get_file_system_properties()
 
         filesystem_list = []
         async for fs in self.dsc.list_file_systems(include_deleted=True):
             filesystem_list.append(fs)
-        self.assertTrue(len(filesystem_list) >= 1)
+        assert len(filesystem_list) >= 1
 
         for filesystem in filesystem_list:
             # find the deleted filesystem and restore it
             if filesystem.deleted and filesystem.name == filesystem_client.file_system_name:
                 restored_fs_client = await dsc.undelete_file_system(filesystem.name, filesystem.deleted_version)
 
                 # to make sure the deleted filesystem is restored
                 props = await restored_fs_client.get_file_system_properties()
-                self.assertIsNotNone(props)
+                assert props is not None
 
     @DataLakePreparer()
-    async def test_delete_none_existing_file_system_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_delete_none_existing_file_system_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         fake_file_system_client = self.dsc.get_file_system_client("fakeclient")
 
         # Act
-        with self.assertRaises(ResourceNotFoundError):
+        with pytest.raises(ResourceNotFoundError):
             await fake_file_system_client.delete_file_system(match_condition=MatchConditions.IfMissing)
 
     @DataLakePreparer()
-    async def test_list_file_systems_with_include_metadata_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_file_systems_with_include_metadata_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         metadata = {'hello': 'world', 'number': '42'}
         await file_system.set_file_system_metadata(metadata)
 
         # Act
         file_systems = []
         async for fs in self.dsc.list_file_systems(name_starts_with=file_system.file_system_name,
                                                    include_metadata=True):
             file_systems.append(fs)
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 1)
-        self.assertIsNotNone(file_systems[0])
+        assert file_systems is not None
+        assert len(file_systems) >= 1
+        assert file_systems[0] is not None
         self.assertNamedItemInContainer(file_systems, file_system.file_system_name)
-        self.assertDictEqual(file_systems[0].metadata, metadata)
+        assert file_systems[0].metadata == metadata
 
     @DataLakePreparer()
-    async def test_set_file_system_acl_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_set_file_system_acl(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+        variables = kwargs.pop('variables', {})
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Act
         file_system = await self._create_file_system()
+        expiry_time = self.get_datetime_variable(variables, 'expiry_time', datetime.utcnow() + timedelta(hours=1))
+        start_time = self.get_datetime_variable(variables, 'start_time', datetime.utcnow())
         access_policy = AccessPolicy(permission=FileSystemSasPermissions(read=True),
-                                     expiry=datetime.utcnow() + timedelta(hours=1),
-                                     start=datetime.utcnow())
+                                     expiry=expiry_time,
+                                     start=start_time)
         signed_identifier1 = {'testid': access_policy}
         response = await file_system.set_file_system_access_policy(
             signed_identifier1, public_access=PublicAccess.FileSystem)
 
-        self.assertIsNotNone(response.get('etag'))
-        self.assertIsNotNone(response.get('last_modified'))
+        assert response.get('etag') is not None
+        assert response.get('last_modified') is not None
 
         acl1 = await file_system.get_file_system_access_policy()
-        self.assertIsNotNone(acl1['public_access'])
-        self.assertEqual(len(acl1['signed_identifiers']), 1)
+        assert acl1['public_access'] is not None
+        assert len(acl1['signed_identifiers']) == 1
 
         # If set signed identifier without specifying the access policy then it will be default to None
         signed_identifier2 = {'testid': access_policy, 'test2': access_policy}
         await file_system.set_file_system_access_policy(signed_identifier2)
         acl2 = await file_system.get_file_system_access_policy()
-        self.assertIsNone(acl2['public_access'])
-        self.assertEqual(len(acl2['signed_identifiers']), 2)
+        assert acl2['public_access'] is None
+        assert len(acl2['signed_identifiers']) == 2
+
+        return variables
 
     @DataLakePreparer()
-    async def test_list_file_systems_by_page_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_file_systems_by_page(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         for i in range(0, 6):
             await self._create_file_system(file_system_prefix="filesystem{}".format(i))
 
         # Act
         file_systems = []
         async for fs in await self.dsc.list_file_systems(
                 results_per_page=3,
                 name_starts_with="file",
                 include_metadata=True).by_page().__anext__():
             file_systems.append(fs)
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 3)
+        assert file_systems is not None
+        assert len(file_systems) >= 3
 
     @DataLakePreparer()
-    async def test_list_file_systems_with_public_access_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_file_systems_with_public_access_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_name = self._get_file_system_reference()
         file_system = self.dsc.get_file_system_client(file_system_name)
         await file_system.create_file_system(public_access="blob")
         metadata = {'hello': 'world', 'number': '42'}
         await file_system.set_file_system_metadata(metadata)
@@ -588,137 +670,169 @@
         # Act
         file_systems = []
         async for fs in self.dsc.list_file_systems(name_starts_with=file_system.file_system_name,
                                                    include_metadata=True):
             file_systems.append(fs)
 
         # Assert
-        self.assertIsNotNone(file_systems)
-        self.assertGreaterEqual(len(file_systems), 1)
-        self.assertIsNotNone(file_systems[0])
+        assert file_systems is not None
+        assert len(file_systems) >= 1
+        assert file_systems[0] is not None
         self.assertNamedItemInContainer(file_systems, file_system.file_system_name)
-        self.assertDictEqual(file_systems[0].metadata, metadata)
-        self.assertTrue(file_systems[0].public_access is PublicAccess.File)
+        assert file_systems[0].metadata == metadata
+        assert file_systems[0].public_access is PublicAccess.File
 
     @DataLakePreparer()
-    async def test_get_file_system_properties_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_get_file_system_properties(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         metadata = {'hello': 'world', 'number': '42'}
         file_system = await self._create_file_system()
         await file_system.set_file_system_metadata(metadata)
 
         # Act
         props = await file_system.get_file_system_properties()
 
         # Assert
-        self.assertIsNotNone(props)
-        self.assertDictEqual(props.metadata, metadata)
-        self.assertIsNotNone(props.has_immutability_policy)
-        self.assertIsNotNone(props.has_legal_hold)
+        assert props is not None
+        assert props.metadata == metadata
+        assert props.has_immutability_policy is not None
+        assert props.has_legal_hold is not None
 
     @DataLakePreparer()
-    async def test_service_client_session_closes_after_filesystem_creation(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_service_client_session_closes_after_filesystem_creation(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         dsc2 = DataLakeServiceClient(self.dsc.url, credential=datalake_storage_account_key)
         async with DataLakeServiceClient(
                 self.dsc.url, credential=datalake_storage_account_key) as ds_client:
             fs1 = await ds_client.create_file_system(self._get_file_system_reference(prefix="fs1"))
             await fs1.delete_file_system()
         await dsc2.create_file_system(self._get_file_system_reference(prefix="fs2"))
         await dsc2.close()
 
     @DataLakePreparer()
-    async def test_list_paths_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_paths(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         for i in range(0, 6):
             await file_system.create_directory("dir1{}".format(i))
 
         paths = []
         async for path in file_system.get_paths(upn=True):
             paths.append(path)
 
-        self.assertEqual(len(paths), 6)
+        assert len(paths) == 6
 
     @DataLakePreparer()
-    async def test_list_paths_create_expiry(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_paths_create_expiry(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+        variables = kwargs.pop('variables', {})
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         file_client = await file_system.create_file('file1')
 
-        expires_on = datetime.utcnow() + timedelta(days=1)
-        await file_client.set_file_expiry("Absolute", expires_on=expires_on)
+        expiry_time = self.get_datetime_variable(variables, 'expiry_time', datetime.utcnow() + timedelta(days=1))
+        await file_client.set_file_expiry("Absolute", expires_on=expiry_time)
 
         # Act
         paths = []
         async for path in file_system.get_paths(upn=True):
             paths.append(path)
 
         # Assert
-        self.assertEqual(1, len(paths))
+        assert 1 == len(paths)
         props = await file_client.get_file_properties()
         # Properties do not include microseconds so let them vary by 1 second
-        self.assertAlmostEqual(props.creation_time, paths[0].creation_time, delta=timedelta(seconds=1))
-        self.assertAlmostEqual(props.expiry_time, paths[0].expiry_time, delta=timedelta(seconds=1))
+        self._is_almost_equal(props.creation_time, paths[0].creation_time, delta=timedelta(seconds=1))
+        self._is_almost_equal(props.expiry_time, paths[0].expiry_time, delta=timedelta(seconds=1))
+
+        return variables
 
     @DataLakePreparer()
-    async def test_list_paths_no_expiry(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_paths_no_expiry(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         await file_system.create_file('file1')
 
         # Act
         paths = []
         async for path in file_system.get_paths(upn=True):
             paths.append(path)
 
         # Assert
-        self.assertEqual(1, len(paths))
-        self.assertIsNone(paths[0].expiry_time)
+        assert 1 == len(paths)
+        assert paths[0].expiry_time is None
 
     @DataLakePreparer()
-    async def test_list_paths_which_are_all_files_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_paths_which_are_all_files_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         for i in range(0, 6):
             await file_system.create_file("file{}".format(i))
 
         paths = []
         async for path in file_system.get_paths(upn=True):
             paths.append(path)
 
-        self.assertEqual(len(paths), 6)
+        assert len(paths) == 6
 
     @DataLakePreparer()
-    async def test_list_system_filesystems_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_system_filesystems(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         dsc = DataLakeServiceClient(self.dsc.url, credential=datalake_storage_account_key)
         # Act
         filesystems = []
         async for fs in dsc.list_file_systems(include_system=True):
             filesystems.append(fs)
         # Assert
         found = False
         for fs in filesystems:
             if fs.name == "$logs":
                 found = True
-        self.assertEqual(found, True)
+        assert found == True
 
     @DataLakePreparer()
-    async def test_list_paths_with_max_per_page_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_paths_with_max_per_page_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         for i in range(0, 6):
             await file_system.create_directory("dir1{}".format(i))
 
         generator1 = file_system.get_paths(max_results=2, upn=True).by_page()
@@ -728,46 +842,53 @@
 
         generator2 = file_system.get_paths(max_results=4, upn=True) \
             .by_page(continuation_token=generator1.continuation_token)
         paths2 = []
         async for path in await generator2.__anext__():
             paths2.append(path)
 
-        self.assertEqual(len(paths1), 2)
-        self.assertEqual(len(paths2), 4)
-        self.assertEqual(paths2[0].name, "dir12")
+        assert len(paths1) == 2
+        assert len(paths2) == 4
+        assert paths2[0].name == "dir12"
 
     @DataLakePreparer()
-    async def test_list_paths_under_specific_path_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_paths_under_specific_path_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         for i in range(0, 6):
             dir = await file_system.create_directory("dir1{}".format(i))
 
             # create a subdirectory under the current directory
             subdir = await dir.create_sub_directory("subdir")
             await subdir.create_sub_directory("subsub")
 
             # create a file under the current directory
             file_client = await subdir.create_file("file")
-            await file_client.append_data(b"abced", 0, 5)
+            await file_client.append_data(b"abced", 0, 5) # cspell:disable-line
             await file_client.flush_data(5)
 
         generator1 = file_system.get_paths(path="dir10/subdir", max_results=2, upn=True).by_page()
         paths = []
         async for path in await generator1.__anext__():
             paths.append(path)
 
-        self.assertEqual(len(paths), 2)
-        self.assertEqual(paths[0].content_length, 5)
+        assert len(paths) == 2
+        assert paths[0].content_length == 5
 
     @DataLakePreparer()
-    async def test_list_paths_recursively_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_paths_recursively(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         for i in range(0, 6):
             dir = await file_system.create_directory("dir1{}".format(i))
 
             # create a subdirectory under the current directory
@@ -778,18 +899,22 @@
             await subdir.create_file("file")
 
         paths = []
         async for path in file_system.get_paths(recursive=True, upn=True):
             paths.append(path)
 
         # there are 24 subpaths in total
-        self.assertEqual(len(paths), 24)
+        assert len(paths) == 24
 
     @DataLakePreparer()
-    async def test_list_paths_pages_correctly(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_list_paths_pages_correctly(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system(file_system_prefix="filesystem1")
         for i in range(0, 6):
             await file_system.create_directory("dir1{}".format(i))
         for i in range(0, 6):
             await file_system.create_file("file{}".format(i))
@@ -798,27 +923,28 @@
         paths1 = []
         async for path in await generator.__anext__():
             paths1.append(path)
         paths2 = []
         async for path in await generator.__anext__():
             paths2.append(path)
 
-        with self.assertRaises(StopAsyncIteration):
+        with pytest.raises(StopAsyncIteration):
             paths3 = []
             async for path in await generator.__anext__():
                 paths3.append(path)
 
-        self.assertEqual(len(paths1), 6)
-        self.assertEqual(len(paths2), 6)
+        assert len(paths1) == 6
+        assert len(paths2) == 6
 
     @DataLakePreparer()
-    async def test_get_deleted_paths(self, datalake_storage_account_name, datalake_storage_account_key):
-        # TODO: Needs soft delete enabled account in ARM template.
-        if not self.is_playback():
-            return
+    @recorded_by_proxy_async
+    async def test_get_deleted_paths(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         file0 = await file_system.create_file("file0")
         file1 = await file_system.create_file("file1")
 
         dir1 = await file_system.create_directory("dir1")
@@ -837,38 +963,42 @@
         async for path in file_system.list_deleted_paths():
             deleted_paths.append(path)
         dir3_paths = []
         async for path in file_system.list_deleted_paths(path_prefix="dir3/"):
             dir3_paths.append(path)
 
         # Assert
-        self.assertEqual(len(deleted_paths), 6)
-        self.assertEqual(len(dir3_paths), 2)
-        self.assertIsNotNone(dir3_paths[0].deletion_id)
-        self.assertIsNotNone(dir3_paths[1].deletion_id)
-        self.assertEqual(dir3_paths[0].name, 'dir3/file_in_dir3')
-        self.assertEqual(dir3_paths[1].name, 'dir3/subdir/file_in_subdir')
+        assert len(deleted_paths) == 6
+        assert len(dir3_paths) == 2
+        assert dir3_paths[0].deletion_id is not None
+        assert dir3_paths[1].deletion_id is not None
+        assert dir3_paths[0].name == 'dir3/file_in_dir3'
+        assert dir3_paths[1].name == 'dir3/subdir/file_in_subdir'
 
         paths_generator1 = file_system.list_deleted_paths(results_per_page=2).by_page()
         paths1 = []
         async for path in await paths_generator1.__anext__():
             paths1.append(path)
 
         paths_generator2 = file_system.list_deleted_paths(results_per_page=4) \
             .by_page(continuation_token=paths_generator1.continuation_token)
         paths2 = []
         async for path in await paths_generator2.__anext__():
             paths2.append(path)
 
         # Assert
-        self.assertEqual(len(paths1), 2)
-        self.assertEqual(len(paths2), 4)
+        assert len(paths1) == 2
+        assert len(paths2) == 4
 
     @DataLakePreparer()
-    async def test_path_properties_encryption_scope_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_path_properties_encryption_scope(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         encryption_scope = EncryptionScopeOptions(default_encryption_scope="hnstestscope1")
         file_system_name = self._get_file_system_reference()
 
         # Act
         file_system_client = self.dsc.get_file_system_client(file_system_name)
@@ -879,66 +1009,78 @@
         dir_props = await file_system_client.get_directory_client('dir1').get_directory_properties()
         file_props = await file_system_client.get_file_client('dir1/file1').get_file_properties()
         paths = []
         async for path in file_system_client.get_paths(recursive=True, upn=True):
             paths.append(path)
 
         # Assert
-        self.assertEqual(dir_props.encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertEqual(file_props.encryption_scope, encryption_scope.default_encryption_scope)
-        self.assertTrue(paths)
-        self.assertIsNotNone(paths[0].encryption_scope)
-        self.assertEqual(paths[0].encryption_scope, encryption_scope.default_encryption_scope)
+        assert dir_props.encryption_scope == encryption_scope.default_encryption_scope
+        assert file_props.encryption_scope == encryption_scope.default_encryption_scope
+        assert paths
+        assert paths[0].encryption_scope is not None
+        assert paths[0].encryption_scope == encryption_scope.default_encryption_scope
 
     @DataLakePreparer()
-    async def test_create_directory_from_file_system_client_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_directory_from_file_system_client_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         await file_system.create_directory("dir1/dir2")
 
         paths = []
         async for path in file_system.get_paths(recursive=False, upn=True):
             paths.append(path)
 
-        self.assertEqual(len(paths), 1)
-        self.assertEqual(paths[0].name, "dir1")
+        assert len(paths) == 1
+        assert paths[0].name == "dir1"
 
     @DataLakePreparer()
-    async def test_create_file_from_file_system_client_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_create_file_from_file_system_client_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system = await self._create_file_system()
         await file_system.create_file("dir1/dir2/file")
 
         paths = []
         async for path in file_system.get_paths(recursive=True, upn=True):
             paths.append(path)
-        self.assertEqual(len(paths), 3)
-        self.assertEqual(paths[0].name, "dir1")
-        self.assertEqual(paths[2].is_directory, False)
+        assert len(paths) == 3
+        assert paths[0].name == "dir1"
+        assert paths[2].is_directory == False
 
     @DataLakePreparer()
-    async def test_get_root_directory_client_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_get_root_directory_client(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         file_system = await self._create_file_system()
         directory_client = file_system._get_root_directory_client()
 
         acl = 'user::rwx,group::r-x,other::rwx'
         await directory_client.set_access_control(acl=acl)
         access_control = await directory_client.get_access_control()
 
-        self.assertEqual(acl, access_control['acl'])
+        assert acl == access_control['acl']
 
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_get_access_control_using_delegation_sas_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_get_access_control_using_delegation_sas_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
 
         url = self.account_url(datalake_storage_account_name, 'dfs')
         token_credential = self.generate_oauth_token()
         dsc = DataLakeServiceClient(url, token_credential, logging_enable=True)
         file_system_name = self._get_file_system_reference()
         directory_client_name = '/'
@@ -949,107 +1091,116 @@
         await directory_client.set_access_control(owner=random_guid,
                                                   permissions='0777')
         acl = await directory_client.get_access_control()
 
         delegation_key = await dsc.get_user_delegation_key(datetime.utcnow(),
                                                            datetime.utcnow() + timedelta(hours=1))
 
-        token = generate_file_system_sas(
+        token = self.generate_sas(
+            generate_file_system_sas,
             dsc.account_name,
             file_system_name,
             delegation_key,
             permission=FileSystemSasPermissions(
                 read=True, execute=True, manage_access_control=True, manage_ownership=True),
             expiry=datetime.utcnow() + timedelta(hours=1),
             agent_object_id=random_guid
         )
         sas_directory_client = DataLakeDirectoryClient(self.dsc.url, file_system_name, directory_client_name,
                                                        credential=token, logging_enable=True)
         access_control = await sas_directory_client.get_access_control()
 
-        self.assertIsNotNone(access_control)
+        assert access_control is not None
 
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_list_paths_using_file_sys_delegation_sas_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_list_paths_using_file_sys_delegation_sas_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         url = self.account_url(datalake_storage_account_name, 'dfs')
         token_credential = self.generate_oauth_token()
         dsc = DataLakeServiceClient(url, token_credential)
         file_system_name = self._get_file_system_reference()
         directory_client_name = '/'
         directory_client = (await dsc.create_file_system(file_system_name)).get_directory_client(directory_client_name)
 
         random_guid = uuid.uuid4()
         await directory_client.set_access_control(owner=random_guid, permissions='0777')
 
         delegation_key = await dsc.get_user_delegation_key(datetime.utcnow(),
                                                            datetime.utcnow() + timedelta(hours=1))
 
-        token = generate_file_system_sas(
+        token = self.generate_sas(
+            generate_file_system_sas,
             dsc.account_name,
             file_system_name,
             delegation_key,
             permission=DirectorySasPermissions(list=True),
             expiry=datetime.utcnow() + timedelta(hours=1),
             agent_object_id=random_guid
         )
         sas_directory_client = FileSystemClient(self.dsc.url, file_system_name,
                                                 credential=token)
         paths = list()
         async for path in sas_directory_client.get_paths():
             paths.append(path)
 
-        self.assertEqual(0, 0)
+        assert 0 == 0
 
     @DataLakePreparer()
-    async def test_file_system_sessions_closes_properly_async(
-            self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy_async
+    async def test_file_system_sessions_closes_properly_async(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_system_client = await self._create_file_system("fs")
         async with file_system_client as fs_client:
             async with fs_client.get_file_client("file1.txt") as f_client:
                 await f_client.create_file()
             async with fs_client.get_file_client("file2.txt") as f_client:
                 await f_client.create_file()
             async with fs_client.get_directory_client("file1") as f_client:
                 await f_client.create_directory()
             async with fs_client.get_directory_client("file2") as f_client:
                 await f_client.create_directory()
 
     @DataLakePreparer()
-    async def test_undelete_dir_with_version_id(self, datalake_storage_account_name, datalake_storage_account_key):
-        # TODO: Needs soft delete enabled account in ARM template.
-        if not self.is_playback():
-            return
+    @recorded_by_proxy_async
+    async def test_undelete_dir_with_version_id(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        file_system_client = await self._create_file_system("fs")
+        file_system_client = await self._create_file_system("fs2")
         dir_path = 'dir10'
         dir_client = await file_system_client.create_directory(dir_path)
         resp = await dir_client.delete_directory()
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await file_system_client.get_file_client(dir_path).get_file_properties()
         restored_dir_client = await file_system_client._undelete_path(dir_path, resp['deletion_id'])
         resp = await restored_dir_client.get_directory_properties()
-        self.assertIsNotNone(resp)
+        assert resp is not None
 
     @DataLakePreparer()
-    async def test_undelete_file_with_version_id(self, datalake_storage_account_name, datalake_storage_account_key):
-        # TODO: Needs soft delete enabled account in ARM template.
-        if not self.is_playback():
-            return
+    @recorded_by_proxy_async
+    async def test_undelete_file_with_version_id(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("storage_data_lake_soft_delete_account_name")
+        datalake_storage_account_key = kwargs.pop("storage_data_lake_soft_delete_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
-        file_system_client = await self._create_file_system("fs")
+        file_system_client = await self._create_file_system("fs2")
         file_path = 'dir10/fileŇ'
         dir_client = await file_system_client.create_file(file_path)
         resp = await dir_client.delete_file()
-        with self.assertRaises(HttpResponseError):
+        with pytest.raises(HttpResponseError):
             await file_system_client.get_file_client(file_path).get_file_properties()
         restored_file_client = await file_system_client._undelete_path(file_path, resp['deletion_id'])
         resp = await restored_file_client.get_file_properties()
-        self.assertIsNotNone(resp)
+        assert resp is not None
 
 # ------------------------------------------------------------------------------
 if __name__ == '__main__':
     unittest.main()
```

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_quick_query.py` & `azure-storage-file-datalake-12.9.1/tests/test_quick_query.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,26 +1,27 @@
-# coding: utf-8
-
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 import base64
 import os
 
 import pytest
-
 from azure.storage.filedatalake import (
-    DelimitedTextDialect,
+    ArrowDialect,
+    ArrowType,
     DelimitedJsonDialect,
-    ArrowDialect, ArrowType, QuickQueryDialect)
+    DelimitedTextDialect,
+    QuickQueryDialect
+)
 
+from devtools_testutils import recorded_by_proxy
+from devtools_testutils.storage import StorageRecordedTestCase
 from settings.testcase import DataLakePreparer
-from devtools_testutils.storage import StorageTestCase
 # ------------------------------------------------------------------------------
 from azure.storage.filedatalake import DataLakeServiceClient
 
 CSV_DATA = b'Service,Package,Version,RepoPath,MissingDocs\r\nApp Configuration,' \
            b'azure-data-appconfiguration,1,appconfiguration,FALSE\r\nEvent Hubs' \
            b'\r\nEvent Hubs - Azure Storage CheckpointStore,' \
            b'azure-messaging-eventhubs-checkpointstore-blob,1.0.1,eventhubs,FALSE\r\nIdentity,azure-identity,' \
@@ -108,45 +109,49 @@
                      b"'1.0.0-beta.2';core;FALSE.Service;Package;Version;RepoPath;MissingDocs.App Configuration;" \
                      b"azure-data-appconfiguration;'1.0.1';appconfiguration;FALSE.Event Hubs;azure-messaging-eventhubs;" \
                      b"'5.0.1';eventhubs;FALSE."
 
 # ------------------------------------------------------------------------------
 
 
-class StorageQuickQueryTest(StorageTestCase):
+class TestStorageQuickQuery(StorageRecordedTestCase):
     def _setUp(self, account_name, account_key):
         url = self.account_url(account_name, 'dfs')
         self.dsc = DataLakeServiceClient(url, credential=account_key, logging_enable=True)
         self.config = self.dsc._config
-        self.filesystem_name = self.get_resource_name('utqqcontainer')
+        self.filesystem_name = self.get_resource_name('utqqcontainer') # cspell:disable-line
 
         if not self.is_playback():
             try:
                 self.dsc.create_file_system(self.filesystem_name)
             except:
                 pass
 
     def tearDown(self):
         if not self.is_playback():
             try:
                 self.dsc.delete_file_system(self.filesystem_name)
             except:
                 pass
 
-        return super(StorageQuickQueryTest, self).tearDown()
+        return super(TestStorageQuickQuery, self).tearDown()
 
     # --Helpers-----------------------------------------------------------------
 
     def _get_file_reference(self):
         return self.get_resource_name("csvfile")
 
     # -- Test cases for APIs supporting CPK ----------------------------------------------
 
     @DataLakePreparer()
-    def test_quick_query_readall(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_readall(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
@@ -154,21 +159,25 @@
 
         def on_error(error):
             errors.append(error)
 
         reader = file_client.query_file("SELECT * from BlobStorage", on_error=on_error)
         data = reader.readall()
 
-        self.assertEqual(len(errors), 0)
-        self.assertEqual(len(reader), len(CSV_DATA))
-        self.assertEqual(len(reader), reader._blob_query_reader._bytes_processed)
-        self.assertEqual(data, CSV_DATA.replace(b'\r\n', b'\n'))
+        assert len(errors) == 0
+        assert len(reader) == len(CSV_DATA)
+        assert len(reader) == reader._blob_query_reader._bytes_processed
+        assert data, CSV_DATA.replace(b'\r\n' == b'\n')
 
     @DataLakePreparer()
-    def test_quick_query_datalake_expression(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_datalake_expression(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(DATALAKE_CSV_DATA, overwrite=True)
 
@@ -178,43 +187,51 @@
             errors.append(error)
 
         input_format = DelimitedTextDialect(has_header=True)
         reader = file_client.query_file("SELECT DataLakeStorage from DataLakeStorage", on_error=on_error,
                                         file_format=input_format)
         reader.readall()
 
-        self.assertEqual(len(errors), 0)
-        self.assertEqual(len(reader), len(DATALAKE_CSV_DATA))
-        self.assertEqual(len(reader), reader._blob_query_reader._bytes_processed)
+        assert len(errors) == 0
+        assert len(reader) == len(DATALAKE_CSV_DATA)
+        assert len(reader) == reader._blob_query_reader._bytes_processed
 
     @DataLakePreparer()
-    def test_quick_query_iter_records(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
         reader = file_client.query_file("SELECT * from BlobStorage")
         read_records = reader.records()
 
         # Assert first line has header
         data = next(read_records)
-        self.assertEqual(data, b'Service,Package,Version,RepoPath,MissingDocs')
+        assert data == b'Service,Package,Version,RepoPath,MissingDocs'
 
         for record in read_records:
             data += record
 
-        self.assertEqual(len(reader), len(CSV_DATA))
-        self.assertEqual(len(reader), reader._blob_query_reader._bytes_processed)
-        self.assertEqual(data, CSV_DATA.replace(b'\r\n', b''))
+        assert len(reader) == len(CSV_DATA)
+        assert len(reader) == reader._blob_query_reader._bytes_processed
+        assert data, CSV_DATA.replace(b'\r\n' == b'')
 
     @DataLakePreparer()
-    def test_quick_query_readall_with_encoding(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_readall_with_encoding(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
@@ -222,109 +239,129 @@
 
         def on_error(error):
             errors.append(error)
 
         reader = file_client.query_file("SELECT * from BlobStorage", on_error=on_error, encoding='utf-8')
         data = reader.readall()
 
-        self.assertEqual(len(errors), 0)
-        self.assertEqual(len(reader), len(CSV_DATA))
-        self.assertEqual(len(reader), reader._blob_query_reader._bytes_processed)
-        self.assertEqual(data, CSV_DATA.replace(b'\r\n', b'\n').decode('utf-8'))
+        assert len(errors) == 0
+        assert len(reader) == len(CSV_DATA)
+        assert len(reader) == reader._blob_query_reader._bytes_processed
+        assert data, CSV_DATA.replace(b'\r\n' == b'\n').decode('utf-8')
 
     @DataLakePreparer()
-    def test_quick_query_iter_records_with_encoding(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records_with_encoding(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
         reader = file_client.query_file("SELECT * from BlobStorage", encoding='utf-8')
         data = ''
         for record in reader.records():
             data += record
 
-        self.assertEqual(len(reader), len(CSV_DATA))
-        self.assertEqual(len(reader), reader._blob_query_reader._bytes_processed)
-        self.assertEqual(data, CSV_DATA.replace(b'\r\n', b'').decode('utf-8'))
+        assert len(reader) == len(CSV_DATA)
+        assert len(reader) == reader._blob_query_reader._bytes_processed
+        assert data, CSV_DATA.replace(b'\r\n' == b'').decode('utf-8')
 
     @DataLakePreparer()
-    def test_quick_query_iter_output_records_excluding_headers(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_output_records_excluding_headers(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
         input_format = DelimitedTextDialect(has_header=True)
         output_format = DelimitedTextDialect(has_header=False)
         reader = file_client.query_file("SELECT * from BlobStorage", file_format=input_format, output_format=output_format)
         read_records = reader.records()
 
         # Assert first line does not include header
         data = next(read_records)
-        self.assertEqual(data, b'App Configuration,azure-data-appconfiguration,1,appconfiguration,FALSE')
+        assert data == b'App Configuration,azure-data-appconfiguration,1,appconfiguration,FALSE'
 
         for record in read_records:
             data += record
 
-        self.assertEqual(len(reader), len(CSV_DATA))
-        self.assertEqual(len(reader), reader._blob_query_reader._bytes_processed)
-        self.assertEqual(data, CSV_DATA.replace(b'\r\n', b'')[44:])
+        assert len(reader) == len(CSV_DATA)
+        assert len(reader) == reader._blob_query_reader._bytes_processed
+        assert data, CSV_DATA.replace(b'\r\n' == b'')[44:]
 
     @DataLakePreparer()
-    def test_quick_query_iter_output_records_including_headers(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_output_records_including_headers(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
         input_format = DelimitedTextDialect(has_header=True)
         reader = file_client.query_file("SELECT * from BlobStorage", file_format=input_format)
         read_records = reader.records()
 
         # Assert first line does not include header
         data = next(read_records)
-        self.assertEqual(data, b'Service,Package,Version,RepoPath,MissingDocs')
+        assert data == b'Service,Package,Version,RepoPath,MissingDocs'
 
         for record in read_records:
             data += record
 
-        self.assertEqual(len(reader), len(CSV_DATA))
-        self.assertEqual(len(reader), reader._blob_query_reader._bytes_processed)
-        self.assertEqual(data, CSV_DATA.replace(b'\r\n', b''))
+        assert len(reader) == len(CSV_DATA)
+        assert len(reader) == reader._blob_query_reader._bytes_processed
+        assert data, CSV_DATA.replace(b'\r\n' == b'')
 
     @DataLakePreparer()
-    def test_quick_query_iter_records_with_progress(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records_with_progress(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
         reader = file_client.query_file("SELECT * from BlobStorage")
         data = b''
         progress = 0
         for record in reader.records():
             if record:
                 data += record
                 progress += len(record) + 2
-        self.assertEqual(len(reader), len(CSV_DATA))
-        self.assertEqual(len(reader), reader._blob_query_reader._bytes_processed)
-        self.assertEqual(data, CSV_DATA.replace(b'\r\n', b''))
-        self.assertEqual(progress, len(reader))
+        assert len(reader) == len(CSV_DATA)
+        assert len(reader) == reader._blob_query_reader._bytes_processed
+        assert data, CSV_DATA.replace(b'\r\n' == b'')
+        assert progress == len(reader)
 
     @DataLakePreparer()
-    def test_quick_query_readall_with_serialization_setting(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_readall_with_serialization_setting(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
@@ -349,20 +386,24 @@
         resp = file_client.query_file(
             "SELECT * from BlobStorage",
             on_error=on_error,
             file_format=input_format,
             output_format=output_format)
         query_result = resp.readall()
 
-        self.assertEqual(len(errors), 0)
-        self.assertEqual(len(resp), len(CSV_DATA))
-        self.assertEqual(query_result, CONVERTED_CSV_DATA)
+        assert len(errors) == 0
+        assert len(resp) == len(CSV_DATA)
+        assert query_result == CONVERTED_CSV_DATA
 
     @DataLakePreparer()
-    def test_quick_query_iter_records_with_serialization_setting(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records_with_serialization_setting(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
@@ -385,20 +426,24 @@
             file_format=input_format,
             output_format=output_format)
         data = []
         for record in reader.records():
             if record:
                 data.append(record)
 
-        self.assertEqual(len(reader), len(CSV_DATA))
-        self.assertEqual(len(reader), reader._blob_query_reader._bytes_processed)
-        self.assertEqual(len(data), 33)
+        assert len(reader) == len(CSV_DATA)
+        assert len(reader) == reader._blob_query_reader._bytes_processed
+        assert len(data) == 33
 
     @DataLakePreparer()
-    def test_quick_query_readall_with_fatal_error_handler(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_readall_with_fatal_error_handler(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data1 = b'{name: owner}'
         data2 = b'{name2: owner2}'
         data3 = b'{version:0,begin:1601-01-01T00:00:00.000Z,intervalSecs:3600,status:Finalized,config:' \
                 b'{version:0,configVersionEtag:0x8d75ef460eb1a12,numShards:1,recordsFormat:avro,formatSchemaVersion:3,' \
                 b'shardDistFnVersion:1},chunkFilePaths:[$blobchangefeed/log/00/1601/01/01/0000/],storageDiagnostics:' \
@@ -426,20 +471,24 @@
         resp = file_client.query_file(
             "SELECT * from BlobStorage",
             on_error=on_error,
             file_format=input_format,
             output_format=output_format)
         query_result = resp.readall()
 
-        self.assertEqual(len(errors), 1)
-        self.assertEqual(len(resp), 43)
-        self.assertEqual(query_result, b'')
+        assert len(errors) == 1
+        assert len(resp) == 43
+        assert query_result == b''
 
     @DataLakePreparer()
-    def test_quick_query_iter_records_with_fatal_error_handler(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records_with_fatal_error_handler(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data1 = b'{name: owner}'
         data2 = b'{name2: owner2}'
         data3 = b'{version:0,begin:1601-01-01T00:00:00.000Z,intervalSecs:3600,status:Finalized,config:' \
                 b'{version:0,configVersionEtag:0x8d75ef460eb1a12,numShards:1,recordsFormat:avro,formatSchemaVersion:3,' \
                 b'shardDistFnVersion:1},chunkFilePaths:[$blobchangefeed/log/00/1601/01/01/0000/],storageDiagnostics:' \
@@ -469,20 +518,24 @@
             on_error=on_error,
             file_format=input_format,
             output_format=output_format)
         data = []
         for record in resp.records():
             data.append(record)
         
-        self.assertEqual(len(errors), 1)
-        self.assertEqual(len(resp), 43)
-        self.assertEqual(data, [b''])
+        assert len(errors) == 1
+        assert len(resp) == 43
+        assert data == [b'']
 
     @DataLakePreparer()
-    def test_quick_query_readall_with_fatal_error_handler_raise(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_readall_with_fatal_error_handler_raise(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data1 = b'{name: owner}'
         data2 = b'{name2: owner2}'
         data3 = b'{version:0,begin:1601-01-01T00:00:00.000Z,intervalSecs:3600,status:Finalized,config:' \
                 b'{version:0,configVersionEtag:0x8d75ef460eb1a12,numShards:1,recordsFormat:avro,formatSchemaVersion:3,' \
                 b'shardDistFnVersion:1},chunkFilePaths:[$blobchangefeed/log/00/1601/01/01/0000/],storageDiagnostics:' \
@@ -512,15 +565,19 @@
             on_error=on_error,
             file_format=input_format,
             output_format=output_format)
         with pytest.raises(Exception):
             query_result = resp.readall()
 
     @DataLakePreparer()
-    def test_quick_query_iter_records_with_fatal_error_handler_raise(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records_with_fatal_error_handler_raise(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data1 = b'{name: owner}'
         data2 = b'{name2: owner2}'
         data3 = b'{version:0,begin:1601-01-01T00:00:00.000Z,intervalSecs:3600,status:Finalized,config:' \
                 b'{version:0,configVersionEtag:0x8d75ef460eb1a12,numShards:1,recordsFormat:avro,formatSchemaVersion:3,' \
                 b'shardDistFnVersion:1},chunkFilePaths:[$blobchangefeed/log/00/1601/01/01/0000/],storageDiagnostics:' \
@@ -552,15 +609,19 @@
             output_format=output_format)
 
         with pytest.raises(Exception):
             for record in resp.records():
                 print(record)
 
     @DataLakePreparer()
-    def test_quick_query_readall_with_fatal_error_ignore(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_readall_with_fatal_error_ignore(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data1 = b'{name: owner}'
         data2 = b'{name2: owner2}'
         data = data1 + b'\n' + data2 + b'\n' + data1
 
         # upload the json file
@@ -578,15 +639,19 @@
         resp = file_client.query_file(
             "SELECT * from BlobStorage",
             file_format=input_format,
             output_format=output_format)
         query_result = resp.readall()
 
     @DataLakePreparer()
-    def test_quick_query_iter_records_with_fatal_error_ignore(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records_with_fatal_error_ignore(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data1 = b'{name: owner}'
         data2 = b'{name2: owner2}'
         data3 = b'{version:0,begin:1601-01-01T00:00:00.000Z,intervalSecs:3600,status:Finalized,config:' \
                 b'{version:0,configVersionEtag:0x8d75ef460eb1a12,numShards:1,recordsFormat:avro,formatSchemaVersion:3,' \
                 b'shardDistFnVersion:1},chunkFilePaths:[$blobchangefeed/log/00/1601/01/01/0000/],storageDiagnostics:' \
@@ -611,15 +676,19 @@
             file_format=input_format,
             output_format=output_format)
 
         for record in resp.records():
             print(record)
 
     @DataLakePreparer()
-    def test_quick_query_readall_with_nonfatal_error_handler(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_readall_with_nonfatal_error_handler(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
@@ -644,20 +713,24 @@
             "SELECT RepoPath from BlobStorage",
             file_format=input_format,
             output_format=output_format,
             on_error=on_error)
         query_result = resp.readall()
 
         # the error is because that line only has one column
-        self.assertEqual(len(errors), 1)
-        self.assertEqual(len(resp), len(CSV_DATA))
-        self.assertTrue(len(query_result) > 0)
+        assert len(errors) == 1
+        assert len(resp) == len(CSV_DATA)
+        assert len(query_result) > 0
 
     @DataLakePreparer()
-    def test_quick_query_iter_records_with_nonfatal_error_handler(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records_with_nonfatal_error_handler(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
@@ -682,20 +755,24 @@
             "SELECT RepoPath from BlobStorage",
             file_format=input_format,
             output_format=output_format,
             on_error=on_error)
         data = list(resp.records())
 
         # the error is because that line only has one column
-        self.assertEqual(len(errors), 1)
-        self.assertEqual(len(resp), len(CSV_DATA))
-        self.assertEqual(len(data), 32)
+        assert len(errors) == 1
+        assert len(resp) == len(CSV_DATA)
+        assert len(data) == 32
 
     @DataLakePreparer()
-    def test_quick_query_readall_with_nonfatal_error_ignore(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_readall_with_nonfatal_error_ignore(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
@@ -713,19 +790,23 @@
             escapechar='\\',
         )
         resp = file_client.query_file(
             "SELECT RepoPath from BlobStorage",
             file_format=input_format,
             output_format=output_format)
         query_result = resp.readall()
-        self.assertEqual(len(resp), len(CSV_DATA))
-        self.assertTrue(len(query_result) > 0)
+        assert len(resp) == len(CSV_DATA)
+        assert len(query_result) > 0
 
     @DataLakePreparer()
-    def test_quick_query_iter_records_with_nonfatal_error_ignore(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records_with_nonfatal_error_ignore(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         # upload the csv file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
         file_client.upload_data(CSV_DATA, overwrite=True)
 
@@ -743,19 +824,23 @@
             escapechar='\\',
         )
         resp = file_client.query_file(
             "SELECT RepoPath from BlobStorage",
             file_format=input_format,
             output_format=output_format)
         data = list(resp.records())
-        self.assertEqual(len(resp), len(CSV_DATA))
-        self.assertEqual(len(data), 32)
+        assert len(resp) == len(CSV_DATA)
+        assert len(data) == 32
 
     @DataLakePreparer()
-    def test_quick_query_readall_with_json_serialization_setting(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_readall_with_json_serialization_setting(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data1 = b'{\"name\": \"owner\", \"id\": 1}'
         data2 = b'{\"name2\": \"owner2\"}'
         data = data1 + b'\n' + data2 + b'\n' + data1
 
         # upload the json file
@@ -773,20 +858,24 @@
         resp = file_client.query_file(
             "SELECT name from BlobStorage",
             on_error=on_error,
             file_format=input_format,
             output_format=output_format)
         query_result = resp.readall()
 
-        self.assertEqual(len(errors), 0)
-        self.assertEqual(len(resp), len(data))
-        self.assertEqual(query_result, b'{"name":"owner"};{};{"name":"owner"};')
+        assert len(errors) == 0
+        assert len(resp) == len(data)
+        assert query_result == b'{"name":"owner"};{};{"name":"owner"};'
 
     @DataLakePreparer()
-    def test_quick_query_iter_records_with_json_serialization_setting(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_iter_records_with_json_serialization_setting(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data1 = b'{\"name\": \"owner\", \"id\": 1}'
         data2 = b'{\"name2\": \"owner2\"}'
         data = data1 + b'\n' + data2 + b'\n' + data1
 
         # upload the json file
@@ -804,20 +893,24 @@
         resp = file_client.query_file(
             "SELECT name from BlobStorage",
             on_error=on_error,
             file_format=input_format,
             output_format=output_format)
         listdata = list(resp.records())
 
-        self.assertEqual(len(errors), 0)
-        self.assertEqual(len(resp), len(data))
-        self.assertEqual(listdata, [b'{"name":"owner"}',b'{}',b'{"name":"owner"}', b''])
+        assert len(errors) == 0
+        assert len(resp) == len(data)
+        assert listdata, [b'{"name":"owner"}',b'{}',b'{"name":"owner"}' == b'']
 
     @DataLakePreparer()
-    def test_quick_query_with_only_input_json_serialization_setting(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_with_only_input_json_serialization_setting(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data1 = b'{\"name\": \"owner\", \"id\": 1}'
         data2 = b'{\"name2\": \"owner2\"}'
         data = data1 + data2 + data1
 
         # upload the json file
@@ -835,20 +928,24 @@
         resp = file_client.query_file(
             "SELECT name from BlobStorage",
             on_error=on_error,
             file_format=input_format,
             output_format=output_format)
         query_result = resp.readall()
 
-        self.assertEqual(len(errors), 0)
-        self.assertEqual(len(resp), len(data))
-        self.assertEqual(query_result, b'{"name":"owner"}\n{}\n{"name":"owner"}\n')
+        assert len(errors) == 0
+        assert len(resp) == len(data)
+        assert query_result == b'{"name":"owner"}\n{}\n{"name":"owner"}\n'
 
     @DataLakePreparer()
-    def test_quick_query_output_in_arrow_format(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_output_in_arrow_format(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         data = b'100,200,300,400\n300,400,500,600\n'
 
         # upload the json file
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
@@ -863,67 +960,79 @@
         resp = file_client.query_file(
             "SELECT _2 from BlobStorage WHERE _1 > 250",
             on_error=on_error,
             output_format=output_format)
         query_result = base64.b64encode(resp.readall())
         # expected_result = b'/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEHEAAAABwAAAAEAAAAAAAAAAMAAABhYmMACAAMAAQACAAIAAAABAAAAAIAAAD/////cAAAABAAAAAAAAoADgAGAAUACAAKAAAAAAMEABAAAAAAAAoADAAAAAQACAAKAAAAMAAAAAQAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAP////+IAAAAFAAAAAAAAAAMABYABgAFAAgADAAMAAAAAAMEABgAAAAQAAAAAAAAAAAACgAYAAwABAAIAAoAAAA8AAAAEAAAAAEAAAAAAAAAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAABAAAAAQAAAAAAAAAAAAAAAAAAAJABAAAAAAAAAAAAAAAAAAA='
 
-        self.assertEqual(len(errors), 0)
+        assert len(errors) == 0
         # Skip this assert for now, requires further investigation: https://github.com/Azure/azure-sdk-for-python/issues/24690
-        # self.assertEqual(query_result, expected_result)
+        # assert query_result == expected_result
 
     @DataLakePreparer()
-    def test_quick_query_input_in_arrow_format(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_input_in_arrow_format(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         # Arrange
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
 
         errors = []
         def on_error(error):
             errors.append(error)
 
         input_format = [ArrowDialect(ArrowType.DECIMAL, name="abc", precision=4, scale=2)]
 
-        with self.assertRaises(ValueError):
+        with pytest.raises(ValueError):
             file_client.query_file(
                 "SELECT _2 from BlobStorage WHERE _1 > 250",
                 on_error=on_error,
                 file_format=input_format)
 
     @DataLakePreparer()
-    def test_quick_query_input_in_parquet_format(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_input_in_parquet_format(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         # Arrange
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
 
         expression = "select * from blobstorage where id < 1;"
-        expected_data = b"0,mdifjt55.ea3,mdifjt55.ea3\n"
+        expected_data = b"0,mdifjt55.ea3,mdifjt55.ea3\n" # cspell:disable-line
 
         parquet_path = os.path.abspath(os.path.join(os.path.abspath(__file__), "..", "./resources/parquet.parquet"))
         with open(parquet_path, "rb") as parquet_data:
             file_client.upload_data(parquet_data, overwrite=True)
 
         reader = file_client.query_file(expression, file_format=QuickQueryDialect.Parquet)
         real_data = reader.readall()
 
-        self.assertEqual(real_data, expected_data)
+        assert real_data == expected_data
 
     @DataLakePreparer()
-    def test_quick_query_output_in_parquet_format(self, datalake_storage_account_name, datalake_storage_account_key):
+    @recorded_by_proxy
+    def test_quick_query_output_in_parquet_format(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         # Arrange
         self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         file_name = self._get_file_reference()
         file_client = self.dsc.get_file_client(self.filesystem_name, file_name)
 
         expression = "SELECT * from BlobStorage"
         parquet_path = os.path.abspath(os.path.join(os.path.abspath(__file__), "..", "./resources/parquet.parquet"))
         with open(parquet_path, "rb") as parquet_data:
             file_client.upload_data(parquet_data, overwrite=True)
 
-        with self.assertRaises(ValueError):
+        with pytest.raises(ValueError):
             file_client.query_file(
                 expression, file_format=QuickQueryDialect.Parquet,
                 output_format=QuickQueryDialect.Parquet)
 
 # ------------------------------------------------------------------------------
```

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_datalake_service_client_async.py` & `azure-storage-file-datalake-12.9.1/tests/test_datalake_service_client_async.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_datalake_service_client.py` & `azure-storage-file-datalake-12.9.1/tests/test_datalake_service_client.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_large_file_async.py` & `azure-storage-file-datalake-12.9.1/tests/test_large_file_async.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-# coding: utf-8
-
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
+import asyncio
+import platform
 import re
 import unittest
-import asyncio
 from io import BytesIO
 from os import urandom
 
 import pytest
-
 from azure.core.exceptions import ResourceExistsError
 from azure.core.pipeline.policies import SansIOHTTPPolicy
 from azure.storage.blob._shared.base_client import _format_shared_key_credential
 from azure.storage.filedatalake.aio import DataLakeServiceClient
-from devtools_testutils.storage.aio import AsyncStorageTestCase as StorageTestCase
+
+from devtools_testutils.storage.aio import AsyncStorageRecordedTestCase
 from settings.testcase import DataLakePreparer
+
 # ------------------------------------------------------------------------------
 TEST_DIRECTORY_PREFIX = 'directory'
 TEST_FILE_PREFIX = 'file'
 FILE_PATH = 'file_output.temp.dat'
 LARGEST_BLOCK_SIZE = 4000 * 1024 * 1024
 # ------------------------------------------------------------------------------
 
 
-class LargeFileTest(StorageTestCase):
+class TestLargeFileAsync(AsyncStorageRecordedTestCase):
     async def _setUp(self, account_name, account_key):
         url = self.account_url(account_name, 'dfs')
         self.payload_dropping_policy = PayloadDroppingPolicy()
         credential_policy = _format_shared_key_credential(account_name, account_key)
         self.dsc = DataLakeServiceClient(url,
                                          credential=account_key,
                                          _additional_pipeline_policies=[self.payload_dropping_policy, credential_policy])
@@ -53,25 +53,28 @@
             try:
                 loop = asyncio.get_event_loop()
                 loop.run_until_complete(self.dsc.delete_file_system(self.file_system_name))
                 loop.run_until_complete(self.dsc.__aexit__())
             except:
                 pass
 
-        return super(LargeFileTest, self).tearDown()
+        return super(TestLargeFileAsync, self).tearDown()
 
     # --Helpers-----------------------------------------------------------------
     def _get_directory_reference(self, prefix=TEST_DIRECTORY_PREFIX):
         directory_name = self.get_resource_name(prefix)
         return directory_name
 
     # --Helpers-----------------------------------------------------------------
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_append_large_stream_without_network(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_append_large_stream_without_network(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self._get_directory_reference()
 
         # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
 
@@ -79,22 +82,25 @@
         await file_client.create_file()
 
         data = LargeStream(LARGEST_BLOCK_SIZE)
 
         # Act
         response = await file_client.append_data(data, 0, LARGEST_BLOCK_SIZE)
 
-        self.assertIsNotNone(response)
-        self.assertEqual(self.payload_dropping_policy.append_counter, 1)
-        self.assertEqual(self.payload_dropping_policy.append_sizes[0], LARGEST_BLOCK_SIZE)
+        assert response is not None
+        assert self.payload_dropping_policy.append_counter == 1
+        assert self.payload_dropping_policy.append_sizes[0] == LARGEST_BLOCK_SIZE
 
-    @pytest.mark.skip(reason="Pypy3 on Linux failed somehow, skip for now to investigate")
+    @pytest.mark.skipif(platform.python_implementation() == "PyPy", reason="Test failing on Pypy3 Linux, skip to investigate")
     @pytest.mark.live_test_only
     @DataLakePreparer()
-    async def test_upload_large_stream_without_network_async(self, datalake_storage_account_name, datalake_storage_account_key):
+    async def test_upload_large_stream_without_network(self, **kwargs):
+        datalake_storage_account_name = kwargs.pop("datalake_storage_account_name")
+        datalake_storage_account_key = kwargs.pop("datalake_storage_account_key")
+
         await self._setUp(datalake_storage_account_name, datalake_storage_account_key)
         directory_name = self.get_resource_name(TEST_DIRECTORY_PREFIX)
 
         # Create a directory to put the file under that
         directory_client = self.dsc.get_directory_client(self.file_system_name, directory_name)
         await directory_client.create_directory()
 
@@ -103,18 +109,18 @@
 
         length = 2*LARGEST_BLOCK_SIZE
         data = LargeStream(length)
 
         # Act
         response = await file_client.upload_data(data, length, overwrite=True, chunk_size=LARGEST_BLOCK_SIZE)
 
-        self.assertIsNotNone(response)
-        self.assertEqual(self.payload_dropping_policy.append_counter, 2)
-        self.assertEqual(self.payload_dropping_policy.append_sizes[0], LARGEST_BLOCK_SIZE)
-        self.assertEqual(self.payload_dropping_policy.append_sizes[1], LARGEST_BLOCK_SIZE)
+        assert response is not None
+        assert self.payload_dropping_policy.append_counter == 2
+        assert self.payload_dropping_policy.append_sizes[0] == LARGEST_BLOCK_SIZE
+        assert self.payload_dropping_policy.append_sizes[1] == LARGEST_BLOCK_SIZE
 
 
 class LargeStream(BytesIO):
     def __init__(self, length, initial_buffer_length=1024 * 1024):
         self._base_data = urandom(initial_buffer_length)
         self._base_data_length = initial_buffer_length
         self._position = 0
```

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_cpk_async.py` & `azure-storage-file-datalake-12.9.1/tests/test_cpk_async.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/test_cpk.py` & `azure-storage-file-datalake-12.9.1/tests/test_cpk.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/append.py` & `azure-storage-file-datalake-12.9.1/tests/perfstress_tests/append.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/upload_from_file.py` & `azure-storage-file-datalake-12.9.1/tests/perfstress_tests/upload_from_file.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/read.py` & `azure-storage-file-datalake-12.9.1/tests/perfstress_tests/read.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/_test_base.py` & `azure-storage-file-datalake-12.9.1/tests/perfstress_tests/_test_base.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/perfstress_tests/upload.py` & `azure-storage-file-datalake-12.9.1/tests/perfstress_tests/upload.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/settings/settings_fake.py` & `azure-storage-file-datalake-12.9.1/tests/settings/settings_fake.py`

 * *Files 26% similar despite different names*

```diff
@@ -3,14 +3,17 @@
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 DATALAKE_STORAGE_ACCOUNT_NAME = "fakename"
 DATALAKE_STORAGE_ACCOUNT_KEY = "fakekey"
 
+STORAGE_DATA_LAKE_SOFT_DELETE_ACCOUNT_NAME = "fakename"
+STORAGE_DATA_LAKE_SOFT_DELETE_ACCOUNT_KEY = "fakekey"
+
 TENANT_ID = "00000000-0000-0000-0000-000000000000"
 CLIENT_ID = "00000000-0000-0000-0000-000000000000"
 CLIENT_SECRET = "00000000-0000-0000-0000-000000000000"
 
 ACCOUNT_URL_SUFFIX = 'core.windows.net'
 RUN_IN_LIVE = "False"
 SKIP_LIVE_RECORDING = "True"
```

## Comparing `azure-storage-file-datalake-12.9.0b1/tests/settings/testcase.py` & `azure-storage-file-datalake-12.9.1/tests/settings/testcase.py`

 * *Files 16% similar despite different names*

```diff
@@ -26,21 +26,26 @@
 
 
 LOGGING_FORMAT = '%(asctime)s %(name)-20s %(levelname)-5s %(message)s'
 LOGGING_FORMAT = '%(asctime)s %(name)-20s %(levelname)-5s %(message)s'
 os.environ['DATALAKE_STORAGE_ACCOUNT_NAME'] = os.environ.get('DATALAKE_STORAGE_ACCOUNT_NAME', None) or DATALAKE_STORAGE_ACCOUNT_NAME
 os.environ['DATALAKE_STORAGE_ACCOUNT_KEY'] = os.environ.get('DATALAKE_STORAGE_ACCOUNT_KEY', None) or DATALAKE_STORAGE_ACCOUNT_KEY
 
+os.environ['STORAGE_DATA_LAKE_SOFT_DELETE_ACCOUNT_NAME'] = os.environ.get('STORAGE_DATA_LAKE_SOFT_DELETE_ACCOUNT_NAME', None) or STORAGE_DATA_LAKE_SOFT_DELETE_ACCOUNT_NAME
+os.environ['STORAGE_DATA_LAKE_SOFT_DELETE_ACCOUNT_KEY'] = os.environ.get('STORAGE_DATA_LAKE_SOFT_DELETE_ACCOUNT_KEY', None) or STORAGE_DATA_LAKE_SOFT_DELETE_ACCOUNT_KEY
+
 os.environ['AZURE_TEST_RUN_LIVE'] = os.environ.get('AZURE_TEST_RUN_LIVE', None) or RUN_IN_LIVE
 os.environ['AZURE_SKIP_LIVE_RECORDING'] = os.environ.get('AZURE_SKIP_LIVE_RECORDING', None) or SKIP_LIVE_RECORDING
 os.environ['PROTOCOL'] = PROTOCOL
 os.environ['ACCOUNT_URL_SUFFIX'] = ACCOUNT_URL_SUFFIX
 
 os.environ['STORAGE_TENANT_ID'] = os.environ.get('STORAGE_TENANT_ID', None) or TENANT_ID
 os.environ['STORAGE_CLIENT_ID'] = os.environ.get('STORAGE_CLIENT_ID', None) or CLIENT_ID
 os.environ['STORAGE_CLIENT_SECRET'] = os.environ.get('STORAGE_CLIENT_SECRET', None) or CLIENT_SECRET
 
 DataLakePreparer = functools.partial(
     PowerShellPreparer, "storage",
     datalake_storage_account_name="storagename",
-    datalake_storage_account_key=STORAGE_ACCOUNT_FAKE_KEY
+    datalake_storage_account_key=STORAGE_ACCOUNT_FAKE_KEY,
+    storage_data_lake_soft_delete_account_name="storagesoftdelname",
+    storage_data_lake_soft_delete_account_key=STORAGE_ACCOUNT_FAKE_KEY,
 )
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_serialize.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_serialize.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_models.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_models.py`

 * *Files 0% similar despite different names*

```diff
@@ -405,15 +405,15 @@
         To specify read, write, or delete permissions you need only to
         include the first letter of the word in the string. E.g. For read and
         write permissions, you would provide a string "rw".
 
         :param str permission: The string which dictates the read, add, create,
             write, or delete permissions.
         :return: A FileSystemSasPermissions object
-        :rtype: ~azure.storage.fildatalake.FileSystemSasPermissions
+        :rtype: ~azure.storage.filedatalake.FileSystemSasPermissions
         """
         p_read = 'r' in permission
         p_add = 'a' in permission
         p_create = 'c' in permission
         p_write = 'w' in permission
         p_delete = 'd' in permission
         p_list = 'l' in permission
@@ -576,15 +576,15 @@
         To specify read, write, or delete permissions you need only to
         include the first letter of the word in the string. E.g. For read and
         write permissions, you would provide a string "rw".
 
         :param str permission: The string which dictates the read, add, create,
             write, or delete permissions.
         :return: A FileSasPermissions object
-        :rtype: ~azure.storage.fildatalake.FileSasPermissions
+        :rtype: ~azure.storage.filedatalake.FileSasPermissions
         """
         p_read = 'r' in permission
         p_add = 'a' in permission
         p_create = 'c' in permission
         p_write = 'w' in permission
         p_delete = 'd' in permission
         p_move = 'm' in permission
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_directory_client.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_directory_client.py`

 * *Files 0% similar despite different names*

```diff
@@ -300,15 +300,15 @@
         Rename the source directory.
 
         :param str new_name:
             the new directory name the user want to rename to.
             The value must have the following format: "{filesystem}/{directory}/{subdirectory}".
         :keyword source_lease:
             A lease ID for the source path. If specified,
-            the source path must have an active lease and the leaase ID must
+            the source path must have an active lease and the lease ID must
             match.
         :paramtype source_lease: ~azure.storage.filedatalake.DataLakeLeaseClient or str
         :keyword lease:
             Required if the file/directory has an active lease. Value can be a LeaseClient object
             or the lease ID as a string.
         :paramtype lease: ~azure.storage.filedatalake.DataLakeLeaseClient or str
         :keyword ~datetime.datetime if_modified_since:
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_download.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_download.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
-from typing import Iterator, Optional
+from typing import IO, Iterator, Optional
 
 from ._deserialize import from_blob_properties
 
 
 class StorageStreamDownloader(object):
     """A streaming object to download from Azure Storage.
 
     :ivar str name:
         The name of the file being downloaded.
     :ivar ~azure.storage.filedatalake.FileProperties properties:
         The properties of the file being downloaded. If only a range of the data is being
         downloaded, this will be reflected in the properties.
     :ivar int size:
-        The size of the total data in the stream. This will be the byte range if speficied,
+        The size of the total data in the stream. This will be the byte range if specified,
         otherwise the total size of the file.
     """
 
     def __init__(self, downloader):
         self._downloader = downloader
         self.name = self._downloader.name
         self.properties = from_blob_properties(self._downloader.properties)  # pylint: disable=protected-access
@@ -39,31 +39,31 @@
 
     def read(self, size: Optional[int] = -1) -> bytes:
         """
         Read up to size bytes from the stream and return them. If size
         is unspecified or is -1, all bytes will be read.
 
         :param size:
-            The number of bytes to download from the stream. Leave unsepcified
+            The number of bytes to download from the stream. Leave unspecified
             or set to -1 to download all bytes.
         :returns:
-            The requsted data as bytes. If the return value is empty, there is no more data to read.
+            The requested data as bytes. If the return value is empty, there is no more data to read.
         :rtype: bytes
         """
         return self._downloader.read(size)
 
     def readall(self) -> bytes:
         """Download the contents of this file.
 
         This operation is blocking until all data is downloaded.
         :rtype: bytes
         """
         return self._downloader.readall()
 
-    def readinto(self, stream) -> int:
+    def readinto(self, stream: IO[bytes]) -> int:
         """Download the contents of this file to a stream.
 
         :param stream:
             The stream to download to. This can be an open file-handle,
             or any writable stream. The stream must be seekable if the download
             uses more than one parallel connection.
         :returns: The number of bytes read.
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_path_client.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_path_client.py`

 * *Files 0% similar despite different names*

```diff
@@ -754,15 +754,15 @@
         :param rename_source:
             The value must have the following format: "/{filesystem}/{path}".
         :type rename_source: str
         :keyword ~azure.storage.filedatalake.ContentSettings content_settings:
             ContentSettings object used to set path properties.
         :keyword source_lease:
             A lease ID for the source path. If specified,
-            the source path must have an active lease and the leaase ID must
+            the source path must have an active lease and the lease ID must
             match.
         :paramtype source_lease: ~azure.storage.filedatalake.DataLakeLeaseClient or str
         :keyword lease:
             Required if the file/directory has an active lease. Value can be a LeaseClient object
             or the lease ID as a string.
         :paramtype lease: ~azure.storage.filedatalake.DataLakeLeaseClient or str
         :keyword ~datetime.datetime if_modified_since:
@@ -910,15 +910,15 @@
         return self._blob_client.set_blob_metadata(metadata=metadata, **kwargs)
 
     def set_http_headers(self, content_settings=None,  # type: Optional[ContentSettings]
                          **kwargs):
         # type: (...) -> Dict[str, Any]
         """Sets system properties on the file or directory.
 
-        If one property is set for the content_settings, all properties will be overriden.
+        If one property is set for the content_settings, all properties will be overridden.
 
         :param ~azure.storage.filedatalake.ContentSettings content_settings:
             ContentSettings object used to set file/directory properties.
         :keyword lease:
             If specified, set_file_system_metadata only succeeds if the
             file system's lease is active and matches this ID.
         :paramtype lease: ~azure.storage.filedatalake.DataLakeLeaseClient or str
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared_access_signature.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared_access_signature.py`

 * *Files 0% similar despite different names*

```diff
@@ -321,15 +321,15 @@
         this can be accomplished
         by calling :func:`~azure.storage.filedatalake.DataLakeServiceClient.get_user_delegation_key`.
         When present, the SAS is signed with the user delegation key instead.
     :type credential: str or ~azure.storage.filedatalake.UserDelegationKey
     :param permission:
         The permissions associated with the shared access signature. The
         user is restricted to operations allowed by the permissions.
-        Permissions must be ordered racwdmeop.
+        Permissions must be ordered racwdlmeop.
         Required unless an id is given referencing a stored access policy
         which contains this field. This field must be omitted if it has been
         specified in an associated stored access policy.
     :type permission: str or ~azure.storage.filedatalake.FileSasPermissions
     :param expiry:
         The time at which the shared access signature becomes invalid.
         Required unless an id is given referencing a stored access policy
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_deserialize.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_deserialize.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_service_client.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_service_client.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_upload_helper.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_upload_helper.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_lease.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_lease.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/__init__.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_data_lake_file_client.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_data_lake_file_client.py`

 * *Files 0% similar despite different names*

```diff
@@ -707,15 +707,15 @@
         Rename the source file.
 
         :param str new_name: the new file name the user want to rename to.
             The value must have the following format: "{filesystem}/{directory}/{subdirectory}/{file}".
         :keyword ~azure.storage.filedatalake.ContentSettings content_settings:
             ContentSettings object used to set path properties.
         :keyword source_lease: A lease ID for the source path. If specified,
-         the source path must have an active lease and the leaase ID must
+         the source path must have an active lease and the lease ID must
          match.
         :paramtype source_lease: ~azure.storage.filedatalake.DataLakeLeaseClient or str
         :keyword lease:
             Required if the file/directory has an active lease. Value can be a LeaseClient object
             or the lease ID as a string.
         :type lease: ~azure.storage.filedatalake.DataLakeLeaseClient or str
         :keyword ~datetime.datetime if_modified_since:
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_quick_query_helper.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_quick_query_helper.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_file_system_client.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_file_system_client.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_list_paths_helper.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_list_paths_helper.py`

 * *Files 1% similar despite different names*

```diff
@@ -117,15 +117,15 @@
     :ivar str path: Filters the results to return only paths under the specified path.
     :ivar int results_per_page: The maximum number of results retrieved per API call.
     :ivar str continuation_token: The continuation token to retrieve the next page of results.
     :ivar list(~azure.storage.filedatalake.PathProperties) current_page: The current page of listed results.
 
     :param callable command: Function to retrieve the next page of items.
     :param str path: Filters the results to return only paths under the specified path.
-    :param int max_results: The maximum number of psths to retrieve per
+    :param int max_results: The maximum number of paths to retrieve per
         call.
     :param str continuation_token: An opaque continuation token.
     """
     def __init__(
             self, command,
             recursive,
             path=None,
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/request_handlers.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/request_handlers.py`

 * *Files 1% similar despite different names*

```diff
@@ -130,15 +130,15 @@
     elif start_range is not None:
         range_header = "bytes={0}-".format(start_range)
 
     # Content MD5 can only be provided for a complete range less than 4MB in size
     range_validation = None
     if check_content_md5:
         if start_range is None or end_range is None:
-            raise ValueError("Both start and end range requied for MD5 content validation.")
+            raise ValueError("Both start and end range required for MD5 content validation.")
         if end_range - start_range > 4 * 1024 * 1024:
             raise ValueError("Getting content MD5 for a range greater than 4MB is not supported.")
         range_validation = 'true'
 
     return range_header, range_validation
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/base_client_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/base_client_async.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/response_handlers.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/response_handlers.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/uploads_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/uploads_async.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/__init__.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/models.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/models.py`

 * *Files 2% similar despite different names*

```diff
@@ -81,15 +81,17 @@
     CONTAINER_DISABLED = "ContainerDisabled"
     CONTAINER_NOT_FOUND = "ContainerNotFound"
     CONTENT_LENGTH_LARGER_THAN_TIER_LIMIT = "ContentLengthLargerThanTierLimit"
     COPY_ACROSS_ACCOUNTS_NOT_SUPPORTED = "CopyAcrossAccountsNotSupported"
     COPY_ID_MISMATCH = "CopyIdMismatch"
     FEATURE_VERSION_MISMATCH = "FeatureVersionMismatch"
     INCREMENTAL_COPY_BLOB_MISMATCH = "IncrementalCopyBlobMismatch"
-    INCREMENTAL_COPY_OF_ERALIER_VERSION_SNAPSHOT_NOT_ALLOWED = "IncrementalCopyOfEralierVersionSnapshotNotAllowed"
+    INCREMENTAL_COPY_OF_EARLIER_VERSION_SNAPSHOT_NOT_ALLOWED = "IncrementalCopyOfEarlierVersionSnapshotNotAllowed"
+    #: Deprecated: Please use INCREMENTAL_COPY_OF_EARLIER_VERSION_SNAPSHOT_NOT_ALLOWED instead.
+    INCREMENTAL_COPY_OF_ERALIER_VERSION_SNAPSHOT_NOT_ALLOWED = "IncrementalCopyOfEarlierVersionSnapshotNotAllowed"
     INCREMENTAL_COPY_SOURCE_MUST_BE_SNAPSHOT = "IncrementalCopySourceMustBeSnapshot"
     INFINITE_LEASE_DURATION_REQUIRED = "InfiniteLeaseDurationRequired"
     INVALID_BLOB_OR_BLOCK = "InvalidBlobOrBlock"
     INVALID_BLOB_TIER = "InvalidBlobTier"
     INVALID_BLOB_TYPE = "InvalidBlobType"
     INVALID_BLOCK_ID = "InvalidBlockId"
     INVALID_BLOCK_LIST = "InvalidBlockList"
@@ -117,15 +119,17 @@
     PENDING_COPY_OPERATION = "PendingCopyOperation"
     PREVIOUS_SNAPSHOT_CANNOT_BE_NEWER = "PreviousSnapshotCannotBeNewer"
     PREVIOUS_SNAPSHOT_NOT_FOUND = "PreviousSnapshotNotFound"
     PREVIOUS_SNAPSHOT_OPERATION_NOT_SUPPORTED = "PreviousSnapshotOperationNotSupported"
     SEQUENCE_NUMBER_CONDITION_NOT_MET = "SequenceNumberConditionNotMet"
     SEQUENCE_NUMBER_INCREMENT_TOO_LARGE = "SequenceNumberIncrementTooLarge"
     SNAPSHOT_COUNT_EXCEEDED = "SnapshotCountExceeded"
-    SNAPHOT_OPERATION_RATE_EXCEEDED = "SnaphotOperationRateExceeded"
+    SNAPSHOT_OPERATION_RATE_EXCEEDED = "SnapshotOperationRateExceeded"
+    #: Deprecated: Please use SNAPSHOT_OPERATION_RATE_EXCEEDED instead.
+    SNAPHOT_OPERATION_RATE_EXCEEDED = "SnapshotOperationRateExceeded"
     SNAPSHOTS_PRESENT = "SnapshotsPresent"
     SOURCE_CONDITION_NOT_MET = "SourceConditionNotMet"
     SYSTEM_IN_USE = "SystemInUse"
     TARGET_CONDITION_NOT_MET = "TargetConditionNotMet"
     UNAUTHORIZED_BLOB_OVERWRITE = "UnauthorizedBlobOverwrite"
     BLOB_BEING_REHYDRATED = "BlobBeingRehydrated"
     BLOB_ARCHIVED = "BlobArchived"
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/uploads.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/uploads.py`

 * *Files 1% similar despite different names*

```diff
@@ -582,15 +582,15 @@
 
     next = __next__  # Python 2 compatibility.
 
     def tell(self, *args, **kwargs):
         raise UnsupportedOperation("Data generator does not support tell.")
 
     def seek(self, *args, **kwargs):
-        raise UnsupportedOperation("Data generator is unseekable.")
+        raise UnsupportedOperation("Data generator is not seekable.")
 
     def read(self, size):
         data = self.leftover
         count = len(self.leftover)
         try:
             while count < size:
                 chunk = self.__next__()
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/policies_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/policies_async.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/base_client.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/base_client.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/policies.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/policies.py`

 * *Files 0% similar despite different names*

```diff
@@ -78,15 +78,15 @@
     variables such as the number of total retries to allow, whether to
     respect the Retry-After header, whether this header is present, and
     whether the returned status code is on the list of status codes to
     be retried upon on the presence of the aforementioned header)
     """
     status = response.http_response.status_code
     if 300 <= status < 500:
-        # An exception occured, but in most cases it was expected. Examples could
+        # An exception occurred, but in most cases it was expected. Examples could
         # include a 309 Conflict or 412 Precondition Failed.
         if status == 404 and mode == LocationMode.SECONDARY:
             # Response code 404 should be retried if secondary was used.
             return True
         if status == 408:
             # Response code 408 is a timeout and should be retried.
             return True
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/shared_access_signature.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/shared_access_signature.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,15 +6,16 @@
 
 from datetime import date
 
 from .parser import _str, _to_utc_datetime
 from .constants import X_MS_VERSION
 from . import sign_string, url_quote
 
-
+# cspell:ignoreRegExp rsc.
+# cspell:ignoreRegExp s..?id
 class QueryStringConstants(object):
     SIGNED_SIGNATURE = 'sig'
     SIGNED_PERMISSION = 'sp'
     SIGNED_START = 'st'
     SIGNED_EXPIRY = 'se'
     SIGNED_RESOURCE = 'sr'
     SIGNED_IDENTIFIER = 'si'
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/authentication.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/authentication.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_shared/parser.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_shared/parser.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_models.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_models.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_file_client_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_file_client_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -548,15 +548,15 @@
         Rename the source file.
 
         :param str new_name: the new file name the user want to rename to.
             The value must have the following format: "{filesystem}/{directory}/{subdirectory}/{file}".
         :keyword ~azure.storage.filedatalake.ContentSettings content_settings:
             ContentSettings object used to set path properties.
         :keyword source_lease: A lease ID for the source path. If specified,
-            the source path must have an active lease and the leaase ID must
+            the source path must have an active lease and the lease ID must
             match.
         :paramtype source_lease: ~azure.storage.filedatalake.aio.DataLakeLeaseClient or str
         :keyword lease:
             Required if the file/directory has an active lease. Value can be a LeaseClient object
             or the lease ID as a string.
         :paramtype lease: ~azure.storage.filedatalake.aio.DataLakeLeaseClient or str
         :type permissions: str
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_directory_client_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_directory_client_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -265,15 +265,15 @@
         Rename the source directory.
 
         :param str new_name:
             the new directory name the user want to rename to.
             The value must have the following format: "{filesystem}/{directory}/{subdirectory}".
         :keyword source_lease:
             A lease ID for the source path. If specified,
-            the source path must have an active lease and the leaase ID must
+            the source path must have an active lease and the lease ID must
             match.
         :paramtype source_lease: ~azure.storage.filedatalake.aio.DataLakeLeaseClient or str
         :keyword lease:
             Required if the file/directory has an active lease. Value can be a LeaseClient object
             or the lease ID as a string.
         :paramtype lease: ~azure.storage.filedatalake.aio.DataLakeLeaseClient or str
         :keyword ~datetime.datetime if_modified_since:
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_upload_helper.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_upload_helper.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_file_system_client_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_file_system_client_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -193,15 +193,15 @@
         :param metadata:
             A dict with name-value pairs to associate with the
             file system as metadata. Example: `{'Category':'test'}`
         :type metadata: dict(str, str)
         :param public_access:
             To specify whether data in the file system may be accessed publicly and the level of access.
         :type public_access: ~azure.storage.filedatalake.PublicAccess
-        :keyword file_system_encryption_scope:
+        :keyword encryption_scope_options:
             Specifies the default encryption scope to set on the file system and use for
             all future writes.
 
             .. versionadded:: 12.9.0
 
         :paramtype encryption_scope_options: dict or ~azure.storage.filedatalake.EncryptionScopeOptions
         :keyword int timeout:
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/__init__.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_service_client_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_service_client_async.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_path_client_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_path_client_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -563,15 +563,15 @@
         Rename directory or file
 
         :param rename_source: The value must have the following format: "/{filesystem}/{path}".
         :type rename_source: str
         :keyword ~azure.storage.filedatalake.ContentSettings content_settings:
             ContentSettings object used to set path properties.
         :keyword source_lease: A lease ID for the source path. If specified,
-            the source path must have an active lease and the leaase ID must
+            the source path must have an active lease and the lease ID must
             match.
         :paramtype source_lease: ~azure.storage.filedatalake.aio.DataLakeLeaseClient or str
         :keyword lease:
             Required if the file/directory has an active lease. Value can be a LeaseClient object
             or the lease ID as a string.
         :paramtype lease: ~azure.storage.filedatalake.aio.DataLakeLeaseClient or str
         :keyword ~datetime.datetime if_modified_since:
@@ -710,15 +710,15 @@
         return await self._blob_client.set_blob_metadata(metadata=metadata, **kwargs)
 
     async def set_http_headers(self, content_settings=None,  # type: Optional[ContentSettings]
                                **kwargs):
         # type: (...) -> Dict[str, Any]
         """Sets system properties on the file or directory.
 
-        If one property is set for the content_settings, all properties will be overriden.
+        If one property is set for the content_settings, all properties will be overridden.
 
         :param ~azure.storage.filedatalake.ContentSettings content_settings:
             ContentSettings object used to set file/directory properties.
         :keyword lease:
             If specified, set_file_system_metadata only succeeds if the
             file system's lease is active and matches this ID.
         :paramtype lease: ~azure.storage.filedatalake.aio.DataLakeLeaseClient or str
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_data_lake_lease_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_data_lake_lease_async.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_list_paths_helper.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_list_paths_helper.py`

 * *Files 0% similar despite different names*

```diff
@@ -120,15 +120,15 @@
     :ivar str path: Filters the results to return only paths under the specified path.
     :ivar int results_per_page: The maximum number of results retrieved per API call.
     :ivar str continuation_token: The continuation token to retrieve the next page of results.
     :ivar list(~azure.storage.filedatalake.PathProperties) current_page: The current page of listed results.
 
     :param callable command: Function to retrieve the next page of items.
     :param str path: Filters the results to return only paths under the specified path.
-    :param int max_results: The maximum number of psths to retrieve per
+    :param int max_results: The maximum number of paths to retrieve per
         call.
     :param str continuation_token: An opaque continuation token.
     """
 
     def __init__(
             self, command,
             recursive,
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/aio/_download_async.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/aio/_download_async.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
-from typing import AsyncIterator, Optional
+from typing import AsyncIterator, IO, Optional
 
 from .._deserialize import from_blob_properties
 
 
 class StorageStreamDownloader(object):
     """A streaming object to download from Azure Storage.
 
     :ivar str name:
         The name of the file being downloaded.
     :ivar ~azure.storage.filedatalake.FileProperties properties:
         The properties of the file being downloaded. If only a range of the data is being
         downloaded, this will be reflected in the properties.
     :ivar int size:
-        The size of the total data in the stream. This will be the byte range if speficied,
+        The size of the total data in the stream. This will be the byte range if specified,
         otherwise the total size of the file.
     """
 
     def __init__(self, downloader):
         self._downloader = downloader
         self.name = self._downloader.name
         self.properties = from_blob_properties(self._downloader.properties)  # pylint: disable=protected-access
@@ -39,31 +39,31 @@
 
     async def read(self, size: Optional[int] = -1) -> bytes:
         """
         Read up to size bytes from the stream and return them. If size
         is unspecified or is -1, all bytes will be read.
 
         :param size:
-            The number of bytes to download from the stream. Leave unsepcified
+            The number of bytes to download from the stream. Leave unspecified
             or set to -1 to download all bytes.
         :returns:
-            The requsted data as bytes. If the return value is empty, there is no more data to read.
+            The requested data as bytes. If the return value is empty, there is no more data to read.
         :rtype: bytes
         """
         return await self._downloader.read(size)
 
     async def readall(self) -> bytes:
         """Download the contents of this file.
 
         This operation is blocking until all data is downloaded.
         :rtype: bytes
         """
         return await self._downloader.readall()
 
-    async def readinto(self, stream) -> int:
+    async def readinto(self, stream: IO[bytes]) -> int:
         """Download the contents of this file to a stream.
 
         :param stream:
             The stream to download to. This can be an open file-handle,
             or any writable stream. The stream must be seekable if the download
             uses more than one parallel connection.
         :returns: The number of bytes read.
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_vendor.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_vendor.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_configuration.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/__init__.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_azure_data_lake_storage_restapi.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_azure_data_lake_storage_restapi.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_patch.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/_serialization.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/_serialization.py`

 * *Files 0% similar despite different names*

```diff
@@ -1501,15 +1501,15 @@
         :param str target_obj: The target object type to deserialize to.
         :param str/dict data: The response data to deseralize.
         :param str content_type: Swagger "produces" if available.
         """
         try:
             return self(target_obj, data, content_type=content_type)
         except:
-            _LOGGER.warning(
+            _LOGGER.debug(
                 "Ran into a deserialization error. Ignoring since this is failsafe deserialization", exc_info=True
             )
             return None
 
     @staticmethod
     def _unpack_content(raw_data, content_type=None):
         """Extract the correct structure for deserialization.
```

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/_models_py3.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/_models_py3.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/__init__.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/_patch.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/models/_azure_data_lake_storage_restapi_enums.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/_azure_data_lake_storage_restapi_enums.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/_configuration.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/_configuration.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/__init__.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/_azure_data_lake_storage_restapi.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/_azure_data_lake_storage_restapi.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/_patch.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_file_system_operations.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_file_system_operations.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/__init__.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_service_operations.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_service_operations.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_path_operations.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_path_operations.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/aio/operations/_patch.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/models/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_file_system_operations.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_file_system_operations.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/__init__.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_service_operations.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_service_operations.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_path_operations.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/operations/_path_operations.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/azure/storage/filedatalake/_generated/operations/_patch.py` & `azure-storage-file-datalake-12.9.1/azure/storage/filedatalake/_generated/aio/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_query.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_query.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_service.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_service.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_upload_download_async.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_upload_download.py`

 * *Files 7% similar despite different names*

```diff
@@ -3,127 +3,125 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 """
-FILE: datalake_samples_upload_download_async.py
+FILE: datalake_samples_upload_download.py
 DESCRIPTION:
     This sample demonstrates:
     * Set up a file system
     * Create file
     * Append data to the file
     * Flush data to the file
     * Get file properties
     * Download the uploaded data
     * Delete file system
 USAGE:
-    python datalake_samples_upload_download_async.py
+    python datalake_samples_upload_download.py
     Set the environment variables with your own values before running the sample:
     1) STORAGE_ACCOUNT_NAME - the storage account name
     2) STORAGE_ACCOUNT_KEY - the storage account key
 """
-import asyncio
+
 import os
 import random
 
-from azure.storage.filedatalake.aio import (
+from azure.storage.filedatalake import (
     DataLakeServiceClient,
 )
 SOURCE_FILE = 'SampleSource.txt'
 
-async def upload_download_sample(filesystem_client):
+def upload_download_sample(filesystem_client):
     # create a file before writing content to it
     file_name = "testfile"
     print("Creating a file named '{}'.".format(file_name))
     # [START create_file]
     file_client = filesystem_client.get_file_client(file_name)
-    await file_client.create_file()
+    file_client.create_file()
     # [END create_file]
 
     # prepare the file content with 4KB of random data
     file_content = get_random_bytes(4*1024)
 
     # append data to the file
     # the data remain uncommitted until flush is performed
     print("Uploading data to '{}'.".format(file_name))
-    await file_client.append_data(data=file_content[0:1024], offset=0, length=1024)
-    await file_client.append_data(data=file_content[1024:2048], offset=1024, length=1024)
+    file_client.append_data(data=file_content[0:1024], offset=0, length=1024)
+    file_client.append_data(data=file_content[1024:2048], offset=1024, length=1024)
     # [START append_data]
-    await file_client.append_data(data=file_content[2048:3072], offset=2048, length=1024)
+    file_client.append_data(data=file_content[2048:3072], offset=2048, length=1024)
     # [END append_data]
-    await file_client.append_data(data=file_content[3072:4096], offset=3072, length=1024)
+    file_client.append_data(data=file_content[3072:4096], offset=3072, length=1024)
 
     # data is only committed when flush is called
-    await file_client.flush_data(len(file_content))
+    file_client.flush_data(len(file_content))
 
     # Get file properties
     # [START get_file_properties]
-    properties = await file_client.get_file_properties()
+    properties = file_client.get_file_properties()
     # [END get_file_properties]
 
     # read the data back
     print("Downloading data from '{}'.".format(file_name))
     # [START read_file]
-    download = await file_client.download_file()
-    downloaded_bytes = await download.readall()
+    download = file_client.download_file()
+    downloaded_bytes = download.readall()
     # [END read_file]
 
     # verify the downloaded content
     if file_content == downloaded_bytes:
         print("The downloaded data is equal to the data uploaded.")
     else:
         print("Something went wrong.")
 
     # Rename the file
     # [START rename_file]
-    new_client = await file_client.rename_file(file_client.file_system_name + '/' + 'newname')
+    new_client = file_client.rename_file(file_client.file_system_name + '/' + 'newname')
     # [END rename_file]
 
     # download the renamed file in to local file
     with open(SOURCE_FILE, 'wb') as stream:
-        download = await new_client.download_file()
-        await download.readinto(stream)
+        download = new_client.download_file()
+        download.readinto(stream)
 
     # [START delete_file]
-    await new_client.delete_file()
+    new_client.delete_file()
     # [END delete_file]
 
 # help method to provide random bytes to serve as file content
 def get_random_bytes(size):
     rand = random.Random()
     result = bytearray(size)
     for i in range(size):
         result[i] = int(rand.random()*255)  # random() is consistent between python 2 and 3
     return bytes(result)
 
 
-async def run():
+def run():
     account_name = os.getenv('STORAGE_ACCOUNT_NAME', "")
     account_key = os.getenv('STORAGE_ACCOUNT_KEY', "")
 
     # set up the service client with the credentials from the environment variables
     service_client = DataLakeServiceClient(account_url="{}://{}.dfs.core.windows.net".format(
         "https",
         account_name
     ), credential=account_key)
 
-    async with service_client:
-        # generate a random name for testing purpose
-        fs_name = "testfs{}".format(random.randint(1, 1000))
-        print("Generating a test filesystem named '{}'.".format(fs_name))
-
-        # create the filesystem
-        filesystem_client = await service_client.create_file_system(file_system=fs_name)
-
-        # invoke the sample code
-        try:
-            await upload_download_sample(filesystem_client)
-        finally:
-            # clean up the demo filesystem
-            await filesystem_client.delete_file_system()
+    # generate a random name for testing purpose
+    fs_name = "testfs{}".format(random.randint(1, 1000))
+    print("Generating a test filesystem named '{}'.".format(fs_name))
+
+    # create the filesystem
+    filesystem_client = service_client.create_file_system(file_system=fs_name)
+
+    # invoke the sample code
+    try:
+        upload_download_sample(filesystem_client)
+    finally:
+        # clean up the demo filesystem
+        filesystem_client.delete_file_system()
 
 
 if __name__ == '__main__':
-    loop = asyncio.get_event_loop()
-    loop.run_until_complete(run())
+    run()
```

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_instantiate_client_async.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_instantiate_client_async.py`

 * *Files 3% similar despite different names*

```diff
@@ -36,9 +36,8 @@
 
 async def main():
     await instantiate_directory_client_from_conn_str()
     await instantiate_file_client_from_conn_str()
 
 
 if __name__ == '__main__':
-    loop = asyncio.get_event_loop()
-    loop.run_until_complete(main())
+    asyncio.run(main())
```

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control_recursive_async.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control_recursive_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -97,15 +97,15 @@
         await file_client.create_file()
 
     futures = [asyncio.ensure_future(create_file()) for _ in itertools.repeat(None, num_child_files)]
     await asyncio.wait(futures)
     print("Created {} files under the directory '{}'.".format(num_child_files, directory_client.path_name))
 
 
-async def run():
+async def main():
     account_name = os.getenv('STORAGE_ACCOUNT_NAME', "")
     account_key = os.getenv('STORAGE_ACCOUNT_KEY', "")
 
     # set up the service client with the credentials from the environment variables
     service_client = DataLakeServiceClient(account_url="{}://{}.dfs.core.windows.net".format(
         "https",
         account_name
@@ -124,9 +124,8 @@
             await recursive_access_control_sample(filesystem_client)
         finally:
             # clean up the demo filesystem
             await filesystem_client.delete_file_system()
 
 
 if __name__ == '__main__':
-    loop = asyncio.get_event_loop()
-    loop.run_until_complete(run())
+    asyncio.run(main())
```

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control_recursive.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control_recursive.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_file_system_async.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_file_system_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -213,20 +213,18 @@
 
             # [START delete_directory_from_file_system]
             await file_system_client.delete_directory("mydirectory")
             # [END delete_directory_from_file_system]
 
             await file_system_client.delete_file_system()
 
-async def run():
+async def main():
     sample = FileSystemSamplesAsync()
     await sample.file_system_sample()
     await sample.acquire_lease_on_file_system()
     await sample.set_metadata_on_file_system()
     await sample.list_paths_in_file_system()
     await sample.get_file_client_from_file_system()
     await sample.create_file_from_file_system()
 
 if __name__ == '__main__':
-    loop = asyncio.get_event_loop()
-    loop.run_until_complete(run())
-    loop.close()
+    asyncio.run(main())
```

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_file_system.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_file_system.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_instantiate_client.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_instantiate_client.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_directory.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_directory.py`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_directory_async.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_directory_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -81,15 +81,15 @@
         await file_client.create_file()
 
     futures = [asyncio.ensure_future(create_file()) for _ in itertools.repeat(None, num_child_files)]
     await asyncio.wait(futures)
     print("Created {} files under the directory '{}'.".format(num_child_files, directory_client.path_name))
 
 
-async def run():
+async def main():
     account_name = os.getenv('STORAGE_ACCOUNT_NAME', "")
     account_key = os.getenv('STORAGE_ACCOUNT_KEY', "")
 
     # set up the service client with the credentials from the environment variables
     service_client = DataLakeServiceClient(account_url="{}://{}.dfs.core.windows.net".format(
         "https",
         account_name
@@ -107,9 +107,8 @@
         try:
             await directory_sample(filesystem_client)
         finally:
             # clean up the demo filesystem
             await filesystem_client.delete_file_system()
 
 if __name__ == '__main__':
-    loop = asyncio.get_event_loop()
-    loop.run_until_complete(run())
+    asyncio.run(main())
```

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/README.md` & `azure-storage-file-datalake-12.9.1/samples/README.md`

 * *Files identical despite different names*

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_access_control_async.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_access_control_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -72,15 +72,15 @@
         await file_client.create_file()
 
     futures = [asyncio.ensure_future(create_file()) for _ in itertools.repeat(None, num_child_files)]
     await asyncio.wait(futures)
     print("Created {} files under the directory '{}'.".format(num_child_files, directory_client.path_name))
 
 
-async def run():
+async def main():
     account_name = os.getenv('STORAGE_ACCOUNT_NAME', "")
     account_key = os.getenv('STORAGE_ACCOUNT_KEY', "")
 
     # set up the service client with the credentials from the environment variables
     service_client = DataLakeServiceClient(account_url="{}://{}.dfs.core.windows.net".format(
         "https",
         account_name
@@ -99,9 +99,8 @@
             await access_control_sample(filesystem_client)
         finally:
             # clean up the demo filesystem
             await filesystem_client.delete_file_system()
 
 
 if __name__ == '__main__':
-    loop = asyncio.get_event_loop()
-    loop.run_until_complete(run())
+    asyncio.run(main())
```

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_upload_download.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_upload_download_async.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,125 +3,126 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 """
-FILE: datalake_samples_upload_download.py
+FILE: datalake_samples_upload_download_async.py
 DESCRIPTION:
     This sample demonstrates:
     * Set up a file system
     * Create file
     * Append data to the file
     * Flush data to the file
     * Get file properties
     * Download the uploaded data
     * Delete file system
 USAGE:
-    python datalake_samples_upload_download.py
+    python datalake_samples_upload_download_async.py
     Set the environment variables with your own values before running the sample:
     1) STORAGE_ACCOUNT_NAME - the storage account name
     2) STORAGE_ACCOUNT_KEY - the storage account key
 """
-
+import asyncio
 import os
 import random
 
-from azure.storage.filedatalake import (
+from azure.storage.filedatalake.aio import (
     DataLakeServiceClient,
 )
 SOURCE_FILE = 'SampleSource.txt'
 
-def upload_download_sample(filesystem_client):
+async def upload_download_sample(filesystem_client):
     # create a file before writing content to it
     file_name = "testfile"
     print("Creating a file named '{}'.".format(file_name))
     # [START create_file]
     file_client = filesystem_client.get_file_client(file_name)
-    file_client.create_file()
+    await file_client.create_file()
     # [END create_file]
 
     # prepare the file content with 4KB of random data
     file_content = get_random_bytes(4*1024)
 
     # append data to the file
     # the data remain uncommitted until flush is performed
     print("Uploading data to '{}'.".format(file_name))
-    file_client.append_data(data=file_content[0:1024], offset=0, length=1024)
-    file_client.append_data(data=file_content[1024:2048], offset=1024, length=1024)
+    await file_client.append_data(data=file_content[0:1024], offset=0, length=1024)
+    await file_client.append_data(data=file_content[1024:2048], offset=1024, length=1024)
     # [START append_data]
-    file_client.append_data(data=file_content[2048:3072], offset=2048, length=1024)
+    await file_client.append_data(data=file_content[2048:3072], offset=2048, length=1024)
     # [END append_data]
-    file_client.append_data(data=file_content[3072:4096], offset=3072, length=1024)
+    await file_client.append_data(data=file_content[3072:4096], offset=3072, length=1024)
 
     # data is only committed when flush is called
-    file_client.flush_data(len(file_content))
+    await file_client.flush_data(len(file_content))
 
     # Get file properties
     # [START get_file_properties]
-    properties = file_client.get_file_properties()
+    properties = await file_client.get_file_properties()
     # [END get_file_properties]
 
     # read the data back
     print("Downloading data from '{}'.".format(file_name))
     # [START read_file]
-    download = file_client.download_file()
-    downloaded_bytes = download.readall()
+    download = await file_client.download_file()
+    downloaded_bytes = await download.readall()
     # [END read_file]
 
     # verify the downloaded content
     if file_content == downloaded_bytes:
         print("The downloaded data is equal to the data uploaded.")
     else:
         print("Something went wrong.")
 
     # Rename the file
     # [START rename_file]
-    new_client = file_client.rename_file(file_client.file_system_name + '/' + 'newname')
+    new_client = await file_client.rename_file(file_client.file_system_name + '/' + 'newname')
     # [END rename_file]
 
     # download the renamed file in to local file
     with open(SOURCE_FILE, 'wb') as stream:
-        download = new_client.download_file()
-        download.readinto(stream)
+        download = await new_client.download_file()
+        await download.readinto(stream)
 
     # [START delete_file]
-    new_client.delete_file()
+    await new_client.delete_file()
     # [END delete_file]
 
 # help method to provide random bytes to serve as file content
 def get_random_bytes(size):
     rand = random.Random()
     result = bytearray(size)
     for i in range(size):
         result[i] = int(rand.random()*255)  # random() is consistent between python 2 and 3
     return bytes(result)
 
 
-def run():
+async def main():
     account_name = os.getenv('STORAGE_ACCOUNT_NAME', "")
     account_key = os.getenv('STORAGE_ACCOUNT_KEY', "")
 
     # set up the service client with the credentials from the environment variables
     service_client = DataLakeServiceClient(account_url="{}://{}.dfs.core.windows.net".format(
         "https",
         account_name
     ), credential=account_key)
 
-    # generate a random name for testing purpose
-    fs_name = "testfs{}".format(random.randint(1, 1000))
-    print("Generating a test filesystem named '{}'.".format(fs_name))
-
-    # create the filesystem
-    filesystem_client = service_client.create_file_system(file_system=fs_name)
-
-    # invoke the sample code
-    try:
-        upload_download_sample(filesystem_client)
-    finally:
-        # clean up the demo filesystem
-        filesystem_client.delete_file_system()
+    async with service_client:
+        # generate a random name for testing purpose
+        fs_name = "testfs{}".format(random.randint(1, 1000))
+        print("Generating a test filesystem named '{}'.".format(fs_name))
+
+        # create the filesystem
+        filesystem_client = await service_client.create_file_system(file_system=fs_name)
+
+        # invoke the sample code
+        try:
+            await upload_download_sample(filesystem_client)
+        finally:
+            # clean up the demo filesystem
+            await filesystem_client.delete_file_system()
 
 
 if __name__ == '__main__':
-    run()
+    asyncio.run(main())
```

## Comparing `azure-storage-file-datalake-12.9.0b1/samples/datalake_samples_service_async.py` & `azure-storage-file-datalake-12.9.1/samples/datalake_samples_service_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,15 +33,15 @@
 account_name = os.getenv('STORAGE_ACCOUNT_NAME', "")
 active_directory_application_id = os.getenv("ACTIVE_DIRECTORY_APPLICATION_ID")
 active_directory_application_secret = os.getenv("ACTIVE_DIRECTORY_APPLICATION_SECRET")
 active_directory_tenant_id = os.getenv("ACTIVE_DIRECTORY_TENANT_ID")
 
 #--Begin DataLake Service Samples-----------------------------------------------------------------
 
-async def data_lake_service_sample():
+async def main():
 
     # Instantiate a DataLakeServiceClient using a connection string
     # [START create_datalake_service_client]
     from azure.storage.filedatalake.aio import DataLakeServiceClient
     datalake_service_client = DataLakeServiceClient.from_connection_string(connection_string)
     # [END create_datalake_service_client]
 
@@ -109,10 +109,8 @@
         await datalake_service_client.delete_file_system("filesystem")
         # [END delete_file_system_from_service_client]
         await file_system_client.delete_file_system()
 
     await token_credential.close()
 
 if __name__ == '__main__':
-    loop = asyncio.get_event_loop()
-    loop.run_until_complete(data_lake_service_sample())
-
+    asyncio.run(main())
```


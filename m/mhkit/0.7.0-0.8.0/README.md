# Comparing `tmp/mhkit-0.7.0.tar.gz` & `tmp/mhkit-0.8.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "mhkit-0.7.0.tar", last modified: Fri Aug 11 19:21:59 2023, max compression
+gzip compressed data, was "mhkit-0.8.0.tar", last modified: Wed May  8 13:42:53 2024, max compression
```

## Comparing `mhkit-0.7.0.tar` & `mhkit-0.8.0.tar`

### file list

```diff
@@ -1,160 +1,175 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.622742 mhkit-0.7.0/
--rw-r--r--   0 runner    (1001) docker     (123)     1833 2023-08-11 19:21:43.000000 mhkit-0.7.0/LICENSE.md
--rw-r--r--   0 runner    (1001) docker     (123)     1710 2023-08-11 19:21:59.622742 mhkit-0.7.0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     6601 2023-08-11 19:21:43.000000 mhkit-0.7.0/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.582740 mhkit-0.7.0/mhkit/
--rw-r--r--   0 runner    (1001) docker     (123)      895 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.586740 mhkit-0.7.0/mhkit/dolfyn/
--rw-r--r--   0 runner    (1001) docker     (123)      472 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.586740 mhkit-0.7.0/mhkit/dolfyn/adp/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/adp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      176 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/adp/api.py
--rw-r--r--   0 runner    (1001) docker     (123)    13145 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/adp/clean.py
--rw-r--r--   0 runner    (1001) docker     (123)    39177 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/adp/turbulence.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.586740 mhkit-0.7.0/mhkit/dolfyn/adv/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/adv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      256 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/adv/api.py
--rw-r--r--   0 runner    (1001) docker     (123)    10426 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/adv/clean.py
--rw-r--r--   0 runner    (1001) docker     (123)    20030 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/adv/motion.py
--rw-r--r--   0 runner    (1001) docker     (123)    24931 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/adv/turbulence.py
--rw-r--r--   0 runner    (1001) docker     (123)    17937 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/binned.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.590741 mhkit-0.7.0/mhkit/dolfyn/io/
--rw-r--r--   0 runner    (1001) docker     (123)      362 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12087 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/api.py
--rw-r--r--   0 runner    (1001) docker     (123)    12436 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/base.py
--rw-r--r--   0 runner    (1001) docker     (123)    41519 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/nortek.py
--rw-r--r--   0 runner    (1001) docker     (123)    23842 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/nortek2.py
--rw-r--r--   0 runner    (1001) docker     (123)    13364 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/nortek2_defs.py
--rw-r--r--   0 runner    (1001) docker     (123)    15882 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/nortek2_lib.py
--rw-r--r--   0 runner    (1001) docker     (123)    12203 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/nortek_defs.py
--rw-r--r--   0 runner    (1001) docker     (123)    54521 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/rdi.py
--rw-r--r--   0 runner    (1001) docker     (123)     9392 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/rdi_defs.py
--rw-r--r--   0 runner    (1001) docker     (123)     3222 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/io/rdi_lib.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.590741 mhkit-0.7.0/mhkit/dolfyn/rotate/
--rw-r--r--   0 runner    (1001) docker     (123)      227 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/rotate/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10651 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/rotate/api.py
--rw-r--r--   0 runner    (1001) docker     (123)       79 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/rotate/awac.py
--rw-r--r--   0 runner    (1001) docker     (123)    10925 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/rotate/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     6155 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/rotate/rdi.py
--rw-r--r--   0 runner    (1001) docker     (123)     4539 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/rotate/signature.py
--rw-r--r--   0 runner    (1001) docker     (123)    11147 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/rotate/vector.py
--rw-r--r--   0 runner    (1001) docker     (123)     6790 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/time.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.590741 mhkit-0.7.0/mhkit/dolfyn/tools/
--rw-r--r--   0 runner    (1001) docker     (123)       71 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/tools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8903 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/tools/fft.py
--rw-r--r--   0 runner    (1001) docker     (123)     9971 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/tools/misc.py
--rw-r--r--   0 runner    (1001) docker     (123)    32287 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/dolfyn/velocity.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.594741 mhkit-0.7.0/mhkit/loads/
--rw-r--r--   0 runner    (1001) docker     (123)       96 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/loads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    24276 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/loads/extreme.py
--rw-r--r--   0 runner    (1001) docker     (123)     5915 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/loads/general.py
--rw-r--r--   0 runner    (1001) docker     (123)     5491 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/loads/graphics.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.594741 mhkit-0.7.0/mhkit/mooring/
--rw-r--r--   0 runner    (1001) docker     (123)       84 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/mooring/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8479 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/mooring/graphics.py
--rw-r--r--   0 runner    (1001) docker     (123)     5959 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/mooring/io.py
--rw-r--r--   0 runner    (1001) docker     (123)     2299 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/mooring/main.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.594741 mhkit-0.7.0/mhkit/power/
--rw-r--r--   0 runner    (1001) docker     (123)       73 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/power/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3553 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/power/characteristics.py
--rw-r--r--   0 runner    (1001) docker     (123)     6255 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/power/quality.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.594741 mhkit-0.7.0/mhkit/qc/
--rw-r--r--   0 runner    (1001) docker     (123)      126 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/qc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.594741 mhkit-0.7.0/mhkit/river/
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/river/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6729 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/river/graphics.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.598741 mhkit-0.7.0/mhkit/river/io/
--rw-r--r--   0 runner    (1001) docker     (123)       64 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/river/io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    26022 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/river/io/d3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3667 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/river/io/usgs.py
--rw-r--r--   0 runner    (1001) docker     (123)     4752 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/river/performance.py
--rw-r--r--   0 runner    (1001) docker     (123)     6821 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/river/resource.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.598741 mhkit-0.7.0/mhkit/tests/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.602741 mhkit-0.7.0/mhkit/tests/dolfyn/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1637 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     6231 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_analysis.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     4114 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_clean.py
--rw-r--r--   0 runner    (1001) docker     (123)     3455 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_motion.py
--rw-r--r--   0 runner    (1001) docker     (123)     4133 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_orient.py
--rw-r--r--   0 runner    (1001) docker     (123)     6387 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_read_adp.py
--rw-r--r--   0 runner    (1001) docker     (123)     1554 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_read_adv.py
--rw-r--r--   0 runner    (1001) docker     (123)     3724 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_read_io.py
--rw-r--r--   0 runner    (1001) docker     (123)     7478 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_rotate_adp.py
--rw-r--r--   0 runner    (1001) docker     (123)     6440 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_rotate_adv.py
--rw-r--r--   0 runner    (1001) docker     (123)     1944 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_shortcuts.py
--rw-r--r--   0 runner    (1001) docker     (123)     1797 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_time.py
--rw-r--r--   0 runner    (1001) docker     (123)     3647 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)     4205 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/dolfyn/test_vs_nortek.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.602741 mhkit-0.7.0/mhkit/tests/loads/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/loads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8892 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/loads/test_loads.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.602741 mhkit-0.7.0/mhkit/tests/power/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/power/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4796 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/power/test_power.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.602741 mhkit-0.7.0/mhkit/tests/river/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/river/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9717 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/river/test_io.py
--rw-r--r--   0 runner    (1001) docker     (123)     2716 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/river/test_performance.py
--rw-r--r--   0 runner    (1001) docker     (123)     7186 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/river/test_resource.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.606741 mhkit-0.7.0/mhkit/tests/tidal/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/tidal/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4126 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/tidal/test_io.py
--rw-r--r--   0 runner    (1001) docker     (123)     4027 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/tidal/test_performance.py
--rw-r--r--   0 runner    (1001) docker     (123)     3599 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/tidal/test_resource.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.606741 mhkit-0.7.0/mhkit/tests/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5570 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/utils/test_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.606741 mhkit-0.7.0/mhkit/tests/wave/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.610741 mhkit-0.7.0/mhkit/tests/wave/io/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/io/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.610741 mhkit-0.7.0/mhkit/tests/wave/io/hindcast/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/io/hindcast/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8040 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/io/hindcast/test_hindcast.py
--rw-r--r--   0 runner    (1001) docker     (123)     6868 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/io/hindcast/test_wind_toolkit.py
--rw-r--r--   0 runner    (1001) docker     (123)     6478 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/io/test_cdip.py
--rw-r--r--   0 runner    (1001) docker     (123)    10603 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/io/test_ndbc.py
--rw-r--r--   0 runner    (1001) docker     (123)     2915 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/io/test_swan.py
--rw-r--r--   0 runner    (1001) docker     (123)     3531 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/io/test_wecsim.py
--rw-r--r--   0 runner    (1001) docker     (123)     9664 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/test_contours.py
--rw-r--r--   0 runner    (1001) docker     (123)     4548 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/test_performance.py
--rw-r--r--   0 runner    (1001) docker     (123)    13691 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/test_resource_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     7613 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tests/wave/test_resource_spectrum.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.610741 mhkit-0.7.0/mhkit/tidal/
--rw-r--r--   0 runner    (1001) docker     (123)      130 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tidal/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)       29 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tidal/d3d.py
--rw-r--r--   0 runner    (1001) docker     (123)    16199 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tidal/graphics.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.614742 mhkit-0.7.0/mhkit/tidal/io/
--rw-r--r--   0 runner    (1001) docker     (123)       32 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tidal/io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7029 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tidal/io/noaa.py
--rw-r--r--   0 runner    (1001) docker     (123)    21955 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tidal/performance.py
--rw-r--r--   0 runner    (1001) docker     (123)     6431 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/tidal/resource.py
--rw-r--r--   0 runner    (1001) docker     (123)     9509 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.614742 mhkit-0.7.0/mhkit/wave/
--rw-r--r--   0 runner    (1001) docker     (123)      156 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    69987 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/contours.py
--rw-r--r--   0 runner    (1001) docker     (123)    26635 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/graphics.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.614742 mhkit-0.7.0/mhkit/wave/io/
--rw-r--r--   0 runner    (1001) docker     (123)      160 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17895 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/io/cdip.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.622742 mhkit-0.7.0/mhkit/wave/io/hindcast/
--rw-r--r--   0 runner    (1001) docker     (123)      310 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/io/hindcast/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15968 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/io/hindcast/hindcast.py
--rw-r--r--   0 runner    (1001) docker     (123)    11619 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/io/hindcast/wind_toolkit.py
--rw-r--r--   0 runner    (1001) docker     (123)    37345 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/io/ndbc.py
--rw-r--r--   0 runner    (1001) docker     (123)     9951 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/io/swan.py
--rw-r--r--   0 runner    (1001) docker     (123)    18982 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/io/wecsim.py
--rw-r--r--   0 runner    (1001) docker     (123)    12974 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/performance.py
--rw-r--r--   0 runner    (1001) docker     (123)    26769 2023-08-11 19:21:46.000000 mhkit-0.7.0/mhkit/wave/resource.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-11 19:21:59.586740 mhkit-0.7.0/mhkit.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     1710 2023-08-11 19:21:59.000000 mhkit-0.7.0/mhkit.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     3643 2023-08-11 19:21:59.000000 mhkit-0.7.0/mhkit.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-08-11 19:21:59.000000 mhkit-0.7.0/mhkit.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-08-11 19:21:59.000000 mhkit-0.7.0/mhkit.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)      239 2023-08-11 19:21:59.000000 mhkit-0.7.0/mhkit.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        6 2023-08-11 19:21:59.000000 mhkit-0.7.0/mhkit.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-08-11 19:21:59.622742 mhkit-0.7.0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     3355 2023-08-11 19:21:46.000000 mhkit-0.7.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.391112 mhkit-0.8.0/
+-rw-r--r--   0 runner    (1001) docker     (127)     1833 2024-05-08 13:42:47.000000 mhkit-0.8.0/LICENSE.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2200 2024-05-08 13:42:53.391112 mhkit-0.8.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     7564 2024-05-08 13:42:47.000000 mhkit-0.8.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.367112 mhkit-0.8.0/mhkit/
+-rw-r--r--   0 runner    (1001) docker     (127)      896 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.367112 mhkit-0.8.0/mhkit/dolfyn/
+-rw-r--r--   0 runner    (1001) docker     (127)      493 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.367112 mhkit-0.8.0/mhkit/dolfyn/adp/
+-rw-r--r--   0 runner    (1001) docker     (127)       18 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/adp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      176 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/adp/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13102 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/adp/clean.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40245 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/adp/turbulence.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.367112 mhkit-0.8.0/mhkit/dolfyn/adv/
+-rw-r--r--   0 runner    (1001) docker     (127)       18 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/adv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      256 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/adv/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10291 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/adv/clean.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19411 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/adv/motion.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25223 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/adv/turbulence.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17923 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/binned.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.371112 mhkit-0.8.0/mhkit/dolfyn/io/
+-rw-r--r--   0 runner    (1001) docker     (127)      362 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12596 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12019 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45250 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/nortek.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29084 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/nortek2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16291 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/nortek2_defs.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17975 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/nortek2_lib.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13412 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/nortek_defs.py
+-rw-r--r--   0 runner    (1001) docker     (127)    57298 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/rdi.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10208 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/rdi_defs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3065 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/io/rdi_lib.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.371112 mhkit-0.8.0/mhkit/dolfyn/rotate/
+-rw-r--r--   0 runner    (1001) docker     (127)      227 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/rotate/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10421 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/rotate/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)       79 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/rotate/awac.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11198 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/rotate/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6127 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/rotate/rdi.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4692 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/rotate/signature.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11151 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/rotate/vector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6811 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/time.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.375112 mhkit-0.8.0/mhkit/dolfyn/tools/
+-rw-r--r--   0 runner    (1001) docker     (127)       71 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9334 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/tools/fft.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9914 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/tools/misc.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33980 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/dolfyn/velocity.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.375112 mhkit-0.8.0/mhkit/loads/
+-rw-r--r--   0 runner    (1001) docker     (127)      494 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/loads/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.375112 mhkit-0.8.0/mhkit/loads/extreme/
+-rw-r--r--   0 runner    (1001) docker     (127)      975 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/loads/extreme/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10575 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/loads/extreme/extremes.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16902 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/loads/extreme/mler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17443 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/loads/extreme/peaks.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1839 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/loads/extreme/sample.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8863 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/loads/general.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7201 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/loads/graphics.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.375112 mhkit-0.8.0/mhkit/mooring/
+-rw-r--r--   0 runner    (1001) docker     (127)       84 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/mooring/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8564 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/mooring/graphics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5899 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/mooring/io.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2312 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/mooring/main.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.375112 mhkit-0.8.0/mhkit/power/
+-rw-r--r--   0 runner    (1001) docker     (127)       94 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/power/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9225 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/power/characteristics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13802 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/power/quality.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.375112 mhkit-0.8.0/mhkit/qc/
+-rw-r--r--   0 runner    (1001) docker     (127)      145 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/qc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.375112 mhkit-0.8.0/mhkit/river/
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/river/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7015 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/river/graphics.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.379112 mhkit-0.8.0/mhkit/river/io/
+-rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/river/io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28996 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/river/io/d3d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5377 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/river/io/usgs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5206 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/river/performance.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9013 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/river/resource.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.379112 mhkit-0.8.0/mhkit/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.379112 mhkit-0.8.0/mhkit/tests/dolfyn/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1650 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7777 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_analysis.py
+-rw-r--r--   0 runner    (1001) docker     (127)      723 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4080 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_clean.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_motion.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4771 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_orient.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7544 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_read_adp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1561 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_read_adv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4088 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_read_io.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7353 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_rotate_adp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6337 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_rotate_adv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1931 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_shortcuts.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1798 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_time.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9020 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4019 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/dolfyn/test_vs_nortek.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.383112 mhkit-0.8.0/mhkit/tests/loads/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/loads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1524 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/loads/test_extreme.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18221 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/loads/test_loads.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.383112 mhkit-0.8.0/mhkit/tests/power/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/power/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7985 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/power/test_power.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.383112 mhkit-0.8.0/mhkit/tests/river/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/river/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10792 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/river/test_io_d3d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2208 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/river/test_io_usgs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2797 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/river/test_performance.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14187 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/river/test_resource.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.383112 mhkit-0.8.0/mhkit/tests/tidal/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/tidal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5498 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/tidal/test_io.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6525 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/tidal/test_performance.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3484 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/tidal/test_resource.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.383112 mhkit-0.8.0/mhkit/tests/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6850 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/utils/test_cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4138 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/utils/test_upcrossing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9569 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/utils/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.383112 mhkit-0.8.0/mhkit/tests/wave/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.387112 mhkit-0.8.0/mhkit/tests/wave/io/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/io/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.387112 mhkit-0.8.0/mhkit/tests/wave/io/hindcast/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/io/hindcast/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7888 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/io/hindcast/test_hindcast.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11951 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/io/hindcast/test_wind_toolkit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6547 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/io/test_cdip.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10993 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/io/test_ndbc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3355 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/io/test_swan.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3584 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/io/test_wecsim.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19001 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/test_contours.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4542 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/test_performance.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14731 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/test_resource_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7505 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tests/wave/test_resource_spectrum.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.387112 mhkit-0.8.0/mhkit/tidal/
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tidal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16123 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tidal/graphics.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.387112 mhkit-0.8.0/mhkit/tidal/io/
+-rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tidal/io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)       33 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tidal/io/d3d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9900 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tidal/io/noaa.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20568 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tidal/performance.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6543 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/tidal/resource.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.387112 mhkit-0.8.0/mhkit/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      526 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9222 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/utils/cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8898 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/utils/stat_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1567 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/utils/time_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7273 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/utils/type_handling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6391 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/utils/upcrossing.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.391112 mhkit-0.8.0/mhkit/wave/
+-rw-r--r--   0 runner    (1001) docker     (127)      157 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    74077 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/contours.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29630 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/graphics.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.391112 mhkit-0.8.0/mhkit/wave/io/
+-rw-r--r--   0 runner    (1001) docker     (127)      161 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21274 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/io/cdip.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.391112 mhkit-0.8.0/mhkit/wave/io/hindcast/
+-rw-r--r--   0 runner    (1001) docker     (127)      321 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/io/hindcast/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18283 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/io/hindcast/hindcast.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18312 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/io/hindcast/wind_toolkit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    42544 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/io/ndbc.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11859 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/io/swan.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19253 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/io/wecsim.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15826 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/performance.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38688 2024-05-08 13:42:49.000000 mhkit-0.8.0/mhkit/wave/resource.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-08 13:42:53.391112 mhkit-0.8.0/mhkit.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)     2200 2024-05-08 13:42:53.000000 mhkit-0.8.0/mhkit.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     4052 2024-05-08 13:42:53.000000 mhkit-0.8.0/mhkit.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-08 13:42:53.000000 mhkit-0.8.0/mhkit.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-08 13:42:53.000000 mhkit-0.8.0/mhkit.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (127)      205 2024-05-08 13:42:53.000000 mhkit-0.8.0/mhkit.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        6 2024-05-08 13:42:53.000000 mhkit-0.8.0/mhkit.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-08 13:42:53.391112 mhkit-0.8.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     2958 2024-05-08 13:42:49.000000 mhkit-0.8.0/setup.py
```

### Comparing `mhkit-0.7.0/LICENSE.md` & `mhkit-0.8.0/LICENSE.md`

 * *Files identical despite different names*

### Comparing `mhkit-0.7.0/README.md` & `mhkit-0.8.0/README.md`

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-![](figures/logo.png) MHKiT-Python
-=====================================
+# ![](logo.png) MHKiT-Python
 
 <p align="left">
     <a href=https://github.com/MHKiT-Software/MHKiT-Python/actions/workflows/main.yml>
         <img src="https://github.com/MHKiT-Software/MHKiT-Python/actions/workflows/main.yml/badge.svg">
     </a>
     <a href=https://coveralls.io/github/MHKiT-Software/MHKiT-Python?branch=master>
         <img src="https://coveralls.io/repos/github/MHKiT-Software/MHKiT-Python/badge.svg?branch=master">
@@ -12,78 +11,96 @@
         <img src="https://pepy.tech/badge/mhkit">
     </a>
     <a href=https://doi.org/10.5281/zenodo.3924683>
         <img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3924683.svg">
     </a>
 </p>
 
-MHKiT-Python is a Python package designed for marine renewable energy applications to assist in 
-data processing and visualization.  The software package include functionality for:
+MHKiT-Python is a Python package designed for marine renewable energy applications to assist in
+data processing and visualization. The software package include functionality for:
 
-* Data processing
-* Data visualization
-* Data quality control
-* Resource assessment
-* Device performance
-* Device loads
+- Data processing
+- Data visualization
+- Data quality control
+- Resource assessment
+- Device performance
+- Device loads
+
+## Documentation
 
-Documentation
-------------------
 MHKiT-Python documentation includes overview information, installation instructions, API documentation, and examples.
 See the [MHKiT documentation](https://mhkit-software.github.io/MHKiT) for more information.
 
-Installation
-------------------------
-MHKiT-Python requires Python (3.7, 3.8, or 3.9) along with several Python 
-package dependencies.  MHKiT-Python can be installed from PyPI using the command ``pip install mhkit``.
+## Installation
+
+MHKiT-Python requires Python (3.8, 3.9, 3.10, 3.11) along with several Python
+package dependencies. MHKiT-Python can be installed from PyPI using the command:
+
+`pip install mhkit`
+
 See [installation instructions](https://mhkit-software.github.io/MHKiT/installation.html) for more information.
 
-Copyright and license
-------------------------
-MHKiT-Python is copyright through the National Renewable Energy Laboratory, 
-Pacific Northwest National Laboratory, and Sandia National Laboratories. 
+## Copyright and license
+
+MHKiT-Python is copyright through the National Renewable Energy Laboratory,
+Pacific Northwest National Laboratory, and Sandia National Laboratories.
 The software is distributed under the Revised BSD License.
 See [copyright and license](LICENSE.md) for more information.
 
-Issues
-------------------------
+## Issues
+
 The GitHub platform has the Issues feature that is used to track ideas, feedback, tasks, and/or bugs. To submit an Issue, follow the steps below. More information about GitHub Issues can be found [here](https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues)
+
 1. Navigate to the [MHKiT-Python main page](https://github.com/MHKiT-Software/MHKiT-Python)
 2. 2.Under the repository name (upper left), click **Issues**.
 3. Click **New Issue**.
 4. If the Issue is a bug, use the **Bug report** template and click **Get started**, otherwise click on the **Open a blank issue** link.
 5. Provide a **Title** and **description** for the issue. Be sure the title is relevant to the issue and that the description is clear and provided with sufficient detail.
 6. When you're finished, click **Submit new issue**. The developers will follow-up once the issue is addressed.
 
-Creating a fork
-------------------------
+## Creating a fork
+
 The GitHub platform has the Fork feature that facilitates code modification and contributions. A fork is a new repository that shares code and visibility settings with the original upstream repository. To fork MHKiT-Python, follow the steps below. More information about GitHub Forks can be found [here](https://docs.github.com/en/get-started/quickstart/fork-a-repo)
+
 1. Navigate to the [MHKiT-Python main page](https://github.com/MHKiT-Software/MHKiT-Python)
 2. Under the repository name (upper left), click **Fork**.
 3. Select an owner for the forked repository.
 4. Specify a name for the fork. By default, forks are named the same as their upstream repositories.
 5. Add a description of your fork (optional).
 6. Choose whether to copy only the default branch or all branches to the new fork. You will only need copy the default branch to contribute to MHKiT-Python.
 7. When you're finished, click **Create fork**. You will now have a fork of the MHKiT-Python repository.
 
-Creating a branch
-------------------------
+## Creating a branch
+
 The GitHub platform has the branch feature that facilitates code contributions and collaboration amongst developers. A branch isolates development work without affecting other branches in the repository. Each repository has one default branch, and can have multiple other branches. To create a branch of your forked MHKiT-Python repository, follow the steps below. More information about GitHub branches can be found [here](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-branches)
+
 1. Navigate to your fork of MHKiT-Python (see instructions above)
 2. Above the list of files, click **Branches**.
-3. Click **New Branch**. 
+3. Click **New Branch**.
 4. Enter a name for the branch. Be sure to select **MHKiT-Software/MHKiT-Python:master** as the source.
 5. Click **Create branch**. You will now have a branch on your fork of MHKiT-Python that you can use to work with the code base.
 
-Creating a pull request
-------------------------
+## Creating a pull request
+
 The GitHub platform has the pull request feature that allows you to propose changes to a repository such as MHKiT-Python. The pull request will allow the repository administrators to evaluate the pull request. To create a pull request for MHKiT-Python repository, follow the steps below. More information about GitHub pull requests can be found [here](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request)
+
 1. Navigate to the [MHKiT-Python main page](https://github.com/MHKiT-Software/MHKiT-Python)
 2. Above the list of files, click **Pull request**.
-3. On the compare page, click **Compare accross forks**. 
-4. In the "base branch" drop-down menu, select the branch of the upstream repository you'd like to merge changes into. 
+3. On the compare page, click **Compare accross forks**.
+4. In the "base branch" drop-down menu, select the branch of the upstream repository you'd like to merge changes into.
 5. In the "head fork" drop-down menu, select your fork, then use the "compare branch" drop-down menu to select the branch you made your changes in.
 6. Type a title and description for your pull request.
 7. If you want to allow anyone with push access to the upstream repository to make changes to your pull request, select **Allow edits from maintainers**.
 8. To create a pull request that is ready for review, click **Create Pull Request**. To create a draft pull request, use the drop-down and select **Create Draft Pull Request**, then click **Draft Pull Request**. More information about draft pull requests can be found [here](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests#draft-pull-requests)
 9. MHKiT-Python adminstrators will review your pull request and contact you if needed.
 
+## Code Formatting in MHKiT
+
+MHKiT adheres to the "black" code formatting standard to maintain a consistent and readable code style. Developers contributing to MHKiT have several options to ensure their code meets this standard:
+
+1. **Manual Formatting with Black**: Install the 'black' formatter and run it manually from the terminal to format your code. This can be done by executing a command like `black [file or directory]`.
+
+2. **IDE Extension**: If you are using an Integrated Development Environment (IDE) like Visual Studio Code (VS Code), you can install the 'black' formatter as an extension. This allows for automatic formatting of code within the IDE.
+
+3. **Pre-Commit Hook**: Enable the pre-commit hook in your development environment. This automatically formats your code with 'black' each time you make a commit, ensuring that all committed code conforms to the formatting standard.
+
+For detailed instructions on installing and using 'black', please refer to the [Black Documentation](https://black.readthedocs.io/en/stable/). This resource provides comprehensive guidance on installation, usage, and configuration of the formatter.
```

#### html2text {}

```diff
@@ -1,82 +1,95 @@
-![](figures/logo.png) MHKiT-Python =====================================
+# ![](logo.png) MHKiT-Python
 _[_h_t_t_p_s_:_/_/_g_i_t_h_u_b_._c_o_m_/_M_H_K_i_T_-_S_o_f_t_w_a_r_e_/_M_H_K_i_T_-_P_y_t_h_o_n_/_a_c_t_i_o_n_s_/_w_o_r_k_f_l_o_w_s_/_m_a_i_n_._y_m_l_/
 _b_a_d_g_e_._s_v_g_]_[_h_t_t_p_s_:_/_/_c_o_v_e_r_a_l_l_s_._i_o_/_r_e_p_o_s_/_g_i_t_h_u_b_/_M_H_K_i_T_-_S_o_f_t_w_a_r_e_/_M_H_K_i_T_-_P_y_t_h_o_n_/
 _b_a_d_g_e_._s_v_g_?_b_r_a_n_c_h_=_m_a_s_t_e_r_]_[_h_t_t_p_s_:_/_/_p_e_p_y_._t_e_c_h_/_b_a_d_g_e_/_m_h_k_i_t_]_[_h_t_t_p_s_:_/_/_z_e_n_o_d_o_._o_r_g_/
 _b_a_d_g_e_/_D_O_I_/_1_0_._5_2_8_1_/_z_e_n_o_d_o_._3_9_2_4_6_8_3_._s_v_g_]
 MHKiT-Python is a Python package designed for marine renewable energy
 applications to assist in data processing and visualization. The software
-package include functionality for: * Data processing * Data visualization *
-Data quality control * Resource assessment * Device performance * Device loads
-Documentation ------------------ MHKiT-Python documentation includes overview
-information, installation instructions, API documentation, and examples. See
-the [MHKiT documentation](https://mhkit-software.github.io/MHKiT) for more
-information. Installation ------------------------ MHKiT-Python requires Python
-(3.7, 3.8, or 3.9) along with several Python package dependencies. MHKiT-Python
-can be installed from PyPI using the command ``pip install mhkit``. See
-[installation instructions](https://mhkit-software.github.io/MHKiT/
-installation.html) for more information. Copyright and license ----------------
--------- MHKiT-Python is copyright through the National Renewable Energy
-Laboratory, Pacific Northwest National Laboratory, and Sandia National
+package include functionality for: - Data processing - Data visualization -
+Data quality control - Resource assessment - Device performance - Device loads
+## Documentation MHKiT-Python documentation includes overview information,
+installation instructions, API documentation, and examples. See the [MHKiT
+documentation](https://mhkit-software.github.io/MHKiT) for more information. ##
+Installation MHKiT-Python requires Python (3.8, 3.9, 3.10, 3.11) along with
+several Python package dependencies. MHKiT-Python can be installed from PyPI
+using the command: `pip install mhkit` See [installation instructions](https://
+mhkit-software.github.io/MHKiT/installation.html) for more information. ##
+Copyright and license MHKiT-Python is copyright through the National Renewable
+Energy Laboratory, Pacific Northwest National Laboratory, and Sandia National
 Laboratories. The software is distributed under the Revised BSD License. See
-[copyright and license](LICENSE.md) for more information. Issues --------------
----------- The GitHub platform has the Issues feature that is used to track
-ideas, feedback, tasks, and/or bugs. To submit an Issue, follow the steps
-below. More information about GitHub Issues can be found [here](https://
-docs.github.com/en/issues/tracking-your-work-with-issues/about-issues) 1.
-Navigate to the [MHKiT-Python main page](https://github.com/MHKiT-Software/
-MHKiT-Python) 2. 2.Under the repository name (upper left), click **Issues**. 3.
-Click **New Issue**. 4. If the Issue is a bug, use the **Bug report** template
-and click **Get started**, otherwise click on the **Open a blank issue** link.
-5. Provide a **Title** and **description** for the issue. Be sure the title is
-relevant to the issue and that the description is clear and provided with
-sufficient detail. 6. When you're finished, click **Submit new issue**. The
-developers will follow-up once the issue is addressed. Creating a fork --------
----------------- The GitHub platform has the Fork feature that facilitates code
-modification and contributions. A fork is a new repository that shares code and
-visibility settings with the original upstream repository. To fork MHKiT-
-Python, follow the steps below. More information about GitHub Forks can be
-found [here](https://docs.github.com/en/get-started/quickstart/fork-a-repo) 1.
-Navigate to the [MHKiT-Python main page](https://github.com/MHKiT-Software/
-MHKiT-Python) 2. Under the repository name (upper left), click **Fork**. 3.
-Select an owner for the forked repository. 4. Specify a name for the fork. By
-default, forks are named the same as their upstream repositories. 5. Add a
-description of your fork (optional). 6. Choose whether to copy only the default
-branch or all branches to the new fork. You will only need copy the default
-branch to contribute to MHKiT-Python. 7. When you're finished, click **Create
-fork**. You will now have a fork of the MHKiT-Python repository. Creating a
-branch ------------------------ The GitHub platform has the branch feature that
-facilitates code contributions and collaboration amongst developers. A branch
-isolates development work without affecting other branches in the repository.
-Each repository has one default branch, and can have multiple other branches.
-To create a branch of your forked MHKiT-Python repository, follow the steps
-below. More information about GitHub branches can be found [here](https://
-docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-
+[copyright and license](LICENSE.md) for more information. ## Issues The GitHub
+platform has the Issues feature that is used to track ideas, feedback, tasks,
+and/or bugs. To submit an Issue, follow the steps below. More information about
+GitHub Issues can be found [here](https://docs.github.com/en/issues/tracking-
+your-work-with-issues/about-issues) 1. Navigate to the [MHKiT-Python main page]
+(https://github.com/MHKiT-Software/MHKiT-Python) 2. 2.Under the repository name
+(upper left), click **Issues**. 3. Click **New Issue**. 4. If the Issue is a
+bug, use the **Bug report** template and click **Get started**, otherwise click
+on the **Open a blank issue** link. 5. Provide a **Title** and **description**
+for the issue. Be sure the title is relevant to the issue and that the
+description is clear and provided with sufficient detail. 6. When you're
+finished, click **Submit new issue**. The developers will follow-up once the
+issue is addressed. ## Creating a fork The GitHub platform has the Fork feature
+that facilitates code modification and contributions. A fork is a new
+repository that shares code and visibility settings with the original upstream
+repository. To fork MHKiT-Python, follow the steps below. More information
+about GitHub Forks can be found [here](https://docs.github.com/en/get-started/
+quickstart/fork-a-repo) 1. Navigate to the [MHKiT-Python main page](https://
+github.com/MHKiT-Software/MHKiT-Python) 2. Under the repository name (upper
+left), click **Fork**. 3. Select an owner for the forked repository. 4. Specify
+a name for the fork. By default, forks are named the same as their upstream
+repositories. 5. Add a description of your fork (optional). 6. Choose whether
+to copy only the default branch or all branches to the new fork. You will only
+need copy the default branch to contribute to MHKiT-Python. 7. When you're
+finished, click **Create fork**. You will now have a fork of the MHKiT-Python
+repository. ## Creating a branch The GitHub platform has the branch feature
+that facilitates code contributions and collaboration amongst developers. A
+branch isolates development work without affecting other branches in the
+repository. Each repository has one default branch, and can have multiple other
+branches. To create a branch of your forked MHKiT-Python repository, follow the
+steps below. More information about GitHub branches can be found [here](https:/
+/docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-
 changes-to-your-work-with-pull-requests/about-branches) 1. Navigate to your
 fork of MHKiT-Python (see instructions above) 2. Above the list of files, click
 **Branches**. 3. Click **New Branch**. 4. Enter a name for the branch. Be sure
 to select **MHKiT-Software/MHKiT-Python:master** as the source. 5. Click
 **Create branch**. You will now have a branch on your fork of MHKiT-Python that
-you can use to work with the code base. Creating a pull request ---------------
---------- The GitHub platform has the pull request feature that allows you to
-propose changes to a repository such as MHKiT-Python. The pull request will
-allow the repository administrators to evaluate the pull request. To create a
-pull request for MHKiT-Python repository, follow the steps below. More
-information about GitHub pull requests can be found [here](https://
+you can use to work with the code base. ## Creating a pull request The GitHub
+platform has the pull request feature that allows you to propose changes to a
+repository such as MHKiT-Python. The pull request will allow the repository
+administrators to evaluate the pull request. To create a pull request for
+MHKiT-Python repository, follow the steps below. More information about GitHub
+pull requests can be found [here](https://docs.github.com/en/pull-requests/
+collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-
+requests/creating-a-pull-request) 1. Navigate to the [MHKiT-Python main page]
+(https://github.com/MHKiT-Software/MHKiT-Python) 2. Above the list of files,
+click **Pull request**. 3. On the compare page, click **Compare accross
+forks**. 4. In the "base branch" drop-down menu, select the branch of the
+upstream repository you'd like to merge changes into. 5. In the "head fork"
+drop-down menu, select your fork, then use the "compare branch" drop-down menu
+to select the branch you made your changes in. 6. Type a title and description
+for your pull request. 7. If you want to allow anyone with push access to the
+upstream repository to make changes to your pull request, select **Allow edits
+from maintainers**. 8. To create a pull request that is ready for review, click
+**Create Pull Request**. To create a draft pull request, use the drop-down and
+select **Create Draft Pull Request**, then click **Draft Pull Request**. More
+information about draft pull requests can be found [here](https://
 docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-
-changes-to-your-work-with-pull-requests/creating-a-pull-request) 1. Navigate to
-the [MHKiT-Python main page](https://github.com/MHKiT-Software/MHKiT-Python) 2.
-Above the list of files, click **Pull request**. 3. On the compare page, click
-**Compare accross forks**. 4. In the "base branch" drop-down menu, select the
-branch of the upstream repository you'd like to merge changes into. 5. In the
-"head fork" drop-down menu, select your fork, then use the "compare branch"
-drop-down menu to select the branch you made your changes in. 6. Type a title
-and description for your pull request. 7. If you want to allow anyone with push
-access to the upstream repository to make changes to your pull request, select
-**Allow edits from maintainers**. 8. To create a pull request that is ready for
-review, click **Create Pull Request**. To create a draft pull request, use the
-drop-down and select **Create Draft Pull Request**, then click **Draft Pull
-Request**. More information about draft pull requests can be found [here]
-(https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/
-proposing-changes-to-your-work-with-pull-requests/about-pull-requests#draft-
-pull-requests) 9. MHKiT-Python adminstrators will review your pull request and
-contact you if needed.
+changes-to-your-work-with-pull-requests/about-pull-requests#draft-pull-
+requests) 9. MHKiT-Python adminstrators will review your pull request and
+contact you if needed. ## Code Formatting in MHKiT MHKiT adheres to the "black"
+code formatting standard to maintain a consistent and readable code style.
+Developers contributing to MHKiT have several options to ensure their code
+meets this standard: 1. **Manual Formatting with Black**: Install the 'black'
+formatter and run it manually from the terminal to format your code. This can
+be done by executing a command like `black [file or directory]`. 2. **IDE
+Extension**: If you are using an Integrated Development Environment (IDE) like
+Visual Studio Code (VS Code), you can install the 'black' formatter as an
+extension. This allows for automatic formatting of code within the IDE. 3.
+**Pre-Commit Hook**: Enable the pre-commit hook in your development
+environment. This automatically formats your code with 'black' each time you
+make a commit, ensuring that all committed code conforms to the formatting
+standard. For detailed instructions on installing and using 'black', please
+refer to the [Black Documentation](https://black.readthedocs.io/en/stable/).
+This resource provides comprehensive guidance on installation, usage, and
+configuration of the formatter.
```

### Comparing `mhkit-0.7.0/mhkit/__init__.py` & `mhkit-0.8.0/mhkit/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,20 +7,21 @@
 from mhkit import power
 from mhkit import loads
 from mhkit import dolfyn
 from mhkit import mooring
 
 # Register datetime converter for a matplotlib plotting methods
 from pandas.plotting import register_matplotlib_converters as _rmc
+
 _rmc()
 
 # Ignore future warnings
-_warn.simplefilter(action='ignore', category=FutureWarning)
+_warn.simplefilter(action="ignore", category=FutureWarning)
 
-__version__ = 'v0.7.0'
+__version__ = "v0.8.0"
 
 __copyright__ = """
 Copyright 2019, Alliance for Sustainable Energy, LLC under the terms of 
 Contract DE-AC36-08GO28308, Battelle Memorial Institute under the terms of 
 Contract DE-AC05-76RL01830, and National Technology & Engineering Solutions of 
 Sandia, LLC under the terms of Contract DE-NA0003525. The U.S. Government 
 retains certain rights in this software."""
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/adp/clean.py` & `mhkit-0.8.0/mhkit/dolfyn/adp/clean.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 """Module containing functions to clean data
 """
+
 import numpy as np
 import xarray as xr
 from scipy.signal import medfilt
 from ..tools.misc import medfiltnan
 from ..rotate.api import rotate2
 from ..rotate.base import _make_model, quaternion2orient
 
@@ -36,23 +37,23 @@
 
     If the ADCP is mounted on a tripod on the seafloor, `h_deploy` will be
     the height of the tripod +/- any extra distance to the transducer faces.
     If the instrument is vessel-mounted, `h_deploy` is the distance between
     the surface and downward-facing ADCP's transducers.
     """
 
-    r = [s for s in ds.dims if 'range' in s]
+    r = [s for s in ds.dims if "range" in s]
     for val in r:
         ds[val] = ds[val].values + h_deploy
-        ds[val].attrs['units'] = 'm'
+        ds[val].attrs["units"] = "m"
 
-    if hasattr(ds, 'h_deploy'):
-        ds.attrs['h_deploy'] += h_deploy
+    if hasattr(ds, "h_deploy"):
+        ds.attrs["h_deploy"] += h_deploy
     else:
-        ds.attrs['h_deploy'] = h_deploy
+        ds.attrs["h_deploy"] = h_deploy
 
 
 def find_surface(ds, thresh=10, nfilt=None):
     """
     Find the surface (water level or seafloor) from amplitude data and
     adds the variable "depth" to the input Dataset.
 
@@ -74,17 +75,21 @@
     """
 
     # This finds the maximum of the echo profile:
     inds = np.argmax(ds.amp.values, axis=1)
     # This finds the first point that increases (away from the profiler) in
     # the echo profile
     edf = np.diff(ds.amp.values.astype(np.int16), axis=1)
-    inds2 = np.max((edf < 0) *
-                   np.arange(ds.vel.shape[1] - 1,
-                             dtype=np.uint8)[None, :, None], axis=1) + 1
+    inds2 = (
+        np.max(
+            (edf < 0) * np.arange(ds.vel.shape[1] - 1, dtype=np.uint8)[None, :, None],
+            axis=1,
+        )
+        + 1
+    )
 
     # Calculate the depth of these quantities
     d1 = ds.range.values[inds]
     d2 = ds.range.values[inds2]
     # Combine them:
     D = np.vstack((d1, d2))
     # Take the median value as the estimate of the surface:
@@ -97,20 +102,25 @@
             d[ip] = np.NaN
 
     if nfilt:
         dfilt = medfiltnan(d, nfilt, thresh=4)
         dfilt[dfilt == 0] = np.NaN
         d = dfilt
 
-    ds['depth'] = xr.DataArray(d.astype('float32'),
-                               dims=['time'],
-                               attrs={'units': 'm',
-                                      'long_name': 'Depth',
-                                      'standard_name': 'depth',
-                                      'positive': 'down'})
+    ds["depth"] = xr.DataArray(
+        d.astype("float32"),
+        dims=["time"],
+        attrs={
+            "units": "m",
+            "long_name": "Depth",
+            "standard_name": "depth",
+            "positive": "down",
+        },
+    )
+
 
 def find_surface_from_P(ds, salinity=35):
     """
     Calculates the distance to the water surface. Temperature and salinity
     are used to calculate seawater density, which is in turn used with the
     pressure data to calculate depth.
 
@@ -133,55 +143,61 @@
 
     Calculates seawater density using a linear approximation of the sea
     water equation of state:
 
     .. math:: \\rho - \\rho_0 = -\\alpha (T-T_0) + \\beta (S-S_0) + \\kappa P
 
     Where :math:`\\rho` is water density, :math:`T` is water temperature,
-    :math:`P` is water pressure, :math:`S` is practical salinity, 
-    :math:`\\alpha` is the thermal expansion coefficient, :math:`\\beta` is 
-    the haline contraction coefficient, and :math:`\\kappa` is adiabatic 
+    :math:`P` is water pressure, :math:`S` is practical salinity,
+    :math:`\\alpha` is the thermal expansion coefficient, :math:`\\beta` is
+    the haline contraction coefficient, and :math:`\\kappa` is adiabatic
     compressibility.
     """
 
     # Density calcation
     P = ds.pressure.values
     T = ds.temp.values  # temperature, degC
     S = salinity  # practical salinity
     rho0 = 1027  # kg/m^3
     T0 = 10  # degC
     S0 = 35  # psu assumed equivalent to ppt
     a = 0.15  # thermal expansion coefficient, kg/m^3/degC
     b = 0.78  # haline contraction coefficient, kg/m^3/ppt
     k = 4.5e-3  # adiabatic compressibility, kg/m^3/dbar
-    rho = rho0 - a*(T-T0) + b*(S-S0) + k*P
+    rho = rho0 - a * (T - T0) + b * (S - S0) + k * P
 
     # Depth = pressure (conversion from dbar to MPa) / water weight
-    d = (ds.pressure*10000)/(9.81*rho)
+    d = (ds.pressure * 10000) / (9.81 * rho)
 
-    if hasattr(ds, 'h_deploy'):
+    if hasattr(ds, "h_deploy"):
         d += ds.h_deploy
         description = "Depth to Seafloor"
     else:
         description = "Depth to Instrument"
 
-    ds['water_density'] = xr.DataArray(
-        rho.astype('float32'),
-        dims=['time'],
-        attrs={'units': 'kg m-3',
-               'long_name': 'Water Density',
-               'standard_name': 'sea_water_density',
-               'description': 'Water density from linear approximation of sea water equation of state'})
-    ds['depth'] = xr.DataArray(
-        d.astype('float32'),
-        dims=['time'],
-        attrs={'units': 'm',
-               'long_name': description,
-               'standard_name': 'depth',
-               'positive': 'down'})
+    ds["water_density"] = xr.DataArray(
+        rho.astype("float32"),
+        dims=["time"],
+        attrs={
+            "units": "kg m-3",
+            "long_name": "Water Density",
+            "standard_name": "sea_water_density",
+            "description": "Water density from linear approximation of sea water equation of state",
+        },
+    )
+    ds["depth"] = xr.DataArray(
+        d.astype("float32"),
+        dims=["time"],
+        attrs={
+            "units": "m",
+            "long_name": description,
+            "standard_name": "depth",
+            "positive": "down",
+        },
+    )
 
 
 def nan_beyond_surface(ds, val=np.nan, beam_angle=None, inplace=False):
     """
     Mask the values of 3D data (vel, amp, corr, echo) that are beyond the surface.
 
     Parameters
@@ -200,45 +216,48 @@
     -------
     ds : xarray.Dataset
       Sets the adcp dataset where relevant arrays with values greater than `depth`
       set to NaN
 
     Notes
     -----
-    Surface interference expected to happen at 
+    Surface interference expected to happen at
     `distance > range * cos(beam angle) - cell size`
     """
 
     if not inplace:
         ds = ds.copy(deep=True)
 
     # Get all variables with 'range' coordinate
-    var = [h for h in ds.keys() if any(s for s in ds[h].dims if 'range' in s)]
+    var = [h for h in ds.keys() if any(s for s in ds[h].dims if "range" in s)]
 
     if beam_angle is None:
-        if hasattr(ds, 'beam_angle'):
-            beam_angle = ds.beam_angle * (np.pi/180)
+        if hasattr(ds, "beam_angle"):
+            beam_angle = ds.beam_angle * (np.pi / 180)
         else:
-            raise Exception("'beam_angle` not found in dataset attributes. "\
-                            "Please supply the ADCP's beam angle.")
+            raise Exception(
+                "'beam_angle` not found in dataset attributes. "
+                "Please supply the ADCP's beam angle."
+            )
 
     # Surface interference distance calculated from distance of transducers to surface
-    if hasattr(ds, 'h_deploy'):
-        range_limit = ((ds.depth-ds.h_deploy) * np.cos(beam_angle) -
-                       ds.cell_size) + ds.h_deploy
+    if hasattr(ds, "h_deploy"):
+        range_limit = (
+            (ds.depth - ds.h_deploy) * np.cos(beam_angle) - ds.cell_size
+        ) + ds.h_deploy
     else:
         range_limit = ds.depth * np.cos(beam_angle) - ds.cell_size
 
     bds = ds.range > range_limit
 
     # Echosounder data needs only be trimmed at water surface
-    if 'echo' in var:
+    if "echo" in var:
         bds_echo = ds.range_echo > ds.depth
-        ds['echo'].values[..., bds_echo] = val
-        var.remove('echo')
+        ds["echo"].values[..., bds_echo] = val
+        var.remove("echo")
 
     # Correct rest of "range" data for surface interference
     for nm in var:
         a = ds[nm].values
         try:  # float dtype
             a[..., bds] = val
         except:  # int dtype
@@ -247,15 +266,15 @@
 
     if not inplace:
         return ds
 
 
 def correlation_filter(ds, thresh=50, inplace=False):
     """
-    Filters out data where correlation is below a threshold in the 
+    Filters out data where correlation is below a threshold in the
     along-beam correlation data.
 
     Parameters
     ----------
     ds : xarray.Dataset
       The adcp dataset to clean.
     thresh : numeric
@@ -264,47 +283,50 @@
     inplace : bool
       When True the existing data object is modified. When False
       a copy is returned. Default = False
 
     Returns
     -------
     ds : xarray.Dataset
-      Elements in velocity, correlation, and amplitude are removed if below the 
+      Elements in velocity, correlation, and amplitude are removed if below the
       correlation threshold
 
     Notes
     -----
     Does not edit correlation or amplitude data.
     """
 
     if not inplace:
         ds = ds.copy(deep=True)
 
     # 4 or 5 beam
-    if hasattr(ds, 'vel_b5'):
-        tag = ['', '_b5']
+    if hasattr(ds, "vel_b5"):
+        tag = ["", "_b5"]
     else:
-        tag = ['']
+        tag = [""]
 
     # copy original ref frame
     coord_sys_orig = ds.coord_sys
 
     # correlation is always in beam coordinates
-    rotate2(ds, 'beam', inplace=True)
+    rotate2(ds, "beam", inplace=True)
     # correlation is always in beam coordinates
     for tg in tag:
-        mask = ds['corr'+tg].values <= thresh
+        mask = ds["corr" + tg].values <= thresh
 
-        for var in ['vel', 'corr', 'amp']:
+        for var in ["vel", "corr", "amp"]:
             try:
-                ds[var+tg].values[mask] = np.nan
+                ds[var + tg].values[mask] = np.nan
             except:
-                ds[var+tg].values[mask] = 0
-            ds[var+tg].attrs['Comments'] = 'Filtered of data with a correlation value below ' + \
-                str(thresh) + ds.corr.units
+                ds[var + tg].values[mask] = 0
+            ds[var + tg].attrs["Comments"] = (
+                "Filtered of data with a correlation value below "
+                + str(thresh)
+                + ds.corr.units
+            )
 
     rotate2(ds, coord_sys_orig, inplace=True)
 
     if not inplace:
         return ds
 
 
@@ -328,30 +350,30 @@
     See Also
     --------
     scipy.signal.medfilt()
     """
 
     ds = ds.copy(deep=True)
 
-    if getattr(ds, 'has_imu'):
+    if getattr(ds, "has_imu"):
         q_filt = np.zeros(ds.quaternions.shape)
         for i in range(ds.quaternions.q.size):
             q_filt[i] = medfilt(ds.quaternions[i].values, nfilt)
         ds.quaternions.values = q_filt
 
-        ds['orientmat'] = quaternion2orient(ds.quaternions)
+        ds["orientmat"] = quaternion2orient(ds.quaternions)
         return ds
 
     else:
         # non Nortek AHRS-equipped instruments
-        do_these = ['pitch', 'roll', 'heading']
+        do_these = ["pitch", "roll", "heading"]
         for nm in do_these:
             ds[nm].values = medfilt(ds[nm].values, nfilt)
 
-        return ds.drop_vars('orientmat')
+        return ds.drop_vars("orientmat")
 
 
 def val_exceeds_thresh(var, thresh=5, val=np.nan):
     """
     Find values of a variable that exceed a threshold value,
     and assign "val" to the velocity data where the threshold is
     exceeded.
@@ -369,23 +391,23 @@
     -------
     ds : xarray.Dataset
       The adcp dataset with datapoints beyond thresh are set to `val`
     """
 
     var = var.copy(deep=True)
 
-    bd = np.zeros(var.shape, dtype='bool')
-    bd |= (np.abs(var.values) > thresh)
+    bd = np.zeros(var.shape, dtype="bool")
+    bd |= np.abs(var.values) > thresh
 
     var.values[bd] = val
 
     return var
 
 
-def fillgaps_time(var, method='cubic', maxgap=None):
+def fillgaps_time(var, method="cubic", maxgap=None):
     """
     Fill gaps (nan values) in var across time using the specified method
 
     Parameters
     ----------
     var : xarray.DataArray
       The variable to clean
@@ -400,22 +422,22 @@
       The input DataArray 'var' with gaps in 'var' interpolated across time
 
     See Also
     --------
     xarray.DataArray.interpolate_na()
     """
 
-    time_dim = [t for t in var.dims if 'time' in t][0]
+    time_dim = [t for t in var.dims if "time" in t][0]
 
-    return var.interpolate_na(dim=time_dim, method=method,
-                              use_coordinate=True,
-                              limit=maxgap)
+    return var.interpolate_na(
+        dim=time_dim, method=method, use_coordinate=True, limit=maxgap
+    )
 
 
-def fillgaps_depth(var, method='cubic', maxgap=None):
+def fillgaps_depth(var, method="cubic", maxgap=None):
     """
     Fill gaps (nan values) in var along the depth profile using the specified method
 
     Parameters
     ----------
     var : xarray.DataArray
       The variable to clean
@@ -430,12 +452,12 @@
       The input DataArray 'var' with gaps in 'var' interpolated across depth
 
     See Also
     --------
     xarray.DataArray.interpolate_na()
     """
 
-    range_dim = [t for t in var.dims if 'range' in t][0]
+    range_dim = [t for t in var.dims if "range" in t][0]
 
-    return var.interpolate_na(dim=range_dim, method=method,
-                              use_coordinate=False,
-                              limit=maxgap)
+    return var.interpolate_na(
+        dim=range_dim, method=method, use_coordinate=False, limit=maxgap
+    )
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/adp/turbulence.py` & `mhkit-0.8.0/mhkit/dolfyn/adp/turbulence.py`

 * *Files 9% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 
     Parameters
     ----------
     dat : array-like
       1 dimensional vector to be differentiated
     z : array-like
       Vertical dimension to differentiate across
-    
+
     Returns
     -------
     out : array-like
       Numerically-derived derivative of `dat`
     """
 
     return np.diff(dat, axis=0) / (np.diff(z)[:, None])
@@ -32,61 +32,73 @@
 
     Parameters
     ----------
     dat : array-like
       1 dimensional vector to be differentiated
     z : array-like
       Vertical dimension to differentiate across
-    
+
     Returns
     -------
     out : array-like
       Numerically-derived derivative of `dat`
 
     Notes
     -----
     Want top - bottom here: (u_x+1 - u_x-1)/dx
     Can use 2*np.diff b/c depth bin size never changes
     """
 
-    return (dat[2:]-dat[:-2]) / (2*np.diff(z)[1:, None])
+    return (dat[2:] - dat[:-2]) / (2 * np.diff(z)[1:, None])
 
 
 def _diffz_centered_extended(dat, z):
     """
     Extended centered difference method.
 
     Parameters
     ----------
     dat : array-like
       1 dimensional vector to be differentiated
     z : array-like
       Vertical dimension to differentiate across
-    
+
     Returns
     -------
     out : array-like
       Numerically-derived derivative of `dat`
 
     Notes
     -----
     Top - bottom centered difference with endpoints determined
-    with a first difference. Ensures the output array is the 
+    with a first difference. Ensures the output array is the
     same size as the input array.
     """
 
-    out = np.concatenate((_diffz_first(dat[:2], z[:2]),
-                          _diffz_centered(dat, z),
-                          _diffz_first(dat[-2:], z[-2:])))
+    out = np.concatenate(
+        (
+            _diffz_first(dat[:2], z[:2]),
+            _diffz_centered(dat, z),
+            _diffz_first(dat[-2:], z[-2:]),
+        )
+    )
     return out
 
 
 class ADPBinner(VelBinner):
-    def __init__(self, n_bin, fs, n_fft=None, n_fft_coh=None,
-                 noise=None, orientation='up', diff_style='centered_extended'):
+    def __init__(
+        self,
+        n_bin,
+        fs,
+        n_fft=None,
+        n_fft_coh=None,
+        noise=None,
+        orientation="up",
+        diff_style="centered_extended",
+    ):
         """
         A class for calculating turbulence statistics from ADCP data
 
         Parameters
         ----------
         n_bin : int
           Number of data points to include in a 'bin' (ensemble), not the
@@ -95,35 +107,37 @@
           Instrument sampling frequency in Hz
         n_fft : int
           Number of data points to use for fft (`n_fft`<=`n_bin`).
           Default: `n_fft`=`n_bin`
         n_fft_coh : int
           Number of data points to use for coherence and cross-spectra ffts
           Default: `n_fft_coh`=`n_fft`
-        noise : float, list or numpy.ndarray
-          Instrument's doppler noise in same units as velocity
+        noise : float or array-like
+          Instrument noise level in same units as velocity. Typically
+          found from `adp.turbulence.doppler_noise_level`.
+          Default: None.
         orientation : str, default='up'
           Instrument's orientation, either 'up' or 'down'
         diff_style : str, default='centered_extended'
-          Style of numerical differentiation using Newton's Method. 
+          Style of numerical differentiation using Newton's Method.
           Either 'first' (first difference), 'centered' (centered difference),
           or 'centered_extended' (centered difference with first and last points
           extended using a first difference).
         """
 
         VelBinner.__init__(self, n_bin, fs, n_fft, n_fft_coh, noise)
         self.diff_style = diff_style
         self.orientation = orientation
 
     def _diff_func(self, vel, u):
-        """ Applies the chosen style of numerical differentiation to velocity data.
+        """Applies the chosen style of numerical differentiation to velocity data.
 
-        This method calculates the derivative of the velocity data 'vel' with respect to the 'range' 
-        using the differentiation style specified in 'self.diff_style'. The styles can be 'first' 
-        for first difference, 'centered' for centered difference, and 'centered_extended' for 
+        This method calculates the derivative of the velocity data 'vel' with respect to the 'range'
+        using the differentiation style specified in 'self.diff_style'. The styles can be 'first'
+        for first difference, 'centered' for centered difference, and 'centered_extended' for
         centered difference with first and last points extended using a first difference.
 
         Parameters
         ----------
         vel : xarray.DataArray
           Velocity data with dimensions 'range' and 'time'.
         u : str or int
@@ -131,22 +145,22 @@
 
         Returns
         -------
         out : numpy.ndarray
             The calculated derivative of the velocity data.
         """
 
-        if self.diff_style == 'first':
-            out = _diffz_first(vel[u].values, vel['range'].values)
+        if self.diff_style == "first":
+            out = _diffz_first(vel[u].values, vel["range"].values)
             return out, vel.range[1:]
-        elif self.diff_style == 'centered':
-            out = _diffz_centered(vel[u].values, vel['range'].values)
+        elif self.diff_style == "centered":
+            out = _diffz_centered(vel[u].values, vel["range"].values)
             return out, vel.range[1:-1]
-        elif self.diff_style == 'centered_extended':
-            out = _diffz_centered_extended(vel[u].values, vel['range'].values)
+        elif self.diff_style == "centered_extended":
+            out = _diffz_centered_extended(vel[u].values, vel["range"].values)
             return out, vel.range
 
     def dudz(self, vel, orientation=None):
         """
         The shear in the first velocity component.
 
         Parameters
@@ -167,24 +181,24 @@
         coordinate ('dz' is actually diff(self['range'])), not necessarily the
         'true vertical' direction.
         """
 
         if not orientation:
             orientation = self.orientation
         sign = 1
-        if orientation == 'down':
+        if orientation == "down":
             sign *= -1
 
-        dudz, rng = sign*self._diff_func(vel, 0)
-        return xr.DataArray(dudz,
-                            coords=[rng, vel.time],
-                            dims=['range', 'time'],
-                            attrs={'units': 's-1',
-                                   'long_name': 'Shear in X-direction'}
-                            )
+        dudz, rng = sign * self._diff_func(vel, 0)
+        return xr.DataArray(
+            dudz,
+            coords=[rng, vel.time],
+            dims=["range", "time"],
+            attrs={"units": "s-1", "long_name": "Shear in X-direction"},
+        )
 
     def dvdz(self, vel):
         """
         The shear in the second velocity component.
 
         Parameters
         ----------
@@ -200,20 +214,20 @@
         -----
         The derivative direction is along the profiler's 'z'
         coordinate ('dz' is actually diff(self['range'])), not necessarily the
         'true vertical' direction.
         """
 
         dvdz, rng = self._diff_func(vel, 1)
-        return xr.DataArray(dvdz,
-                            coords=[rng, vel.time],
-                            dims=['range', 'time'],
-                            attrs={'units': 's-1',
-                                   'long_name': 'Shear in Y-direction'}
-                            )
+        return xr.DataArray(
+            dvdz,
+            coords=[rng, vel.time],
+            dims=["range", "time"],
+            attrs={"units": "s-1", "long_name": "Shear in Y-direction"},
+        )
 
     def dwdz(self, vel):
         """
         The shear in the third velocity component.
 
         Parameters
         ----------
@@ -229,20 +243,20 @@
         -----
         The derivative direction is along the profiler's 'z'
         coordinate ('dz' is actually diff(self['range'])), not necessarily the
         'true vertical' direction.
         """
 
         dwdz, rng = self._diff_func(vel, 2)
-        return xr.DataArray(dwdz,
-                            coords=[rng, vel.time],
-                            dims=['range', 'time'],
-                            attrs={'units': 's-1',
-                                   'long_name': 'Shear in Z-direction'}
-                            )
+        return xr.DataArray(
+            dwdz,
+            coords=[rng, vel.time],
+            dims=["range", "time"],
+            attrs={"units": "s-1", "long_name": "Shear in Z-direction"},
+        )
 
     def shear_squared(self, vel):
         """
         The horizontal shear squared.
 
         Parameters
         ----------
@@ -262,16 +276,16 @@
 
         See Also
         --------
         :math:`dudz`, :math:`dvdz`
         """
 
         shear2 = self.dudz(vel) ** 2 + self.dvdz(vel) ** 2
-        shear2.attrs['units'] = 's-2'
-        shear2.attrs['long_name'] = 'Horizontal Shear Squared'
+        shear2.attrs["units"] = "s-2"
+        shear2.attrs["long_name"] = "Horizontal Shear Squared"
 
         return shear2
 
     def doppler_noise_level(self, psd, pct_fN=0.8):
         """
         Calculate bias due to Doppler noise using the noise floor
         of the velocity spectra.
@@ -282,80 +296,83 @@
           The velocity spectra from a single depth bin (range), typically
           in the mid-water range
         pct_fN : float
           Percent of Nyquist frequency to calculate characeristic frequency
 
         Returns
         -------
-        doppler_noise (xarray.DataArray): 
+        doppler_noise (xarray.DataArray):
           Doppler noise level in units of m/s
 
         Notes
         -----
         Approximates bias from
 
         .. :math: \\sigma^{2}_{noise} = N x f_{c}
 
         where :math: `\\sigma_{noise}` is the bias due to Doppler noise,
         `N` is the constant variance or spectral density, and `f_{c}`
         is the characteristic frequency.
 
-        The characteristic frequency is then found as 
+        The characteristic frequency is then found as
 
         .. :math: f_{c} = pct_fN * (f_{s}/2)
 
         where `f_{s}/2` is the Nyquist frequency.
 
 
-        Richard, Jean-Baptiste, et al. "Method for identification of Doppler noise 
-        levels in turbulent flow measurements dedicated to tidal energy." International 
+        Richard, Jean-Baptiste, et al. "Method for identification of Doppler noise
+        levels in turbulent flow measurements dedicated to tidal energy." International
         Journal of Marine Energy 3 (2013): 52-64.
 
-        Thibaut, Maxime, et al. "Investigating the flow dynamics and turbulence at a 
-        tidal-stream energy site in a highly energetic estuary." Renewable Energy 195 
+        Thibaut, Maxime, et al. "Investigating the flow dynamics and turbulence at a
+        tidal-stream energy site in a highly energetic estuary." Renewable Energy 195
         (2022): 252-262.
         """
 
         if not isinstance(psd, xr.DataArray):
             raise TypeError("`psd` must be an instance of `xarray.DataArray`.")
         if not isinstance(pct_fN, float) or not 0 <= pct_fN <= 1:
             raise ValueError("`pct_fN` must be a float within the range [0, 1].")
         if len(psd.shape) != 2:
-            raise Exception('PSD should be 2-dimensional (time, frequency)')
+            raise Exception("PSD should be 2-dimensional (time, frequency)")
 
         # Characteristic frequency set to 80% of Nyquist frequency
-        fN = self.fs/2
+        fN = self.fs / 2
         fc = pct_fN * fN
 
         # Get units right
         if psd.freq.units == "Hz":
             f_range = slice(fc, fN)
         else:
-            f_range = slice(2*np.pi*fc, 2*np.pi*fN)
+            f_range = slice(2 * np.pi * fc, 2 * np.pi * fN)
 
         # Noise floor
         N2 = psd.sel(freq=f_range) * psd.freq.sel(freq=f_range)
-        noise_level = np.sqrt(N2.mean(dim='freq'))
+        noise_level = np.sqrt(N2.mean(dim="freq"))
 
+        time_coord = psd.dims[0]  # no reason this shouldn't be time or time_b5
         return xr.DataArray(
-            noise_level.values.astype('float32'),
-            dims=['time'],
-            attrs={'units': 'm s-1',
-                   'long_name': 'Doppler Noise Level',
-                   'description': 'Doppler noise level calculated '
-                   'from PSD white noise'})
+            noise_level.values.astype("float32"),
+            coords={time_coord: psd.coords[time_coord]},
+            attrs={
+                "units": "m s-1",
+                "long_name": "Doppler Noise Level",
+                "description": "Doppler noise level calculated " "from PSD white noise",
+            },
+        )
 
     def _stress_func_warnings(self, ds, beam_angle, noise, tilt_thresh):
         """
         Performs a series of checks and raises warnings for ADCP stress calculations.
 
-        This method checks several conditions relevant for ADCP stress calculations and raises 
-        warnings if these conditions are not met. It checks if the beam angle is defined, 
-        if the instrument's coordinate system is aligned with the principal flow directions, 
-        if the tilt is above a threshold, if the noise level is specified, and if the data 
+        This method checks several conditions relevant for ADCP stress calculations and raises
+        warnings if these conditions are not met. It checks if the beam angle is defined,
+        if the instrument's coordinate system is aligned with the principal flow directions,
+        if the tilt is above a threshold, if the noise level is specified, and if the data
         set is in the 'beam' coordinate system.
 
         Parameters
         ----------
         ds : xarray.Dataset
           Raw dataset in beam coordinates
         beam_angle : int, default=ds.attrs['beam_angle']
@@ -370,49 +387,58 @@
         b_angle : float
           If 'beam_angle' was None, it tries to find it in 'ds'.
         noise : float
           If 'noise' was None, it is set to 0.
         """
 
         # Error 1. Beam Angle
-        b_angle = getattr(ds, 'beam_angle', beam_angle)
+        b_angle = getattr(ds, "beam_angle", beam_angle)
         if b_angle is None:
             raise Exception(
-                "    Beam angle not found in dataset and no beam angle supplied.")
+                "    Beam angle not found in dataset and no beam angle supplied."
+            )
 
         # Warning 1. Memo
-        warnings.warn("    The beam-variance algorithms assume the instrument's "
-                      "(XYZ) coordinate system is aligned with the principal "
-                      "flow directions.")
+        warnings.warn(
+            "    The beam-variance algorithms assume the instrument's "
+            "(XYZ) coordinate system is aligned with the principal "
+            "flow directions."
+        )
 
         # Warning 2. Check tilt
-        tilt_mask = calc_tilt(ds['pitch'], ds['roll']) > tilt_thresh
+        tilt_mask = calc_tilt(ds["pitch"], ds["roll"]) > tilt_thresh
         if sum(tilt_mask):
             pct_above_thresh = round(sum(tilt_mask) / len(tilt_mask) * 100, 2)
-            warnings.warn(f"    {pct_above_thresh} % of measurements have a tilt "
-                          f"greater than {tilt_thresh} degrees.")
+            warnings.warn(
+                f"    {pct_above_thresh} % of measurements have a tilt "
+                f"greater than {tilt_thresh} degrees."
+            )
 
         # Warning 3. Noise level of instrument is important considering 50 % of variance
         # in ADCP data can be noise
         if noise is None:
-            warnings.warn('    No "noise" input supplied. Consider calculating "noise" '
-                          'using `calc_doppler_noise`')
+            warnings.warn(
+                '    No "noise" input supplied. Consider calculating "noise" '
+                "using `calc_doppler_noise`"
+            )
             noise = 0
 
         # Warning 4. Likely not in beam coordinates after running a typical analysis workflow
-        if 'beam' not in ds.coord_sys:
-            warnings.warn("    Raw dataset must be in the 'beam' coordinate system. "
-                          "Rotating raw dataset...")
-            ds.velds.rotate2('beam')
+        if "beam" not in ds.coord_sys:
+            warnings.warn(
+                "    Raw dataset must be in the 'beam' coordinate system. "
+                "Rotating raw dataset..."
+            )
+            ds.velds.rotate2("beam")
 
         return b_angle, noise
-    
+
     def _check_orientation(self, ds, orientation, beam5=False):
         """
-        Determines the beam order for the beam-stress rotation algorithm based on 
+        Determines the beam order for the beam-stress rotation algorithm based on
         the instrument orientation.
 
         Note: Stacey defines the beams for down-looking Workhorse ADCPs.
               According to the workhorse coordinate transformation
               documentation, the instrument's:
                                x-axis points from beam 1 to 2, and
                                y-axis points from beam 4 to 3.
@@ -420,68 +446,70 @@
                          y-axis points from beam 2 to 4
 
         Parameters
         ----------
         ds : xarray.Dataset
           Raw dataset in beam coordinates
         orientation : str
-          The orientation of the instrument, either 'up' or 'down'. 
-          If None, the orientation will be retrieved from the dataset or the 
+          The orientation of the instrument, either 'up' or 'down'.
+          If None, the orientation will be retrieved from the dataset or the
           instance's default orientation.
         beam5 : bool, default=False
-          A flag indicating whether a fifth beam is present. 
+          A flag indicating whether a fifth beam is present.
           If True, the number 4 will be appended to the beam order.
 
         Returns
         -------
         beams : list of int
           Beam order.
         phi2 : float, optional
           The mean of the roll values in radians. Only returned if 'beam5' is True.
         phi3 : float, optional
-          The mean of the pitch values in radians, negated for Nortek instruments. 
+          The mean of the pitch values in radians, negated for Nortek instruments.
           Only returned if 'beam5' is True.
         """
 
         if orientation is None:
-            orientation = getattr(ds, 'orientation', self.orientation)
+            orientation = getattr(ds, "orientation", self.orientation)
 
-        if 'TRDI' in ds.inst_make:
-            phi2 = np.deg2rad(self.mean(ds['pitch'].values))
-            phi3 = np.deg2rad(self.mean(ds['roll'].values))
-            if 'down' in orientation.lower():
+        if "TRDI" in ds.inst_make:
+            phi2 = np.deg2rad(self.mean(ds["pitch"].values))
+            phi3 = np.deg2rad(self.mean(ds["roll"].values))
+            if "down" in orientation.lower():
                 # this order is correct given the note above
                 beams = [0, 1, 2, 3]  # for down-facing RDIs
-            elif 'up' in orientation.lower():
+            elif "up" in orientation.lower():
                 beams = [0, 1, 3, 2]  # for up-facing RDIs
             else:
                 raise Exception(
-                    "Please provide instrument orientation ['up' or 'down']")
+                    "Please provide instrument orientation ['up' or 'down']"
+                )
 
         # For Nortek Signatures
-        elif ('Signature' in ds.inst_model) or ('AD2CP' in ds.inst_model):
-            phi2 = np.deg2rad(self.mean(ds['roll'].values))
-            phi3 = -np.deg2rad(self.mean(ds['pitch'].values))
-            if 'down' in orientation.lower():
+        elif ("Signature" in ds.inst_model) or ("AD2CP" in ds.inst_model):
+            phi2 = np.deg2rad(self.mean(ds["roll"].values))
+            phi3 = -np.deg2rad(self.mean(ds["pitch"].values))
+            if "down" in orientation.lower():
                 beams = [2, 0, 3, 1]  # for down-facing Norteks
-            elif 'up' in orientation.lower():
+            elif "up" in orientation.lower():
                 beams = [0, 2, 3, 1]  # for up-facing Norteks
             else:
                 raise Exception(
-                    "Please provide instrument orientation ['up' or 'down']")
+                    "Please provide instrument orientation ['up' or 'down']"
+                )
 
         if beam5:
             beams.append(4)
             return beams, phi2, phi3
         else:
             return beams
 
     def _beam_variance(self, ds, time, noise, beam_order, n_beams):
         """
-        Calculates the variance of the along-beam velocities and then subtracts 
+        Calculates the variance of the along-beam velocities and then subtracts
         noise from the result.
 
         Parameters
         ----------
         ds : xarray.Dataset
           Raw dataset in beam coordinates
         time : xarray.DataArray
@@ -492,40 +520,41 @@
           Beam order in pairs, per manufacturer and orientation
         n_beams : int
           Number of beams
 
         Returns
         -------
         bp2_ : xarray.DataArray
-          Enxemble-averaged along-beam velocity variance, 
+          Enxemble-averaged along-beam velocity variance,
           written "beam-velocity prime squared bar" in units of m^2/s^2
         """
 
         # Concatenate 5th beam velocity if need be
         if n_beams == 4:
-            beam_vel = ds['vel'].values
+            beam_vel = ds["vel"].values
         elif n_beams == 5:
-            beam_vel = np.concatenate((ds['vel'].values,
-                                       ds['vel_b5'].values[None, ...]))
+            beam_vel = np.concatenate(
+                (ds["vel"].values, ds["vel_b5"].values[None, ...])
+            )
 
         # Calculate along-beam velocity prime squared bar
-        bp2_ = np.empty((n_beams, len(ds.range), len(time)))*np.nan
+        bp2_ = np.empty((n_beams, len(ds.range), len(time))) * np.nan
         for i, beam in enumerate(beam_order):
             bp2_[i] = np.nanvar(self.reshape(beam_vel[beam]), axis=-1)
 
         # Remove doppler_noise
         if type(noise) == type(ds.vel):
             noise = noise.values
         bp2_ -= noise**2
 
         return bp2_
 
     def reynolds_stress_4beam(self, ds, noise=None, orientation=None, beam_angle=None):
         """
-        Calculate the stresses from the covariance of along-beam 
+        Calculate the stresses from the covariance of along-beam
         velocity measurements
 
         Parameters
         ----------
         ds : xarray.Dataset
           Raw dataset in beam coordinates
         noise : int or xarray.DataArray (time)
@@ -543,46 +572,51 @@
         Notes
         -----
         Assumes zero mean pitch and roll.
 
         Assumes ADCP instrument coordinate system is aligned with principal flow
         directions.
 
-        Stacey, Mark T., Stephen G. Monismith, and Jon R. Burau. "Measurements 
-        of Reynolds stress profiles in unstratified tidal flow." Journal of 
+        Stacey, Mark T., Stephen G. Monismith, and Jon R. Burau. "Measurements
+        of Reynolds stress profiles in unstratified tidal flow." Journal of
         Geophysical Research: Oceans 104.C5 (1999): 10933-10949.
         """
 
         # Run through warnings
         b_angle, noise = self._stress_func_warnings(
-            ds, beam_angle, noise, tilt_thresh=5)
+            ds, beam_angle, noise, tilt_thresh=5
+        )
 
         # Fetch beam order
         beam_order = self._check_orientation(ds, orientation, beam5=False)
 
         # Calculate beam variance and subtract noise
-        time = self.mean(ds['time'].values)
+        time = self.mean(ds["time"].values)
         bp2_ = self._beam_variance(ds, time, noise, beam_order, n_beams=4)
 
         # Run stress calculations
         denm = 4 * np.sin(np.deg2rad(b_angle)) * np.cos(np.deg2rad(b_angle))
         upwp_ = (bp2_[0] - bp2_[1]) / denm
         vpwp_ = (bp2_[2] - bp2_[3]) / denm
 
         return xr.DataArray(
-            np.stack([upwp_*np.nan, upwp_, vpwp_]).astype('float32'),
-            coords={'tau': ["upvp_", "upwp_", "vpwp_"],
-                    'range': ds.range,
-                    'time': time},
-            attrs={'units': 'm2 s-2',
-                   'long_name': 'Specific Reynolds Stress Vector'})
-
-    def stress_tensor_5beam(self, ds, noise=None, orientation=None, beam_angle=None, tke_only=False):
+            np.stack([upwp_ * np.nan, upwp_, vpwp_]).astype("float32"),
+            coords={
+                "tau": ["upvp_", "upwp_", "vpwp_"],
+                "range": ds.range,
+                "time": time,
+            },
+            attrs={"units": "m2 s-2", "long_name": "Specific Reynolds Stress Vector"},
+        )
+
+    def stress_tensor_5beam(
+        self, ds, noise=None, orientation=None, beam_angle=None, tke_only=False
+    ):
         """
-        Calculate the stresses from the covariance of along-beam 
+        Calculate the stresses from the covariance of along-beam
         velocity measurements
 
         Parameters
         ----------
         ds : xarray.Dataset
           Raw dataset in beam coordinates
         noise : int or xarray.DataArray with dim 'time', default=0
@@ -601,112 +635,143 @@
           Otherwise only `tke_vec` is returned
 
         Notes
         -----
         Assumes small-angle approximation is applicable.
 
         Assumes ADCP instrument coordinate system is aligned with principal flow
-        directions, i.e. u', v' and w' are aligned to the instrument's (XYZ) 
+        directions, i.e. u', v' and w' are aligned to the instrument's (XYZ)
         frame of reference.
 
         The stress equations here utilize u'v'_ to account for small variations
         in pitch and roll. u'v'_ cannot be directly calculated by a 5-beam ADCP,
         so it is approximated by the covariance of `u` and `v`. The uncertainty
         introduced by using this approximation is small if deviations from pitch
         and roll are small (< 10 degrees).
 
         Dewey, R., and S. Stringer. "Reynolds stresses and turbulent kinetic
         energy estimates from various ADCP beam configurations: Theory." J. of
         Phys. Ocean (2007): 1-35.
 
-        Guerra, Maricarmen, and Jim Thomson. "Turbulence measurements from 
-        five-beam acoustic Doppler current profilers." Journal of Atmospheric 
+        Guerra, Maricarmen, and Jim Thomson. "Turbulence measurements from
+        five-beam acoustic Doppler current profilers." Journal of Atmospheric
         and Oceanic Technology 34.6 (2017): 1267-1284.
         """
 
         # Check that beam 5 velocity exists
-        if 'vel_b5' not in ds.data_vars:
+        if "vel_b5" not in ds.data_vars:
             raise Exception("Must have 5th beam data to use this function.")
 
         # Run through warnings
         b_angle, noise = self._stress_func_warnings(
-            ds, beam_angle, noise, tilt_thresh=10)
+            ds, beam_angle, noise, tilt_thresh=10
+        )
 
         # Fetch beam order
-        beam_order, phi2, phi3 = self._check_orientation(
-            ds, orientation, beam5=True)
+        beam_order, phi2, phi3 = self._check_orientation(ds, orientation, beam5=True)
 
         # Calculate beam variance and subtract noise
-        time = self.mean(ds['time'].values)
+        time = self.mean(ds["time"].values)
         bp2_ = self._beam_variance(ds, time, noise, beam_order, n_beams=5)
 
         # Run tke and stress calculations
         th = np.deg2rad(b_angle)
         sin = np.sin
         cos = np.cos
-        denm = -4 * sin(th)**6 * cos(th)**2
+        denm = -4 * sin(th) ** 6 * cos(th) ** 2
 
-        upup_ = (-2*sin(th)**4*cos(th)**2*(bp2_[1]+bp2_[0]-2*cos(th)**2*bp2_[4]) +
-                 2*sin(th)**5*cos(th)*phi3*(bp2_[1]-bp2_[0])) / denm
-
-        vpvp_ = (-2*sin(th)**4*cos(th)**2*(bp2_[3]+bp2_[0]-2*cos(th)**2*bp2_[4]) -
-                 2*sin(th)**4*cos(th)**2*phi3*(bp2_[1]-bp2_[0]) +
-                 2*sin(th)**3*cos(th)**3*phi3*(bp2_[1]-bp2_[0]) -
-                 2*sin(th)**5*cos(th)*phi2*(bp2_[3]-bp2_[2])) / denm
-
-        wpwp_ = (-2*sin(th)**5*cos(th) *
-                 (bp2_[1]-bp2_[0] + 2*sin(th)**5*cos(th)*phi2*(bp2_[3]-bp2_[2]) -
-                  4*sin(th)**6*cos(th)**2*bp2_[4])) / denm
+        upup_ = (
+            -2
+            * sin(th) ** 4
+            * cos(th) ** 2
+            * (bp2_[1] + bp2_[0] - 2 * cos(th) ** 2 * bp2_[4])
+            + 2 * sin(th) ** 5 * cos(th) * phi3 * (bp2_[1] - bp2_[0])
+        ) / denm
+
+        vpvp_ = (
+            -2
+            * sin(th) ** 4
+            * cos(th) ** 2
+            * (bp2_[3] + bp2_[0] - 2 * cos(th) ** 2 * bp2_[4])
+            - 2 * sin(th) ** 4 * cos(th) ** 2 * phi3 * (bp2_[1] - bp2_[0])
+            + 2 * sin(th) ** 3 * cos(th) ** 3 * phi3 * (bp2_[1] - bp2_[0])
+            - 2 * sin(th) ** 5 * cos(th) * phi2 * (bp2_[3] - bp2_[2])
+        ) / denm
+
+        wpwp_ = (
+            -2
+            * sin(th) ** 5
+            * cos(th)
+            * (
+                bp2_[1]
+                - bp2_[0]
+                + 2 * sin(th) ** 5 * cos(th) * phi2 * (bp2_[3] - bp2_[2])
+                - 4 * sin(th) ** 6 * cos(th) ** 2 * bp2_[4]
+            )
+        ) / denm
 
         tke_vec = xr.DataArray(
-            np.stack([upup_, vpvp_, wpwp_]).astype('float32'),
-            coords={'tke': ["upup_", "vpvp_", "wpwp_"],
-                    'range': ds.range,
-                    'time': time},
-            attrs={'units': 'm2 s-2',
-                   'long_name': 'TKE Vector',
-                   'standard_name': 'specific_turbulent_kinetic_energy_of_sea_water'})
+            np.stack([upup_, vpvp_, wpwp_]).astype("float32"),
+            coords={
+                "tke": ["upup_", "vpvp_", "wpwp_"],
+                "range": ds.range,
+                "time": time,
+            },
+            attrs={
+                "units": "m2 s-2",
+                "long_name": "TKE Vector",
+                "standard_name": "specific_turbulent_kinetic_energy_of_sea_water",
+            },
+        )
 
         if tke_only:
             return tke_vec
 
         else:
             # Guerra Thomson calculate u'v' bar from from the covariance of u' and v'
-            ds.velds.rotate2('inst')
+            ds.velds.rotate2("inst")
             vel = self.detrend(ds.vel.values)
-            upvp_ = np.nanmean(vel[0] * vel[1], axis=-1,
-                               dtype=np.float64).astype(np.float32)
-
-            upwp_ = (sin(th)**5*cos(th)*(bp2_[1]-bp2_[0]) +
-                     2*sin(th)**4*cos(th)*2*phi3*(bp2_[1]+bp2_[0]) -
-                     4*sin(th)**4*cos(th)*2*phi3*bp2_[4] -
-                     4*sin(th)**6*cos(th)*2*phi2*upvp_) / denm
-
-            vpwp_ = (sin(th)**5*cos(th)*(bp2_[3]-bp2_[2]) -
-                     2*sin(th)**4*cos(th)*2*phi2*(bp2_[3]+bp2_[2]) +
-                     4*sin(th)**4*cos(th)*2*phi2*bp2_[4] +
-                     4*sin(th)**6*cos(th)*2*phi3*upvp_) / denm
+            upvp_ = np.nanmean(vel[0] * vel[1], axis=-1, dtype=np.float64).astype(
+                np.float32
+            )
+
+            upwp_ = (
+                sin(th) ** 5 * cos(th) * (bp2_[1] - bp2_[0])
+                + 2 * sin(th) ** 4 * cos(th) * 2 * phi3 * (bp2_[1] + bp2_[0])
+                - 4 * sin(th) ** 4 * cos(th) * 2 * phi3 * bp2_[4]
+                - 4 * sin(th) ** 6 * cos(th) * 2 * phi2 * upvp_
+            ) / denm
+
+            vpwp_ = (
+                sin(th) ** 5 * cos(th) * (bp2_[3] - bp2_[2])
+                - 2 * sin(th) ** 4 * cos(th) * 2 * phi2 * (bp2_[3] + bp2_[2])
+                + 4 * sin(th) ** 4 * cos(th) * 2 * phi2 * bp2_[4]
+                + 4 * sin(th) ** 6 * cos(th) * 2 * phi3 * upvp_
+            ) / denm
 
             stress_vec = xr.DataArray(
-                np.stack([upvp_, upwp_, vpwp_]).astype('float32'),
-                coords={'tau': ["upvp_", "upwp_", "vpwp_"],
-                        'range': ds.range,
-                        'time': time},
-                attrs={'units': 'm2 s-2',
-                       'long_name': 'Specific Reynolds Stress Vector'})
+                np.stack([upvp_, upwp_, vpwp_]).astype("float32"),
+                coords={
+                    "tau": ["upvp_", "upwp_", "vpwp_"],
+                    "range": ds.range,
+                    "time": time,
+                },
+                attrs={
+                    "units": "m2 s-2",
+                    "long_name": "Specific Reynolds Stress Vector",
+                },
+            )
 
             return tke_vec, stress_vec
 
-    def total_turbulent_kinetic_energy(self, 
-                                       ds, 
-                                       noise=None, 
-                                       orientation=None, 
-                                       beam_angle=None):
+    def total_turbulent_kinetic_energy(
+        self, ds, noise=None, orientation=None, beam_angle=None
+    ):
         """
-        Calculate magnitude of turbulent kinetic energy from 5-beam ADCP. 
+        Calculate magnitude of turbulent kinetic energy from 5-beam ADCP.
 
         Parameters
         ----------
         ds : xarray.Dataset
           Raw dataset in beam coordinates
         noise : int or xarray.DataArray, default=0 (time)
           Doppler noise level in units of m/s
@@ -722,107 +787,112 @@
 
         Notes
         -----
         This function is a wrapper around 'calc_stress_5beam' that then
         combines the TKE components.
 
         Warning: the integral length scale of turbulence captured by the
-        ADCP measurements (i.e. the size of turbulent structures) increases 
+        ADCP measurements (i.e. the size of turbulent structures) increases
         with increasing range from the instrument.
         """
 
         tke_vec = self.stress_tensor_5beam(
-            ds, noise, orientation, beam_angle, tke_only=True)
+            ds, noise, orientation, beam_angle, tke_only=True
+        )
 
-        tke = tke_vec.sum('tke') / 2
-        tke.attrs['units'] = 'm2 s-2'
-        tke.attrs['long_name'] = 'TKE Magnitude',
-        tke.attrs['standard_name'] = 'specific_turbulent_kinetic_energy_of_sea_water'
+        tke = tke_vec.sum("tke") / 2
+        tke.attrs["units"] = "m2 s-2"
+        tke.attrs["long_name"] = ("TKE Magnitude",)
+        tke.attrs["standard_name"] = "specific_turbulent_kinetic_energy_of_sea_water"
 
-        return tke.astype('float32')
+        return tke.astype("float32")
 
     def check_turbulence_cascade_slope(self, psd, freq_range=[0.2, 0.4]):
         """
-        This function calculates the slope of the PSD, the power spectra 
+        This function calculates the slope of the PSD, the power spectra
         of velocity, within the given frequency range. The purpose of this
-        function is to check that the region of the PSD containing the 
+        function is to check that the region of the PSD containing the
         isotropic turbulence cascade decreases at a rate of :math:`f^{-5/3}`.
 
         Parameters
         ----------
         psd : xarray.DataArray ([[range,] time,] freq)
           The power spectral density (1D, 2D or 3D)
         freq_range : iterable(2) (default: [6.28, 12.57])
-          The range over which the isotropic turbulence cascade occurs, in 
+          The range over which the isotropic turbulence cascade occurs, in
           units of the psd frequency vector (Hz or rad/s)
 
         Returns
         -------
         (m, b): tuple (slope, y-intercept)
-          A tuple containing the coefficients of the log-adjusted linear 
+          A tuple containing the coefficients of the log-adjusted linear
           regression between PSD and frequency
 
         Notes
         -----
         Calculates slope based on the `standard` formula for dissipation:
 
         .. math:: S(k) = \\alpha \\epsilon^{2/3} k^{-5/3} + N
 
-        The slope of the isotropic turbulence cascade, which should be 
-        equal to :math:`k^{-5/3}` or :math:`f^{-5/3}`, where k and f are 
-        the wavenumber and frequency vectors, is estimated using linear 
+        The slope of the isotropic turbulence cascade, which should be
+        equal to :math:`k^{-5/3}` or :math:`f^{-5/3}`, where k and f are
+        the wavenumber and frequency vectors, is estimated using linear
         regression with a log transformation:
 
         .. math:: log10(y) = m*log10(x) + b
 
         Which is equivalent to
 
         .. math:: y = 10^{b} x^{m}
 
-        Where :math:`y` is S(k) or S(f), :math:`x` is k or f, :math:`m` 
-        is the slope (ideally -5/3), and :math:`10^{b}` is the intercept of 
+        Where :math:`y` is S(k) or S(f), :math:`x` is k or f, :math:`m`
+        is the slope (ideally -5/3), and :math:`10^{b}` is the intercept of
         y at x^m=1.
         """
 
         if not isinstance(psd, xr.DataArray):
             raise TypeError("`psd` must be an instance of `xarray.DataArray`.")
-        if not hasattr(freq_range, '__iter__') or len(freq_range) != 2: 
+        if not hasattr(freq_range, "__iter__") or len(freq_range) != 2:
             raise ValueError("`freq_range` must be an iterable of length 2.")
-        
+
         idx = np.where((freq_range[0] < psd.freq) & (psd.freq < freq_range[1]))
         idx = idx[0]
 
-        x = np.log10(psd['freq'].isel(freq=idx))
+        x = np.log10(psd["freq"].isel(freq=idx))
         y = np.log10(psd.isel(freq=idx))
 
-        y_bar = y.mean('freq')
-        x_bar = x.mean('freq')
+        y_bar = y.mean("freq")
+        x_bar = x.mean("freq")
 
         # using the formula to calculate the slope and intercept
         n = np.sum((x - x_bar) * (y - y_bar), axis=0)
-        d = np.sum((x - x_bar)**2, axis=0)
+        d = np.sum((x - x_bar) ** 2, axis=0)
 
-        m = n/d
-        b = y_bar - m*x_bar
+        m = n / d
+        b = y_bar - m * x_bar
 
         return m, b
 
-    def dissipation_rate_LT83(self, psd, U_mag, freq_range=[0.2, 0.4]):
+    def dissipation_rate_LT83(self, psd, U_mag, freq_range=[0.2, 0.4], noise=None):
         """
         Calculate the TKE dissipation rate from the velocity spectra.
 
         Parameters
         ----------
         psd : xarray.DataArray (time,f)
           The power spectral density from a single depth bin (range)
         U_mag : xarray.DataArray (time)
           The bin-averaged horizontal velocity (a.k.a. speed) from a single depth bin (range)
         f_range : iterable(2)
-          The range over which to integrate/average the spectrum, in units 
+          The range over which to integrate/average the spectrum, in units
           of the psd frequency vector (Hz or rad/s)
+        noise : float or array-like
+          Instrument noise level in same units as velocity. Typically
+          found from `adp.turbulence.doppler_noise_level`.
+          Default: None.
 
         Returns
         -------
         dissipation_rate : xarray.DataArray (...,n_time)
           Turbulent kinetic energy dissipation rate
 
         Notes
@@ -846,41 +916,55 @@
         .. math:: S(\\omega) = \\alpha \\epsilon^{2/3} f^{-5/3} (U/(2*\\pi))^{2/3} + N
 
         LT83 : Lumley and Terray, "Kinematics of turbulence convected
         by a random wave field". JPO, 1983, vol13, pp2000-2007.
         """
 
         if len(psd.shape) != 2:
-            raise Exception('PSD should be 2-dimensional (time, frequency)')
+            raise Exception("PSD should be 2-dimensional (time, frequency)")
         if len(U_mag.shape) != 1:
-            raise Exception('U_mag should be 1-dimensional (time)')
-        if not hasattr(freq_range, '__iter__') or len(freq_range) != 2: 
+            raise Exception("U_mag should be 1-dimensional (time)")
+        if not hasattr(freq_range, "__iter__") or len(freq_range) != 2:
             raise ValueError("`freq_range` must be an iterable of length 2.")
-        
+        if noise is not None:
+            if np.shape(noise)[0] != np.shape(psd)[0]:
+                raise Exception("Noise should have same first dimension as PSD")
+        else:
+            noise = np.array(0)
+
+        # Noise subtraction from binner.TimeBinner._psd_base
+        psd = psd.copy()
+        if noise is not None:
+            psd -= noise**2 / (self.fs / 2)
+            psd = psd.where(psd > 0, np.min(np.abs(psd)) / 100)
+
         freq = psd.freq
         idx = np.where((freq_range[0] < freq) & (freq < freq_range[1]))
         idx = idx[0]
 
-        if freq.units == 'Hz':
-            U = U_mag/(2*np.pi)
+        if freq.units == "Hz":
+            U = U_mag / (2 * np.pi)
         else:
             U = U_mag
 
         a = 0.5
-        out = (psd[:, idx] * freq[idx]**(5/3) /
-               a).mean(axis=-1)**(3/2) / U.values
+        out = (psd[:, idx] * freq[idx] ** (5 / 3) / a).mean(axis=-1) ** (
+            3 / 2
+        ) / U.values
 
         return xr.DataArray(
-            out.astype('float32'),
-            attrs={'units': 'm2 s-3',
-                   'long_name': 'TKE Dissipation Rate',
-                   'standard_name': 'specific_turbulent_kinetic_energy_dissipation_in_sea_water',
-                   'description': 'TKE dissipation rate calculated using '
-                                  'the method from Lumley and Terray, 1983',
-                   })
+            out.astype("float32"),
+            attrs={
+                "units": "m2 s-3",
+                "long_name": "TKE Dissipation Rate",
+                "standard_name": "specific_turbulent_kinetic_energy_dissipation_in_sea_water",
+                "description": "TKE dissipation rate calculated using "
+                "the method from Lumley and Terray, 1983",
+            },
+        )
 
     def dissipation_rate_SF(self, vel_raw, r_range=[1, 5]):
         """
         Calculate TKE dissipation rate from ADCP along-beam velocity using the
         "structure function" (SF) method.
 
         Parameters
@@ -900,26 +984,26 @@
         noise : xarray.DataArray (range, time)
           Noise offset estimated from the structure function at r = 0
         structure_function : xarray.DataArray (range, r, time)
           Structure function D(z,r)
 
         Notes
         -----
-        Dissipation rate outputted by this function is only valid if the isotropic 
-        turbulence cascade can be seen in the TKE spectra. 
+        Dissipation rate outputted by this function is only valid if the isotropic
+        turbulence cascade can be seen in the TKE spectra.
 
-        Velocity data must be in beam coordinates and should be cleaned of surface 
+        Velocity data must be in beam coordinates and should be cleaned of surface
         interference.
 
         This method calculates the 2nd order structure function:
 
         .. math:: D(z,r) = [(u'(z) - u`(z+r))^2]
 
-        where `u'` is the velocity fluctuation `z` is the depth bin, 
-        `r` is the separation between depth bins, and [] denotes a time average 
+        where `u'` is the velocity fluctuation `z` is the depth bin,
+        `r` is the separation between depth bins, and [] denotes a time average
         (size 'ADPBinner.n_bin').
 
         The stucture function can then be used to estimate the dissipation rate:
 
         .. math:: D(z,r) = C^2 \\epsilon^{2/3} r^{2/3} + N
 
         where `C` is a constant (set to 2.1), `\\epsilon` is the dissipation rate,
@@ -930,105 +1014,107 @@
         Wiles, et al, "A novel technique for measuring the rate of
         turbulent dissipation in the marine environment"
         GRL, 2006, 33, L21608.
         """
 
         if not isinstance(vel_raw, xr.DataArray):
             raise TypeError("`vel_raw` must be an instance of `xarray.DataArray`.")
-        if not hasattr(r_range, '__iter__') or len(r_range) != 2: 
+        if not hasattr(r_range, "__iter__") or len(r_range) != 2:
             raise ValueError("`r_range` must be an iterable of length 2.")
 
         if len(vel_raw.shape) != 2:
             raise Exception(
-                "Function input must be single beam and in 'beam' coordinate system")
+                "Function input must be single beam and in 'beam' coordinate system"
+            )
 
-        if 'range_b5' in vel_raw.dims:
+        if "range_b5" in vel_raw.dims:
             rng = vel_raw.range_b5
             time = self.mean(vel_raw.time_b5.values)
         else:
             rng = vel_raw.range
             time = self.mean(vel_raw.time.values)
 
         # bm shape is [range, ensemble time, 'data within ensemble']
         bm = self.demean(vel_raw.values)  # take out the ensemble mean
 
-        e = np.empty(bm.shape[:2], dtype='float32')*np.nan
-        n = np.empty(bm.shape[:2], dtype='float32')*np.nan
+        e = np.empty(bm.shape[:2], dtype="float32") * np.nan
+        n = np.empty(bm.shape[:2], dtype="float32") * np.nan
 
         bin_size = round(np.diff(rng)[0], 3)
-        R = int(r_range[0]/bin_size)
-        r = np.arange(bin_size, r_range[1]+bin_size, bin_size)
+        R = int(r_range[0] / bin_size)
+        r = np.arange(bin_size, r_range[1] + bin_size, bin_size)
 
         # D(z,r,time)
         D = np.zeros((bm.shape[0], r.size, bm.shape[1]))
         for r_value in r:
             # the i in d is the index based on r and bin size
             # bin size index, > 1
-            i = int(r_value/bin_size)
+            i = int(r_value / bin_size)
             for idx in range(bm.shape[1]):  # for each ensemble
                 # subtract the variance of adjacent depth cells
-                d = np.nanmean(
-                    (bm[:-i, idx, :] - bm[i:, idx, :]) ** 2, axis=-1)
+                d = np.nanmean((bm[:-i, idx, :] - bm[i:, idx, :]) ** 2, axis=-1)
 
                 # have to insert 0/nan in first bin to match length
                 spaces = np.empty((i,))
                 spaces[:] = np.NaN
-                D[:, i-1, idx] = np.concatenate((spaces, d))
+                D[:, i - 1, idx] = np.concatenate((spaces, d))
 
         # find best fit line y = mx + b (aka D(z,r) = A*r^2/3 + N) to solve
         # epsilon for each depth and ensemble
         for idx in range(bm.shape[1]):  # for each ensemble
             # start at minimum r_range and work up to surface
             for i in range(D.shape[1], D.shape[0]):
                 # average ensembles together
                 if not all(np.isnan(D[i, R:, idx])):  # if no nan's
-                    e[i, idx], n[i, idx] = np.polyfit(r[R:] ** 2/3,
-                                                      D[i, R:, idx],
-                                                      deg=1)
+                    e[i, idx], n[i, idx] = np.polyfit(
+                        r[R:] ** 2 / 3, D[i, R:, idx], deg=1
+                    )
                 else:
                     e[i, idx], n[i, idx] = np.nan, np.nan
         # A taken as 2.1, n = y-intercept
-        epsilon = (e/2.1)**(3/2)
-        noise = np.sqrt(n/2)
+        epsilon = (e / 2.1) ** (3 / 2)
+        noise = np.sqrt(n / 2)
 
         epsilon = xr.DataArray(
-            epsilon.astype('float32'),
-            coords={vel_raw.dims[0]: rng,
-                    vel_raw.dims[1]: time},
+            epsilon.astype("float32"),
+            coords={vel_raw.dims[0]: rng, vel_raw.dims[1]: time},
             dims=vel_raw.dims,
-            attrs={'units': 'm2 s-3',
-                   'long_name': 'TKE Dissipation Rate',
-                   'standard_name': 'specific_turbulent_kinetic_energy_dissipation_in_sea_water',
-                   'description': 'TKE dissipation rate calculated from the '
-                                  '"structure function" method from Wiles et al, 2006.'
-                   })
+            attrs={
+                "units": "m2 s-3",
+                "long_name": "TKE Dissipation Rate",
+                "standard_name": "specific_turbulent_kinetic_energy_dissipation_in_sea_water",
+                "description": "TKE dissipation rate calculated from the "
+                '"structure function" method from Wiles et al, 2006.',
+            },
+        )
 
         noise = xr.DataArray(
-            noise.astype('float32'),
-            coords={vel_raw.dims[0]: rng,
-                    vel_raw.dims[1]: time},
-            attrs={'units': 'm s-1',
-                   'long_name': 'Structure Function Noise Offset',
-                   })
+            noise.astype("float32"),
+            coords={vel_raw.dims[0]: rng, vel_raw.dims[1]: time},
+            attrs={
+                "units": "m s-1",
+                "long_name": "Structure Function Noise Offset",
+            },
+        )
 
         SF = xr.DataArray(
-            D.astype('float32'),
-            coords={vel_raw.dims[0]: rng,
-                    'range_SF': r,
-                    vel_raw.dims[1]: time},
-            attrs={'units': 'm2 s-2',
-                   'long_name': 'Structure Function D(z,r)',
-                   'description': '"Structure function" from Wiles et al, 2006.'
-                   })
+            D.astype("float32"),
+            coords={vel_raw.dims[0]: rng, "range_SF": r, vel_raw.dims[1]: time},
+            attrs={
+                "units": "m2 s-2",
+                "long_name": "Structure Function D(z,r)",
+                "description": '"Structure function" from Wiles et al, 2006.',
+            },
+        )
 
         return epsilon, noise, SF
 
     def friction_velocity(self, ds_avg, upwp_, z_inds=slice(1, 5), H=None):
         """
-        Approximate friction velocity from shear stress using a 
+        Approximate friction velocity from shear stress using a
         logarithmic profile.
 
         Parameters
         ----------
         ds_avg : xarray.Dataset
           Bin-averaged dataset containing `stress_vec`
         upwp_ : xarray.DataArray
@@ -1047,22 +1133,24 @@
 
         if not isinstance(ds_avg, xr.Dataset):
             raise TypeError("`ds_avg` must be an instance of `xarray.Dataset`.")
         if not isinstance(upwp_, xr.DataArray):
             raise TypeError("`upwp_` must be an instance of `xarray.DataArray`.")
         if not isinstance(z_inds, slice):
             raise TypeError("`z_inds` must be an instance of `slice(int,int)`.")
-        
+
         if not H:
             H = ds_avg.depth.values
-        z = ds_avg['range'].values
+        z = ds_avg["range"].values
         upwp_ = upwp_.values
 
         sign = np.nanmean(np.sign(upwp_[z_inds, :]), axis=0)
-        u_star = np.nanmean(sign * upwp_[z_inds, :] /
-                            (1 - z[z_inds, None] / H), axis=0) ** 0.5
+        u_star = (
+            np.nanmean(sign * upwp_[z_inds, :] / (1 - z[z_inds, None] / H), axis=0)
+            ** 0.5
+        )
 
         return xr.DataArray(
-            u_star.astype('float32'),
-            coords={'time': ds_avg.time},
-            attrs={'units': 'm s-1',
-                   'long_name': 'Friction Velocity'})
+            u_star.astype("float32"),
+            coords={"time": ds_avg.time},
+            attrs={"units": "m s-1", "long_name": "Friction Velocity"},
+        )
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/adv/clean.py` & `mhkit-0.8.0/mhkit/dolfyn/adv/clean.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,32 +1,34 @@
 """Module containing functions to clean data
 """
+
 import numpy as np
 import warnings
 from ..velocity import VelBinner
 from ..tools.misc import group, slice1d_along_axis
-warnings.filterwarnings('ignore', category=np.RankWarning)
+
+warnings.filterwarnings("ignore", category=np.RankWarning)
 
 sin = np.sin
 cos = np.cos
 
 
-def clean_fill(u, mask, npt=12, method='cubic', maxgap=6):
+def clean_fill(u, mask, npt=12, method="cubic", maxgap=6):
     """
     Interpolate over mask values in timeseries data using the specified method
 
     Parameters
     ----------
     u : xarray.DataArray
       The dataArray to clean.
     mask : bool
       Logical tensor of elements to "nan" out (from `spikeThresh`, `rangeLimit`,
       or `GN2002`) and replace
     npt : int
-      The number of points on either side of the bad values that 
+      The number of points on either side of the bad values that
       interpolation occurs over
     method : string
       Interpolation method to use (linear, cubic, pchip, etc). Default is 'cubic'
     maxgap : numeric
       Maximum gap of missing data to interpolate across. Default is None
 
     Returns
@@ -39,15 +41,15 @@
     xarray.DataArray.interpolate_na()
     """
 
     # Apply mask
     u.values[..., mask] = np.nan
 
     # Remove bad data for 2D+ and 1D timeseries variables
-    if 'dir' in u.dims:
+    if "dir" in u.dims:
         for i in range(u.shape[0]):
             u[i] = _interp_nan(u[i], npt, method, maxgap)
     else:
         u = _interp_nan(u, npt, method, maxgap)
 
     return u
 
@@ -97,21 +99,20 @@
                 # Reset ntail
                 ntail = 0
             else:  # No
                 # Add to the tail of the block.
                 ntail += 1
             pos += 1
 
-            if (ntail == npt or pos == len(da)):
+            if ntail == npt or pos == len(da):
                 # This is the block we are interpolating over
                 i_int = i[start:pos]
-                da[i_int] = da[i_int].interpolate_na(dim=da.dims[-1],
-                                                     method=method,
-                                                     use_coordinate=True,
-                                                     limit=maxgap)
+                da[i_int] = da[i_int].interpolate_na(
+                    dim=da.dims[-1], method=method, use_coordinate=True, limit=maxgap
+                )
                 # Reset
                 searching = True
                 ntail = 0
     return da
 
 
 def fill_nan_ensemble_mean(u, mask, fs, window):
@@ -137,15 +138,15 @@
 
     Notes
     -----
     Gaps larger than the ensemble size will not get filled in.
     """
 
     u = u.where(~mask)
-    bnr = VelBinner(n_bin=window*fs, fs=fs)
+    bnr = VelBinner(n_bin=window * fs, fs=fs)
 
     if len(u.shape) == 1:
         var = u.values[None, :]
     else:
         var = u.values
 
     vel = np.empty(var.shape)
@@ -154,33 +155,33 @@
 
     # If there are extra datapoints trimmed off after the last ensemble,
     # take them into account by filling in another ensemble with means
     diff = vel.shape[-1] - vel_reshaped.size // vel.shape[0]
     # diff = number of extra points
     extra_nans = vel_reshaped.shape[-1] - diff
     if diff:
-        vel = np.empty((var.shape[0], var.shape[-1]+extra_nans))
+        vel = np.empty((var.shape[0], var.shape[-1] + extra_nans))
         extra = var[:, -diff:]
-        empty = np.empty((vel.shape[0], extra_nans))*np.nan
+        empty = np.empty((vel.shape[0], extra_nans)) * np.nan
         extra = np.concatenate((extra, empty), axis=-1)
-        vel_reshaped = np.concatenate(
-            (vel_reshaped, extra[:, None, :]), axis=1)
+        vel_reshaped = np.concatenate((vel_reshaped, extra[:, None, :]), axis=1)
         extra_mean = np.nanmean(extra, axis=-1)
         vel_mean = np.concatenate((vel_mean, extra_mean[:, None]), axis=-1)
 
     # Create a matrix the same size as the reshaped array, and mask out the
     # non-missing values. Then add the two matrices together.
     vel_mean_matrix = np.tile(vel_mean[..., None], (1, 1, bnr.n_bin))
     vel_missing = np.isnan(vel_reshaped)
     vel_mask = np.ma.masked_array(vel_mean_matrix, ~vel_missing).filled(np.nan)
-    vel_filled = np.where(np.isnan(vel_reshaped), vel_mask,
-                          vel_reshaped + np.nan_to_num(vel_mask))
+    vel_filled = np.where(
+        np.isnan(vel_reshaped), vel_mask, vel_reshaped + np.nan_to_num(vel_mask)
+    )
     # "Unshape" the data
     for i in range(var.shape[0]):
-        vel[i] = np.ravel(vel_filled[i], 'C')
+        vel[i] = np.ravel(vel_filled[i], "C")
 
     if diff:  # Trim off the extra means
         u.values = np.squeeze(vel[:, :-extra_nans])
     else:
         u.values = np.squeeze(vel)
 
     return u
@@ -208,15 +209,15 @@
     mask = (du > thresh) + (du < -thresh)
 
     return mask
 
 
 def range_limit(u, range=[-5, 5]):
     """
-    Returns a logical vector that is True where the values of `u` are 
+    Returns a logical vector that is True where the values of `u` are
     outside of `range`.
 
     Parameters
     ----------
     u : xarray.DataArray
       The timeseries data to clean.
     range : list
@@ -228,55 +229,57 @@
       Logical vector with spikes labeled as 'True'
     """
 
     return ~((range[0] < u.values) & (u.values < range[1]))
 
 
 def _calcab(al, Lu_std_u, Lu_std_d2u):
-    """Solve equations 10 and 11 of Goring+Nikora2002
-    """
-    return tuple(np.linalg.solve(
-        np.array([[cos(al) ** 2, sin(al) ** 2],
-                  [sin(al) ** 2, cos(al) ** 2]]),
-        np.array([(Lu_std_u) ** 2, (Lu_std_d2u) ** 2])))
+    """Solve equations 10 and 11 of Goring+Nikora2002"""
+    return tuple(
+        np.linalg.solve(
+            np.array([[cos(al) ** 2, sin(al) ** 2], [sin(al) ** 2, cos(al) ** 2]]),
+            np.array([(Lu_std_u) ** 2, (Lu_std_d2u) ** 2]),
+        )
+    )
 
 
 def _phaseSpaceThresh(u):
     if u.ndim == 1:
         u = u[:, None]
     u = np.array(u)
     Lu = (2 * np.log(u.shape[0])) ** 0.5
     u = u - u.mean(0)
     du = np.zeros_like(u)
     d2u = np.zeros_like(u)
     # Take the centered difference.
     du[1:-1] = (u[2:] - u[:-2]) / 2
     # And again.
     d2u[2:-2] = (du[1:-1][2:] - du[1:-1][:-2]) / 2
-    p = (u ** 2 + du ** 2 + d2u ** 2)
+    p = u**2 + du**2 + d2u**2
     std_u = np.std(u, axis=0)
     std_du = np.std(du, axis=0)
     std_d2u = np.std(d2u, axis=0)
-    alpha = np.arctan2(np.sum(u * d2u, axis=0), np.sum(u ** 2, axis=0))
+    alpha = np.arctan2(np.sum(u * d2u, axis=0), np.sum(u**2, axis=0))
     a = np.empty_like(alpha)
     b = np.empty_like(alpha)
     with warnings.catch_warnings() as w:
         warnings.filterwarnings(
-            'ignore', category=RuntimeWarning, message='invalid value encountered in ')
+            "ignore", category=RuntimeWarning, message="invalid value encountered in "
+        )
         for idx, al in enumerate(alpha):
             a[idx], b[idx] = _calcab(al, Lu * std_u[idx], Lu * std_d2u[idx])
         theta = np.arctan2(du, u)
-        phi = np.arctan2((du ** 2 + u ** 2) ** 0.5, d2u)
-        pe = (((sin(phi) * cos(theta) * cos(alpha) +
-                cos(phi) * sin(alpha)) ** 2) / a +
-              ((sin(phi) * cos(theta) * sin(alpha) -
-                cos(phi) * cos(alpha)) ** 2) / b +
-              ((sin(phi) * sin(theta)) ** 2) / (Lu * std_du) ** 2) ** -1
+        phi = np.arctan2((du**2 + u**2) ** 0.5, d2u)
+        pe = (
+            ((sin(phi) * cos(theta) * cos(alpha) + cos(phi) * sin(alpha)) ** 2) / a
+            + ((sin(phi) * cos(theta) * sin(alpha) - cos(phi) * cos(alpha)) ** 2) / b
+            + ((sin(phi) * sin(theta)) ** 2) / (Lu * std_du) ** 2
+        ) ** -1
     pe[:, np.isnan(pe[0, :])] = 0
-    return (p > pe).flatten('F')
+    return (p > pe).flatten("F")
 
 
 def GN2002(u, npt=5000):
     """
     The Goring & Nikora 2002 'despiking' method, with Wahl2003 correction.
     Returns a logical vector that is true where spikes are identified.
 
@@ -293,24 +296,24 @@
       Logical vector with spikes labeled as 'True'
     """
 
     if not isinstance(u, np.ndarray):
         return GN2002(u.values, npt=npt)
 
     if u.ndim > 1:
-        mask = np.zeros(u.shape, dtype='bool')
+        mask = np.zeros(u.shape, dtype="bool")
         for slc in slice1d_along_axis(u.shape, -1):
             mask[slc] = GN2002(u[slc], npt=npt)
         return mask
 
-    mask = np.zeros(len(u), dtype='bool')
+    mask = np.zeros(len(u), dtype="bool")
 
     # Find large bad segments (>npt/10):
     # group returns a vector of slice objects.
-    bad_segs = group(np.isnan(u), min_length=int(npt//10))
+    bad_segs = group(np.isnan(u), min_length=int(npt // 10))
     if bad_segs.size > 2:
         # Break them up into separate regions:
         sp = 0
         ep = len(u)
 
         # Skip start and end bad_segs:
         if bad_segs[0].start == sp:
@@ -319,29 +322,30 @@
         if bad_segs[-1].stop == ep:
             ep = bad_segs[-1].start
             bad_segs = bad_segs[:-1]
 
         for ind in range(len(bad_segs)):
             bs = bad_segs[ind]  # bs is a slice object.
             # Clean the good region:
-            mask[sp:bs.start] = GN2002(u[sp:bs.start], npt=npt)
+            mask[sp : bs.start] = GN2002(u[sp : bs.start], npt=npt)
             sp = bs.stop
         # Clean the last good region.
         mask[sp:ep] = GN2002(u[sp:ep], npt=npt)
         return mask
 
     c = 0
     ntot = len(u)
     nbins = int(ntot // npt)
     mask_last = np.zeros_like(mask) + np.inf
     mask[0] = True  # make sure we start.
     while mask.any():
-        mask[:nbins * npt] = _phaseSpaceThresh(
-            np.array(np.reshape(u[:(nbins * npt)], (npt, nbins), order='F')))
+        mask[: nbins * npt] = _phaseSpaceThresh(
+            np.array(np.reshape(u[: (nbins * npt)], (npt, nbins), order="F"))
+        )
         mask[-npt:] = _phaseSpaceThresh(u[-npt:])
         c += 1
         if c >= 100:
-            raise Exception('GN2002 loop-limit exceeded.')
+            raise Exception("GN2002 loop-limit exceeded.")
         if mask.sum() >= mask_last.sum():
             break
         mask_last = mask.copy()
     return mask
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/adv/motion.py` & `mhkit-0.8.0/mhkit/dolfyn/adv/motion.py`

 * *Files 8% similar despite different names*

```diff
@@ -7,29 +7,32 @@
 from ..rotate import vector as rot
 from ..rotate.api import _make_model, rotate2
 
 
 class MissingDataError(ValueError):
     pass
 
+
 class DataAlreadyProcessedError(Exception):
     pass
 
+
 class MissingRequiredDataError(Exception):
     pass
 
+
 def _get_body2imu(make_model):
-    if make_model == 'nortek vector':
+    if make_model == "nortek vector":
         # In inches it is: (0.25, 0.25, 5.9)
         return np.array([0.00635, 0.00635, 0.14986])
     else:
         raise Exception("The imu->body vector is unknown for this instrument.")
 
 
-class CalcMotion():
+class CalcMotion:
     """
     A 'calculator' for computing the velocity of points that are
     rigidly connected to an ADV-body with an IMU.
 
     Parameters
     ----------
     ds : xarray.Dataset
@@ -40,120 +43,132 @@
     vel_filtfreq : float (optional)
       a second frequency to high-pass filter the integrated
       acceleration.  Default = 1/3 of `accel_filtfreq`
     """
 
     _default_accel_filtfreq = 0.03
 
-    def __init__(self, ds,
-                 accel_filtfreq=None,
-                 vel_filtfreq=None,
-                 to_earth=True):
-
+    def __init__(self, ds, accel_filtfreq=None, vel_filtfreq=None, to_earth=True):
         self.ds = ds
-        self._check_filtfreqs(accel_filtfreq,
-                              vel_filtfreq)
+        self._check_filtfreqs(accel_filtfreq, vel_filtfreq)
         self.to_earth = to_earth
 
         self._set_accel()
         self._set_acclow()
-        self.angrt = ds['angrt'].values  # No copy because not modified.
+        self.angrt = ds["angrt"].values  # No copy because not modified.
 
     def _check_filtfreqs(self, accel_filtfreq, vel_filtfreq):
-        datval = self.ds.attrs.get('motion accel_filtfreq Hz', None)
+        datval = self.ds.attrs.get("motion accel_filtfreq Hz", None)
         if datval is None:
             if accel_filtfreq is None:
                 accel_filtfreq = self._default_accel_filtfreq
                 # else use the accel_filtfreq value
         else:
             if accel_filtfreq is None:
                 accel_filtfreq = datval
             else:
                 if datval != accel_filtfreq:
                     warnings.warn(
                         f"The default accel_filtfreq is {datval} Hz. "
                         "Overriding this with the user-specified "
-                        "value: {accel_filtfreq} Hz.")
+                        "value: {accel_filtfreq} Hz."
+                    )
         if vel_filtfreq is None:
-            vel_filtfreq = self.ds.attrs.get('motion vel_filtfreq Hz', None)
+            vel_filtfreq = self.ds.attrs.get("motion vel_filtfreq Hz", None)
         if vel_filtfreq is None:
             vel_filtfreq = accel_filtfreq / 3.0
         self.accel_filtfreq = accel_filtfreq
         self.accelvel_filtfreq = vel_filtfreq
 
-    def _set_accel(self, ):
+    def _set_accel(
+        self,
+    ):
         ds = self.ds
-        if ds.coord_sys == 'inst':
-            self.accel = np.einsum('ij...,i...->j...',
-                                   ds['orientmat'].values,
-                                   ds['accel'].values)
-        elif self.ds.coord_sys == 'earth':
-            self.accel = ds['accel'].values.copy()
+        if ds.coord_sys == "inst":
+            self.accel = np.einsum(
+                "ij...,i...->j...", ds["orientmat"].values, ds["accel"].values
+            )
+        elif self.ds.coord_sys == "earth":
+            self.accel = ds["accel"].values.copy()
         else:
-            raise Exception(("Invalid coordinate system '%s'. The coordinate "
-                             "system must either be 'earth' or 'inst' to "
-                             "perform motion correction.")
-                            % (self.ds.coord_sys))
-
-    def _check_duty_cycle(self, ):
+            raise Exception(
+                (
+                    "Invalid coordinate system '%s'. The coordinate "
+                    "system must either be 'earth' or 'inst' to "
+                    "perform motion correction."
+                )
+                % (self.ds.coord_sys)
+            )
+
+    def _check_duty_cycle(
+        self,
+    ):
         """
         Function to check if duty cycle exists and if it is followed
         consistently in the datafile
         """
 
-        n_burst = self.ds.attrs.get('duty_cycle_n_burst')
+        n_burst = self.ds.attrs.get("duty_cycle_n_burst")
         if not n_burst:
             return
 
         # duty cycle interval in seconds
-        interval = self.ds.attrs.get('duty_cycle_interval')
+        interval = self.ds.attrs.get("duty_cycle_interval")
         actual_interval = (
-            self.ds.time[n_burst:].values - self.ds.time[:-n_burst].values)/1e9
+            self.ds.time[n_burst:].values - self.ds.time[:-n_burst].values
+        ) / 1e9
 
         rng = actual_interval.max() - actual_interval.min()
         mean = actual_interval.mean()
         # Range will vary depending on how datetime64 rounds the timestamp
         # But isn't an issue if it does
-        if rng > 2 or (mean > interval+1 and mean < interval-1):
+        if rng > 2 or (mean > interval + 1 and mean < interval - 1):
             raise Exception("Bad duty cycle detected")
 
         # If this passes, it means we're safe to blindly skip n_burst for every integral
         return n_burst
 
     def reshape(self, dat, n_bin):
         # Assumes shape is (3, time)
-        length = dat.shape[-1]//n_bin
-        return np.reshape(dat[..., :length*n_bin], (dat.shape[0], length, n_bin))
+        length = dat.shape[-1] // n_bin
+        return np.reshape(dat[..., : length * n_bin], (dat.shape[0], length, n_bin))
 
-    def _set_acclow(self, ):
+    def _set_acclow(
+        self,
+    ):
         # Check if file is duty cycled
         n = self._check_duty_cycle()
 
         if n:
-            warnings.warn("   Duty Cycle detected. "
-                          "Motion corrected data may contain edge effects "
-                          "at the beginning and end of each duty cycle.")
+            warnings.warn(
+                "   Duty Cycle detected. "
+                "Motion corrected data may contain edge effects "
+                "at the beginning and end of each duty cycle."
+            )
             self.accel = self.reshape(self.accel, n_bin=n)
 
         self.acclow = acc = self.accel.copy()
         if self.accel_filtfreq == 0:
             acc[:] = acc.mean(-1)[..., None]
         else:
             flt = ss.butter(1, self.accel_filtfreq / (self.ds.fs / 2))
             for idx in range(3):
                 acc[idx] = ss.filtfilt(flt[0], flt[1], acc[idx], axis=-1)
 
             # Fill nan with zeros - happens for some filter frequencies
             if np.isnan(acc).any():
                 warnings.warn(
                     "Error filtering acceleration data. "
-                    "Please decrease `accel_filtfreq`.")
+                    "Please decrease `accel_filtfreq`."
+                )
                 acc = np.nan_to_num(acc)
 
-    def calc_velacc(self, ):
+    def calc_velacc(
+        self,
+    ):
         """
         Calculates the translational velocity from the high-pass
         filtered acceleration signal.
 
         Returns
         -------
         velacc : numpy.ndarray (3 x n_time)
@@ -166,54 +181,60 @@
         n = self._check_duty_cycle()
         # accel & accel-low will already be reshaped if n isn't none
 
         # Get high-pass accelerations
         hp = self.accel - self.acclow
 
         # Integrate in time to get velocities
-        dat = np.concatenate((np.zeros(list(hp.shape[:-1]) + [1]),
-                              cumtrapz(hp, dx=1 / samp_freq, axis=-1)), axis=-1)
+        dat = np.concatenate(
+            (
+                np.zeros(list(hp.shape[:-1]) + [1]),
+                cumtrapz(hp, dx=1 / samp_freq, axis=-1),
+            ),
+            axis=-1,
+        )
 
         if self.accelvel_filtfreq > 0:
             filt_freq = self.accelvel_filtfreq
             # 2nd order Butterworth filter
             # Applied twice by 'filtfilt' = 4th order butterworth
             filt = ss.butter(2, float(filt_freq) / (samp_freq / 2))
             for idx in range(hp.shape[0]):
-                dat[idx] = dat[idx] - \
-                    ss.filtfilt(filt[0], filt[1], dat[idx], axis=-1)
+                dat[idx] = dat[idx] - ss.filtfilt(filt[0], filt[1], dat[idx], axis=-1)
 
             # Fill nan with zeros - happens for some filter frequencies
             if np.isnan(dat).any():
-                warnings.warn("Error filtering acceleration data. "
-                              "Please decrease `vel_filtfreq`. "
-                              "(default is 1/3 `accel_filtfreq`)")
+                warnings.warn(
+                    "Error filtering acceleration data. "
+                    "Please decrease `vel_filtfreq`. "
+                    "(default is 1/3 `accel_filtfreq`)"
+                )
                 dat = np.nan_to_num(dat)
 
         if n:
             # remove reshape
             velacc_shaped = np.empty(self.angrt.shape)
             acclow_shaped = np.empty(self.angrt.shape)
             accel_shaped = np.empty(self.angrt.shape)
             for idx in range(hp.shape[0]):
-                velacc_shaped[idx] = np.ravel(dat[idx], 'C')
-                acclow_shaped[idx] = np.ravel(self.acclow[idx], 'C')
-                accel_shaped[idx] = np.ravel(self.accel[idx], 'C')
+                velacc_shaped[idx] = np.ravel(dat[idx], "C")
+                acclow_shaped[idx] = np.ravel(self.acclow[idx], "C")
+                accel_shaped[idx] = np.ravel(self.accel[idx], "C")
 
             # return acclow and velacc
             self.acclow = acclow_shaped
             self.accel = accel_shaped
             return velacc_shaped
 
         else:
             return dat
 
     def calc_velrot(self, vec, to_earth=None):
         """
-        Calculate the induced velocity due to rotations of the 
+        Calculate the induced velocity due to rotations of the
         instrument about the IMU center.
 
         Parameters
         ----------
         vec : numpy.ndarray (len(3) or 3 x M)
           The vector in meters (or vectors) from the body-origin
           (center of head end-cap) to the point of interest (in the
@@ -241,25 +262,24 @@
         # imu2head = body2head - body2imu
         vec = vec - _get_body2imu(_make_model(self.ds))[:, None]
 
         # This motion of the point *vec* due to rotations should be the
         # cross-product of omega (rotation vector) and the vector.
         #   u=dz*omegaY-dy*omegaZ,v=dx*omegaZ-dz*omegaX,w=dy*omegaX-dx*omegaY
         # where vec=[dx,dy,dz], and angrt=[omegaX,omegaY,omegaZ]
-        velrot = np.array([(vec[2][:, None] * self.angrt[1] -
-                            vec[1][:, None] * self.angrt[2]),
-                           (vec[0][:, None] * self.angrt[2] -
-                            vec[2][:, None] * self.angrt[0]),
-                           (vec[1][:, None] * self.angrt[0] -
-                            vec[0][:, None] * self.angrt[1]),
-                           ])
+        velrot = np.array(
+            [
+                (vec[2][:, None] * self.angrt[1] - vec[1][:, None] * self.angrt[2]),
+                (vec[0][:, None] * self.angrt[2] - vec[2][:, None] * self.angrt[0]),
+                (vec[1][:, None] * self.angrt[0] - vec[0][:, None] * self.angrt[1]),
+            ]
+        )
 
         if to_earth:
-            velrot = np.einsum('ji...,j...->i...',
-                               self.ds['orientmat'].values, velrot)
+            velrot = np.einsum("ji...,j...->i...", self.ds["orientmat"].values, velrot)
 
         if dimflag:
             return velrot[:, 0, :]
 
         return velrot
 
 
@@ -267,57 +287,59 @@
     """
     Calculates the position of probe (or "head") of an ADV.
 
     Paratmeters
     -----------
     ds : xarray.Dataset
       ADV dataset
-    separate_probes : bool 
-      If a Nortek Vector ADV, this function returns the 
-      transformation matrix of positions of the probe's 
+    separate_probes : bool
+      If a Nortek Vector ADV, this function returns the
+      transformation matrix of positions of the probe's
       acoustic recievers to the ADV's instrument frame of
       reference. Optional, default = False
 
     Returns
     -------
     vec : 3x3 numpy.ndarray
-      Transformation matrix to convert from ADV probe to 
+      Transformation matrix to convert from ADV probe to
       instrument frame of reference
     """
 
     vec = ds.inst2head_vec
     if type(vec) != np.ndarray:
         vec = np.array(vec)
 
     # According to the ADV technical drawing, the probe-length radius
     # is 8.6 cm @ 120 deg from probe-stem axis.  If I subtract 1 cm
     # to get the center of a acoustic receiver, this is 7.6 cm.
     # In the coordinate system of the center of the probe (origin at
     # the acoustic transmitter) then, the positions of the centers of
     # the receivers is:
-    if separate_probes and _make_model(ds) == 'nortek vector':
+    if separate_probes and _make_model(ds) == "nortek vector":
         r = 0.076
         # The angle between the x-y plane and the probes
         phi = np.deg2rad(-30)
         # The angles of the probes from the x-axis:
-        theta = np.deg2rad(np.array([0., 120., 240.]))
-        return (np.dot(ds['inst2head_rotmat'].values.T,
-                       np.array([r * np.cos(theta),
-                                 r * np.sin(theta),
-                                 r * np.tan(phi) * np.ones(3)])) +
-                vec[:, None])
+        theta = np.deg2rad(np.array([0.0, 120.0, 240.0]))
+        return (
+            np.dot(
+                ds["inst2head_rotmat"].values.T,
+                np.array(
+                    [r * np.cos(theta), r * np.sin(theta), r * np.tan(phi) * np.ones(3)]
+                ),
+            )
+            + vec[:, None]
+        )
     else:
         return vec
 
 
-def correct_motion(ds,
-                   accel_filtfreq=None,
-                   vel_filtfreq=None,
-                   to_earth=True,
-                   separate_probes=False):
+def correct_motion(
+    ds, accel_filtfreq=None, vel_filtfreq=None, to_earth=True, separate_probes=False
+):
     """
     This function performs motion correction on an IMU-ADV data
     object. The IMU and ADV data should be tightly synchronized and
     contained in a single data object.
 
     Parameters
     ----------
@@ -328,15 +350,15 @@
       the frequency at which to high-pass filter the acceleration
       sigal to remove low-frequency drift.
 
     vel_filtfreq : float
       a second frequency to high-pass filter the integrated
       acceleration.  Optional, default = 1/3 of `accel_filtfreq`
 
-    to_earth : bool 
+    to_earth : bool
       All variables in the ds.props['rotate_vars'] list will be
       rotated into either the earth frame (to_earth=True) or the
       instrument frame (to_earth=False). Optional, default = True
 
     separate_probes : bool
       a flag to perform motion-correction at the probe tips, and
       perform motion correction in beam-coordinates, then transform
@@ -353,15 +375,15 @@
 
       ``velrot`` is the rotational component of the head motion (from
                  angrt)
 
       ``velacc`` is the translational component of the head motion (from
                  accel, the high-pass filtered accel sigal)
 
-      ``acclow`` is the low-pass filtered accel sigal (i.e., 
+      ``acclow`` is the low-pass filtered accel sigal (i.e.,
 
     The primary velocity vector attribute, ``vel``, is motion corrected
     such that:
 
           vel = velraw + velrot + velacc
 
     The sigs are correct in this equation. The measured velocity
@@ -404,123 +426,128 @@
     remove that sigal from the ADV sigal in post-processing.
     """
 
     # Ensure acting on new dataset
     ds = ds.copy(deep=True)
 
     # Check that no nan's exist
-    if ds['accel'].isnull().sum():
+    if ds["accel"].isnull().sum():
         raise MissingDataError("There should be no missing data in `accel` variable")
-    if ds['angrt'].isnull().sum():
+    if ds["angrt"].isnull().sum():
         raise MissingDataError("There should be no missing data in `angrt` variable")
 
-    if hasattr(ds, 'velrot') or ds.attrs.get('motion corrected', False):
-        raise DataAlreadyProcessedError('The data appears to already have been '
-                        'motion corrected.')
+    if hasattr(ds, "velrot") or ds.attrs.get("motion corrected", False):
+        raise DataAlreadyProcessedError(
+            "The data appears to already have been " "motion corrected."
+        )
 
-    if not hasattr(ds, 'has_imu') or ('accel' not in ds):
-        raise MissingRequiredDataError('The instrument does not appear to have an IMU.')
+    if not hasattr(ds, "has_imu") or ("accel" not in ds):
+        raise MissingRequiredDataError("The instrument does not appear to have an IMU.")
 
-    if ds.coord_sys != 'inst':
-        rotate2(ds, 'inst', inplace=True)
+    if ds.coord_sys != "inst":
+        rotate2(ds, "inst", inplace=True)
 
     # Returns True/False if head2inst_rotmat has been set/not-set.
     # Bad configs raises errors (this is to check for those)
     rot._check_inst2head_rotmat(ds)
 
     # Create the motion 'calculator':
-    calcobj = CalcMotion(ds,
-                         accel_filtfreq=accel_filtfreq,
-                         vel_filtfreq=vel_filtfreq,
-                         to_earth=to_earth)
+    calcobj = CalcMotion(
+        ds, accel_filtfreq=accel_filtfreq, vel_filtfreq=vel_filtfreq, to_earth=to_earth
+    )
 
     ##########
     # Calculate the translational velocity (from the accel):
-    ds['velacc'] = xr.DataArray(calcobj.calc_velacc(),
-                                dims=['dirIMU', 'time'],
-                                attrs={'units': 'm s-1',
-                                       'long_name': 'Velocity from IMU Accelerometer'}
-                                ).astype('float32')
+    ds["velacc"] = xr.DataArray(
+        calcobj.calc_velacc(),
+        dims=["dirIMU", "time"],
+        attrs={"units": "m s-1", "long_name": "Velocity from IMU Accelerometer"},
+    ).astype("float32")
     # Copy acclow to the adv-object.
-    ds['acclow'] = xr.DataArray(calcobj.acclow,
-                                dims=['dirIMU', 'time'],
-                                attrs={'units': 'm s-2',
-                                       'long_name': 'Low-Frequency Acceleration from IMU'}
-                                ).astype('float32')
+    ds["acclow"] = xr.DataArray(
+        calcobj.acclow,
+        dims=["dirIMU", "time"],
+        attrs={"units": "m s-2", "long_name": "Low-Frequency Acceleration from IMU"},
+    ).astype("float32")
 
     ##########
     # Calculate rotational velocity (from angrt):
     pos = _calc_probe_pos(ds, separate_probes)
     # Calculate the velocity of the head (or probes).
     velrot = calcobj.calc_velrot(pos, to_earth=False)
     if separate_probes:
         # The head->beam transformation matrix
-        transMat = ds.get('beam2inst_orientmat', None)
+        transMat = ds.get("beam2inst_orientmat", None)
         # The inst->head transformation matrix
-        rmat = ds['inst2head_rotmat']
+        rmat = ds["inst2head_rotmat"]
 
         # 1) Rotate body-coordinate velocities to head-coord.
         velrot = np.dot(rmat, velrot)
         # 2) Rotate body-coord to beam-coord (einsum),
         # 3) Take along beam-component (diagonal),
         # 4) Rotate back to head-coord (einsum),
-        velrot = np.einsum('ij,kj->ik',
-                           transMat,
-                           np.diagonal(np.einsum('ij,j...->i...',
-                                                 np.linalg.inv(transMat),
-                                                 velrot)))
+        velrot = np.einsum(
+            "ij,kj->ik",
+            transMat,
+            np.diagonal(np.einsum("ij,j...->i...", np.linalg.inv(transMat), velrot)),
+        )
         # 5) Rotate back to body-coord.
         velrot = np.dot(rmat.T, velrot)
-    ds['velrot'] = xr.DataArray(velrot,
-                                dims=['dirIMU', 'time'],
-                                attrs={'units': 'm s-1',
-                                       'long_name': 'Velocity from IMU Gyroscope'}
-                                ).astype('float32')
+    ds["velrot"] = xr.DataArray(
+        velrot,
+        dims=["dirIMU", "time"],
+        attrs={"units": "m s-1", "long_name": "Velocity from IMU Gyroscope"},
+    ).astype("float32")
 
     ##########
     # Rotate the data into the correct coordinate system.
     # inst2earth expects a 'rotate_vars' property.
     # Add velrot, velacc, acclow, to it.
-    if 'rotate_vars' not in ds.attrs:
-        ds.attrs['rotate_vars'] = ['vel', 'velrot', 'velacc', 'accel',
-                                   'acclow', 'angrt', 'mag']
+    if "rotate_vars" not in ds.attrs:
+        ds.attrs["rotate_vars"] = [
+            "vel",
+            "velrot",
+            "velacc",
+            "accel",
+            "acclow",
+            "angrt",
+            "mag",
+        ]
     else:
-        ds.attrs['rotate_vars'].extend(['velrot', 'velacc', 'acclow'])
+        ds.attrs["rotate_vars"].extend(["velrot", "velacc", "acclow"])
 
     # NOTE: accel, acclow, and velacc are in the earth-frame after
     #       calc_velacc() call.
     inst2earth = rot._inst2earth
     if to_earth:
         # accel was converted to earth coordinates
-        ds['accel'].values = calcobj.accel
-        to_remove = ['accel', 'acclow', 'velacc']
-        ds = inst2earth(ds, rotate_vars=[e for e in
-                                         ds.attrs['rotate_vars']
-                                         if e not in to_remove])
+        ds["accel"].values = calcobj.accel
+        to_remove = ["accel", "acclow", "velacc"]
+        ds = inst2earth(
+            ds, rotate_vars=[e for e in ds.attrs["rotate_vars"] if e not in to_remove]
+        )
     else:
         # rotate these variables back to the instrument frame.
-        ds = inst2earth(ds, reverse=True,
-                        rotate_vars=['acclow', 'velacc'],
-                        force=True)
+        ds = inst2earth(ds, reverse=True, rotate_vars=["acclow", "velacc"], force=True)
 
     ##########
     # Copy vel -> velraw prior to motion correction:
-    ds['vel_raw'] = ds.vel.copy(deep=True)
+    ds["vel_raw"] = ds.vel.copy(deep=True)
 
     # Add it to rotate_vars:
-    ds.attrs['rotate_vars'].append('vel_raw')
+    ds.attrs["rotate_vars"].append("vel_raw")
 
     ##########
     # Remove motion from measured velocity
     # NOTE: The plus sign is because the measured-induced velocities
     #       are in the opposite direction of the head motion.
     #       i.e. when the head moves one way in stationary flow, it
     #       measures a velocity in the opposite direction.
 
     # use xarray to keep dimensions consistent
-    velmot = ds['velrot'] + ds['velacc']
-    ds['vel'].values += velmot.values
+    velmot = ds["velrot"] + ds["velacc"]
+    ds["vel"].values += velmot.values
 
-    ds.attrs['motion corrected'] = 1
-    ds.attrs['motion accel_filtfreq Hz'] = calcobj.accel_filtfreq
+    ds.attrs["motion corrected"] = 1
+    ds.attrs["motion accel_filtfreq Hz"] = calcobj.accel_filtfreq
 
     return ds
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/adv/turbulence.py` & `mhkit-0.8.0/mhkit/dolfyn/adv/turbulence.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 from ..tools.misc import slice1d_along_axis, _nans_like
 from scipy.special import cbrt
 import xarray as xr
 
 
 class ADVBinner(VelBinner):
     """
-    A class that builds upon `VelBinner` for calculating turbulence 
+    A class that builds upon `VelBinner` for calculating turbulence
     statistics and velocity spectra from ADV data
 
     Parameters
     ----------
     n_bin : int
       The length of each `bin`, in number of points, for this averaging
       operator.
@@ -20,43 +20,44 @@
       Instrument sampling frequency in Hz
     n_fft : int
       The length of the FFT for computing spectra (must be <= n_bin).
       Optional, default `n_fft` = `n_bin`
     n_fft_coh : int
       Number of data points to use for coherence and cross-spectra fft's.
       Optional, default `n_fft_coh` = `n_fft`
-    noise : float, list or numpy.ndarray
-      Instrument's doppler noise in same units as velocity
+        noise : float or array-like
+          Instrument noise level in same units as velocity. Typically
+          found from `adv.turbulence.doppler_noise_level`.
+          Default: None.
     """
 
-    def __call__(self, ds, freq_units='rad/s', window='hann'):
+    def __call__(self, ds, freq_units="rad/s", window="hann"):
         out = type(ds)()
         out = self.bin_average(ds, out)
 
-        noise = ds.get('doppler_noise', [0, 0, 0])
-        out['tke_vec'] = self.turbulent_kinetic_energy(ds['vel'], noise=noise)
-        out['stress_vec'] = self.reynolds_stress(ds['vel'])
-
-        out['psd'] = self.power_spectral_density(ds['vel'],
-                                                 window=window,
-                                                 freq_units=freq_units,
-                                                 noise=noise)
+        noise = ds.get("doppler_noise", [0, 0, 0])
+        out["tke_vec"] = self.turbulent_kinetic_energy(ds["vel"], noise=noise)
+        out["stress_vec"] = self.reynolds_stress(ds["vel"])
+
+        out["psd"] = self.power_spectral_density(
+            ds["vel"], window=window, freq_units=freq_units, noise=noise
+        )
         for key in list(ds.attrs.keys()):
-            if 'config' in key:
+            if "config" in key:
                 ds.attrs.pop(key)
         out.attrs = ds.attrs
-        out.attrs['n_bin'] = self.n_bin
-        out.attrs['n_fft'] = self.n_fft
-        out.attrs['n_fft_coh'] = self.n_fft_coh
+        out.attrs["n_bin"] = self.n_bin
+        out.attrs["n_fft"] = self.n_fft
+        out.attrs["n_fft_coh"] = self.n_fft_coh
 
         return out
 
     def reynolds_stress(self, veldat, detrend=True):
         """
-        Calculate the specific Reynolds stresses 
+        Calculate the specific Reynolds stresses
         (cross-covariances of u,v,w in m^2/s^2)
 
         Parameters
         ----------
         veldat : xr.DataArray
           A velocity data array. The last dimension is assumed
           to be time.
@@ -74,51 +75,54 @@
 
         if not isinstance(veldat, xr.DataArray):
             raise TypeError("`veldat` must be an instance of `xarray.DataArray`.")
 
         time = self.mean(veldat.time.values)
         vel = veldat.values
 
-        out = np.empty(self._outshape(vel[:3].shape)[:-1],
-                       dtype=np.float32)
+        out = np.empty(self._outshape(vel[:3].shape)[:-1], dtype=np.float32)
 
         if detrend:
             vel = self.detrend(vel)
         else:
             vel = self.demean(vel)
 
         for idx, p in enumerate(self._cross_pairs):
-            out[idx] = np.nanmean(vel[p[0]] * vel[p[1]],
-                                  -1, dtype=np.float64
-                                  ).astype(np.float32)
-
-        da = xr.DataArray(out.astype('float32'), 
-                          dims=veldat.dims, 
-                          attrs={'units': 'm2 s-2',
-                                 'long_name': 'Specific Reynolds Stress Vector'})
-        da = da.rename({'dir': 'tau'})
-        da = da.assign_coords({'tau': self.tau, 'time': time})
-        
+            out[idx] = np.nanmean(vel[p[0]] * vel[p[1]], -1, dtype=np.float64).astype(
+                np.float32
+            )
+
+        da = xr.DataArray(
+            out.astype("float32"),
+            dims=veldat.dims,
+            attrs={"units": "m2 s-2", "long_name": "Specific Reynolds Stress Vector"},
+        )
+        da = da.rename({"dir": "tau"})
+        da = da.assign_coords({"tau": self.tau, "time": time})
+
         return da
 
-    def cross_spectral_density(self, veldat,
-                               freq_units='rad/s',
-                               fs=None,
-                               window='hann',
-                               n_bin=None,
-                               n_fft_coh=None):
+    def cross_spectral_density(
+        self,
+        veldat,
+        freq_units="rad/s",
+        fs=None,
+        window="hann",
+        n_bin=None,
+        n_fft_coh=None,
+    ):
         """
         Calculate the cross-spectral density of velocity components.
 
         Parameters
         ----------
         veldat : xarray.DataArray
           The raw 3D velocity data.
         freq_units : string
-          Frequency units of the returned spectra in either Hz or rad/s 
+          Frequency units of the returned spectra in either Hz or rad/s
           (`f` or :math:`\\omega`)
         fs : float (optional)
           The sample rate. Default = `binner.fs`
         window : string or array
           Specify the window function.
          Options: 1, None, 'hann', 'hamm'
         n_bin : int (optional)
@@ -131,62 +135,73 @@
         csd : xarray.DataArray (3, M, N_FFT)
           The first-dimension of the cross-spectrum is the three
           different cross-spectra: 'uv', 'uw', 'vw'.
         """
 
         if not isinstance(veldat, xr.DataArray):
             raise TypeError("`veldat` must be an instance of `xarray.DataArray`.")
-        if ('rad' not in freq_units) and ('Hz' not in freq_units):
+        if ("rad" not in freq_units) and ("Hz" not in freq_units):
             raise ValueError("`freq_units` should be one of 'Hz' or 'rad/s'")
 
         fs_in = self._parse_fs(fs)
         n_fft = self._parse_nfft_coh(n_fft_coh)
         time = self.mean(veldat.time.values)
         veldat = veldat.values
         if len(np.shape(veldat)) != 2:
-            raise Exception("This function is only valid for calculating TKE using "
-                            "the 3D velocity vector from an ADV.")
-
-        out = np.empty(self._outshape_fft(veldat[:3].shape, n_fft=n_fft, n_bin=n_bin),
-                       dtype='complex')
+            raise Exception(
+                "This function is only valid for calculating TKE using "
+                "the 3D velocity vector from an ADV."
+            )
+
+        out = np.empty(
+            self._outshape_fft(veldat[:3].shape, n_fft=n_fft, n_bin=n_bin),
+            dtype="complex",
+        )
 
         # Create frequency vector, also checks whether using f or omega
-        if 'rad' in freq_units:
-            fs = 2*np.pi*fs_in
-            freq_units = 'rad s-1'
-            units = 'm2 s-1 rad-1'
+        if "rad" in freq_units:
+            fs = 2 * np.pi * fs_in
+            freq_units = "rad s-1"
+            units = "m2 s-1 rad-1"
         else:
             fs = fs_in
-            freq_units = 'Hz'
-            units = 'm2 s-2 Hz-1'
-        coh_freq = xr.DataArray(self._fft_freq(fs=fs_in, units=freq_units, n_fft=n_fft, coh=True),
-                                dims=['coh_freq'],
-                                name='coh_freq',
-                                attrs={'units': freq_units,
-                                       'long_name': 'FFT Frequency Vector',
-                                       'coverage_content_type': 'coordinate'}
-                                ).astype('float32')
+            freq_units = "Hz"
+            units = "m2 s-2 Hz-1"
+        coh_freq = xr.DataArray(
+            self._fft_freq(fs=fs_in, units=freq_units, n_fft=n_fft, coh=True),
+            dims=["coh_freq"],
+            name="coh_freq",
+            attrs={
+                "units": freq_units,
+                "long_name": "FFT Frequency Vector",
+                "coverage_content_type": "coordinate",
+            },
+        ).astype("float32")
 
         for ip, ipair in enumerate(self._cross_pairs):
-            out[ip] = self._csd_base(veldat[ipair[0]],
-                                     veldat[ipair[1]],
-                                     fs=fs,
-                                     window=window,
-                                     n_bin=n_bin,
-                                     n_fft=n_fft)
-
-        csd = xr.DataArray(out.astype('complex64'),
-                           coords={'C': self.C,
-                                   'time': time,
-                                   'coh_freq': coh_freq},
-                           dims=['C', 'time', 'coh_freq'],
-                           attrs={'units': units, 
-                                  'n_fft_coh': n_fft,
-                                  'long_name': 'Cross Spectral Density'})
-        csd['coh_freq'].attrs['units'] = freq_units
+            out[ip] = self._csd_base(
+                veldat[ipair[0]],
+                veldat[ipair[1]],
+                fs=fs,
+                window=window,
+                n_bin=n_bin,
+                n_fft=n_fft,
+            )
+
+        csd = xr.DataArray(
+            out.astype("complex64"),
+            coords={"C": self.C, "time": time, "coh_freq": coh_freq},
+            dims=["C", "time", "coh_freq"],
+            attrs={
+                "units": units,
+                "n_fft_coh": n_fft,
+                "long_name": "Cross Spectral Density",
+            },
+        )
+        csd["coh_freq"].attrs["units"] = freq_units
 
         return csd
 
     def doppler_noise_level(self, psd, pct_fN=0.8):
         """
         Calculate bias due to Doppler noise using the noise floor
         of the velocity spectra.
@@ -196,150 +211,156 @@
         psd : xarray.DataArray (dir, time, f)
           The ADV power spectral density of velocity (auto-spectra)
         pct_fN : float
           Percent of Nyquist frequency to calculate characeristic frequency
 
         Returns
         -------
-        doppler_noise (xarray.DataArray): 
+        doppler_noise (xarray.DataArray):
           Doppler noise level in units of m/s
 
         Notes
         -----
         Approximates bias from
 
         .. :math: \\sigma^{2}_{noise} = N x f_{c}
 
         where :math: `\\sigma_{noise}` is the bias due to Doppler noise,
         `N` is the constant variance or spectral density, and `f_{c}`
         is the characteristic frequency.
 
-        The characteristic frequency is then found as 
+        The characteristic frequency is then found as
 
         .. :math: f_{c} = pct_fN * (f_{s}/2)
 
         where `f_{s}/2` is the Nyquist frequency.
 
 
-        Richard, Jean-Baptiste, et al. "Method for identification of Doppler noise 
-        levels in turbulent flow measurements dedicated to tidal energy." International 
+        Richard, Jean-Baptiste, et al. "Method for identification of Doppler noise
+        levels in turbulent flow measurements dedicated to tidal energy." International
         Journal of Marine Energy 3 (2013): 52-64.
 
-        Thibaut, Maxime, et al. "Investigating the flow dynamics and turbulence at a 
-        tidal-stream energy site in a highly energetic estuary." Renewable Energy 195 
+        Thibaut, Maxime, et al. "Investigating the flow dynamics and turbulence at a
+        tidal-stream energy site in a highly energetic estuary." Renewable Energy 195
         (2022): 252-262.
         """
-        
+
         if not isinstance(psd, xr.DataArray):
             raise TypeError("`psd` must be an instance of `xarray.DataArray`.")
         if not isinstance(pct_fN, float) or not 0 <= pct_fN <= 1:
             raise ValueError("`pct_fN` must be a float within the range [0, 1].")
 
         # Characteristic frequency set to 80% of Nyquist frequency
-        fN = self.fs/2
+        fN = self.fs / 2
         fc = pct_fN * fN
 
         # Get units right
         if psd.freq.units == "Hz":
             f_range = slice(fc, fN)
         else:
-            f_range = slice(2*np.pi*fc, 2*np.pi*fN)
+            f_range = slice(2 * np.pi * fc, 2 * np.pi * fN)
 
         # Noise floor
         N2 = psd.sel(freq=f_range) * psd.freq.sel(freq=f_range)
-        noise_level = np.sqrt(N2.mean(dim='freq'))
+        noise_level = np.sqrt(N2.mean(dim="freq"))
 
         return xr.DataArray(
-            noise_level.values.astype('float32'),
-            dims=['dir', 'time'],
-            attrs={'units': 'm/s',
-                   'long_name': 'Doppler Noise Level',
-                   'description': 'Doppler noise level calculated '
-                                  'from PSD white noise'})
+            noise_level.values.astype("float32"),
+            coords={"S": psd["S"], "time": psd["time"]},
+            attrs={
+                "units": "m/s",
+                "long_name": "Doppler Noise Level",
+                "description": "Doppler noise level calculated " "from PSD white noise",
+            },
+        )
 
     def check_turbulence_cascade_slope(self, psd, freq_range=[6.28, 12.57]):
         """
-        This function calculates the slope of the PSD, the power spectra 
+        This function calculates the slope of the PSD, the power spectra
         of velocity, within the given frequency range. The purpose of this
-        function is to check that the region of the PSD containing the 
+        function is to check that the region of the PSD containing the
         isotropic turbulence cascade decreases at a rate of :math:`f^{-5/3}`.
 
         Parameters
         ----------
         psd : xarray.DataArray ([time,] freq)
           The power spectral density (1D or 2D)
         freq_range : iterable(2) (default: [6.28, 12.57])
-          The range over which the isotropic turbulence cascade occurs, in 
+          The range over which the isotropic turbulence cascade occurs, in
           units of the psd frequency vector (Hz or rad/s)
 
         Returns
         -------
         (m, b): tuple (slope, y-intercept)
-          A tuple containing the coefficients of the log-adjusted linear 
-          regression between PSD and frequency 
+          A tuple containing the coefficients of the log-adjusted linear
+          regression between PSD and frequency
 
         Notes
         -----
         Calculates slope based on the `standard` formula for dissipation:
 
         .. math:: S(k) = \\alpha \\epsilon^{2/3} k^{-5/3} + N
 
-        The slope of the isotropic turbulence cascade, which should be 
-        equal to :math:`k^{-5/3}` or :math:`f^{-5/3}`, where k and f are 
-        the wavenumber and frequency vectors, is estimated using linear 
+        The slope of the isotropic turbulence cascade, which should be
+        equal to :math:`k^{-5/3}` or :math:`f^{-5/3}`, where k and f are
+        the wavenumber and frequency vectors, is estimated using linear
         regression with a log transformation:
 
         .. math:: log10(y) = m*log10(x) + b
 
         Which is equivalent to
 
         .. math:: y = 10^{b} x^{m}
-        
-        Where :math:`y` is S(k) or S(f), :math:`x` is k or f, :math:`m` 
-        is the slope (ideally -5/3), and :math:`10^{b}` is the intercept of 
+
+        Where :math:`y` is S(k) or S(f), :math:`x` is k or f, :math:`m`
+        is the slope (ideally -5/3), and :math:`10^{b}` is the intercept of
         y at x^m=1.
         """
 
         if not isinstance(psd, xr.DataArray):
             raise TypeError("`psd` must be an instance of `xarray.DataArray`.")
-        if not hasattr(freq_range, '__iter__') or len(freq_range) != 2: 
+        if not hasattr(freq_range, "__iter__") or len(freq_range) != 2:
             raise ValueError("`freq_range` must be an iterable of length 2.")
-        
+
         idx = np.where((freq_range[0] < psd.freq) & (psd.freq < freq_range[1]))
         idx = idx[0]
 
-        x = np.log10(psd['freq'].isel(freq=idx))
+        x = np.log10(psd["freq"].isel(freq=idx))
         y = np.log10(psd.isel(freq=idx))
 
-        y_bar = y.mean('freq')
-        x_bar = x.mean('freq')
+        y_bar = y.mean("freq")
+        x_bar = x.mean("freq")
 
         # using the formula to calculate the slope and intercept
         n = np.sum((x - x_bar) * (y - y_bar), axis=0)
-        d = np.sum((x - x_bar)**2, axis=0)
+        d = np.sum((x - x_bar) ** 2, axis=0)
 
-        m = n/d
-        b = y_bar - m*x_bar
+        m = n / d
+        b = y_bar - m * x_bar
 
         return m, b
 
-    def dissipation_rate_LT83(self, psd, U_mag, freq_range=[6.28, 12.57]):
+    def dissipation_rate_LT83(self, psd, U_mag, freq_range=[6.28, 12.57], noise=None):
         """
         Calculate the dissipation rate from the PSD
 
         Parameters
         ----------
         psd : xarray.DataArray (...,time,f)
           The power spectral density
         U_mag : xarray.DataArray (...,time)
           The bin-averaged horizontal velocity [m/s] (from dataset shortcut)
         freq_range : iterable(2)
-          The range over which to integrate/average the spectrum, in units 
-          of the psd frequency vector (Hz or rad/s). 
+          The range over which to integrate/average the spectrum, in units
+          of the psd frequency vector (Hz or rad/s).
           Default = [6.28, 12.57] rad/s
+        noise : float or array-like
+          Instrument noise level in same units as velocity. Typically
+          found from `adv.turbulence.calc_doppler_noise`.
+          Default: None.
 
         Returns
         -------
         epsilon : xarray.DataArray (...,n_time)
           dataArray of the dissipation rate
 
         Notes
@@ -365,103 +386,120 @@
         LT83 : Lumley and Terray, "Kinematics of turbulence convected
         by a random wave field". JPO, 1983, vol13, pp2000-2007.
         """
 
         if not isinstance(psd, xr.DataArray):
             raise TypeError("`psd` must be an instance of `xarray.DataArray`.")
         if len(U_mag.shape) != 1:
-            raise Exception('U_mag should be 1-dimensional (time)')
-        if len(psd.time)!=len(U_mag.time):
+            raise Exception("U_mag should be 1-dimensional (time)")
+        if len(psd.time) != len(U_mag.time):
             raise Exception("`U_mag` should be from ensembled-averaged dataset")
-        if not hasattr(freq_range, '__iter__') or len(freq_range) != 2: 
+        if not hasattr(freq_range, "__iter__") or len(freq_range) != 2:
             raise ValueError("`freq_range` must be an iterable of length 2.")
 
+        if noise is not None:
+            if np.shape(noise)[0] != 3:
+                raise Exception("Noise should have same first dimension as velocity")
+        else:
+            noise = np.array([0, 0, 0])[:, None, None]
+
+        # Noise subtraction from binner.TimeBinner.calc_psd_base
+        psd = psd.copy()
+        if noise is not None:
+            psd -= noise**2 / (self.fs / 2)
+            psd = psd.where(psd > 0, np.min(np.abs(psd)) / 100)
+
         freq = psd.freq
         idx = np.where((freq_range[0] < freq) & (freq < freq_range[1]))
         idx = idx[0]
 
-        if freq.units == 'Hz':
-            U = U_mag/(2*np.pi)
+        if freq.units == "Hz":
+            U = U_mag / (2 * np.pi)
         else:
             U = U_mag
 
         a = 0.5
-        out = (psd.isel(freq=idx) *
-               freq.isel(freq=idx)**(5/3) / a).mean(axis=-1)**(3/2) / U
+        out = (psd.isel(freq=idx) * freq.isel(freq=idx) ** (5 / 3) / a).mean(
+            axis=-1
+        ) ** (3 / 2) / U
 
         return xr.DataArray(
-            out.astype('float32'),
-            attrs={'units': 'm2 s-3',
-                   'long_name': 'TKE Dissipation Rate',
-                   'standard_name': 'specific_turbulent_kinetic_energy_dissipation_in_sea_water',
-                   'description': 'TKE dissipation rate calculated using '
-                                  'the method from Lumley and Terray, 1983',
-                   })
+            out.astype("float32"),
+            attrs={
+                "units": "m2 s-3",
+                "long_name": "TKE Dissipation Rate",
+                "standard_name": "specific_turbulent_kinetic_energy_dissipation_in_sea_water",
+                "description": "TKE dissipation rate calculated using "
+                "the method from Lumley and Terray, 1983",
+            },
+        )
 
-    def dissipation_rate_SF(self, vel_raw, U_mag, fs=None, freq_range=[2., 4.]):
+    def dissipation_rate_SF(self, vel_raw, U_mag, fs=None, freq_range=[2.0, 4.0]):
         """
         Calculate dissipation rate using the "structure function" (SF) method
 
         Parameters
         ----------
         vel_raw : xarray.DataArray (time)
-          The raw velocity data upon which to perform the SF technique. 
+          The raw velocity data upon which to perform the SF technique.
         U_mag : xarray.DataArray
           The bin-averaged horizontal velocity (from dataset shortcut)
         fs : float
           The sample rate of `vel_raw` [Hz]
         freq_range : iterable(2)
           The frequency range over which to compute the SF [Hz]
-          (i.e. the frequency range within which the isotropic 
+          (i.e. the frequency range within which the isotropic
           turbulence cascade falls).
           Default = [2., 4.] Hz
 
         Returns
         -------
         epsilon : xarray.DataArray
           dataArray of the dissipation rate
         """
 
         if not isinstance(vel_raw, xr.DataArray):
             raise TypeError("`vel_raw` must be an instance of `xarray.DataArray`.")
-        if len(vel_raw.time)==len(U_mag.time):
+        if len(vel_raw.time) == len(U_mag.time):
             raise Exception("`U_mag` should be from ensembled-averaged dataset")
-        if not hasattr(freq_range, '__iter__') or len(freq_range) != 2: 
+        if not hasattr(freq_range, "__iter__") or len(freq_range) != 2:
             raise ValueError("`freq_range` must be an iterable of length 2.")
 
         veldat = vel_raw.values
         if len(veldat.shape) > 1:
             raise Exception("Function input should be a 1D velocity vector")
 
         fs = self._parse_fs(fs)
         if freq_range[1] > fs:
-            warnings.warn('Max freq_range cannot be greater than fs')
+            warnings.warn("Max freq_range cannot be greater than fs")
 
         dt = self.reshape(veldat)
         out = np.empty(dt.shape[:-1], dtype=dt.dtype)
         for slc in slice1d_along_axis(dt.shape, -1):
             up = dt[slc]
             lag = U_mag.values[slc[:-1]] / fs * np.arange(up.shape[0])
             DAA = _nans_like(lag)
             for L in range(int(fs / freq_range[1]), int(fs / freq_range[0])):
                 DAA[L] = np.nanmean((up[L:] - up[:-L]) ** 2, dtype=np.float64)
             cv2 = DAA / (lag ** (2 / 3))
             cv2m = np.median(cv2[np.logical_not(np.isnan(cv2))])
             out[slc[:-1]] = (cv2m / 2.1) ** (3 / 2)
 
         return xr.DataArray(
-            out.astype('float32'),
+            out.astype("float32"),
             coords=U_mag.coords,
             dims=U_mag.dims,
-            attrs={'units': 'm2 s-3',
-                   'long_name': 'TKE Dissipation Rate',
-                   'standard_name': 'specific_turbulent_kinetic_energy_dissipation_in_sea_water',
-                   'description': 'TKE dissipation rate calculated using the '
-                                  '"structure function" method',
-                   })
+            attrs={
+                "units": "m2 s-3",
+                "long_name": "TKE Dissipation Rate",
+                "standard_name": "specific_turbulent_kinetic_energy_dissipation_in_sea_water",
+                "description": "TKE dissipation rate calculated using the "
+                '"structure function" method',
+            },
+        )
 
     def _up_angle(self, U_complex):
         """
         Calculate the angle of the turbulence fluctuations.
 
         Parameters
         ----------
@@ -494,88 +532,94 @@
           velocity fluctuations
         """
 
         x = np.arange(-20, 20, 1e-2)  # I think this is a long enough range.
         out = np.empty_like(I_tke.flatten())
         for i, (b, t) in enumerate(zip(I_tke.flatten(), theta.flatten())):
             out[i] = np.trapz(
-                cbrt(x**2 - 2/b*np.cos(t)*x + b**(-2)) *
-                np.exp(-0.5 * x ** 2), x)
+                cbrt(x**2 - 2 / b * np.cos(t) * x + b ** (-2)) * np.exp(-0.5 * x**2),
+                x,
+            )
 
-        return out.reshape(I_tke.shape) * \
-            (2 * np.pi) ** (-0.5) * I_tke ** (2 / 3)
+        return out.reshape(I_tke.shape) * (2 * np.pi) ** (-0.5) * I_tke ** (2 / 3)
 
     def dissipation_rate_TE01(self, dat_raw, dat_avg, freq_range=[6.28, 12.57]):
         """
         Calculate the dissipation rate according to TE01.
 
         Parameters
         ----------
         dat_raw : xarray.Dataset
           The raw (off the instrument) adv dataset
         dat_avg : xarray.Dataset
           The bin-averaged adv dataset (calc'd from 'calc_turbulence' or
-          'do_avg'). The spectra (psd) and basic turbulence statistics 
+          'do_avg'). The spectra (psd) and basic turbulence statistics
           ('tke_vec' and 'stress_vec') must already be computed.
         freq_range : iterable(2)
-          The range over which to integrate/average the spectrum, in units 
+          The range over which to integrate/average the spectrum, in units
           of the psd frequency vector (Hz or rad/s).
           Default = [6.28, 12.57] rad/s
 
         Notes
         -----
         TE01 : Trowbridge, J and Elgar, S, "Turbulence measurements in
         the Surf Zone". JPO, 2001, vol31, pp2403-2417.
         """
 
         if not isinstance(dat_raw, xr.Dataset):
             raise TypeError("`dat_raw` must be an instance of `xarray.Dataset`.")
         if not isinstance(dat_avg, xr.Dataset):
             raise TypeError("`dat_avg` must be an instance of `xarray.Dataset`.")
-        if not hasattr(freq_range, '__iter__') or len(freq_range) != 2: 
+        if not hasattr(freq_range, "__iter__") or len(freq_range) != 2:
             raise ValueError("`freq_range` must be an iterable of length 2.")
 
         # Assign local names
         U_mag = dat_avg.velds.U_mag.values
         I_tke = dat_avg.velds.I_tke.values
-        theta = np.angle(dat_avg.velds.U.values) - \
-            self._up_angle(dat_raw.velds.U.values)
-        freq = dat_avg['psd'].freq.values
+        theta = np.angle(dat_avg.velds.U.values) - self._up_angle(
+            dat_raw.velds.U.values
+        )
+        freq = dat_avg["psd"].freq.values
 
         # Calculate constants
         alpha = 1.5
         intgrl = self._integral_TE01(I_tke, theta)
 
         # Index data to be used
         inds = (freq_range[0] < freq) & (freq < freq_range[1])
         psd = dat_avg.psd[..., inds].values
         freq = freq[inds].reshape([1] * (dat_avg.psd.ndim - 2) + [sum(inds)])
 
         # Estimate values
         # u & v components (equation 6)
-        out = (np.nanmean((psd[0] + psd[1]) * freq**(5/3), -1) /
-               (21/55 * alpha * intgrl))**(3/2) / U_mag
+        out = (
+            np.nanmean((psd[0] + psd[1]) * freq ** (5 / 3), -1)
+            / (21 / 55 * alpha * intgrl)
+        ) ** (3 / 2) / U_mag
 
         # Add w component
-        out += (np.nanmean(psd[2] * freq**(5/3), -1) /
-                (12/55 * alpha * intgrl))**(3/2) / U_mag
+        out += (
+            np.nanmean(psd[2] * freq ** (5 / 3), -1) / (12 / 55 * alpha * intgrl)
+        ) ** (3 / 2) / U_mag
 
         # Average the two estimates
         out *= 0.5
 
         return xr.DataArray(
-            out.astype('float32'),
-            coords={'time': dat_avg.psd.time},
-            dims='time',
-            attrs={'units': 'm2 s-3',
-                   'long_name': 'TKE Dissipation Rate',
-                   'standard_name': 'specific_turbulent_kinetic_energy_dissipation_in_sea_water',
-                   'description': 'TKE dissipation rate calculated using the '
-                                  'method from Trowbridge and Elgar, 2001'
-                   })
+            out.astype("float32"),
+            coords={"time": dat_avg.psd.time},
+            dims="time",
+            attrs={
+                "units": "m2 s-3",
+                "long_name": "TKE Dissipation Rate",
+                "standard_name": "specific_turbulent_kinetic_energy_dissipation_in_sea_water",
+                "description": "TKE dissipation rate calculated using the "
+                "method from Trowbridge and Elgar, 2001",
+            },
+        )
 
     def integral_length_scales(self, a_cov, U_mag, fs=None):
         """
         Calculate integral length scales.
 
         Parameters
         ----------
@@ -597,43 +641,48 @@
         auto-covariance falls to 1/e.
 
         If T_int is not reached, L_int will default to '0'.
         """
 
         if not isinstance(a_cov, xr.DataArray):
             raise TypeError("`a_cov` must be an instance of `xarray.DataArray`.")
-        if len(a_cov.time)!=len(U_mag.time):
+        if len(a_cov.time) != len(U_mag.time):
             raise Exception("`U_mag` should be from ensembled-averaged dataset")
 
         acov = a_cov.values
         fs = self._parse_fs(fs)
 
-        scale = np.argmin((acov/acov[..., :1]) > (1/np.e), axis=-1)
+        scale = np.argmin((acov / acov[..., :1]) > (1 / np.e), axis=-1)
         L_int = U_mag.values / fs * scale
 
         return xr.DataArray(
-            L_int.astype('float32'),
-            coords={'dir': a_cov.dir, 'time': a_cov.time},
-            attrs={'units': 'm',
-                   'long_name': 'Integral Length Scale',
-                   'standard_name': 'turbulent_mixing_length_of_sea_water'})
+            L_int.astype("float32"),
+            coords={"dir": a_cov.dir, "time": a_cov.time},
+            attrs={
+                "units": "m",
+                "long_name": "Integral Length Scale",
+                "standard_name": "turbulent_mixing_length_of_sea_water",
+            },
+        )
 
 
-def turbulence_statistics(ds_raw, n_bin, fs, n_fft=None, freq_units='rad/s', window='hann'):
+def turbulence_statistics(
+    ds_raw, n_bin, fs, n_fft=None, freq_units="rad/s", window="hann"
+):
     """
-    Functional version of `ADVBinner` that computes a suite of turbulence 
+    Functional version of `ADVBinner` that computes a suite of turbulence
     statistics for the input dataset, and returns a `binned` data object.
 
     Parameters
     ----------
     ds_raw : xarray.Dataset
       The raw adv datset to `bin`, average and compute
       turbulence statistics of.
     freq_units : string
-      Frequency units of the returned spectra in either Hz or rad/s 
+      Frequency units of the returned spectra in either Hz or rad/s
       (`f` or :math:`\\omega`). Default is 'rad/s'
     window : string or array
       The window to use for calculating spectra.
 
 
     Returns
     -------
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/binned.py` & `mhkit-0.8.0/mhkit/dolfyn/binned.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 import numpy as np
 import warnings
 from .tools.fft import fft_frequency, psd_1D, cpsd_1D, cpsd_quasisync_1D
 from .tools.misc import slice1d_along_axis, detrend_array
 from .time import epoch2dt64, dt642epoch
-warnings.simplefilter('ignore', RuntimeWarning)
+
+warnings.simplefilter("ignore", RuntimeWarning)
 
 
 class TimeBinner:
-    def __init__(self, n_bin, fs, n_fft=None, n_fft_coh=None,
-                 noise=[0, 0, 0]):
+    def __init__(self, n_bin, fs, n_fft=None, n_fft_coh=None, noise=[0, 0, 0]):
         """
         Initialize an averaging object
 
         Parameters
         ----------
         n_bin : int
-          Number of data points to include in a 'bin' (ensemble), not the 
+          Number of data points to include in a 'bin' (ensemble), not the
           number of bins
         fs : int
           Instrument sampling frequency in Hz
         n_fft : int
           Number of data points to use for fft (`n_fft`<=`n_bin`).
           Default: `n_fft`=`n_bin`
         n_fft_coh : int
@@ -34,22 +34,23 @@
         self.n_fft = n_fft
         self.n_fft_coh = n_fft_coh
         self.noise = noise
         if n_fft is None:
             self.n_fft = n_bin
         elif n_fft > n_bin:
             self.n_fft = n_bin
-            warnings.warn(
-                "n_fft must be smaller than n_bin, setting n_fft = n_bin")
+            warnings.warn("n_fft must be smaller than n_bin, setting n_fft = n_bin")
         if n_fft_coh is None:
             self.n_fft_coh = int(self.n_fft)
         elif n_fft_coh > n_bin:
             self.n_fft_coh = int(n_bin)
-            warnings.warn("n_fft_coh must be smaller than or equal to n_bin, "
-                          "setting n_fft_coh = n_bin")
+            warnings.warn(
+                "n_fft_coh must be smaller than or equal to n_bin, "
+                "setting n_fft_coh = n_bin"
+            )
 
     def _outshape(self, inshape, n_pad=0, n_bin=None):
         """
         Returns `outshape` (the 'reshape'd shape) for an `inshape` array.
         """
         n_bin = int(self._parse_nbin(n_bin))
         return list(inshape[:-1]) + [int(inshape[-1] // n_bin), int(n_bin + n_pad)]
@@ -73,25 +74,26 @@
         return n_bin
 
     def _parse_nfft(self, n_fft=None):
         if n_fft is None:
             return self.n_fft
         if n_fft > self.n_bin:
             n_fft = self.n_bin
-            warnings.warn(
-                "n_fft must be smaller than n_bin, setting n_fft = n_bin")
+            warnings.warn("n_fft must be smaller than n_bin, setting n_fft = n_bin")
         return n_fft
 
     def _parse_nfft_coh(self, n_fft_coh=None):
         if n_fft_coh is None:
             return self.n_fft_coh
         if n_fft_coh > self.n_bin:
             n_fft_coh = int(self.n_bin)
-            warnings.warn("n_fft_coh must be smaller than or equal to n_bin, "
-                          "setting n_fft_coh = n_bin")
+            warnings.warn(
+                "n_fft_coh must be smaller than or equal to n_bin, "
+                "setting n_fft_coh = n_bin"
+            )
         return n_fft_coh
 
     def _check_ds(self, raw_ds, out_ds):
         """
         Check that the attributes between two datasets match up.
 
         Parameters
@@ -105,63 +107,70 @@
         Returns
         -------
         out_ds : xarray.Dataset
         """
 
         for v in raw_ds.data_vars:
             if np.any(np.array(raw_ds[v].shape) == 0):
-                raise RuntimeError(f"{v} cannot be averaged "
-                                   "because it is empty.")
-        if 'DutyCycle_NBurst' in raw_ds.attrs and \
-                raw_ds.attrs['DutyCycle_NBurst'] < self.n_bin:
-            warnings.warn(f"The averaging interval (n_bin = {self.n_bin})"
-                          "is larger than the burst interval "
-                          "(NBurst = {dat.attrs['DutyCycle_NBurst']})")
+                raise RuntimeError(f"{v} cannot be averaged " "because it is empty.")
+        if (
+            "DutyCycle_NBurst" in raw_ds.attrs
+            and raw_ds.attrs["DutyCycle_NBurst"] < self.n_bin
+        ):
+            warnings.warn(
+                f"The averaging interval (n_bin = {self.n_bin})"
+                "is larger than the burst interval "
+                "(NBurst = {dat.attrs['DutyCycle_NBurst']})"
+            )
         if raw_ds.fs != self.fs:
-            raise Exception(f"The input data sample rate ({raw_ds.fs}) does not "
-                            "match the sample rate of this binning-object "
-                            "({self.fs})")
+            raise Exception(
+                f"The input data sample rate ({raw_ds.fs}) does not "
+                "match the sample rate of this binning-object "
+                "({self.fs})"
+            )
 
         if out_ds is None:
             out_ds = type(raw_ds)()
 
         o_attrs = out_ds.attrs
 
         props = {}
-        props['fs'] = self.fs
-        props['n_bin'] = self.n_bin
-        props['n_fft'] = self.n_fft
-        props['description'] = 'Binned averages calculated from ' \
-            'ensembles of size "n_bin"'
+        props["fs"] = self.fs
+        props["n_bin"] = self.n_bin
+        props["n_fft"] = self.n_fft
+        props["description"] = (
+            "Binned averages calculated from " 'ensembles of size "n_bin"'
+        )
         props.update(raw_ds.attrs)
 
         for ky in props:
             if ky in o_attrs and o_attrs[ky] != props[ky]:
                 # The values in out_ds must match `props` (raw_ds.attrs,
                 # plus those defined above)
                 raise AttributeError(
                     "The attribute '{}' of `out_ds` is inconsistent "
-                    "with this `VelBinner` or the input data (`raw_ds`)".format(ky))
+                    "with this `VelBinner` or the input data (`raw_ds`)".format(ky)
+                )
             else:
                 o_attrs[ky] = props[ky]
         return out_ds
 
     def _new_coords(self, array):
         """
-        Function for setting up a new xarray.DataArray regardless of how 
+        Function for setting up a new xarray.DataArray regardless of how
         many dimensions the input data-array has
         """
         dims = array.dims
         dims_list = []
         coords_dict = {}
-        if len(array.shape) == 1 & ('dir' in array.coords):
-            array = array.drop_vars('dir')
+        if len(array.shape) == 1 & ("dir" in array.coords):
+            array = array.drop_vars("dir")
         for ky in dims:
             dims_list.append(ky)
-            if 'time' in ky:
+            if "time" in ky:
                 coords_dict[ky] = self.mean(array.time.values)
             else:
                 coords_dict[ky] = array.coords[ky].values
 
         return dims_list, coords_dict
 
     def reshape(self, arr, n_pad=0, n_bin=None):
@@ -194,42 +203,41 @@
         example:
         - for n_bin=2048.2 every 1/5 bins will have a skipped point.
         - for n_bin=4096.9 every 9/10 bins will have a skipped point.
         """
 
         n_bin = self._parse_nbin(n_bin)
         if arr.shape[-1] < n_bin:
-            raise Exception('n_bin is larger than length of input array')
+            raise Exception("n_bin is larger than length of input array")
         npd0 = int(n_pad // 2)
         npd1 = int((n_pad + 1) // 2)
         shp = self._outshape(arr.shape, n_pad=0, n_bin=n_bin)
         out = np.zeros(
-            self._outshape(arr.shape, n_pad=n_pad, n_bin=n_bin),
-            dtype=arr.dtype)
+            self._outshape(arr.shape, n_pad=n_pad, n_bin=n_bin), dtype=arr.dtype
+        )
         if np.mod(n_bin, 1) == 0:
             # n_bin needs to be int
             n_bin = int(n_bin)
             # If n_bin is an integer, we can do this simply.
-            out[..., npd0: n_bin + npd0] = (
-                arr[..., :(shp[-2] * shp[-1])]).reshape(shp, order='C')
+            out[..., npd0 : n_bin + npd0] = (arr[..., : (shp[-2] * shp[-1])]).reshape(
+                shp, order="C"
+            )
         else:
-            inds = (np.arange(np.prod(shp[-2:])) * n_bin // int(n_bin)
-                    ).astype(int)
+            inds = (np.arange(np.prod(shp[-2:])) * n_bin // int(n_bin)).astype(int)
             # If there are too many indices, drop one bin
             if inds[-1] >= arr.shape[-1]:
-                inds = inds[:-int(n_bin)]
+                inds = inds[: -int(n_bin)]
                 shp[-2] -= 1
                 out = out[..., 1:, :]
             n_bin = int(n_bin)
-            out[..., npd0:n_bin + npd0] = (arr[..., inds]
-                                           ).reshape(shp, order='C')
+            out[..., npd0 : n_bin + npd0] = (arr[..., inds]).reshape(shp, order="C")
             n_bin = int(n_bin)
         if n_pad != 0:
-            out[..., 1:, :npd0] = out[..., :-1, n_bin:n_bin + npd0]
-            out[..., :-1, -npd1:] = out[..., 1:, npd0:npd0 + npd1]
+            out[..., 1:, :npd0] = out[..., :-1, n_bin : n_bin + npd0]
+            out[..., :-1, -npd1:] = out[..., 1:, npd0 : npd0 + npd1]
 
         return out
 
     def detrend(self, arr, axis=-1, n_pad=0, n_bin=None):
         """
         Reshape the array `arr` to shape (...,n,n_bin+n_pad)
         and remove the best-fit trend line from each bin.
@@ -332,15 +340,15 @@
         """
 
         return np.nanvar(self.reshape(arr, n_bin=n_bin), axis=axis, dtype=np.float32)
 
     def standard_deviation(self, arr, axis=-1, n_bin=None):
         """
         Reshape the array `arr` to shape (...,n,n_bin+n_pad)
-        and take the standard deviation of each bin along the 
+        and take the standard deviation of each bin along the
         specified `axis`.
 
         Parameters
         ----------
         arr : numpy.ndarray
         axis : int
           Axis along which to take std dev. Default = -1
@@ -350,16 +358,25 @@
         Returns
         -------
         out : numpy.ndarray
         """
 
         return np.nanstd(self.reshape(arr, n_bin=n_bin), axis=axis, dtype=np.float32)
 
-    def _psd_base(self, dat, fs=None, window='hann', noise=0,
-                  n_bin=None, n_fft=None, n_pad=None, step=None):
+    def _psd_base(
+        self,
+        dat,
+        fs=None,
+        window="hann",
+        noise=0,
+        n_bin=None,
+        n_fft=None,
+        n_pad=None,
+        step=None,
+    ):
         """
         Calculate power spectral density of `dat`
 
         Parameters
         ----------
         dat : xarray.DataArray
           The raw dataArray of which to calculate the psd.
@@ -367,18 +384,18 @@
           The sample rate (Hz).
         window : str
           String indicating the window function to use. Default is 'hanning'
         noise  : float
           The white-noise level of the measurement (in the same units
           as `dat`).
         n_bin : int
-          n_bin of veldat2, number of elements per bin if 'None' is taken 
+          n_bin of veldat2, number of elements per bin if 'None' is taken
           from VelBinner
         n_fft : int
-          n_fft of veldat2, number of elements per bin if 'None' is taken 
+          n_fft of veldat2, number of elements per bin if 'None' is taken
           from VelBinner
         n_pad : int (optional)
           The number of values to pad with zero. Default = 0
         step : int (optional)
           Controls amount of overlap in fft. Default: the step size is
           chosen to maximize data use, minimize nens, and have a
           minimum of 50% overlap.
@@ -399,98 +416,95 @@
         if n_pad is None:
             n_pad = min(n_bin - n_fft, n_fft)
         out = np.empty(self._outshape_fft(dat.shape, n_fft=n_fft, n_bin=n_bin))
         # The data is detrended in psd, so we don't need to do it here.
         dat = self.reshape(dat, n_pad=n_pad)
 
         for slc in slice1d_along_axis(dat.shape, -1):
-            out[slc] = psd_1D(dat[slc], n_fft, fs,
-                              window=window, step=step)
-        if noise != 0:
-            out -= noise**2 / (fs/2)
+            out[slc] = psd_1D(dat[slc], n_fft, fs, window=window, step=step)
+        if np.any(noise):
+            out -= noise**2 / (fs / 2)
             # Make sure all values of the PSD are >0 (but still small):
             out[out < 0] = np.min(np.abs(out)) / 100
         return out
 
-    def _csd_base(self, dat1, dat2, fs=None, window='hann',
-                  n_fft=None, n_bin=None):
+    def _csd_base(self, dat1, dat2, fs=None, window="hann", n_fft=None, n_bin=None):
         """
         Calculate the cross power spectral density of `dat`.
 
         Parameters
         ----------
         dat1 : numpy.ndarray
-          The first (shorter, if applicable) raw dataArray of which to 
+          The first (shorter, if applicable) raw dataArray of which to
           calculate the cpsd.
         dat2 : numpy.ndarray
-          The second (the shorter, if applicable) raw dataArray of which to 
+          The second (the shorter, if applicable) raw dataArray of which to
           calculate the cpsd.
         fs : float (optional)
           The sample rate (Hz).
         window : str
           String indicating the window function to use. Default is 'hanning'
         n_fft : int
-          n_fft of veldat2, number of elements per bin if 'None' is taken 
+          n_fft of veldat2, number of elements per bin if 'None' is taken
           from VelBinner
         n_bin : int
-          n_bin of veldat2, number of elements per bin if 'None' is taken 
+          n_bin of veldat2, number of elements per bin if 'None' is taken
           from VelBinner
 
         Returns
         -------
         out : numpy.ndarray
           The cross power spectral density of `dat1` and `dat2`
 
         Notes
         -----
         PSD's are calculated based on sample rate units
 
-        The two velocity inputs do not have to be perfectly synchronized, but 
+        The two velocity inputs do not have to be perfectly synchronized, but
         they should have the same start and end timestamps
         """
 
         fs = self._parse_fs(fs)
         if n_fft is None:
             n_fft = self.n_fft_coh
         # want each slice to carry the same timespan
         n_bin2 = self._parse_nbin(n_bin)  # bins for shorter array
-        n_bin1 = int(dat1.shape[-1]/(dat2.shape[-1]/n_bin2))
+        n_bin1 = int(dat1.shape[-1] / (dat2.shape[-1] / n_bin2))
 
         oshp = self._outshape_fft(dat1.shape, n_fft=n_fft, n_bin=n_bin1)
         oshp[-2] = np.min([oshp[-2], int(dat2.shape[-1] // n_bin2)])
 
         # The data is detrended in psd, so we don't need to do it here:
         dat1 = self.reshape(dat1, n_pad=n_fft)
         dat2 = self.reshape(dat2, n_pad=n_fft)
-        out = np.empty(oshp, dtype='c{}'.format(dat1.dtype.itemsize * 2))
+        out = np.empty(oshp, dtype="c{}".format(dat1.dtype.itemsize * 2))
         if dat1.shape == dat2.shape:
             cross = cpsd_1D
         else:
             cross = cpsd_quasisync_1D
         for slc in slice1d_along_axis(out.shape, -1):
-            out[slc] = cross(dat1[slc], dat2[slc], n_fft,
-                             fs, window=window)
+            out[slc] = cross(dat1[slc], dat2[slc], n_fft, fs, window=window)
         return out
 
-    def _fft_freq(self, fs=None, units='Hz', n_fft=None, coh=False):
+    def _fft_freq(self, fs=None, units="Hz", n_fft=None, coh=False):
         """
         Wrapper to calculate the ordinary or radial frequency vector
 
         Parameters
         ----------
         fs : float (optional)
           The sample rate (Hz).
         units : string
           Frequency units in either Hz or rad/s (f or omega)
         coh : bool
           Calculate the frequency vector for coherence/cross-spectra
           (default: False) i.e. use self.n_fft_coh instead of
           self.n_fft.
         n_fft : int
-          n_fft of veldat2, number of elements per bin if 'None' is taken 
+          n_fft of veldat2, number of elements per bin if 'None' is taken
           from VelBinner
 
         Returns
         -------
         out: numpy.ndarray
           Spectrum frequency array in units of 'Hz' or 'rad/s'
         """
@@ -498,15 +512,17 @@
         if n_fft is None:
             n_fft = self.n_fft
             if coh:
                 n_fft = self.n_fft_coh
 
         fs = self._parse_fs(fs)
 
-        if ('Hz' not in units) and ('rad' not in units):
-            raise Exception('Valid fft frequency vector units are Hz \
-                            or rad/s')
+        if ("Hz" not in units) and ("rad" not in units):
+            raise Exception(
+                "Valid fft frequency vector units are Hz \
+                            or rad/s"
+            )
 
-        if 'rad' in units:
-            return fft_frequency(n_fft, 2*np.pi*fs)
+        if "rad" in units:
+            return fft_frequency(n_fft, 2 * np.pi * fs)
         else:
             return fft_frequency(n_fft, fs)
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/io/api.py` & `mhkit-0.8.0/mhkit/dolfyn/io/api.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,28 +3,35 @@
 import xarray as xr
 from os.path import abspath, dirname, join, normpath, relpath
 from .nortek import read_nortek
 from .nortek2 import read_signature
 from .rdi import read_rdi
 from .base import _create_dataset, _get_filetype
 from ..rotate.base import _set_coords
-from ..time import date2matlab, matlab2date, date2dt64, dt642date, date2epoch, epoch2date
+from ..time import (
+    date2matlab,
+    matlab2date,
+    date2dt64,
+    dt642date,
+    date2epoch,
+    epoch2date,
+)
 
 
 def _check_file_ext(path, ext):
     filename = path.replace("\\", "/").rsplit("/")[-1]  # windows/linux
     # for a filename like mcrl.water_velocity-1s.b1.20200813.150000.nc
     file_ext = filename.rsplit(".")[-1]
-    if '.' in filename:
+    if "." in filename:
         if file_ext != ext:
             raise IOError("File extension must be of the type {}".format(ext))
         if file_ext == ext:
             return path
 
-    return path + '.' + ext
+    return path + "." + ext
 
 
 def _decode_cf(dataset: xr.Dataset) -> xr.Dataset:
     """
     Wrapper around `xarray.decode_cf()` which handles additional edge cases.
 
     This helps ensure that the dataset is formatted and encoded correctly after it has
@@ -72,39 +79,41 @@
     Parameters
     ----------
     filename : string
       Filename of instrument file to read.
     userdata : True, False, or string of userdata.json filename (default ``True``)
       Whether to read the '<base-filename>.userdata.json' file.
     nens : None, int or 2-element tuple (start, stop)
-      Number of pings or ensembles to read from the file. 
+      Number of pings or ensembles to read from the file.
       Default is None, read entire file
     **kwargs : dict
       Passed to instrument-specific parser.
 
     Returns
     -------
     ds : xarray.Dataset
       An xarray dataset from instrument datafile.
     """
 
     file_type = _get_filetype(fname)
-    if file_type == '<GIT-LFS pointer>':
-        raise IOError("File '{}' looks like a git-lfs pointer. You may need to "
-                      "install and initialize git-lfs. See https://git-lfs.github.com"
-                      " for details.".format(fname))
+    if file_type == "<GIT-LFS pointer>":
+        raise IOError(
+            "File '{}' looks like a git-lfs pointer. You may need to "
+            "install and initialize git-lfs. See https://git-lfs.github.com"
+            " for details.".format(fname)
+        )
     elif file_type is None:
-        raise IOError("File '{}' is not recognized as a file-type that is readable by "
-                      "DOLfYN. If you think it should be readable, try using the "
-                      "appropriate read function (`read_rdi`, `read_nortek`, or "
-                      "`read_signature`) found in dolfyn.io.api.".format(fname))
+        raise IOError(
+            "File '{}' is not recognized as a file-type that is readable by "
+            "DOLfYN. If you think it should be readable, try using the "
+            "appropriate read function (`read_rdi`, `read_nortek`, or "
+            "`read_signature`) found in dolfyn.io.api.".format(fname)
+        )
     else:
-        func_map = dict(RDI=read_rdi,
-                        nortek=read_nortek,
-                        signature=read_signature)
+        func_map = dict(RDI=read_rdi, nortek=read_nortek, signature=read_signature)
         func = func_map[file_type]
     return func(fname, userdata=userdata, nens=nens, **kwargs)
 
 
 def read_example(name, **kwargs):
     """
     Read an ADCP or ADV datafile from the examples directory.
@@ -126,24 +135,21 @@
     Returns
     -------
     ds : xarray.Dataset
       An xarray dataset from the binary instrument data.
     """
 
     testdir = dirname(abspath(__file__))
-    exdir = normpath(join(testdir, relpath('../../../examples/data/dolfyn/')))
-    filename = exdir + '/' + name
+    exdir = normpath(join(testdir, relpath("../../../examples/data/dolfyn/")))
+    filename = exdir + "/" + name
 
     return read(filename, **kwargs)
 
 
-def save(ds, filename,
-         format='NETCDF4', engine='netcdf4',
-         compression=False,
-         **kwargs):
+def save(ds, filename, format="NETCDF4", engine="netcdf4", compression=False, **kwargs):
     """
     Save xarray dataset as netCDF (.nc).
 
     Parameters
     ----------
     ds : xarray.Dataset
       Dataset to save
@@ -163,39 +169,49 @@
 
     More detailed compression options can be specified by specifying
     'encoding' in kwargs. The values in encoding will take precedence
     over whatever is set according to the compression option above.
     See the xarray.to_netcdf documentation for more details.
     """
 
-    filename = _check_file_ext(filename, 'nc')
+    filename = _check_file_ext(filename, "nc")
 
     # Handling complex values for netCDF4
-    ds.attrs['complex_vars'] = []
+    ds.attrs["complex_vars"] = []
     for var in ds.data_vars:
         if np.iscomplexobj(ds[var]):
-            ds[var+'_real'] = ds[var].real
-            ds[var+'_imag'] = ds[var].imag
+            ds[var + "_real"] = ds[var].real
+            ds[var + "_imag"] = ds[var].imag
 
             ds = ds.drop_vars(var)
-            ds.attrs['complex_vars'].append(var)
+            ds.attrs["complex_vars"].append(var)
 
         # For variables that get rewritten to float64
         elif ds[var].dtype == np.float64:
-            ds[var] = ds[var].astype('float32')
+            ds[var] = ds[var].astype("float32")
 
-    if compression:
-        enc = dict()
-        for ky in ds.variables:
-            enc[ky] = dict(zlib=True, complevel=1)
-        if 'encoding' in kwargs:
-            # Overwrite ('update') values in enc with whatever is in kwargs['encoding']
-            enc.update(kwargs['encoding'])
-        else:
-            kwargs['encoding'] = enc
+    # Write variable encoding
+    enc = dict()
+    if "encoding" in kwargs:
+        enc.update(kwargs["encoding"])
+    for ky in ds.variables:
+        # Save prior encoding
+        enc[ky] = ds[ky].encoding
+        # Remove unexpected netCDF4 encoding parameters
+        # https://github.com/pydata/xarray/discussions/5709
+        params = ["szip", "zstd", "bzip2", "blosc", "contiguous", "chunksizes"]
+        [enc[ky].pop(p) for p in params if p in enc[ky]]
+
+        if compression:
+            # New netcdf4-c cannot compress variable length strings
+            if ds[ky].size <= 1 or isinstance(ds[ky].data[0], str):
+                continue
+            enc[ky].update(dict(zlib=True, complevel=1))
+
+    kwargs["encoding"] = enc
 
     # Fix encoding on datetime64 variables.
     ds = _decode_cf(ds)
 
     ds.to_netcdf(filename, format=format, engine=engine, **kwargs)
 
 
@@ -210,33 +226,34 @@
 
     Returns
     -------
     ds : xarray.Dataset
       An xarray dataset from the binary instrument data.
     """
 
-    filename = _check_file_ext(filename, 'nc')
+    filename = _check_file_ext(filename, "nc")
 
-    ds = xr.load_dataset(filename, engine='netcdf4')
+    ds = xr.load_dataset(filename, engine="netcdf4")
 
     # Convert numpy arrays and strings back to lists
     for nm in ds.attrs:
-        if type(ds.attrs[nm]) == np.ndarray and ds.attrs[nm].size > 1:
+        if isinstance(ds.attrs[nm], np.ndarray) and ds.attrs[nm].size > 1:
             ds.attrs[nm] = list(ds.attrs[nm])
-        elif type(ds.attrs[nm]) == str and nm in ['rotate_vars']:
+        elif isinstance(ds.attrs[nm], str) and nm in ["rotate_vars"]:
             ds.attrs[nm] = [ds.attrs[nm]]
 
     # Rejoin complex numbers
-    if hasattr(ds, 'complex_vars') and len(ds.complex_vars):
-        if len(ds.complex_vars[0]) == 1:
-            ds.attrs['complex_vars'] = [ds.complex_vars]
-        for var in ds.complex_vars:
-            ds[var] = ds[var+'_real'] + ds[var+'_imag'] * 1j
-            ds = ds.drop_vars([var+'_real', var+'_imag'])
-    ds.attrs.pop('complex_vars')
+    if hasattr(ds, "complex_vars"):
+        if len(ds.complex_vars):
+            if len(ds.complex_vars[0]) == 1:
+                ds.attrs["complex_vars"] = [ds.complex_vars]
+            for var in ds.complex_vars:
+                ds[var] = ds[var + "_real"] + ds[var + "_imag"] * 1j
+                ds = ds.drop_vars([var + "_real", var + "_imag"])
+        ds.attrs.pop("complex_vars")
 
     return ds
 
 
 def save_mat(ds, filename, datenum=True):
     """
     Save xarray dataset as a MATLAB (.mat) file
@@ -258,54 +275,58 @@
 
     See Also
     --------
     scipy.io.savemat()
     """
 
     def copy_attrs(matfile, ds, key):
-        if hasattr(ds[key], 'units'):
-            matfile['units'][key] = ds[key].units
-        if hasattr(ds[key], 'long_name'):
-            matfile['long_name'][key] = ds[key].long_name
-        if hasattr(ds[key], 'standard_name'):
-            matfile['standard_name'][key] = ds[key].standard_name
+        if hasattr(ds[key], "units"):
+            matfile["units"][key] = ds[key].units
+        if hasattr(ds[key], "long_name"):
+            matfile["long_name"][key] = ds[key].long_name
+        if hasattr(ds[key], "standard_name"):
+            matfile["standard_name"][key] = ds[key].standard_name
 
-    filename = _check_file_ext(filename, 'mat')
+    filename = _check_file_ext(filename, "mat")
 
     # Convert time to datenum
-    t_coords = [t for t in ds.coords if np.issubdtype(
-        ds[t].dtype, np.datetime64)]
-    t_data = [t for t in ds.data_vars if np.issubdtype(
-        ds[t].dtype, np.datetime64)]
+    t_coords = [t for t in ds.coords if np.issubdtype(ds[t].dtype, np.datetime64)]
+    t_data = [t for t in ds.data_vars if np.issubdtype(ds[t].dtype, np.datetime64)]
 
     if datenum:
         func = date2matlab
     else:
         func = date2epoch
 
     for ky in t_coords:
         dt = func(dt642date(ds[ky]))
         ds = ds.assign_coords({ky: (ky, dt, ds[ky].attrs)})
     for ky in t_data:
         dt = func(dt642date(ds[ky]))
         ds[ky].data = dt
 
-    ds.attrs['time_coords'] = t_coords
-    ds.attrs['time_data_vars'] = t_data
+    ds.attrs["time_coords"] = t_coords
+    ds.attrs["time_data_vars"] = t_data
 
     # Save xarray structure with more descriptive structure names
-    matfile = {'vars': {}, 'coords': {}, 'config': {},
-               'units': {}, 'long_name': {}, 'standard_name': {}}
+    matfile = {
+        "vars": {},
+        "coords": {},
+        "config": {},
+        "units": {},
+        "long_name": {},
+        "standard_name": {},
+    }
     for ky in ds.data_vars:
-        matfile['vars'][ky] = ds[ky].values
+        matfile["vars"][ky] = ds[ky].values
         copy_attrs(matfile, ds, ky)
     for ky in ds.coords:
-        matfile['coords'][ky] = ds[ky].values
+        matfile["coords"][ky] = ds[ky].values
         copy_attrs(matfile, ds, ky)
-    matfile['config'] = ds.attrs
+    matfile["config"] = ds.attrs
 
     sio.savemat(filename, matfile)
 
 
 def load_mat(filename, datenum=True):
     """
     Load xarray dataset from MATLAB (.mat) file, complimentary to `save_mat()`
@@ -314,69 +335,79 @@
     where 'coords' contain the dimensions of all variables in 'vars'.
 
     Parameters
     ----------
     filename : str
       Filename and/or path with the '.mat' extension
     datenum : bool
-      If true, converts time from datenum. If false, converts time from 
+      If true, converts time from datenum. If false, converts time from
       "epoch time".
 
     Returns
     -------
     ds : xarray.Dataset
       An xarray dataset from the binary instrument data.
 
     See Also
     --------
     scipy.io.loadmat()
     """
 
-    filename = _check_file_ext(filename, 'mat')
+    filename = _check_file_ext(filename, "mat")
 
     data = sio.loadmat(filename, struct_as_record=False, squeeze_me=True)
 
-    ds_dict = {'vars': {}, 'coords': {}, 'config': {},
-               'units': {}, 'long_name': {}, 'standard_name': {}}
+    ds_dict = {
+        "vars": {},
+        "coords": {},
+        "config": {},
+        "units": {},
+        "long_name": {},
+        "standard_name": {},
+    }
     for nm in ds_dict:
         key_list = data[nm]._fieldnames
         for ky in key_list:
             ds_dict[nm][ky] = getattr(data[nm], ky)
 
-    ds_dict['data_vars'] = ds_dict.pop('vars')
-    ds_dict['attrs'] = ds_dict.pop('config')
+    ds_dict["data_vars"] = ds_dict.pop("vars")
+    ds_dict["attrs"] = ds_dict.pop("config")
 
     # Recreate dataset
     ds = _create_dataset(ds_dict)
     ds = _set_coords(ds, ds.coord_sys)
 
     # Convert numpy arrays and strings back to lists
     for nm in ds.attrs:
-        if type(ds.attrs[nm]) == np.ndarray and ds.attrs[nm].size > 1:
+        if isinstance(ds.attrs[nm], np.ndarray) and ds.attrs[nm].size > 1:
             try:
-                ds.attrs[nm] = [x.strip(' ') for x in list(ds.attrs[nm])]
+                ds.attrs[nm] = [x.strip(" ") for x in list(ds.attrs[nm])]
             except:
                 ds.attrs[nm] = list(ds.attrs[nm])
-        elif type(ds.attrs[nm]) == str and nm in ['time_coords', 'time_data_vars', 'rotate_vars']:
+        elif isinstance(ds.attrs[nm], str) and nm in [
+            "time_coords",
+            "time_data_vars",
+            "rotate_vars",
+        ]:
             ds.attrs[nm] = [ds.attrs[nm]]
 
-    if hasattr(ds, 'orientation_down'):
-        ds['orientation_down'] = ds['orientation_down'].astype(bool)
+    if hasattr(ds, "orientation_down"):
+        ds["orientation_down"] = ds["orientation_down"].astype(bool)
 
     if datenum:
         func = matlab2date
     else:
         func = epoch2date
 
     # Restore datnum to np.dt64
-    if hasattr(ds, 'time_coords'):
-        for ky in ds.attrs['time_coords']:
+    if hasattr(ds, "time_coords"):
+        for ky in ds.attrs["time_coords"]:
             dt = date2dt64(func(ds[ky].values))
             ds = ds.assign_coords({ky: dt})
-        ds.attrs.pop('time_coords')
-    if hasattr(ds, 'time_data_vars'):
-        for ky in ds.attrs['time_data_vars']:
+        ds.attrs.pop("time_coords")
+    if hasattr(ds, "time_data_vars"):
+        for ky in ds.attrs["time_data_vars"]:
             dt = date2dt64(func(ds[ky].values))
             ds[ky].data = dt
-        ds.attrs.pop('time_data_vars')
+        ds.attrs.pop("time_data_vars")
 
     return ds
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/io/base.py` & `mhkit-0.8.0/mhkit/dolfyn/io/base.py`

 * *Files 26% similar despite different names*

```diff
@@ -19,273 +19,309 @@
        None - Doesn't match any known pattern
        'signature' - for Nortek signature files
        'nortek' - for Nortek (Vec, AWAC) files
        'RDI' - for RDI files
        '<GIT-LFS pointer> - if the file looks like a GIT-LFS pointer.
     """
 
-    with open(fname, 'rb') as rdr:
+    with open(fname, "rb") as rdr:
         bytes = rdr.read(40)
     code = bytes[:2].hex()
-    if code in ['7f79', '7f7f']:
-        return 'RDI'
-    elif code in ['a50a']:
-        return 'signature'
-    elif code in ['a505']:
+    if code in ["7f79", "7f7f"]:
+        return "RDI"
+    elif code in ["a50a"]:
+        return "signature"
+    elif code in ["a505"]:
         # AWAC
-        return 'nortek'
-    elif bytes == b'version https://git-lfs.github.com/spec/':
-        return '<GIT-LFS pointer>'
+        return "nortek"
+    elif bytes == b"version https://git-lfs.github.com/spec/":
+        return "<GIT-LFS pointer>"
     else:
         return None
 
 
 def _find_userdata(filename, userdata=True):
     # This function finds the file to read
     if userdata:
-        for basefile in [filename.rsplit('.', 1)[0],
-                         filename]:
-            jsonfile = basefile + '.userdata.json'
+        for basefile in [filename.rsplit(".", 1)[0], filename]:
+            jsonfile = basefile + ".userdata.json"
             if os.path.isfile(jsonfile):
                 return _read_userdata(jsonfile)
 
-    elif isinstance(userdata, (str, )) or hasattr(userdata, 'read'):
+    elif isinstance(userdata, (str,)) or hasattr(userdata, "read"):
         return _read_userdata(userdata)
     return {}
 
 
 def _read_userdata(fname):
     """
     Reads a userdata.json file and returns the data it contains as a
     dictionary.
     """
     with open(fname) as data_file:
         data = json.load(data_file)
-    for nm in ['body2head_rotmat', 'body2head_vec']:
+    for nm in ["body2head_rotmat", "body2head_vec"]:
         if nm in data:
-            new_name = 'inst' + nm[4:]
+            new_name = "inst" + nm[4:]
             warnings.warn(
-                f'{nm} has been deprecated, please change this to {new_name} \
-                    in {fname}.')
+                f"{nm} has been deprecated, please change this to {new_name} \
+                    in {fname}."
+            )
             data[new_name] = data.pop(nm)
-    if 'inst2head_rotmat' in data:
-        if data['inst2head_rotmat'] in ['identity', 'eye', 1, 1.]:
-            data['inst2head_rotmat'] = np.eye(3)
+    if "inst2head_rotmat" in data:
+        if data["inst2head_rotmat"] in ["identity", "eye", 1, 1.0]:
+            data["inst2head_rotmat"] = np.eye(3)
         else:
-            data['inst2head_rotmat'] = np.array(data['inst2head_rotmat'])
-    if 'inst2head_vec' in data and type(data['inst2head_vec']) != list:
-        data['inst2head_vec'] = list(data['inst2head_vec'])
+            data["inst2head_rotmat"] = np.array(data["inst2head_rotmat"])
+    if "inst2head_vec" in data and type(data["inst2head_vec"]) != list:
+        data["inst2head_vec"] = list(data["inst2head_vec"])
 
     return data
 
 
 def _handle_nan(data):
     """
-    Finds trailing nan's that cause issues in running the rotation 
+    Finds trailing nan's that cause issues in running the rotation
     algorithms and deletes them.
     """
-    nan = np.zeros(data['coords']['time'].shape, dtype=bool)
-    l = data['coords']['time'].size
+    nan = np.zeros(data["coords"]["time"].shape, dtype=bool)
+    l = data["coords"]["time"].size
 
-    if any(np.isnan(data['coords']['time'])):
-        nan += np.isnan(data['coords']['time'])
+    if any(np.isnan(data["coords"]["time"])):
+        nan += np.isnan(data["coords"]["time"])
 
     # Required for motion-correction algorithm
-    var = ['accel', 'angrt', 'mag']
-    for key in data['data_vars']:
+    var = ["accel", "angrt", "mag"]
+    for key in data["data_vars"]:
         if any(val in key for val in var):
-            shp = data['data_vars'][key].shape
+            shp = data["data_vars"][key].shape
             if shp[-1] == l:
                 if len(shp) == 1:
-                    if any(np.isnan(data['data_vars'][key])):
-                        nan += np.isnan(data['data_vars'][key])
+                    if any(np.isnan(data["data_vars"][key])):
+                        nan += np.isnan(data["data_vars"][key])
                 elif len(shp) == 2:
-                    if any(np.isnan(data['data_vars'][key][-1])):
-                        nan += np.isnan(data['data_vars'][key][-1])
+                    if any(np.isnan(data["data_vars"][key][-1])):
+                        nan += np.isnan(data["data_vars"][key][-1])
     trailing = np.cumsum(nan)[-1]
 
     if trailing > 0:
-        data['coords']['time'] = data['coords']['time'][:-trailing]
-        for key in data['data_vars']:
-            if data['data_vars'][key].shape[-1] == l:
-                data['data_vars'][key] = data['data_vars'][key][..., :-trailing]
+        data["coords"]["time"] = data["coords"]["time"][:-trailing]
+        for key in data["data_vars"]:
+            if data["data_vars"][key].shape[-1] == l:
+                data["data_vars"][key] = data["data_vars"][key][..., :-trailing]
 
     return data
 
 
 def _create_dataset(data):
-    """Creates an xarray dataset from dictionary created from binary
+    """
+    Creates an xarray dataset from dictionary created from binary
     readers.
     Direction 'dir' coordinates are set in `set_coords`
     """
-    ds = xr.Dataset()
-    tag = ['_avg', '_b5', '_echo', '_bt', '_gps', '_ast', '_sl']
 
-    FoR = {}
-    try:
-        beams = data['attrs']['n_beams']
-    except:
-        beams = data['attrs']['n_beams_avg']
+    tag = ["_avg", "_b5", "_echo", "_bt", "_gps", "_altraw", "_altraw_avg", "_sl"]
+
+    ds_dict = {}
+    for key in data["coords"]:
+        ds_dict[key] = {"dims": (key), "data": data["coords"][key]}
+
+    # Set various coordinate frames
+    if "n_beams_avg" in data["attrs"]:
+        beams = data["attrs"]["n_beams_avg"]
+    else:
+        beams = data["attrs"]["n_beams"]
     n_beams = max(min(beams, 4), 3)
-    beams = np.arange(1, n_beams+1, dtype=np.int32)
-    FoR['beam'] = xr.DataArray(beams, dims=['beam'], name='beam', attrs={
-                               'units': '1', 'long_name': 'Beam Reference Frame'})
-    FoR['dir'] = xr.DataArray(beams, dims=['dir'], name='dir', attrs={
-                              'units': '1', 'long_name': 'Reference Frame'})
+    beams = np.arange(1, n_beams + 1, dtype=np.int32)
+
+    ds_dict["beam"] = {"dims": ("beam"), "data": beams}
+    ds_dict["dir"] = {"dims": ("dir"), "data": beams}
+    data["units"].update({"beam": "1", "dir": "1"})
+    data["long_name"].update({"beam": "Beam Reference Frame", "dir": "Reference Frame"})
 
-    for key in data['data_vars']:
+    # Iterate through data variables and add them to new dictionary
+    for key in data["data_vars"]:
         # orientation matrices
-        if 'mat' in key:
-            if 'inst' in key:  # beam2inst & inst2head orientation matrices
-                ds[key] = xr.DataArray(data['data_vars'][key],
-                                       coords={'x1': beams, 'x2': beams},
-                                       dims=['x1', 'x2'],
-                                       attrs={'units': '1',
-                                              'long_name': 'Rotation Matrix'})
-            elif 'orientmat' in key:  # earth2inst orientation matrix
+        if "mat" in key:
+            if "inst" in key:  # beam2inst & inst2head orientation matrices
+                if "x1" not in ds_dict:
+                    ds_dict["x1"] = {"dims": ("x1"), "data": beams}
+                    ds_dict["x2"] = {"dims": ("x2"), "data": beams}
+
+                ds_dict[key] = {"dims": ("x1", "x2"), "data": data["data_vars"][key]}
+                data["units"].update({key: "1"})
+                data["long_name"].update({key: "Rotation Matrix"})
+
+            elif "orientmat" in key:  # earth2inst orientation matrix
                 if any(val in key for val in tag):
-                    tg = '_' + key.rsplit('_')[-1]
+                    tg = "_" + key.rsplit("_")[-1]
                 else:
-                    tg = ''
-                earth = xr.DataArray(['E', 'N', 'U'], dims=['earth'], name='earth', attrs={
-                    'units': '1', 'long_name': 'Earth Reference Frame'})
-                inst = xr.DataArray(['X', 'Y', 'Z'], dims=['inst'], name='inst', attrs={
-                    'units': '1', 'long_name': 'Instrument Reference Frame'})
-                time = data['coords']['time'+tg]
-                ds[key] = xr.DataArray(data['data_vars'][key],
-                                       coords={'earth': earth,
-                                               'inst': inst, 'time'+tg: time},
-                                       dims=['earth', 'inst', 'time'+tg],
-                                       attrs={'units': data['units']['orientmat'],
-                                              'long_name': data['long_name']['orientmat']})
+                    tg = ""
+
+                ds_dict["earth"] = {"dims": ("earth"), "data": ["E", "N", "U"]}
+                ds_dict["inst"] = {"dims": ("inst"), "data": ["X", "Y", "Z"]}
+                ds_dict[key] = {
+                    "dims": ("earth", "inst", "time" + tg),
+                    "data": data["data_vars"][key],
+                }
+                data["units"].update(
+                    {"earth": "1", "inst": "1", key: data["units"]["orientmat"]}
+                )
+                data["long_name"].update(
+                    {
+                        "earth": "Earth Reference Frame",
+                        "inst": "Instrument Reference Frame",
+                        key: data["long_name"]["orientmat"],
+                    }
+                )
 
         # quaternion units never change
-        elif 'quaternions' in key:
+        elif "quaternions" in key:
             if any(val in key for val in tag):
-                tg = '_' + key.rsplit('_')[-1]
+                tg = "_" + key.rsplit("_")[-1]
             else:
-                tg = ''
-            q = xr.DataArray(['w', 'x', 'y', 'z'], dims=['q'], name='q', attrs={
-                             'units': '1', 'long_name': 'Quaternion Vector Components'})
-            time = data['coords']['time'+tg]
-            ds[key] = xr.DataArray(data['data_vars'][key],
-                                   coords={'q': q,
-                                           'time'+tg: time},
-                                   dims=['q', 'time'+tg],
-                                   attrs={'units': data['units']['quaternions'],
-                                          'long_name': data['long_name']['quaternions']})
+                tg = ""
+
+            if "q" not in ds_dict:
+                ds_dict["q"] = {"dims": ("q"), "data": ["w", "x", "y", "z"]}
+                data["units"].update({"q": "1"})
+                data["long_name"].update({"q": "Quaternion Vector Components"})
+
+            ds_dict[key] = {"dims": ("q", "time" + tg), "data": data["data_vars"][key]}
+            data["units"].update({key: data["units"]["quaternions"]})
+            data["long_name"].update({key: data["long_name"]["quaternions"]})
+
         else:
-            # Assign each variable to a dataArray
-            ds[key] = xr.DataArray(data['data_vars'][key])
-            # Assign metadata to each dataArray
-            for md in ['units', 'long_name', 'standard_name']:
-                if key in data[md]:
-                    ds[key].attrs[md] = data[md][key]
-                try:  # make sure ones with tags get units
-                    tg = '_' + key.rsplit('_')[-1]
-                    if any(val in key for val in tag):
-                        ds[key].attrs[md] = data[md][key[:-len(tg)]]
-                except:
-                    pass
-
-            # Fill in dimensions and coordinates for each dataArray
-            shp = data['data_vars'][key].shape
-            l = len(shp)
-            if l == 1:  # 1D variables
-                if any(val in key for val in tag):
-                    tg = '_' + key.rsplit('_')[-1]
+            shp = data["data_vars"][key].shape
+            if len(shp) == 1:  # 1D variables
+                if "_altraw_avg" in key:
+                    tg = "_altraw_avg"
+                elif any(val in key for val in tag):
+                    tg = "_" + key.rsplit("_")[-1]
                 else:
-                    tg = ''
-                ds[key] = ds[key].rename({'dim_0': 'time'+tg})
-                ds[key] = ds[key].assign_coords(
-                    {'time'+tg: data['coords']['time'+tg]})
-
-            elif l == 2:  # 2D variables
-                if key == 'echo':
-                    ds[key] = ds[key].rename({'dim_0': 'range_echo',
-                                              'dim_1': 'time_echo'})
-                    ds[key] = ds[key].assign_coords({'range_echo': data['coords']['range_echo'],
-                                                     'time_echo': data['coords']['time_echo']})
+                    tg = ""
+                ds_dict[key] = {"dims": ("time" + tg), "data": data["data_vars"][key]}
+
+            elif len(shp) == 2:  # 2D variables
+                if key == "echo":
+                    ds_dict[key] = {
+                        "dims": ("range_echo", "time_echo"),
+                        "data": data["data_vars"][key],
+                    }
+                elif key == "samp_altraw":
+                    ds_dict[key] = {
+                        "dims": ("n_altraw", "time_altraw"),
+                        "data": data["data_vars"][key],
+                    }
+                elif key == "samp_altraw_avg":
+                    ds_dict[key] = {
+                        "dims": ("n_altraw_avg", "time_altraw_avg"),
+                        "data": data["data_vars"][key],
+                    }
+
                 # ADV/ADCP instrument vector data, bottom tracking
                 elif shp[0] == n_beams and not any(val in key for val in tag[:3]):
-                    if 'bt' in key and 'time_bt' in data['coords']:
-                        tg = '_bt'
+                    if "bt" in key and "time_bt" in data["coords"]:
+                        tg = "_bt"
                     else:
-                        tg = ''
-                    if any(key.rsplit('_')[0] in s for s in ['amp', 'corr', 'dist', 'prcnt_gd']):
-                        dim0 = 'beam'
+                        tg = ""
+                    if any(
+                        key.rsplit("_")[0] in s
+                        for s in ["amp", "corr", "dist", "prcnt_gd"]
+                    ):
+                        dim0 = "beam"
                     else:
-                        dim0 = 'dir'
-                    ds[key] = ds[key].rename({'dim_0': dim0,
-                                              'dim_1': 'time'+tg})
-                    ds[key] = ds[key].assign_coords({dim0: FoR[dim0],
-                                                     'time'+tg: data['coords']['time'+tg]})
+                        dim0 = "dir"
+                    ds_dict[key] = {
+                        "dims": (dim0, "time" + tg),
+                        "data": data["data_vars"][key],
+                    }
+
                 # ADCP IMU data
                 elif shp[0] == 3:
                     if not any(val in key for val in tag):
-                        tg = ''
+                        tg = ""
                     else:
                         tg = [val for val in tag if val in key]
                         tg = tg[0]
-                    dirIMU = xr.DataArray([1, 2, 3], dims=['dirIMU'], name='dirIMU', attrs={
-                        'units': '1', 'long_name': 'Reference Frame'})
-                    ds[key] = ds[key].rename({'dim_0': 'dirIMU',
-                                              'dim_1': 'time'+tg})
-                    ds[key] = ds[key].assign_coords({'dirIMU': dirIMU,
-                                                     'time'+tg: data['coords']['time'+tg]})
-
-                ds[key].attrs['coverage_content_type'] = 'physicalMeasurement'
-
-            elif l == 3:  # 3D variables
-                if 'vel' in key:
-                    dim0 = 'dir'
+
+                    if "dirIMU" not in ds_dict:
+                        ds_dict["dirIMU"] = {"dims": ("dirIMU"), "data": [1, 2, 3]}
+                        data["units"].update({"dirIMU": "1"})
+                        data["long_name"].update({"dirIMU": "Reference Frame"})
+
+                    ds_dict[key] = {
+                        "dims": ("dirIMU", "time" + tg),
+                        "data": data["data_vars"][key],
+                    }
+
+                elif "b5" in tg:
+                    ds_dict[key] = {
+                        "dims": ("range_b5", "time_b5"),
+                        "data": data["data_vars"][key],
+                    }
+
+            elif len(shp) == 3:  # 3D variables
+                if "vel" in key:
+                    dim0 = "dir"
                 else:  # amp, corr, prcnt_gd, status
-                    dim0 = 'beam'
+                    dim0 = "beam"
 
-                if not any(val in key for val in tag) or ('_avg' in key):
-                    if '_avg' in key:
-                        tg = '_avg'
+                if not any(val in key for val in tag) or ("_avg" in key):
+                    if "_avg" in key:
+                        tg = "_avg"
                     else:
-                        tg = ''
-                    ds[key] = ds[key].rename({'dim_0': dim0,
-                                              'dim_1': 'range'+tg,
-                                              'dim_2': 'time'+tg})
-                    ds[key] = ds[key].assign_coords({dim0: FoR[dim0],
-                                                     'range'+tg: data['coords']['range'+tg],
-                                                     'time'+tg: data['coords']['time'+tg]})
-                elif 'b5' in key:
-                    # xarray can't handle coords of length 1
-                    ds[key] = ds[key][0]
-                    ds[key] = ds[key].rename({'dim_1': 'range_b5',
-                                              'dim_2': 'time_b5'})
-                    ds[key] = ds[key].assign_coords({'range_b5': data['coords']['range_b5'],
-                                                     'time_b5': data['coords']['time_b5']})
-                elif 'sl' in key:
-                    ds[key] = ds[key].rename({'dim_0': dim0,
-                                              'dim_1': 'range_sl',
-                                              'dim_2': 'time'})
-                    ds[key] = ds[key].assign_coords({'range_sl': data['coords']['range_sl'],
-                                                     'time': data['coords']['time']})
+                        tg = ""
+                    ds_dict[key] = {
+                        "dims": (dim0, "range" + tg, "time" + tg),
+                        "data": data["data_vars"][key],
+                    }
+
+                elif "b5" in key:
+                    # "vel_b5" sometimes stored as (1, range_b5, time_b5)
+                    ds_dict[key] = {
+                        "dims": ("range_b5", "time_b5"),
+                        "data": data["data_vars"][key][0],
+                    }
+                elif "sl" in key:
+                    ds_dict[key] = {
+                        "dims": (dim0, "range_sl", "time"),
+                        "data": data["data_vars"][key],
+                    }
                 else:
-                    ds = ds.drop_vars(key)
-                    warnings.warn(f'Variable not included in dataset: {key}')
+                    warnings.warn(f"Variable not included in dataset: {key}")
+
+    # Create dataset
+    ds = xr.Dataset.from_dict(ds_dict)
 
-                ds[key].attrs['coverage_content_type'] = 'physicalMeasurement'
+    # Assign data array attributes
+    for key in ds.variables:
+        for md in ["units", "long_name", "standard_name"]:
+            if key in data[md]:
+                ds[key].attrs[md] = data[md][key]
+            if len(ds[key].shape) > 1:
+                ds[key].attrs["coverage_content_type"] = "physicalMeasurement"
+            try:  # make sure ones with tags get units
+                tg = "_" + key.rsplit("_")[-1]
+                if any(val in key for val in tag):
+                    ds[key].attrs[md] = data[md][key[: -len(tg)]]
+            except:
+                pass
 
-    # coordinate attributes
+    # Assign coordinate attributes
     for ky in ds.dims:
-        ds[ky].attrs['coverage_content_type'] = 'coordinate'
-    r_list = [r for r in ds.coords if 'range' in r]
+        ds[ky].attrs["coverage_content_type"] = "coordinate"
+    r_list = [r for r in ds.coords if "range" in r]
     for ky in r_list:
-        ds[ky].attrs['units'] = 'm'
-        ds[ky].attrs['long_name'] = 'Profile Range'
-        ds[ky].attrs['description'] = 'Distance to the center of each depth bin'
-    time_list = [t for t in ds.coords if 'time' in t]
+        ds[ky].attrs["units"] = "m"
+        ds[ky].attrs["long_name"] = "Profile Range"
+        ds[ky].attrs["description"] = "Distance to the center of each depth bin"
+    time_list = [t for t in ds.coords if "time" in t]
     for ky in time_list:
-        ds[ky].attrs['units'] = 'seconds since 1970-01-01 00:00:00'
-        ds[ky].attrs['long_name'] = 'Time'
-        ds[ky].attrs['standard_name'] = 'time'
+        ds[ky].attrs["units"] = "seconds since 1970-01-01 00:00:00"
+        ds[ky].attrs["long_name"] = "Time"
+        ds[ky].attrs["standard_name"] = "time"
 
-    # dataset metadata
-    ds.attrs = data['attrs']
+    # Set dataset metadata
+    ds.attrs = data["attrs"]
 
     return ds
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/io/nortek.py` & `mhkit-0.8.0/mhkit/dolfyn/io/nortek.py`

 * *Files 17% similar despite different names*

```diff
@@ -10,16 +10,17 @@
 from .base import _find_userdata, _create_dataset, _handle_nan, _abspath
 from ..tools import misc as tbx
 from ..rotate.vector import _calc_omat
 from ..rotate.base import _set_coords
 from ..rotate import api as rot
 
 
-def read_nortek(filename, userdata=True, debug=False, do_checksum=False,
-                nens=None, **kwargs):
+def read_nortek(
+    filename, userdata=True, debug=False, do_checksum=False, nens=None, **kwargs
+):
     """
     Read a classic Nortek (AWAC and Vector) datafile
 
     Parameters
     ----------
     filename : string
       Filename of Nortek file to read.
@@ -27,81 +28,85 @@
       Whether to read the '<base-filename>.userdata.json' file.
       Default = True
     debug : bool
       Logs debugger ouput if true. Default = False
     do_checksum : bool
       Whether to perform the checksum of each data block. Default = False
     nens : None, int or 2-element tuple (start, stop)
-      Number of pings or ensembles to read from the file. 
+      Number of pings or ensembles to read from the file.
       Default is None, read entire file
 
     Returns
     -------
     ds : xarray.Dataset
       An xarray dataset from the binary instrument data
     """
 
     # Start debugger logging
     if debug:
         for handler in logging.root.handlers[:]:
             logging.root.removeHandler(handler)
         filepath = Path(filename)
-        logfile = filepath.with_suffix('.dolfyn.log')
-        logging.basicConfig(filename=str(logfile),
-                            filemode='w',
-                            level=logging.NOTSET,
-                            format='%(name)s - %(levelname)s - %(message)s')
+        logfile = filepath.with_suffix(".dolfyn.log")
+        logging.basicConfig(
+            filename=str(logfile),
+            filemode="w",
+            level=logging.NOTSET,
+            format="%(name)s - %(levelname)s - %(message)s",
+        )
 
     userdata = _find_userdata(filename, userdata)
 
-    with _NortekReader(filename, debug=debug, do_checksum=do_checksum,
-                       nens=nens) as rdr:
-        rdr.readfile()
+    rdr = _NortekReader(filename, debug=debug, do_checksum=do_checksum, nens=nens)
+    rdr.readfile()
     rdr.dat2sci()
     dat = rdr.data
 
     # Remove trailing nan's in time and orientation data
     dat = _handle_nan(dat)
 
     # Search for missing timestamps and interpolate them
-    coords = dat['coords']
-    t_list = [t for t in coords if 'time' in t]
+    coords = dat["coords"]
+    t_list = [t for t in coords if "time" in t]
     for ky in t_list:
         tdat = coords[ky]
         tdat[tdat == 0] = np.NaN
         if np.isnan(tdat).any():
-            tag = ky.lstrip('time')
-            warnings.warn("Zero/NaN values found in '{}'. Interpolating and "
-                          "extrapolating them. To identify which values were filled later, "
-                          "look for 0 values in 'status{}'".format(ky, tag))
-            tdat = time._fill_time_gaps(
-                tdat, sample_rate_hz=dat['attrs']['fs'])
-        coords[ky] = time.epoch2dt64(tdat).astype('datetime64[ns]')
+            tag = ky.lstrip("time")
+            warnings.warn(
+                "Zero/NaN values found in '{}'. Interpolating and "
+                "extrapolating them. To identify which values were filled later, "
+                "look for 0 values in 'status{}'".format(ky, tag)
+            )
+            tdat = time._fill_time_gaps(tdat, sample_rate_hz=dat["attrs"]["fs"])
+        coords[ky] = time.epoch2dt64(tdat).astype("datetime64[ns]")
 
     # Apply rotation matrix and declination
     rotmat = None
     declin = None
     for nm in userdata:
-        if 'rotmat' in nm:
+        if "rotmat" in nm:
             rotmat = userdata[nm]
-        elif 'dec' in nm:
+        elif "dec" in nm:
             declin = userdata[nm]
         else:
-            dat['attrs'][nm] = userdata[nm]
+            dat["attrs"][nm] = userdata[nm]
 
     # Create xarray dataset from upper level dictionary
     ds = _create_dataset(dat)
     ds = _set_coords(ds, ref_frame=ds.coord_sys)
 
-    if 'orientmat' not in ds:
-        ds['orientmat'] = _calc_omat(ds['time'],
-                                     ds['heading'],
-                                     ds['pitch'],
-                                     ds['roll'],
-                                     ds.get('orientation_down', None))
+    if "orientmat" not in ds:
+        ds["orientmat"] = _calc_omat(
+            ds["time"],
+            ds["heading"],
+            ds["pitch"],
+            ds["roll"],
+            ds.get("orientation_down", None),
+        )
 
     if rotmat is not None:
         rot.set_inst2head_rotmat(ds, rotmat, inplace=True)
     if declin is not None:
         rot.set_declination(ds, declin, inplace=True)
 
     # Close handler
@@ -110,35 +115,35 @@
             logging.root.removeHandler(handler)
             handler.close()
 
     return ds
 
 
 def _bcd2char(cBCD):
-    """Taken from the Nortek System Integrator Manual 
+    """Taken from the Nortek System Integrator Manual
     "Example Program" Chapter.
     """
     cBCD = min(cBCD, 153)
-    c = (cBCD & 15)
+    c = cBCD & 15
     c += 10 * (cBCD >> 4)
     return c
 
 
 def _bitshift8(val):
     return val >> 8
 
 
 def _int2binarray(val, n):
-    out = np.zeros(n, dtype='bool')
+    out = np.zeros(n, dtype="bool")
     for idx, n in enumerate(range(n)):
-        out[idx] = val & (2 ** n)
+        out[idx] = val & (2**n)
     return out
 
 
-class _NortekReader():
+class _NortekReader:
     """
     A class for reading reading nortek binary files.
     This reader currently only supports AWAC and Vector data formats.
 
     Parameters
     ----------
     fname : string
@@ -149,278 +154,309 @@
     debug : {True, False*} (optional)
       Print debug/progress information?
     do_checksum : {True*, False} (optional)
       Specifies whether to perform the checksum.
     bufsize : int
       The size of the read buffer to use. Default = 100000
     nens : None, int or 2-element tuple (start, stop)
-      Number of pings or ensembles to read from the file. 
+      Number of pings or ensembles to read from the file.
       Default is None, read entire file
     """
 
     _lastread = [None, None, None, None, None]
-    fun_map = {'0x00': 'read_user_cfg',
-               '0x04': 'read_head_cfg',
-               '0x05': 'read_hw_cfg',
-               '0x07': 'read_vec_checkdata',
-               '0x10': 'read_vec_data',
-               '0x11': 'read_vec_sysdata',
-               '0x12': 'read_vec_hdr',
-               '0x71': 'read_microstrain',
-               '0x20': 'read_awac_profile',
-               }
-
-    def __init__(self, fname, endian=None, debug=False,
-                 do_checksum=True, bufsize=100000, nens=None):
+    fun_map = {
+        "0x00": "read_user_cfg",
+        "0x04": "read_head_cfg",
+        "0x05": "read_hw_cfg",
+        "0x07": "read_vec_checkdata",
+        "0x10": "read_vec_data",
+        "0x11": "read_vec_sysdata",
+        "0x12": "read_vec_hdr",
+        "0x20": "read_awac_profile",
+        "0x30": "read_awac_waves",
+        "0x31": "read_awac_waves_hdr",
+        "0x36": "read_awac_waves",  # "SUV"
+        "0x71": "read_microstrain",
+    }
+
+    def __init__(
+        self,
+        fname,
+        endian=None,
+        debug=False,
+        do_checksum=True,
+        bufsize=100000,
+        nens=None,
+    ):
         self.fname = fname
         self._bufsize = bufsize
-        self.f = open(_abspath(fname), 'rb', 1000)
+        self.f = open(_abspath(fname), "rb", 1000)
         self.do_checksum = do_checksum
         self.filesize  # initialize the filesize.
         self.debug = debug
         self.c = 0
         self._dtypes = []
         self._n_start = 0
         try:
             len(nens)
         except TypeError:
             # not a tuple, so we assume None or int
             self._npings = nens
         else:
             if len(nens) != 2:
-                raise TypeError('nens must be: None (), int, or len 2')
-            warnings.warn("A 'start ensemble' is not yet supported "
-                          "for the Nortek reader. This function will read "
-                          "the entire file, then crop the beginning at "
-                          "nens[0].")
+                raise TypeError("nens must be: None (), int, or len 2")
+            warnings.warn(
+                "A 'start ensemble' is not yet supported "
+                "for the Nortek reader. This function will read "
+                "the entire file, then crop the beginning at "
+                "nens[0]."
+            )
             self._npings = nens[1]
             self._n_start = nens[0]
         if endian is None:
-            if unpack('<HH', self.read(4)) == (1445, 24):
-                endian = '<'
-            elif unpack('>HH', self.read(4)) == (1445, 24):
-                endian = '>'
+            if unpack("<HH", self.read(4)) == (1445, 24):
+                endian = "<"
+            elif unpack(">HH", self.read(4)) == (1445, 24):
+                endian = ">"
             else:
-                raise Exception("I/O error: could not determine the "
-                                "'endianness' of the file.  Are you sure this is a Nortek "
-                                "file?")
+                raise Exception(
+                    "I/O error: could not determine the "
+                    "'endianness' of the file.  Are you sure this is a Nortek "
+                    "file?"
+                )
         self.endian = endian
         self.f.seek(0, 0)
 
         # This is the configuration data:
         self.config = {}
-        err_msg = ("I/O error: The file does not "
-                   "appear to be a Nortek data file.")
+        err_msg = "I/O error: The file does not " "appear to be a Nortek data file."
         # Read the header:
         if self.read_id() == 5:
             self.read_hw_cfg()
         else:
             raise Exception()
         if self.read_id() == 4:
             self.read_head_cfg()
         else:
             raise Exception(err_msg)
         if self.read_id() == 0:
             self.read_user_cfg()
         else:
             raise Exception(err_msg)
-        if self.config['hdw']['serial_number'][0:3].upper() == 'WPR':
-            self.config['config_type'] = 'AWAC'
-        elif self.config['hdw']['serial_number'][0:3].upper() == 'VEC':
-            self.config['config_type'] = 'ADV'
+        if self.config["hdw"]["serial_number"][0:3].upper() == "WPR":
+            self.config["config_type"] = "AWAC"
+        elif self.config["hdw"]["serial_number"][0:3].upper() == "VEC":
+            self.config["config_type"] = "ADV"
         # Initialize the instrument type:
-        self._inst = self.config.pop('config_type')
+        self._inst = self.config.pop("config_type")
         # This is the position after reading the 'hardware',
         # 'head', and 'user' configuration.
         pnow = self.pos
 
         # Run the appropriate initialization routine (e.g. init_ADV).
-        getattr(self, 'init_' + self._inst)()
+        getattr(self, "init_" + self._inst)()
         self.f.close()  # This has a small buffer, so close it.
         # This has a large buffer...
-        self.f = open(_abspath(fname), 'rb', bufsize)
+        self.f = open(_abspath(fname), "rb", bufsize)
         self.close = self.f.close
         if self._npings is not None:
             self.n_samp_guess = self._npings
         self.f.seek(pnow, 0)  # Seek to the previous position.
 
-        da = self.data['attrs']
-        if self.config['n_burst'] > 0:
-            fs = round(self.config['fs'], 7)
-            da['duty_cycle_n_burst'] = self.config['n_burst']
-            da['duty_cycle_interval'] = self.config['burst_interval']
+        da = self.data["attrs"]
+        if self.config["n_burst"] > 0:
+            fs = round(self.config["fs"], 7)
+            da["duty_cycle_n_burst"] = self.config["n_burst"]
+            da["duty_cycle_interval"] = self.config["burst_interval"]
             if fs > 1:
-                burst_seconds = self.config['n_burst']/fs
+                burst_seconds = self.config["n_burst"] / fs
             else:
-                burst_seconds = round(1/fs, 3)
-            da['duty_cycle_description'] = "{} second bursts collected at {} Hz, with bursts taken every {} minutes".format(
-                burst_seconds, fs, self.config['burst_interval']/60)
-        self.burst_start = np.zeros(self.n_samp_guess, dtype='bool')
-        da['fs'] = self.config['fs']
-        da['coord_sys'] = {'XYZ': 'inst',
-                           'ENU': 'earth',
-                           'beam': 'beam'}[self.config['coord_sys_axes']]
-        da['has_imu'] = 0  # Initiate attribute
+                burst_seconds = round(1 / fs, 3)
+            da["duty_cycle_description"] = (
+                "{} second bursts collected at {} Hz, with bursts taken every {} minutes".format(
+                    burst_seconds, fs, self.config["burst_interval"] / 60
+                )
+            )
+        self.burst_start = np.zeros(self.n_samp_guess, dtype="bool")
+        da["fs"] = self.config["fs"]
+        da["coord_sys"] = {"XYZ": "inst", "ENU": "earth", "beam": "beam"}[
+            self.config["coord_sys_axes"]
+        ]
+        da["has_imu"] = 0  # Initiate attribute
         if self.debug:
-            logging.info('Init completed')
+            logging.info("Init completed")
 
     @property
-    def filesize(self,):
-        if not hasattr(self, '_filesz'):
+    def filesize(
+        self,
+    ):
+        if not hasattr(self, "_filesz"):
             pos = self.pos
             self.f.seek(0, 2)
             # Seek to the end of the file to determine the filesize.
             self._filesz = self.pos
             self.f.seek(pos, 0)  # Return to the initial position.
         return self._filesz
 
     @property
-    def pos(self,):
+    def pos(self):
         return self.f.tell()
 
-    def init_ADV(self,):
-        dat = self.data = {'data_vars': {}, 'coords': {}, 'attrs': {},
-                           'units': {}, 'long_name': {}, 'standard_name': {},
-                           'sys': {}}
-        da = dat['attrs']
-        dv = dat['data_vars']
-        da['inst_make'] = 'Nortek'
-        da['inst_model'] = 'Vector'
-        da['inst_type'] = 'ADV'
-        da['rotate_vars'] = ['vel']
-        dv['beam2inst_orientmat'] = self.config.pop('beam2inst_orientmat')
-        self.config['fs'] = 512 / self.config['awac']['avg_interval']
-        da.update(self.config['usr'])
-        da.update(self.config['adv'])
-        da.update(self.config['head'])
-        da.update(self.config['hdw'])
+    def init_ADV(self):
+        dat = self.data = {
+            "data_vars": {},
+            "coords": {},
+            "attrs": {},
+            "units": {},
+            "long_name": {},
+            "standard_name": {},
+            "sys": {},
+        }
+        da = dat["attrs"]
+        dv = dat["data_vars"]
+        da["inst_make"] = "Nortek"
+        da["inst_model"] = "Vector"
+        da["inst_type"] = "ADV"
+        da["rotate_vars"] = ["vel"]
+        dv["beam2inst_orientmat"] = self.config.pop("beam2inst_orientmat")
+        self.config["fs"] = 512 / self.config["awac"]["avg_interval"]
+        da.update(self.config["usr"])
+        da.update(self.config["adv"])
+        da.update(self.config["head"])
+        da.update(self.config["hdw"])
 
         # No apparent way to determine how many samples are in a file
-        dlta = self.code_spacing('0x11')
+        dlta = self.code_spacing("0x11")
         self.n_samp_guess = int(self.filesize / dlta + 1)
-        self.n_samp_guess *= int(self.config['fs'])
+        self.n_samp_guess *= int(self.config["fs"])
 
-    def init_AWAC(self,):
-        dat = self.data = {'data_vars': {}, 'coords': {}, 'attrs': {},
-                           'units': {}, 'long_name': {}, 'standard_name': {},
-                           'sys': {}}
-        da = dat['attrs']
-        dv = dat['data_vars']
-        da['inst_make'] = 'Nortek'
-        da['inst_model'] = 'AWAC'
-        da['inst_type'] = 'ADCP'
-        dv['beam2inst_orientmat'] = self.config.pop('beam2inst_orientmat')
-        da['rotate_vars'] = ['vel']
-        self.config['fs'] = 1. / self.config['awac']['avg_interval']
-        da.update(self.config['usr'])
-        da.update(self.config['awac'])
-        da.update(self.config['head'])
-        da.update(self.config['hdw'])
+    def init_AWAC(self):
+        dat = self.data = {
+            "data_vars": {},
+            "coords": {},
+            "attrs": {},
+            "units": {},
+            "long_name": {},
+            "standard_name": {},
+            "sys": {},
+        }
+        da = dat["attrs"]
+        dv = dat["data_vars"]
+        da["inst_make"] = "Nortek"
+        da["inst_model"] = "AWAC"
+        da["inst_type"] = "ADCP"
+        dv["beam2inst_orientmat"] = self.config.pop("beam2inst_orientmat")
+        da["rotate_vars"] = ["vel"]
+        self.config["fs"] = 1.0 / self.config["awac"]["avg_interval"]
+        da.update(self.config["usr"])
+        da.update(self.config["awac"])
+        da.update(self.config["head"])
+        da.update(self.config["hdw"])
 
-        space = self.code_spacing('0x20')
+        space = self.code_spacing("0x20")
         if space == 0:
             # code spacing is zero if there's only 1 profile
             self.n_samp_guess = 1
         else:
             self.n_samp_guess = int(self.filesize / space + 1)
 
     def read(self, nbyte):
         byts = self.f.read(nbyte)
         if not (len(byts) == nbyte):
-            raise EOFError('Reached the end of the file')
+            raise EOFError("Reached the end of the file")
         return byts
 
     def findnext(self, do_cs=True):
         """Find the next data block by checking the checksum and the
         sync byte(0xa5)
         """
-        sum = np.uint16(int('0xb58c', 0))  # Initialize the sum
+        sum = np.uint16(int("0xb58c", 0))  # Initialize the sum
         cs = 0
         func = _bitshift8
         func2 = np.uint8
-        if self.endian == '<':
+        if self.endian == "<":
             func = np.uint8
             func2 = _bitshift8
         while True:
-            val = unpack(self.endian + 'H', self.read(2))[0]
+            val = unpack(self.endian + "H", self.read(2))[0]
             if func(val) == 165 and (not do_cs or cs == np.uint16(sum)):
                 self.f.seek(-2, 1)
                 return hex(func2(val))
             sum += cs
             cs = val
 
-    def read_id(self,):
-        """Read the next 'ID' from the file.
-        """
+    def read_id(self):
+        """Read the next 'ID' from the file."""
         self._thisid_bytes = bts = self.read(2)
-        tmp = unpack(self.endian + 'BB', bts)
+        tmp = unpack(self.endian + "BB", bts)
         if self.debug:
-            logging.info('Position: {}, codes: {}'.format(self.f.tell(), tmp))
+            logging.info("Position: {}, codes: {}".format(self.f.tell(), tmp))
         if tmp[0] != 165:  # This catches a corrupted data block.
             if self.debug:
-                logging.warning("Corrupted data block sync code (%d, %d) found "
-                                "in ping %d. Searching for next valid code..." %
-                                (tmp[0], tmp[1], self.c))
+                logging.warning(
+                    "Corrupted data block sync code (%d, %d) found "
+                    "in ping %d. Searching for next valid code..."
+                    % (tmp[0], tmp[1], self.c)
+                )
             val = int(self.findnext(do_cs=False), 0)
             self.f.seek(2, 1)
             if self.debug:
-                logging.debug(
-                    ' ...FOUND {} at position: {}.'.format(val, self.pos))
+                logging.debug(" ...FOUND {} at position: {}.".format(val, self.pos))
             return val
         return tmp[1]
 
-    def readnext(self,):
-        id = '0x%02x' % self.read_id()
+    def readnext(self):
+        id = "0x%02x" % self.read_id()
         if id in self.fun_map:
             func_name = self.fun_map[id]
             out = getattr(self, func_name)()  # Should return None
             self._lastread = [func_name[5:]] + self._lastread[:-1]
             return out
         else:
-            logging.warning('Unrecognized identifier: ' + id)
+            logging.warning("Unrecognized identifier: " + id)
             self.f.seek(-2, 1)
             return 10
 
     def readfile(self, nlines=None):
-        print('Reading file %s ...' % self.fname)
+        print("Reading file %s ..." % self.fname)
         retval = None
         try:
             while not retval:
                 if self.c == nlines:
                     break
                 retval = self.readnext()
                 if retval == 10:
                     self.findnext()
                     retval = None
                 if self._npings is not None and self.c >= self._npings:
-                    if 'microstrain' in self._dtypes:
+                    if "microstrain" in self._dtypes:
                         try:
                             self.readnext()
                         except:
                             pass
                     break
         except EOFError:
             if self.debug:
-                logging.info(' end of file at {} bytes.'.format(self.pos))
+                logging.info(" end of file at {} bytes.".format(self.pos))
         else:
             if self.debug:
-                logging.info(' stopped at {} bytes.'.format(self.pos))
+                logging.info(" stopped at {} bytes.".format(self.pos))
         self.c -= 1
         _crop_data(self.data, slice(0, self.c), self.n_samp_guess)
 
     def findnextid(self, id):
         if id.__class__ is str:
             id = int(id, 0)
         nowid = None
         while nowid != id:
             nowid = self.read_id()
             if nowid == 16:
                 shift = 22
             else:
-                sz = 2 * unpack(self.endian + 'H', self.read(2))[0]
+                sz = 2 * unpack(self.endian + "H", self.read(2))[0]
                 shift = sz - 4
             self.f.seek(shift, 1)
         return self.pos
 
     def code_spacing(self, searchcode, iternum=50):
         """
         Find the spacing, in bytes, between a specific hardware code.
@@ -430,257 +466,293 @@
         p0 = self.findnextid(searchcode)
         for i in range(iternum):
             try:
                 self.findnextid(searchcode)
             except EOFError:
                 break
         if self.debug:
-            logging.info('p0={}, pos={}, i={}'.format(p0, self.pos, i))
+            logging.info("p0={}, pos={}, i={}".format(p0, self.pos, i))
         # Compute the average of the data size:
         return (self.pos - p0) / (i + 1)
 
     def checksum(self, byts):
-        """Perform a checksum on `byts` and read the checksum value.
-        """
+        """Perform a checksum on `byts` and read the checksum value."""
         if self.do_checksum:
-            if not np.sum(unpack(self.endian + str(int(1 + len(byts) / 2)) + 'H',
-                                 self._thisid_bytes + byts)) + \
-                    46476 - unpack(self.endian + 'H', self.read(2)):
-
+            if (
+                not np.sum(
+                    unpack(
+                        self.endian + str(int(1 + len(byts) / 2)) + "H",
+                        self._thisid_bytes + byts,
+                    )
+                )
+                + 46476
+                - unpack(self.endian + "H", self.read(2))
+            ):
                 raise Exception("CheckSum Failed at {}".format(self.pos))
         else:
             self.f.seek(2, 1)
 
-    def read_user_cfg(self,):
+    def read_user_cfg(self):
         # ID: '0x00 = 00
         if self.debug:
-            logging.info('Reading user configuration (0x00) ping #{} @ {}...'
-                         .format(self.c, self.pos))
+            logging.info(
+                "Reading user configuration (0x00) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
         cfg_u = self.config
         byts = self.read(508)
         # the first two bytes are the size.
-        tmp = unpack(self.endian +
-                     '2x18H6s4HI9H90H80s48xH50x6H4xH2x2H2xH30x8H',
-                     byts)
-        cfg_u['usr'] = {}
-        cfg_u['adv'] = {}
-        cfg_u['awac'] = {}
-
-        cfg_u['transmit_pulse_length_m'] = tmp[0]  # counts
-        cfg_u['blank_dist'] = tmp[1]  # overridden below
-        cfg_u['receive_length_m'] = tmp[2]  # counts
-        cfg_u['time_between_pings'] = tmp[3]  # counts
-        cfg_u['time_between_bursts'] = tmp[4]  # counts
-        cfg_u['adv']['n_pings_per_burst'] = tmp[5]
-        cfg_u['awac']['avg_interval'] = tmp[6]
-        cfg_u['usr']['n_beams'] = tmp[7]
+        tmp = unpack(self.endian + "2x18H6s4HI9H90H80s48xH50x6H4xH2x2H2xH30x8H", byts)
+        cfg_u["usr"] = {}
+        cfg_u["adv"] = {}
+        cfg_u["awac"] = {}
+
+        cfg_u["transmit_pulse_length_m"] = tmp[0]  # counts
+        cfg_u["blank_dist"] = tmp[1]  # overridden below
+        cfg_u["receive_length_m"] = tmp[2]  # counts
+        cfg_u["time_between_pings"] = tmp[3]  # counts
+        cfg_u["time_between_bursts"] = tmp[4]  # counts
+        cfg_u["adv"]["n_pings_per_burst"] = tmp[5]
+        cfg_u["awac"]["avg_interval"] = tmp[6]
+        cfg_u["usr"]["n_beams"] = tmp[7]
         TimCtrlReg = _int2binarray(tmp[8], 16).astype(int)
         # From the nortek system integrator manual
         # (note: bit numbering is zero-based)
-        cfg_u['usr']['profile_mode'] = [
-            'single', 'continuous'][TimCtrlReg[1]]
-        cfg_u['usr']['burst_mode'] = str(bool(~TimCtrlReg[2]))
-        cfg_u['usr']['power_level'] = TimCtrlReg[5] + 2 * TimCtrlReg[6] + 1
-        cfg_u['usr']['sync_out_pos'] = ['middle', 'end', ][TimCtrlReg[7]]
-        cfg_u['usr']['sample_on_sync'] = str(bool(TimCtrlReg[8]))
-        cfg_u['usr']['start_on_sync'] = str(bool(TimCtrlReg[9]))
-        cfg_u['PwrCtrlReg'] = _int2binarray(tmp[9], 16)
-        cfg_u['A1'] = tmp[10]
-        cfg_u['B0'] = tmp[11]
-        cfg_u['B1'] = tmp[12]
-        cfg_u['usr']['compass_update_rate'] = tmp[13]
-        cfg_u['coord_sys_axes'] = ['ENU', 'XYZ', 'beam'][tmp[14]]
-        cfg_u['usr']['n_bins'] = tmp[15]
-        cfg_u['bin_length'] = tmp[16]
-        cfg_u['burst_interval'] = tmp[17]
-        cfg_u['usr']['deployment_name'] = tmp[18].partition(b'\x00')[
-            0].decode('utf-8')
-        cfg_u['usr']['wrap_mode'] = str(bool(tmp[19]))
-        cfg_u['deployment_time'] = np.array(tmp[20:23])
-        cfg_u['diagnotics_interval'] = tmp[23]
+        cfg_u["usr"]["profile_mode"] = ["single", "continuous"][TimCtrlReg[1]]
+        cfg_u["usr"]["burst_mode"] = str(bool(~TimCtrlReg[2]))
+        cfg_u["usr"]["power_level"] = TimCtrlReg[5] + 2 * TimCtrlReg[6] + 1
+        cfg_u["usr"]["sync_out_pos"] = [
+            "middle",
+            "end",
+        ][TimCtrlReg[7]]
+        cfg_u["usr"]["sample_on_sync"] = str(bool(TimCtrlReg[8]))
+        cfg_u["usr"]["start_on_sync"] = str(bool(TimCtrlReg[9]))
+        cfg_u["PwrCtrlReg"] = _int2binarray(tmp[9], 16)
+        cfg_u["A1"] = tmp[10]
+        cfg_u["B0"] = tmp[11]
+        cfg_u["B1"] = tmp[12]
+        cfg_u["usr"]["compass_update_rate"] = tmp[13]
+        cfg_u["coord_sys_axes"] = ["ENU", "XYZ", "beam"][tmp[14]]
+        cfg_u["usr"]["n_bins"] = tmp[15]
+        cfg_u["bin_length"] = tmp[16]
+        cfg_u["burst_interval"] = tmp[17]
+        cfg_u["usr"]["deployment_name"] = tmp[18].partition(b"\x00")[0].decode("utf-8")
+        cfg_u["usr"]["wrap_mode"] = str(bool(tmp[19]))
+        cfg_u["deployment_time"] = np.array(tmp[20:23])
+        cfg_u["diagnotics_interval"] = tmp[23]
         Mode0 = _int2binarray(tmp[24], 16)
-        cfg_u['user_soundspeed_adj_factor'] = tmp[25]
-        cfg_u['n_samples_diag'] = tmp[26]
-        cfg_u['n_beams_cells_diag'] = tmp[27]
-        cfg_u['n_pings_diag_wave'] = tmp[28]
+        cfg_u["user_soundspeed_adj_factor"] = tmp[25]
+        cfg_u["n_samples_diag"] = tmp[26]
+        cfg_u["n_beams_cells_diag"] = tmp[27]
+        cfg_u["n_pings_diag_wave"] = tmp[28]
         ModeTest = _int2binarray(tmp[29], 16)
-        cfg_u['usr']['analog_in'] = tmp[30]
+        cfg_u["usr"]["analog_in"] = tmp[30]
         sfw_ver = str(tmp[31])
-        cfg_u['usr']['software_version'] = sfw_ver[0] + \
-            '.'+sfw_ver[1:3]+'.'+sfw_ver[3:]
-        cfg_u['usr']['salinity'] = tmp[32]/10
-        cfg_u['VelAdjTable'] = np.array(tmp[33:123])
-        cfg_u['usr']['comments'] = tmp[123].partition(b'\x00')[
-            0].decode('utf-8')
-        cfg_u['awac']['wave_processing_method'] = [
-            'PUV', 'SUV', 'MLM', 'MLMST', 'None'][tmp[124]]
+        cfg_u["usr"]["software_version"] = (
+            sfw_ver[0] + "." + sfw_ver[1:3] + "." + sfw_ver[3:]
+        )
+        cfg_u["usr"]["salinity"] = tmp[32] / 10
+        cfg_u["VelAdjTable"] = np.array(tmp[33:123])
+        cfg_u["usr"]["comments"] = tmp[123].partition(b"\x00")[0].decode("utf-8")
+        cfg_u["awac"]["wave_processing_method"] = [
+            "PUV",
+            "SUV",
+            "MLM",
+            "MLMST",
+            "None",
+        ][tmp[124]]
         Mode1 = _int2binarray(tmp[125], 16)
-        cfg_u['awac']['prc_dyn_wave_cell_pos'] = int(tmp[126]/32767 * 100)
-        cfg_u['wave_transmit_pulse'] = tmp[127]
-        cfg_u['wave_blank_dist'] = tmp[128]
-        cfg_u['awac']['wave_cell_size'] = tmp[129]
-        cfg_u['awac']['n_samples_wave'] = tmp[130]
-        cfg_u['n_burst'] = tmp[131]
-        cfg_u['analog_out_scale'] = tmp[132]
-        cfg_u['corr_thresh'] = tmp[133]
-        cfg_u['transmit_pulse_lag2'] = tmp[134]  # counts
-        cfg_u['QualConst'] = np.array(tmp[135:143])
+        cfg_u["awac"]["prc_dyn_wave_cell_pos"] = int(tmp[126] / 32767 * 100)
+        cfg_u["wave_transmit_pulse"] = tmp[127]
+        cfg_u["wave_blank_dist"] = tmp[128]
+        cfg_u["awac"]["wave_cell_size"] = tmp[129]
+        cfg_u["awac"]["n_samples_wave"] = tmp[130]
+        cfg_u["n_burst"] = tmp[131]
+        cfg_u["analog_out_scale"] = tmp[132]
+        cfg_u["corr_thresh"] = tmp[133]
+        cfg_u["transmit_pulse_lag2"] = tmp[134]  # counts
+        cfg_u["QualConst"] = np.array(tmp[135:143])
         self.checksum(byts)
-        cfg_u['usr']['user_specified_sound_speed'] = str(Mode0[0])
-        cfg_u['awac']['wave_mode'] = ['Disabled', 'Enabled'][int(Mode0[1])]
-        cfg_u['usr']['analog_output'] = str(Mode0[2])
-        cfg_u['usr']['output_format'] = ['Vector', 'ADV'][int(Mode0[3])]  # noqa
-        cfg_u['vel_scale_mm'] = [1, 0.1][int(Mode0[4])]
-        cfg_u['usr']['serial_output'] = str(Mode0[5])
-        cfg_u['reserved_EasyQ'] = str(Mode0[6])
-        cfg_u['usr']['power_output_analog'] = str(Mode0[8])
-        cfg_u['mode_test_use_DSP'] = str(ModeTest[0])
-        cfg_u['mode_test_filter_output'] = ['total', 'correction_only'][int(ModeTest[1])]  # noqa
-        cfg_u['awac']['wave_fs'] = ['1 Hz', '2 Hz'][int(Mode1[0])]
-        cfg_u['awac']['wave_cell_position'] = ['fixed', 'dynamic'][int(Mode1[1])]  # noqa
-        cfg_u['awac']['type_wave_cell_pos'] = ['pct_of_mean_pressure', 'pct_of_min_re'][int(Mode1[2])]  # noqa
+        cfg_u["usr"]["user_specified_sound_speed"] = str(Mode0[0])
+        cfg_u["awac"]["wave_mode"] = ["Disabled", "Enabled"][int(Mode0[1])]
+        cfg_u["usr"]["analog_output"] = str(Mode0[2])
+        cfg_u["usr"]["output_format"] = ["Vector", "ADV"][int(Mode0[3])]  # noqa
+        cfg_u["vel_scale_mm"] = [1, 0.1][int(Mode0[4])]
+        cfg_u["usr"]["serial_output"] = str(Mode0[5])
+        cfg_u["reserved_EasyQ"] = str(Mode0[6])
+        cfg_u["usr"]["power_output_analog"] = str(Mode0[8])
+        cfg_u["mode_test_use_DSP"] = str(ModeTest[0])
+        cfg_u["mode_test_filter_output"] = ["total", "correction_only"][
+            int(ModeTest[1])
+        ]  # noqa
+        cfg_u["awac"]["wave_fs"] = ["1 Hz", "2 Hz"][int(Mode1[0])]
+        cfg_u["awac"]["wave_cell_position"] = ["fixed", "dynamic"][
+            int(Mode1[1])
+        ]  # noqa
+        cfg_u["awac"]["type_wave_cell_pos"] = ["pct_of_mean_pressure", "pct_of_min_re"][
+            int(Mode1[2])
+        ]  # noqa
 
-    def read_head_cfg(self,):
+    def read_head_cfg(self):
         # ID: '0x04 = 04
         if self.debug:
-            logging.info('Reading head configuration (0x04) ping #{} @ {}...'
-                         .format(self.c, self.pos))
+            logging.info(
+                "Reading head configuration (0x04) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
         cfg = self.config
-        cfg['head'] = {}
+        cfg["head"] = {}
         byts = self.read(220)
-        tmp = unpack(self.endian + '2x3H12s176s22sH', byts)
+        tmp = unpack(self.endian + "2x3H12s176s22sH", byts)
         head_config = _int2binarray(tmp[0], 16).astype(int)
-        cfg['head']['pressure_sensor'] = ['no', 'yes'][head_config[0]]
-        cfg['head']['compass'] = ['no', 'yes'][head_config[1]]
-        cfg['head']['tilt_sensor'] = ['no', 'yes'][head_config[2]]
-        cfg['head']['carrier_freq_kHz'] = tmp[1]
-        cfg['beam2inst_orientmat'] = np.array(
-            unpack(self.endian + '9h', tmp[4][8:26])).reshape(3, 3) / 4096.
+        cfg["head"]["pressure_sensor"] = ["no", "yes"][head_config[0]]
+        cfg["head"]["compass"] = ["no", "yes"][head_config[1]]
+        cfg["head"]["tilt_sensor"] = ["no", "yes"][head_config[2]]
+        cfg["head"]["carrier_freq_kHz"] = tmp[1]
+        cfg["beam2inst_orientmat"] = (
+            np.array(unpack(self.endian + "9h", tmp[4][8:26])).reshape(3, 3) / 4096.0
+        )
         self.checksum(byts)
 
-    def read_hw_cfg(self,):
+    def read_hw_cfg(self):
         # ID 0x05 = 05
         if self.debug:
-            logging.info('Reading hardware configuration (0x05) ping #{} @ {}...'
-                         .format(self.c, self.pos))
+            logging.info(
+                "Reading hardware configuration (0x05) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
         cfg_hw = self.config
-        cfg_hw['hdw'] = {}
+        cfg_hw["hdw"] = {}
         byts = self.read(44)
-        tmp = unpack(self.endian + '2x14s6H12x4s', byts)
-        cfg_hw['hdw']['serial_number'] = tmp[0][:8].decode('utf-8')
-        cfg_hw['ProLogID'] = unpack('B', tmp[0][8:9])[0]
-        cfg_hw['hdw']['ProLogFWver'] = tmp[0][10:].decode('utf-8')
-        cfg_hw['board_config'] = tmp[1]
-        cfg_hw['board_freq'] = tmp[2]
-        cfg_hw['hdw']['PIC_version'] = tmp[3]
-        cfg_hw['hdw']['hardware_rev'] = tmp[4]
-        cfg_hw['hdw']['recorder_size_bytes'] = tmp[5] * 65536
+        tmp = unpack(self.endian + "2x14s6H12x4s", byts)
+        cfg_hw["hdw"]["serial_number"] = tmp[0][:8].decode("utf-8")
+        cfg_hw["ProLogID"] = unpack("B", tmp[0][8:9])[0]
+        cfg_hw["hdw"]["ProLogFWver"] = tmp[0][10:].decode("utf-8")
+        cfg_hw["board_config"] = tmp[1]
+        cfg_hw["board_freq"] = tmp[2]
+        cfg_hw["hdw"]["PIC_version"] = tmp[3]
+        cfg_hw["hdw"]["hardware_rev"] = tmp[4]
+        cfg_hw["hdw"]["recorder_size_bytes"] = tmp[5] * 65536
         status = _int2binarray(tmp[6], 16).astype(int)
-        cfg_hw['hdw']['vel_range'] = ['normal', 'high'][status[0]]
-        cfg_hw['hdw']['firmware_version'] = tmp[7].decode('utf-8')
+        cfg_hw["hdw"]["vel_range"] = ["normal", "high"][status[0]]
+        cfg_hw["hdw"]["firmware_version"] = tmp[7].decode("utf-8")
         self.checksum(byts)
 
     def rd_time(self, strng):
-        """Read the time from the first 6bytes of the input string.
-        """
-        min, sec, day, hour, year, month = unpack('BBBBBB', strng[:6])
-        return time.date2epoch(datetime(time._fullyear(_bcd2char(year)),
-                                        _bcd2char(month),
-                                        _bcd2char(day),
-                                        _bcd2char(hour),
-                                        _bcd2char(min),
-                                        _bcd2char(sec)))[0]
+        """Read the time from the first 6bytes of the input string."""
+        min, sec, day, hour, year, month = unpack("BBBBBB", strng[:6])
+        return time.date2epoch(
+            datetime(
+                time._fullyear(_bcd2char(year)),
+                _bcd2char(month),
+                _bcd2char(day),
+                _bcd2char(hour),
+                _bcd2char(min),
+                _bcd2char(sec),
+            )
+        )[0]
 
     def _init_data(self, vardict):
         """Initialize the data object according to vardict.
 
         Parameters
         ----------
         vardict : (dict of :class:`<VarAttrs>`)
           The variable definitions in the :class:`<VarAttrs>` specify
           how to initialize each data variable.
 
         """
-        shape_args = {'n': self.n_samp_guess}
+        shape_args = {"n": self.n_samp_guess}
         try:
-            shape_args['nbins'] = self.config['usr']['n_bins']
+            shape_args["nbins"] = self.config["usr"]["n_bins"]
         except KeyError:
             pass
         for nm, va in list(vardict.items()):
             if va.group is None:
                 # These have to stay separated.
                 if nm not in self.data:
                     self.data[nm] = va._empty_array(**shape_args)
             else:
                 if nm not in self.data[va.group]:
                     self.data[va.group][nm] = va._empty_array(**shape_args)
-                    self.data['units'][nm] = va.units
-                    self.data['long_name'][nm] = va.long_name
+                    self.data["units"][nm] = va.units
+                    self.data["long_name"][nm] = va.long_name
                     if va.standard_name:
-                        self.data['standard_name'][nm] = va.standard_name
+                        self.data["standard_name"][nm] = va.standard_name
 
-    def read_vec_data(self,):
+    def read_vec_data(self):
         # ID: 0x10 = 16
         c = self.c
         dat = self.data
         if self.debug:
-            logging.info('Reading vector velocity data (0x10) ping #{} @ {}...'
-                         .format(self.c, self.pos))
+            logging.info(
+                "Reading vector velocity data (0x10) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
 
-        if 'vel' not in dat['data_vars']:
+        if "vel" not in dat["data_vars"]:
             self._init_data(nortek_defs.vec_data)
-            self._dtypes += ['vec_data']
+            self._dtypes += ["vec_data"]
 
         byts = self.read(20)
-        ds = dat['sys']
-        dv = dat['data_vars']
-        (ds['AnaIn2LSB'][c],
-         ds['Count'][c],
-         dv['PressureMSB'][c],
-         ds['AnaIn2MSB'][c],
-         dv['PressureLSW'][c],
-         ds['AnaIn1'][c],
-         dv['vel'][0, c],
-         dv['vel'][1, c],
-         dv['vel'][2, c],
-         dv['amp'][0, c],
-         dv['amp'][1, c],
-         dv['amp'][2, c],
-         dv['corr'][0, c],
-         dv['corr'][1, c],
-         dv['corr'][2, c]) = unpack(self.endian + '4B2H3h6B', byts)
+        ds = dat["sys"]
+        dv = dat["data_vars"]
+        (
+            ds["AnaIn2LSB"][c],
+            ds["Count"][c],
+            dv["PressureMSB"][c],
+            ds["AnaIn2MSB"][c],
+            dv["PressureLSW"][c],
+            ds["AnaIn1"][c],
+            dv["vel"][0, c],
+            dv["vel"][1, c],
+            dv["vel"][2, c],
+            dv["amp"][0, c],
+            dv["amp"][1, c],
+            dv["amp"][2, c],
+            dv["corr"][0, c],
+            dv["corr"][1, c],
+            dv["corr"][2, c],
+        ) = unpack(self.endian + "4B2H3h6B", byts)
 
         self.checksum(byts)
         self.c += 1
 
-    def read_vec_checkdata(self,):
+    def read_vec_checkdata(self):
         # ID: 0x07 = 07
         if self.debug:
-            logging.info('Reading vector check data (0x07) ping #{} @ {}...'
-                         .format(self.c, self.pos))
+            logging.info(
+                "Reading vector check data (0x07) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
         byts0 = self.read(6)
         checknow = {}
-        tmp = unpack(self.endian + '2x2H', byts0)  # The first two are size.
-        checknow['Samples'] = tmp[0]
-        n = checknow['Samples']
-        checknow['First_samp'] = tmp[1]
-        checknow['Amp1'] = tbx._nans(n, dtype=np.uint8) + 8
-        checknow['Amp2'] = tbx._nans(n, dtype=np.uint8) + 8
-        checknow['Amp3'] = tbx._nans(n, dtype=np.uint8) + 8
+        tmp = unpack(self.endian + "2x2H", byts0)  # The first two are size.
+        checknow["Samples"] = tmp[0]
+        n = checknow["Samples"]
+        checknow["First_samp"] = tmp[1]
+        checknow["Amp1"] = tbx._nans(n, dtype=np.uint8) + 8
+        checknow["Amp2"] = tbx._nans(n, dtype=np.uint8) + 8
+        checknow["Amp3"] = tbx._nans(n, dtype=np.uint8) + 8
         byts1 = self.read(3 * n)
-        tmp = unpack(self.endian + (3 * n * 'B'), byts1)
-        for idx, nm in enumerate(['Amp1', 'Amp2', 'Amp3']):
-            checknow[nm] = np.array(tmp[idx * n:(idx + 1) * n], dtype=np.uint8)
+        tmp = unpack(self.endian + (3 * n * "B"), byts1)
+        for idx, nm in enumerate(["Amp1", "Amp2", "Amp3"]):
+            checknow[nm] = np.array(tmp[idx * n : (idx + 1) * n], dtype=np.uint8)
         self.checksum(byts0 + byts1)
-        if 'checkdata' not in self.config:
-            self.config['checkdata'] = checknow
+        if "checkdata" not in self.config:
+            self.config["checkdata"] = checknow
         else:
-            if not isinstance(self.config['checkdata'], list):
-                self.config['checkdata'] = [self.config['checkdata']]
-            self.config['checkdata'] += [checknow]
+            if not isinstance(self.config["checkdata"], list):
+                self.config["checkdata"] = [self.config["checkdata"]]
+            self.config["checkdata"] += [checknow]
 
     def _sci_data(self, vardict):
         """
         Convert the data to scientific units accordint to vardict.
 
         Parameters
         ----------
@@ -696,365 +768,462 @@
                 dat = self.data[vd.group]
             retval = vd.sci_func(dat[nm])
             # This checks whether a new data object was created:
             # sci_func returns None if it modifies the existing data.
             if retval is not None:
                 dat[nm] = retval
 
-    def sci_vec_data(self,):
+    def sci_vec_data(self):
         self._sci_data(nortek_defs.vec_data)
         dat = self.data
 
-        dat['data_vars']['pressure'] = (
-            dat['data_vars']['PressureMSB'].astype('float32') * 65536 +
-            dat['data_vars']['PressureLSW'].astype('float32')) / 1000.
-        dat['units']['pressure'] = 'dbar'
-        dat['long_name']['pressure'] = 'Pressure'
-        dat['standard_name']['pressure'] = 'sea_water_pressure'
+        dat["data_vars"]["pressure"] = (
+            dat["data_vars"]["PressureMSB"].astype("float32") * 65536
+            + dat["data_vars"]["PressureLSW"].astype("float32")
+        ) / 1000.0
+        dat["units"]["pressure"] = "dbar"
+        dat["long_name"]["pressure"] = "Pressure"
+        dat["standard_name"]["pressure"] = "sea_water_pressure"
 
-        dat['data_vars'].pop('PressureMSB')
-        dat['data_vars'].pop('PressureLSW')
+        dat["data_vars"].pop("PressureMSB")
+        dat["data_vars"].pop("PressureLSW")
 
         # Apply velocity scaling (1 or 0.1)
-        dat['data_vars']['vel'] *= self.config['vel_scale_mm']
+        dat["data_vars"]["vel"] *= self.config["vel_scale_mm"]
 
-    def read_vec_hdr(self,):
+    def read_vec_hdr(self):
         # ID: '0x12 = 18
         if self.debug:
-            logging.info('Reading vector header data (0x12) ping #{} @ {}...'
-                         .format(self.c, self.pos))
+            logging.info(
+                "Reading vector header data (0x12) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
         byts = self.read(38)
         # The first two are size, the next 6 are time.
-        tmp = unpack(self.endian + '8xH7B21x', byts)
+        tmp = unpack(self.endian + "8xH7B21x", byts)
         hdrnow = {}
-        hdrnow['time'] = self.rd_time(byts[2:8])
-        hdrnow['NRecords'] = tmp[0]
-        hdrnow['Noise1'] = tmp[1]
-        hdrnow['Noise2'] = tmp[2]
-        hdrnow['Noise3'] = tmp[3]
-        hdrnow['Spare0'] = byts[13:14].decode('utf-8')
-        hdrnow['Corr1'] = tmp[5]
-        hdrnow['Corr2'] = tmp[6]
-        hdrnow['Corr3'] = tmp[7]
-        hdrnow['Spare1'] = byts[17:].decode('utf-8')
+        hdrnow["time"] = self.rd_time(byts[2:8])
+        hdrnow["NRecords"] = tmp[0]
+        hdrnow["Noise1"] = tmp[1]
+        hdrnow["Noise2"] = tmp[2]
+        hdrnow["Noise3"] = tmp[3]
+        hdrnow["Spare0"] = byts[13:14].decode("utf-8")
+        hdrnow["Corr1"] = tmp[5]
+        hdrnow["Corr2"] = tmp[6]
+        hdrnow["Corr3"] = tmp[7]
+        hdrnow["Spare1"] = byts[17:].decode("utf-8")
         self.checksum(byts)
-        if 'data_header' not in self.config:
-            self.config['data_header'] = hdrnow
+        if "data_header" not in self.config:
+            self.config["data_header"] = hdrnow
         else:
-            if not isinstance(self.config['data_header'], list):
-                self.config['data_header'] = [self.config['data_header']]
-            self.config['data_header'] += [hdrnow]
+            if not isinstance(self.config["data_header"], list):
+                self.config["data_header"] = [self.config["data_header"]]
+            self.config["data_header"] += [hdrnow]
 
-    def read_vec_sysdata(self,):
+    def read_vec_sysdata(self):
         # ID: 0x11 = 17
         c = self.c
         if self.debug:
-            logging.info('Reading vector system data (0x11) ping #{} @ {}...'
-                         .format(self.c, self.pos))
+            logging.info(
+                "Reading vector system data (0x11) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
         dat = self.data
-        if self._lastread[:2] == ['vec_checkdata', 'vec_hdr', ]:
+        if self._lastread[:2] == [
+            "vec_checkdata",
+            "vec_hdr",
+        ]:
             self.burst_start[c] = True
-        if 'time' not in dat['coords']:
+        if "time" not in dat["coords"]:
             self._init_data(nortek_defs.vec_sysdata)
-            self._dtypes += ['vec_sysdata']
+            self._dtypes += ["vec_sysdata"]
         byts = self.read(24)
         # The first two are size (skip them).
-        dat['coords']['time'][c] = self.rd_time(byts[2:8])
-        ds = dat['sys']
-        dv = dat['data_vars']
-        (dv['batt'][c],
-         dv['c_sound'][c],
-         dv['heading'][c],
-         dv['pitch'][c],
-         dv['roll'][c],
-         dv['temp'][c],
-         dv['error'][c],
-         dv['status'][c],
-         ds['AnaIn'][c]) = unpack(self.endian + '2H3hH2BH', byts[8:])
+        dat["coords"]["time"][c] = self.rd_time(byts[2:8])
+        ds = dat["sys"]
+        dv = dat["data_vars"]
+        (
+            dv["batt"][c],
+            dv["c_sound"][c],
+            dv["heading"][c],
+            dv["pitch"][c],
+            dv["roll"][c],
+            dv["temp"][c],
+            dv["error"][c],
+            dv["status"][c],
+            ds["AnaIn"][c],
+        ) = unpack(self.endian + "3H3h2BH", byts[8:])
         self.checksum(byts)
 
-    def sci_vec_sysdata(self,):
+    def sci_vec_sysdata(self):
         """Translate the data in the vec_sysdata structure into
         scientific units.
         """
         dat = self.data
-        fs = dat['attrs']['fs']
+        fs = dat["attrs"]["fs"]
         self._sci_data(nortek_defs.vec_sysdata)
-        t = dat['coords']['time']
-        dv = dat['data_vars']
-        dat['sys']['_sysi'] = ~np.isnan(t)
+        t = dat["coords"]["time"]
+        dv = dat["data_vars"]
+        dat["sys"]["_sysi"] = ~np.isnan(t)
         # These are the indices in the sysdata variables
         # that are not interpolated.
-        nburst = self.config['n_burst']
-        dv['orientation_down'] = tbx._nans(len(t), dtype='bool')
+        nburst = self.config["n_burst"]
+        dv["orientation_down"] = tbx._nans(len(t), dtype="bool")
         if nburst == 0:
             num_bursts = 1
             nburst = len(t)
         else:
             num_bursts = int(len(t) // nburst + 1)
         for nb in range(num_bursts):
             iburst = slice(nb * nburst, (nb + 1) * nburst)
-            sysi = dat['sys']['_sysi'][iburst]
+            sysi = dat["sys"]["_sysi"][iburst]
             if len(sysi) == 0:
                 break
             # Skip the first entry for the interpolation process
             inds = np.nonzero(sysi)[0][1:]
             arng = np.arange(len(t[iburst]), dtype=np.float64)
             if len(inds) >= 2:
                 p = np.poly1d(np.polyfit(inds, t[iburst][inds], 1))
                 t[iburst] = p(arng)
             elif len(inds) == 1:
-                t[iburst] = ((arng - inds[0]) / (fs * 3600 * 24) +
-                             t[iburst][inds[0]])
+                t[iburst] = (arng - inds[0]) / (fs * 3600 * 24) + t[iburst][inds[0]]
             else:
-                t[iburst] = (t[iburst][0] + arng / (fs * 24 * 3600))
+                t[iburst] = t[iburst][0] + arng / (fs * 24 * 3600)
 
-            tmpd = tbx._nans_like(dv['heading'][iburst])
+            tmpd = tbx._nans_like(dv["heading"][iburst])
             # The first status bit should be the orientation.
-            tmpd[sysi] = dv['status'][iburst][sysi] & 1
+            tmpd[sysi] = dv["status"][iburst][sysi] & 1
             tbx.fillgaps(tmpd, extrapFlg=True)
             tmpd = np.nan_to_num(tmpd, nan=0)  # nans in pitch roll heading
             slope = np.diff(tmpd)
             tmpd[1:][slope < 0] = 1
             tmpd[:-1][slope > 0] = 0
-            dv['orientation_down'][iburst] = tmpd.astype('bool')
-        tbx.interpgaps(dv['batt'], t)
-        tbx.interpgaps(dv['c_sound'], t)
-        tbx.interpgaps(dv['heading'], t)
-        tbx.interpgaps(dv['pitch'], t)
-        tbx.interpgaps(dv['roll'], t)
-        tbx.interpgaps(dv['temp'], t)
+            dv["orientation_down"][iburst] = tmpd.astype("bool")
+        tbx.interpgaps(dv["batt"], t)
+        tbx.interpgaps(dv["c_sound"], t)
+        tbx.interpgaps(dv["heading"], t)
+        tbx.interpgaps(dv["pitch"], t)
+        tbx.interpgaps(dv["roll"], t)
+        tbx.interpgaps(dv["temp"], t)
+
+    def read_microstrain(self):
+        """Read ADV microstrain sensor (IMU) data"""
 
-    def read_microstrain(self,):
-        """Read ADV microstrain sensor (IMU) data
-        """
         def update_defs(dat, mag=False, orientmat=False):
-            imu_data = {'accel': ['m s-2', 'Acceleration'],
-                        'angrt': ['rad s-1', 'Angular Velocity'],
-                        'mag': ['gauss', 'Compass'],
-                        'orientmat': ['1', 'Orientation Matrix']}
+            imu_data = {
+                "accel": ["m s-2", "Acceleration"],
+                "angrt": ["rad s-1", "Angular Velocity"],
+                "mag": ["gauss", "Compass"],
+                "orientmat": ["1", "Orientation Matrix"],
+            }
             for ky in imu_data:
-                dat['units'].update({ky: imu_data[ky][0]})
-                dat['long_name'].update({ky: imu_data[ky][1]})
+                dat["units"].update({ky: imu_data[ky][0]})
+                dat["long_name"].update({ky: imu_data[ky][1]})
             if not mag:
-                dat['units'].pop('mag')
-                dat['long_name'].pop('mag')
+                dat["units"].pop("mag")
+                dat["long_name"].pop("mag")
             if not orientmat:
-                dat['units'].pop('orientmat')
-                dat['long_name'].pop('orientmat')
+                dat["units"].pop("orientmat")
+                dat["long_name"].pop("orientmat")
 
         # 0x71 = 113
         if self.c == 0:
-            logging.warning('First "microstrain data" block '
-                            'is before first "vector system data" block.')
+            logging.warning(
+                'First "microstrain data" block '
+                'is before first "vector system data" block.'
+            )
         else:
             self.c -= 1
         if self.debug:
-            logging.info('Reading vector microstrain data (0x71) ping #{} @ {}...'
-                         .format(self.c, self.pos))
+            logging.info(
+                "Reading vector microstrain data (0x71) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
         byts0 = self.read(4)
         # The first 2 are the size, 3rd is count, 4th is the id.
-        ahrsid = unpack(self.endian + '3xB', byts0)[0]
-        if hasattr(self, '_ahrsid') and self._ahrsid != ahrsid:
-            logging.warning('AHRS_ID changes mid-file!')
+        ahrsid = unpack(self.endian + "3xB", byts0)[0]
+        if hasattr(self, "_ahrsid") and self._ahrsid != ahrsid:
+            logging.warning("AHRS_ID changes mid-file!")
 
         if ahrsid in [195, 204, 210, 211]:
             self._ahrsid = ahrsid
 
         c = self.c
         dat = self.data
-        dv = dat['data_vars']
-        da = dat['attrs']
-        da['has_imu'] = 1  # logical
-        if 'accel' not in dv:
-            self._dtypes += ['microstrain']
+        dv = dat["data_vars"]
+        da = dat["attrs"]
+        da["has_imu"] = 1  # logical
+        if "accel" not in dv:
+            self._dtypes += ["microstrain"]
             if ahrsid == 195:
-                self._orient_dnames = ['accel', 'angrt', 'orientmat']
-                dv['accel'] = tbx._nans((3, self.n_samp_guess),
-                                        dtype=np.float32)
-                dv['angrt'] = tbx._nans((3, self.n_samp_guess),
-                                        dtype=np.float32)
-                dv['orientmat'] = tbx._nans((3, 3, self.n_samp_guess),
-                                            dtype=np.float32)
-                rv = ['accel', 'angrt']
-                if not all(x in da['rotate_vars'] for x in rv):
-                    da['rotate_vars'].extend(rv)
+                self._orient_dnames = ["accel", "angrt", "orientmat"]
+                dv["accel"] = tbx._nans((3, self.n_samp_guess), dtype=np.float32)
+                dv["angrt"] = tbx._nans((3, self.n_samp_guess), dtype=np.float32)
+                dv["orientmat"] = tbx._nans((3, 3, self.n_samp_guess), dtype=np.float32)
+                rv = ["accel", "angrt"]
+                if not all(x in da["rotate_vars"] for x in rv):
+                    da["rotate_vars"].extend(rv)
                 update_defs(dat, mag=False, orientmat=True)
 
             if ahrsid in [204, 210]:
-                self._orient_dnames = ['accel', 'angrt', 'mag', 'orientmat']
-                dv['accel'] = tbx._nans((3, self.n_samp_guess),
-                                        dtype=np.float32)
-                dv['angrt'] = tbx._nans((3, self.n_samp_guess),
-                                        dtype=np.float32)
-                dv['mag'] = tbx._nans((3, self.n_samp_guess),
-                                      dtype=np.float32)
-                rv = ['accel', 'angrt', 'mag']
-                if not all(x in da['rotate_vars'] for x in rv):
-                    da['rotate_vars'].extend(rv)
+                self._orient_dnames = ["accel", "angrt", "mag", "orientmat"]
+                dv["accel"] = tbx._nans((3, self.n_samp_guess), dtype=np.float32)
+                dv["angrt"] = tbx._nans((3, self.n_samp_guess), dtype=np.float32)
+                dv["mag"] = tbx._nans((3, self.n_samp_guess), dtype=np.float32)
+                rv = ["accel", "angrt", "mag"]
+                if not all(x in da["rotate_vars"] for x in rv):
+                    da["rotate_vars"].extend(rv)
                 if ahrsid == 204:
-                    dv['orientmat'] = tbx._nans((3, 3, self.n_samp_guess),
-                                                dtype=np.float32)
+                    dv["orientmat"] = tbx._nans(
+                        (3, 3, self.n_samp_guess), dtype=np.float32
+                    )
                 update_defs(dat, mag=True, orientmat=True)
 
             if ahrsid == 211:
-                self._orient_dnames = ['angrt', 'accel', 'mag']
-                dv['angrt'] = tbx._nans((3, self.n_samp_guess),
-                                        dtype=np.float32)
-                dv['accel'] = tbx._nans((3, self.n_samp_guess),
-                                        dtype=np.float32)
-                dv['mag'] = tbx._nans((3, self.n_samp_guess),
-                                      dtype=np.float32)
-                rv = ['angrt', 'accel', 'mag']
-                if not all(x in da['rotate_vars'] for x in rv):
-                    da['rotate_vars'].extend(rv)
+                self._orient_dnames = ["angrt", "accel", "mag"]
+                dv["angrt"] = tbx._nans((3, self.n_samp_guess), dtype=np.float32)
+                dv["accel"] = tbx._nans((3, self.n_samp_guess), dtype=np.float32)
+                dv["mag"] = tbx._nans((3, self.n_samp_guess), dtype=np.float32)
+                rv = ["angrt", "accel", "mag"]
+                if not all(x in da["rotate_vars"] for x in rv):
+                    da["rotate_vars"].extend(rv)
                 update_defs(dat, mag=True, orientmat=False)
 
-        byts = ''
+        byts = ""
         if ahrsid == 195:  # 0xc3
             byts = self.read(64)
-            dt = unpack(self.endian + '6f9f4x', byts)
-            (dv['angrt'][:, c],
-             dv['accel'][:, c]) = (dt[0:3], dt[3:6],)
-            dv['orientmat'][:, :, c] = ((dt[6:9], dt[9:12], dt[12:15]))
+            dt = unpack(self.endian + "6f9f4x", byts)
+            (dv["angrt"][:, c], dv["accel"][:, c]) = (
+                dt[0:3],
+                dt[3:6],
+            )
+            dv["orientmat"][:, :, c] = (dt[6:9], dt[9:12], dt[12:15])
         elif ahrsid == 204:  # 0xcc
             byts = self.read(78)
             # This skips the "DWORD" (4 bytes) and the AHRS checksum
             # (2 bytes)
-            dt = unpack(self.endian + '18f6x', byts)
-            (dv['accel'][:, c],
-             dv['angrt'][:, c],
-             dv['mag'][:, c]) = (dt[0:3], dt[3:6], dt[6:9],)
-            dv['orientmat'][:, :, c] = ((dt[9:12], dt[12:15], dt[15:18]))
+            dt = unpack(self.endian + "18f6x", byts)
+            (dv["accel"][:, c], dv["angrt"][:, c], dv["mag"][:, c]) = (
+                dt[0:3],
+                dt[3:6],
+                dt[6:9],
+            )
+            dv["orientmat"][:, :, c] = (dt[9:12], dt[12:15], dt[15:18])
         elif ahrsid == 211:
             byts = self.read(42)
-            dt = unpack(self.endian + '9f6x', byts)
-            (dv['angrt'][:, c],
-             dv['accel'][:, c],
-             dv['mag'][:, c]) = (dt[0:3], dt[3:6], dt[6:9],)
+            dt = unpack(self.endian + "9f6x", byts)
+            (dv["angrt"][:, c], dv["accel"][:, c], dv["mag"][:, c]) = (
+                dt[0:3],
+                dt[3:6],
+                dt[6:9],
+            )
         else:
-            logging.warning('Unrecognized IMU identifier: ' + str(ahrsid))
+            logging.warning("Unrecognized IMU identifier: " + str(ahrsid))
             self.f.seek(-2, 1)
             return 10
         self.checksum(byts0 + byts)
         self.c += 1  # reset the increment
 
-    def sci_microstrain(self,):
-        """Rotate orientation data into ADV coordinate system.
-        """
+    def sci_microstrain(self):
+        """Rotate orientation data into ADV coordinate system."""
         # MS = MicroStrain
-        dv = self.data['data_vars']
+        dv = self.data["data_vars"]
         for nm in self._orient_dnames:
             # Rotate the MS orientation data (in MS coordinate system)
             # to be consistent with the ADV coordinate system.
             # (x,y,-z)_ms = (z,y,x)_adv
-            (dv[nm][2],
-             dv[nm][0]) = (dv[nm][0],
-                           -dv[nm][2].copy())
-        if 'orientmat' in self._orient_dnames:
+            (dv[nm][2], dv[nm][0]) = (dv[nm][0], -dv[nm][2].copy())
+        if "orientmat" in self._orient_dnames:
             # MS coordinate system is in North-East-Down (NED),
             # we want East-North-Up (ENU)
-            dv['orientmat'][:, 2] *= -1
-            (dv['orientmat'][:, 0],
-             dv['orientmat'][:, 1]) = (dv['orientmat'][:, 1],
-                                       dv['orientmat'][:, 0].copy())
-        if 'accel' in dv:
+            dv["orientmat"][:, 2] *= -1
+            (dv["orientmat"][:, 0], dv["orientmat"][:, 1]) = (
+                dv["orientmat"][:, 1],
+                dv["orientmat"][:, 0].copy(),
+            )
+        if "accel" in dv:
             # This value comes from the MS 3DM-GX3 MIP manual
-            dv['accel'] *= 9.80665
+            dv["accel"] *= 9.80665
         if self._ahrsid in [195, 211]:
             # These are DAng and DVel, so we convert them to angrt, accel here
-            dv['angrt'] *= self.config['fs']
-            dv['accel'] *= self.config['fs']
+            dv["angrt"] *= self.config["fs"]
+            dv["accel"] *= self.config["fs"]
 
-    def read_awac_profile(self,):
+    def read_awac_profile(self):
         # ID: '0x20' = 32
         dat = self.data
         if self.debug:
-            logging.info('Reading AWAC velocity data (0x20) ping #{} @ {}...'
-                         .format(self.c, self.pos))
-        nbins = self.config['usr']['n_bins']
-        if 'temp' not in dat['data_vars']:
+            logging.info(
+                "Reading AWAC velocity data (0x20) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
+        nbins = self.config["usr"]["n_bins"]
+        if "temp" not in dat["data_vars"]:
             self._init_data(nortek_defs.awac_profile)
-            self._dtypes += ['awac_profile']
+            self._dtypes += ["awac_profile"]
 
         # Note: docs state there is 'fill' byte at the end, if nbins is odd,
         # but doesn't appear to be the case
-        n = self.config['usr']['n_beams']
-        byts = self.read(116 + n*3 * nbins)
+        n = self.config["usr"]["n_beams"]
+        byts = self.read(116 + n * 3 * nbins)
         c = self.c
-        dat['coords']['time'][c] = self.rd_time(byts[2:8])
-        ds = dat['sys']
-        dv = dat['data_vars']
-        (dv['error'][c],
-         ds['AnaIn1'][c],
-         dv['batt'][c],
-         dv['c_sound'][c],
-         dv['heading'][c],
-         dv['pitch'][c],
-         dv['roll'][c],
-         p_msb,
-         dv['status'][c],
-         p_lsw,
-         dv['temp'][c],) = unpack(self.endian + '7HBB2H', byts[8:28])
-        dv['pressure'][c] = (65536 * p_msb + p_lsw)
+        dat["coords"]["time"][c] = self.rd_time(byts[2:8])
+        ds = dat["sys"]
+        dv = dat["data_vars"]
+        (
+            dv["error"][c],
+            ds["AnaIn1"][c],
+            dv["batt"][c],
+            dv["c_sound"][c],
+            dv["heading"][c],
+            dv["pitch"][c],
+            dv["roll"][c],
+            p_msb,
+            dv["status"][c],
+            p_lsw,
+            dv["temp"][c],
+        ) = unpack(self.endian + "5H2hBBHh", byts[8:28])
+        dv["pressure"][c] = 65536 * p_msb + p_lsw
         # The nortek system integrator manual specifies an 88byte 'spare'
         # field, therefore we start at 116.
-        tmp = unpack(self.endian + str(n * nbins) + 'h' +
-                     str(n * nbins) + 'B', byts[116:116 + n*3 * nbins])
+        tmp = unpack(
+            self.endian + str(n * nbins) + "h" + str(n * nbins) + "B",
+            byts[116 : 116 + n * 3 * nbins],
+        )
         for idx in range(n):
-            dv['vel'][idx, :, c] = tmp[idx * nbins: (idx + 1) * nbins]
-            dv['amp'][idx, :, c] = tmp[(idx + n) * nbins: (idx + n+1) * nbins]
+            dv["vel"][idx, :, c] = tmp[idx * nbins : (idx + 1) * nbins]
+            dv["amp"][idx, :, c] = tmp[(idx + n) * nbins : (idx + n + 1) * nbins]
         self.checksum(byts)
         self.c += 1
 
-    def sci_awac_profile(self,):
+    def sci_awac_profile(self):
         self._sci_data(nortek_defs.awac_profile)
         # Calculate the ranges.
-        cs_coefs = {2000: 0.0239,
-                    1000: 0.0478,
-                    600: 0.0797,
-                    400: 0.1195}
+        cs_coefs = {2000: 0.0239, 1000: 0.0478, 600: 0.0797, 400: 0.1195}
         h_ang = 25 * (np.pi / 180)  # Head angle is 25 degrees for all awacs.
         # Cell size
-        cs = round(float(self.config['bin_length']) / 256. *
-                   cs_coefs[self.config['head']['carrier_freq_kHz']] * np.cos(h_ang), ndigits=2)
+        cs = round(
+            float(self.config["bin_length"])
+            / 256.0
+            * cs_coefs[self.config["head"]["carrier_freq_kHz"]]
+            * np.cos(h_ang),
+            ndigits=2,
+        )
         # Blanking distance
-        bd = round(self.config['blank_dist'] *
-                   0.0229 * np.cos(h_ang) - cs, ndigits=2)
+        bd = round(self.config["blank_dist"] * 0.0229 * np.cos(h_ang) - cs, ndigits=2)
+
+        r = (np.float32(np.arange(self.config["usr"]["n_bins"])) + 1) * cs + bd
+        self.data["coords"]["range"] = r
+        self.data["attrs"]["cell_size"] = cs
+        self.data["attrs"]["blank_dist"] = bd
 
-        r = (np.float32(np.arange(self.config['usr']['n_bins']))+1)*cs + bd
-        self.data['coords']['range'] = r
-        self.data['attrs']['cell_size'] = cs
-        self.data['attrs']['blank_dist'] = bd
+    def read_awac_waves_hdr(self):
+        # ID: '0x31'
+        c = self.c
+        if self.debug:
+            print(
+                "Reading vector header data (0x31) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
+        hdrnow = {}
+        dat = self.data
+        ds = dat["sys"]
+        dv = dat["data_vars"]
+        if "time" not in dat["coords"]:
+            self._init_data(nortek_defs.waves_hdrdata)
+        byts = self.read(56)
+        # The first two are size, the next 6 are time.
+        tmp = unpack(self.endian + "8x4H3h2HhH4B6H5h", byts)
+        dat["coords"]["time"][c] = self.rd_time(byts[2:8])
+        hdrnow["n_records_alt"] = tmp[0]
+        hdrnow["blank_dist_alt"] = tmp[1]  # counts
+        ds["batt_alt"][c] = tmp[2]  # voltage (0.1 V)
+        dv["c_sound_alt"][c] = tmp[3]  # c (0.1 m/s)
+        dv["heading_alt"][c] = tmp[4]  # (0.1 deg)
+        dv["pitch_alt"][c] = tmp[5]  # (0.1 deg)
+        dv["roll_alt"][c] = tmp[6]  # (0.1 deg)
+        dv["pressure1_alt"][c] = tmp[7]  # min pressure previous profile (0.001 dbar)
+        dv["pressure2_alt"][c] = tmp[8]  # max pressure previous profile (0.001 dbar)
+        dv["temp_alt"][c] = tmp[9]  # (0.01 deg C)
+        hdrnow["cell_size_alt"][c] = tmp[10]  # (counts of T3)
+        hdrnow["noise_alt"][c] = tmp[11:15]  # noise amplitude beam 1-4 (counts)
+        hdrnow["proc_magn_alt"][c] = tmp[15:19]  # processing magnitude beam 1-4
+        hdrnow["n_past_window_alt"] = tmp[
+            19
+        ]  # number of samples of AST window past boundary
+        hdrnow["n_window_alt"] = tmp[20]  # AST window size (# samples)
+        hdrnow["Spare1"] = tmp[21:]
+        self.checksum(byts)
+        if "data_header" not in self.config:
+            self.config["data_header"] = hdrnow
+        else:
+            if not isinstance(self.config["data_header"], list):
+                self.config["data_header"] = [self.config["data_header"]]
+            self.config["data_header"] += [hdrnow]
+
+    def read_awac_waves(self):
+        """Read awac wave and suv data"""
+        # IDs: 0x30 & 0x36
+        c = self.c
+        dat = self.data
+        if self.debug:
+            print(
+                "Reading awac wave data (0x30) ping #{} @ {}...".format(
+                    self.c, self.pos
+                )
+            )
+        if "dist1_alt" not in dat["data_vars"]:
+            self._init_data(nortek_defs.wave_data)
+            self._dtypes += ["wave_data"]
+        # The first two are size
+        byts = self.read(20)
+        ds = dat["sys"]
+        dv = dat["data_vars"]
+        (
+            dv["pressure"][c],  # (0.001 dbar)
+            dv["dist1_alt"][c],  # distance 1 to surface, vertical beam (mm)
+            ds["AnaIn_alt"][c],  # analog input 1
+            dv["vel_alt"][0, c],  # velocity beam 1 (mm/s) East for SUV
+            dv["vel_alt"][1, c],  # North for SUV
+            dv["vel_alt"][2, c],  # Up for SUV
+            dv["dist2_alt"][
+                c
+            ],  # distance 2 to surface, vertical beam (mm) or vel 4 for non-AST
+            dv["amp_alt"][0, c],  # amplitude beam 1 (counts)
+            dv["amp_alt"][1, c],  # amplitude beam 2 (counts)
+            dv["amp_alt"][2, c],  # amplitude beam 3 (counts)
+            # AST quality (counts) or amplitude beam 4 for non-AST
+            dv["quality_alt"][c],
+        ) = unpack(self.endian + "3H4h4B", byts)
+        self.checksum(byts)
+        self.c += 1
 
-    def dat2sci(self,):
+    def dat2sci(self):
         for nm in self._dtypes:
-            getattr(self, 'sci_' + nm)()
-        for nm in ['data_header', 'checkdata']:
+            getattr(self, "sci_" + nm)()
+        for nm in ["data_header", "checkdata"]:
             if nm in self.config and isinstance(self.config[nm], list):
                 self.config[nm] = _recatenate(self.config[nm])
 
-    def __exit__(self, type, value, trace):
-        self.close()
-
-    def __enter__(self):
-        return self
-
 
 def _crop_data(obj, range, n_lastdim):
     for nm, dat in obj.items():
         if isinstance(dat, np.ndarray) and (dat.shape[-1] == n_lastdim):
             obj[nm] = dat[..., range]
 
 
 def _recatenate(obj):
     out = type(obj[0])()
     for ky in list(obj[0].keys()):
-        if ky in ['__data_groups__', '_type']:
+        if ky in ["__data_groups__", "_type"]:
             continue
         val0 = obj[0][ky]
         if isinstance(val0, np.ndarray) and val0.size > 1:
-            out[ky] = np.concatenate([val[ky][..., None] for val in obj],
-                                     axis=-1)
+            out[ky] = np.concatenate([val[ky][..., None] for val in obj], axis=-1)
         else:
             out[ky] = np.array([val[ky] for val in obj])
     return out
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/io/nortek2.py` & `mhkit-0.8.0/mhkit/dolfyn/io/nortek2.py`

 * *Files 21% similar despite different names*

```diff
@@ -10,239 +10,316 @@
 from .base import _find_userdata, _create_dataset, _abspath
 from ..rotate.vector import _euler2orient
 from ..rotate.base import _set_coords
 from ..rotate.api import set_declination
 from ..time import epoch2dt64, _fill_time_gaps
 
 
-def read_signature(filename, userdata=True, nens=None, rebuild_index=False,
-                   debug=False, **kwargs):
+def read_signature(
+    filename,
+    userdata=True,
+    nens=None,
+    rebuild_index=False,
+    debug=False,
+    dual_profile=False,
+    **kwargs
+):
     """
     Read a Nortek Signature (.ad2cp) datafile
 
     Parameters
     ----------
     filename : string
       The filename of the file to load.
     userdata : bool
       To search for and use a .userdata.json or not
     nens : None, int or 2-element tuple (start, stop)
-      Number of pings or ensembles to read from the file. 
+      Number of pings or ensembles to read from the file.
       Default is None, read entire file
     rebuild_index : bool
       Force rebuild of dolfyn-written datafile index. Useful for code updates.
       Default = False
     debug : bool
       Logs debugger ouput if true. Default = False
+    dual_profile : bool
+      Set to true if instrument is running multiple profiles. Default = False
 
     Returns
     -------
     ds : xarray.Dataset
       An xarray dataset from the binary instrument data
     """
 
     # Start debugger logging
     if debug:
         for handler in logging.root.handlers[:]:
             logging.root.removeHandler(handler)
         filepath = Path(filename)
-        logfile = filepath.with_suffix('.dolfyn.log')
-        logging.basicConfig(filename=str(logfile),
-                            filemode='w',
-                            level=logging.NOTSET,
-                            format='%(name)s - %(levelname)s - %(message)s')
+        logfile = filepath.with_suffix(".dolfyn.log")
+        logging.basicConfig(
+            filename=str(logfile),
+            filemode="w",
+            level=logging.NOTSET,
+            format="%(name)s - %(levelname)s - %(message)s",
+        )
 
     if nens is None:
         nens = [0, None]
     else:
         try:
             n = len(nens)
         except TypeError:
             nens = [0, nens]
         else:
             # passes: it's a list/tuple/array
             if n != 2:
-                raise TypeError('nens must be: None (), int, or len 2')
+                raise TypeError("nens must be: None (), int, or len 2")
 
     userdata = _find_userdata(filename, userdata)
 
-    rdr = _Ad2cpReader(filename, rebuild_index=rebuild_index, debug=debug)
+    rdr = _Ad2cpReader(
+        filename, rebuild_index=rebuild_index, debug=debug, dual_profile=dual_profile
+    )
     d = rdr.readfile(nens[0], nens[1])
     rdr.sci_data(d)
+    if rdr._dp:
+        _clean_dp_skips(d)
     out = _reorg(d)
     _reduce(out)
 
     # Convert time to dt64 and fill gaps
-    coords = out['coords']
-    t_list = [t for t in coords if 'time' in t]
+    coords = out["coords"]
+    t_list = [t for t in coords if "time" in t]
     for ky in t_list:
         tdat = coords[ky]
         tdat[tdat == 0] = np.NaN
         if np.isnan(tdat).any():
-            tag = ky.lstrip('time')
-            warnings.warn("Zero/NaN values found in '{}'. Interpolating and "
-                          "extrapolating them. To identify which values were filled later, "
-                          "look for 0 values in 'status{}'".format(ky, tag))
-            tdat = _fill_time_gaps(tdat, sample_rate_hz=out['attrs']['fs'])
-        coords[ky] = epoch2dt64(tdat).astype('datetime64[ns]')
+            tag = ky.lstrip("time")
+            warnings.warn(
+                "Zero/NaN values found in '{}'. Interpolating and "
+                "extrapolating them. To identify which values were filled later, "
+                "look for 0 values in 'status{}'".format(ky, tag)
+            )
+            tdat = _fill_time_gaps(tdat, sample_rate_hz=out["attrs"]["fs"])
+        coords[ky] = epoch2dt64(tdat).astype("datetime64[ns]")
 
     declin = None
     for nm in userdata:
-        if 'dec' in nm:
+        if "dec" in nm:
             declin = userdata[nm]
         else:
-            out['attrs'][nm] = userdata[nm]
+            out["attrs"][nm] = userdata[nm]
 
     # Create xarray dataset from upper level dictionary
     ds = _create_dataset(out)
     ds = _set_coords(ds, ref_frame=ds.coord_sys)
 
-    if 'orientmat' not in ds:
-        ds['orientmat'] = _euler2orient(
-            ds['time'], ds['heading'], ds['pitch'], ds['roll'])
+    if "orientmat" not in ds:
+        ds["orientmat"] = _euler2orient(
+            ds["time"], ds["heading"], ds["pitch"], ds["roll"]
+        )
 
     if declin is not None:
         set_declination(ds, declin, inplace=True)
 
     # Convert config dictionary to json string
     for key in list(ds.attrs.keys()):
-        if 'config' in key:
+        if "config" in key:
             ds.attrs[key] = json.dumps(ds.attrs[key])
 
     # Close handler
     if debug:
         for handler in logging.root.handlers[:]:
             logging.root.removeHandler(handler)
             handler.close()
 
-    return ds
+    # Return two datasets if dual profile
+    if rdr._dp:
+        return split_dp_datasets(ds)
+    else:
+        return ds
 
 
-class _Ad2cpReader():
-    def __init__(self, fname, endian=None, bufsize=None, rebuild_index=False,
-                 debug=False):
+class _Ad2cpReader:
+    def __init__(
+        self,
+        fname,
+        endian=None,
+        bufsize=None,
+        rebuild_index=False,
+        debug=False,
+        dual_profile=False,
+    ):
         self.fname = fname
         self.debug = debug
         self._check_nortek(endian)
         self.f.seek(0, 2)  # Seek to end
         self._eof = self.f.tell()
-        self._index = lib.get_index(fname,
-                                    reload=rebuild_index,
-                                    debug=debug)
+        self.start_pos = self._check_header()
+        self._index, self._dp = lib.get_index(
+            fname,
+            pos=self.start_pos,
+            eof=self._eof,
+            rebuild=rebuild_index,
+            debug=debug,
+            dp=dual_profile,
+        )
         self._reopen(bufsize)
         self.filehead_config = self._read_filehead_config_string()
-        self._ens_pos = self._index['pos'][lib._boolarray_firstensemble_ping(
-            self._index)]
+        self._ens_pos = self._index["pos"][
+            lib._boolarray_firstensemble_ping(self._index)
+        ]
         self._lastblock_iswhole = self._calc_lastblock_iswhole()
         self._config = lib._calc_config(self._index)
         self._init_burst_readers()
         self.unknown_ID_count = {}
 
-    def _calc_lastblock_iswhole(self, ):
-        blocksize, blocksize_count = np.unique(np.diff(self._ens_pos),
-                                               return_counts=True)
+    def _calc_lastblock_iswhole(
+        self,
+    ):
+        blocksize, blocksize_count = np.unique(
+            np.diff(self._ens_pos), return_counts=True
+        )
         standard_blocksize = blocksize[blocksize_count.argmax()]
         return (self._eof - self._ens_pos[-1]) == standard_blocksize
 
     def _check_nortek(self, endian):
         self._reopen(10)
         byts = self.f.read(2)
         if endian is None:
-            if unpack('<' + 'BB', byts) == (165, 10):
-                endian = '<'
-            elif unpack('>' + 'BB', byts) == (165, 10):
-                endian = '>'
+            if unpack("<" + "BB", byts) == (165, 10):
+                endian = "<"
+            elif unpack(">" + "BB", byts) == (165, 10):
+                endian = ">"
             else:
                 raise Exception(
                     "I/O error: could not determine the 'endianness' "
                     "of the file.  Are you sure this is a Nortek "
-                    "AD2CP file?")
+                    "AD2CP file?"
+                )
         self.endian = endian
 
+    def _check_header(self):
+        def find_all(s, c):
+            idx = s.find(c)
+            while idx != -1:
+                yield idx
+                idx = s.find(c, idx + 1)
+
+        # Open the entire file
+        self._reopen(self._eof)
+        pk = self.f.peek(1)
+        # Search for multiple saved headers
+        found = [i for i in find_all(pk, b"GETCLOCKSTR")]
+        if len(found) < 2:
+            return 0
+        else:
+            start_idx = found[-1] - 11
+            return start_idx
+
     def _reopen(self, bufsize=None):
         if bufsize is None:
             bufsize = 1000000
         try:
             self.f.close()
         except AttributeError:
             pass
-        self.f = open(_abspath(self.fname), 'rb', bufsize)
+        self.f = open(_abspath(self.fname), "rb", bufsize)
 
-    def _read_filehead_config_string(self, ):
+    def _read_filehead_config_string(
+        self,
+    ):
         hdr = self._read_hdr()
         out = {}
-        s_id, string = self._read_str(hdr['sz'])
-        string = string.decode('utf-8')
+        s_id, string = self._read_str(hdr["sz"])
+        string = string.decode("utf-8")
         for ln in string.splitlines():
-            ky, val = ln.split(',', 1)
+            ky, val = ln.split(",", 1)
             if ky in out:
                 # There are more than one of this key
                 if not isinstance(out[ky], list):
                     tmp = out[ky]
                     out[ky] = []
                     out[ky].append(tmp)
                 out[ky].append(val)
             else:
                 out[ky] = val
         out2 = {}
         for ky in out:
-            if ky.startswith('GET'):
+            if ky.startswith("GET"):
                 dat = out[ky]
-                d = out2[ky.lstrip('GET')] = dict()
-                for itm in dat.split(','):
-                    k, val = itm.split('=')
+                d = out2[ky.lstrip("GET")] = dict()
+                for itm in dat.split(","):
+                    k, val = itm.split("=")
                     try:
                         val = int(val)
                     except ValueError:
                         try:
                             val = float(val)
                         except ValueError:
                             pass
                     d[k] = val
             else:
                 out2[ky] = out[ky]
         return out2
 
-    def _init_burst_readers(self, ):
+    def _init_burst_readers(
+        self,
+    ):
         self._burst_readers = {}
         for rdr_id, cfg in self._config.items():
             if rdr_id == 28:
                 self._burst_readers[rdr_id] = defs._calc_echo_struct(
-                    cfg['_config'], cfg['n_cells'])
+                    cfg["_config"], cfg["n_cells"]
+                )
             elif rdr_id == 23:
                 self._burst_readers[rdr_id] = defs._calc_bt_struct(
-                    cfg['_config'], cfg['n_beams'])
+                    cfg["_config"], cfg["n_beams"]
+                )
             else:
                 self._burst_readers[rdr_id] = defs._calc_burst_struct(
-                    cfg['_config'], cfg['n_beams'], cfg['n_cells'])
+                    cfg["_config"], cfg["n_beams"], cfg["n_cells"]
+                )
 
     def init_data(self, ens_start, ens_stop):
         outdat = {}
         nens = int(ens_stop - ens_start)
-        n26 = ((self._index['ID'] == 26) &
-               (self._index['ens'] >= ens_start) &
-               (self._index['ens'] < ens_stop)).sum()
+
+        # ID 26 and 31 recorded infrequently
+        def n_id(id):
+            return (
+                (self._index["ID"] == id)
+                & (self._index["ens"] >= ens_start)
+                & (self._index["ens"] < ens_stop)
+            ).sum()
+
+        n_altraw = {26: n_id(26), 31: n_id(31)}
+        if not n_altraw[26] and 26 in self._burst_readers:
+            self._burst_readers.pop(26)
+        if not n_altraw[31] and 31 in self._burst_readers:
+            self._burst_readers.pop(31)
+
         for ky in self._burst_readers:
-            if ky == 26:
-                n = n26
-                ens = np.zeros(n, dtype='uint32')
+            if (ky == 26) or (ky == 31):
+                n = n_altraw[ky]
+                ens = np.zeros(n, dtype="uint32")
             else:
-                ens = np.arange(ens_start,
-                                ens_stop).astype('uint32')
+                ens = np.arange(ens_start, ens_stop).astype("uint32")
                 n = nens
             outdat[ky] = self._burst_readers[ky].init_data(n)
-            outdat[ky]['ensemble'] = ens
-            outdat[ky]['units'] = self._burst_readers[ky].data_units()
-            outdat[ky]['long_name'] = self._burst_readers[ky].data_longnames()
-            outdat[ky]['standard_name'] = self._burst_readers[ky].data_stdnames()
+            outdat[ky]["ensemble"] = ens
+            outdat[ky]["units"] = self._burst_readers[ky].data_units()
+            outdat[ky]["long_name"] = self._burst_readers[ky].data_longnames()
+            outdat[ky]["standard_name"] = self._burst_readers[ky].data_stdnames()
+
         return outdat
 
     def _read_hdr(self, do_cs=False):
         res = defs.header.read2dict(self.f, cs=do_cs)
-        if res['sync'] != 165:
+        if res["sync"] != 165:
             raise Exception("Out of sync!")
         return res
 
     def _read_str(self, size):
         string = self.f.read(size)
         id = string[0]
         string = string[1:-1]
@@ -258,109 +335,114 @@
         nens_total = len(self._ens_pos) - int(not self._lastblock_iswhole)
         if ens_stop is None or ens_stop > nens_total:
             ens_stop = nens_total
         ens_start = int(ens_start)
         ens_stop = int(ens_stop)
         nens = ens_stop - ens_start
         outdat = self.init_data(ens_start, ens_stop)
-        outdat['filehead_config'] = self.filehead_config
-        print('Reading file %s ...' % self.fname)
+        outdat["filehead_config"] = self.filehead_config
+        print("Reading file %s ..." % self.fname)
         c = 0
-        c26 = 0
+        c_altraw = {26: 0, 31: 0}
         self.f.seek(self._ens_pos[ens_start], 0)
         while True:
             try:
                 hdr = self._read_hdr()
             except IOError:
                 return outdat
-            id = hdr['id']
-            if id in [21, 22, 23, 24, 28]:  # vel, bt, vel_b5, echo
+            id = hdr["id"]
+            if id in [21, 22, 23, 24, 28]:  # "burst data record" (vel + ast),
+                # "avg data record" (vel_avg + ast_avg), "bottom track data record" (bt),
+                # "interleaved burst data record" (vel_b5), "echosounder record" (echo)
                 self._read_burst(id, outdat[id], c)
-            elif id in [26]:  # alt_raw (altimeter burst)
-                rdr = self._burst_readers[26]
-                if not hasattr(rdr, '_nsamp_index'):
+            elif id in [26, 31]:
+                # "burst altimeter raw record" (_altraw), "avg altimeter raw record" (_altraw_avg)
+                rdr = self._burst_readers[id]
+                if not hasattr(rdr, "_nsamp_index"):
                     first_pass = True
-                    tmp_idx = rdr._nsamp_index = rdr._names.index('altraw_nsamp')  # noqa
+                    tmp_idx = rdr._nsamp_index = rdr._names.index("nsamp_alt")
                     shift = rdr._nsamp_shift = calcsize(
-                        defs._format(rdr._format[:tmp_idx],
-                                     rdr._N[:tmp_idx]))
+                        defs._format(rdr._format[:tmp_idx], rdr._N[:tmp_idx])
+                    )
                 else:
                     first_pass = False
                     tmp_idx = rdr._nsamp_index
                     shift = rdr._nsamp_shift
                 tmp_idx = tmp_idx + 2  # Don't add in-place
                 self.f.seek(shift, 1)
                 # Now read the num_samples
-                sz = unpack('<I', self.f.read(4))[0]
+                sz = unpack("<I", self.f.read(4))[0]
                 self.f.seek(-shift - 4, 1)
                 if first_pass:
                     # Fix the reader
                     rdr._shape[tmp_idx].append(sz)
                     rdr._N[tmp_idx] = sz
-                    rdr._struct = defs.Struct('<' + rdr.format)
+                    rdr._struct = defs.Struct("<" + rdr.format)
                     rdr.nbyte = calcsize(rdr.format)
                     rdr._cs_struct = defs.Struct(
-                        '<' + '{}H'.format(int(rdr.nbyte // 2)))
+                        "<" + "{}H".format(int(rdr.nbyte // 2))
+                    )
                     # Initialize the array
-                    outdat[26]['altraw_samp'] = defs._nans(
-                        [rdr._N[tmp_idx],
-                         len(outdat[26]['altraw_samp'])],
-                        dtype=np.uint16)
+                    outdat[id]["samp_alt"] = defs._nans(
+                        [rdr._N[tmp_idx], len(outdat[id]["samp_alt"])], dtype=np.uint16
+                    )
                 else:
                     if sz != rdr._N[tmp_idx]:
                         raise Exception(
                             "The number of samples in this 'Altimeter Raw' "
-                            "burst is different from prior bursts.")
-                self._read_burst(id, outdat[id], c26)
-                outdat[id]['ensemble'][c26] = c
-                c26 += 1
-
-            elif id in [27, 29, 30, 31, 35, 36]:  # bt record, DVL,
-                # alt record, avg alt_raw record, raw echo, raw echo transmit
+                            "burst is different from prior bursts."
+                        )
+                self._read_burst(id, outdat[id], c_altraw[id])
+                outdat[id]["ensemble"][c_altraw[id]] = c
+                c_altraw[id] += 1
+
+            elif id in [27, 29, 30, 35, 36]:  # unknown how to handle
+                # "bottom track record", DVL, "altimeter record",
+                # "raw echosounder data record", "raw echosounder transmit data record"
                 if self.debug:
-                    logging.debug(
-                        "Skipped ID: 0x{:02X} ({:02d})\n".format(id, id))
-                self.f.seek(hdr['sz'], 1)
+                    logging.debug("Skipped ID: 0x{:02X} ({:02d})\n".format(id, id))
+                self.f.seek(hdr["sz"], 1)
             elif id == 160:
                 # 0xa0 (i.e., 160) is a 'string data record'
                 if id not in outdat:
                     outdat[id] = dict()
-                s_id, s = self._read_str(hdr['sz'], )
+                s_id, s = self._read_str(
+                    hdr["sz"],
+                )
                 outdat[id][(c, s_id)] = s
             else:
                 if id not in self.unknown_ID_count:
                     self.unknown_ID_count[id] = 1
                     if self.debug:
-                        logging.warning('Unknown ID: 0x{:02X}!'.format(id))
+                        logging.warning("Unknown ID: 0x{:02X}!".format(id))
                 else:
                     self.unknown_ID_count[id] += 1
-                self.f.seek(hdr['sz'], 1)
+                self.f.seek(hdr["sz"], 1)
 
             c = self._advance_ens_count(c, ens_start, nens_total)
 
             if c >= nens:
                 return outdat
 
     def _advance_ens_count(self, c, ens_start, nens_total):
-        """This method advances the counter when appropriate to do so.
-        """
+        """This method advances the counter when appropriate to do so."""
         # It's unfortunate that all of this count checking is so
         # complex, but this is the best I could come up with right
         # now.
         try:
             # Checks to makes sure we're not already at the end of the
             # self._ens_pos array
             _posnow = self._ens_pos[c + ens_start + 1]
         except IndexError:
             # We are at the end of the array, set _posnow
             # We use "+1" here because we want the >= in the while
             # loop to fail for this case so that we go ahead and read
             # the next ping without advancing the ens counter.
             _posnow = self._eof + 1
-        while (self.f.tell() >= _posnow):
+        while self.f.tell() >= _posnow:
             c += 1
             if c + ens_start + 1 >= nens_total:
                 # Again check end of count list
                 break
             try:
                 # Same check as above.
                 _posnow = self._ens_pos[c + ens_start + 1]
@@ -371,250 +453,402 @@
     def sci_data(self, dat):
         for id in dat:
             dnow = dat[id]
             if id not in self._burst_readers:
                 continue
             rdr = self._burst_readers[id]
             rdr.sci_data(dnow)
-            if 'vel' in dnow and 'vel_scale' in dnow:
-                dnow['vel'] = (dnow['vel'] *
-                               10.0 ** dnow['vel_scale']).astype('float32')
-
-    def __exit__(self, type, value, trace,):
-        self.f.close()
-
-    def __enter__(self,):
-        return self
+            if "vel" in dnow and "vel_scale" in dnow:
+                dnow["vel"] = (dnow["vel"] * 10.0 ** dnow["vel_scale"]).astype(
+                    "float32"
+                )
+
+
+def _altraw_reorg(outdat, tag=""):
+    """Submethod for `_reorg` particular to raw altimeter pings (ID 26 and 31)"""
+    for ky in list(outdat["data_vars"]):
+        if ky.endswith("raw" + tag) and not ky.endswith("_altraw" + tag):
+            outdat["data_vars"].pop(ky)
+    outdat["coords"]["time_altraw" + tag] = outdat["coords"].pop("timeraw" + tag)
+    # convert "signed fractional" to float
+    outdat["data_vars"]["samp_altraw" + tag] = (
+        outdat["data_vars"]["samp_altraw" + tag].astype("float32") / 2**8
+    )
+
+    # Read altimeter status
+    outdat["data_vars"].pop("status_altraw" + tag)
+    status_alt = lib._alt_status2data(outdat["data_vars"]["status_alt" + tag])
+    for ky in status_alt:
+        outdat["attrs"][ky + tag] = lib._collapse(
+            status_alt[ky].astype("uint8"), name=ky
+        )
+    outdat["data_vars"].pop("status_alt" + tag)
+
+    # Power level index
+    power = {0: "high", 1: "med-high", 2: "med-low", 3: "low"}
+    outdat["attrs"]["power_level_alt" + tag] = power[
+        outdat["attrs"].pop("power_level_idx_alt" + tag)
+    ]
+
+    # Other attrs
+    for ky in list(outdat["attrs"]):
+        if ky.endswith("raw" + tag):
+            outdat["attrs"][ky.split("raw")[0] + "_alt" + tag] = outdat["attrs"].pop(ky)
 
 
 def _reorg(dat):
     """
     This function grabs the data from the dictionary of data types
     (organized by ID), and combines them into a single dictionary.
     """
 
-    outdat = {'data_vars': {}, 'coords': {}, 'attrs': {},
-              'units': {}, 'long_name': {}, 'standard_name': {},
-              'sys': {}, 'altraw': {}}
-    cfg = outdat['attrs']
-    cfh = cfg['filehead_config'] = dat['filehead_config']
-    cfg['inst_model'] = (cfh['ID'].split(',')[0][5:-1])
-    cfg['inst_make'] = 'Nortek'
-    cfg['inst_type'] = 'ADCP'
-
-    for id, tag in [(21, ''), (22, '_avg'), (23, '_bt'), 
-                    (24, '_b5'), (26, '_ast'), (28, '_echo')]:
+    outdat = {
+        "data_vars": {},
+        "coords": {},
+        "attrs": {},
+        "units": {},
+        "long_name": {},
+        "standard_name": {},
+        "sys": {},
+        "altraw": {},
+    }
+    cfg = outdat["attrs"]
+    cfh = cfg["filehead_config"] = dat["filehead_config"]
+    cfg["inst_model"] = cfh["ID"].split(",")[0][5:-1]
+    cfg["inst_make"] = "Nortek"
+    cfg["inst_type"] = "ADCP"
+
+    for id, tag in [
+        (21, ""),
+        (22, "_avg"),
+        (23, "_bt"),
+        (24, "_b5"),
+        (26, "raw"),
+        (28, "_echo"),
+        (31, "raw_avg"),
+    ]:
         if id in [24, 26]:
             collapse_exclude = [0]
         else:
             collapse_exclude = []
         if id not in dat:
             continue
         dnow = dat[id]
-        outdat['units'].update(dnow['units'])
-        outdat['long_name'].update(dnow['long_name'])
-        for ky in dnow['units']:
-            if not dnow['standard_name'][ky]:
-                dnow['standard_name'].pop(ky)
-        outdat['standard_name'].update(dnow['standard_name'])
-        cfg['burst_config' + tag] = lib._headconfig_int2dict(
-            lib._collapse(dnow['config'], exclude=collapse_exclude,
-                          name='config'))
-        outdat['coords']['time' + tag] = lib._calc_time(
-            dnow['year'] + 1900,
-            dnow['month'],
-            dnow['day'],
-            dnow['hour'],
-            dnow['minute'],
-            dnow['second'],
-            dnow['usec100'].astype('uint32') * 100)
+        outdat["units"].update(dnow["units"])
+        outdat["long_name"].update(dnow["long_name"])
+        for ky in dnow["units"]:
+            if not dnow["standard_name"][ky]:
+                dnow["standard_name"].pop(ky)
+        outdat["standard_name"].update(dnow["standard_name"])
+        cfg["burst_config" + tag] = lib._headconfig_int2dict(
+            lib._collapse(dnow["config"], exclude=collapse_exclude, name="config")
+        )
+        outdat["coords"]["time" + tag] = lib._calc_time(
+            dnow["year"] + 1900,
+            dnow["month"],
+            dnow["day"],
+            dnow["hour"],
+            dnow["minute"],
+            dnow["second"],
+            dnow["usec100"].astype("uint32") * 100,
+        )
         tmp = lib._beams_cy_int2dict(
-            lib._collapse(dnow['beam_config'], exclude=collapse_exclude,
-                          name='beam_config'), 21)
-        cfg['n_cells' + tag] = tmp['n_cells']
-        cfg['coord_sys_axes' + tag] = tmp['cy']
-        cfg['n_beams' + tag] = tmp['n_beams']
-        cfg['ambig_vel' +
-            tag] = lib._collapse(dnow['ambig_vel'], name='ambig_vel')
-
-        for ky in ['SerialNum', 'cell_size', 'blank_dist', 'nominal_corr',
-                   'power_level_dB']:
-            cfg[ky + tag] = lib._collapse(dnow[ky],
-                                          exclude=collapse_exclude,
-                                          name=ky)
-
-        for ky in ['c_sound', 'temp', 'pressure', 'heading', 'pitch', 'roll',
-                   'mag', 'accel', 'batt', 'temp_clock', 'error',
-                   'status', 'ensemble',
-                   ]:
-            outdat['data_vars'][ky + tag] = dnow[ky]
-            if 'ensemble' in ky:
-                outdat['data_vars'][ky + tag] += 1
-                outdat['units'][ky + tag] = '#'
-                outdat['long_name'][ky + tag] = 'Ensemble Number'
-                outdat['standard_name'][ky + tag] = 'number_of_observations'
-
-        for ky in ['vel', 'amp', 'corr', 'prcnt_gd', 'echo', 'dist',
-                   'orientmat', 'angrt', 'quaternions', 'ast_pressure',
-                   'alt_dist', 'alt_quality', 'alt_status',
-                   'ast_dist', 'ast_quality', 'ast_offset_time',
-                   'altraw_nsamp', 'altraw_dsamp', 'altraw_samp',
-                   'status0', 'fom', 'temp_press', 'press_std',
-                   'pitch_std', 'roll_std', 'heading_std', 'xmit_energy',
-                   ]:
+            lib._collapse(
+                dnow["beam_config"], exclude=collapse_exclude, name="beam_config"
+            ),
+            21,  # always 21 here
+        )
+        cfg["n_cells" + tag] = tmp["n_cells"]
+        cfg["coord_sys_axes" + tag] = tmp["cy"]
+        cfg["n_beams" + tag] = tmp["n_beams"]
+        cfg["ambig_vel" + tag] = lib._collapse(dnow["ambig_vel"], name="ambig_vel")
+
+        for ky in [
+            "SerialNum",
+            "cell_size",
+            "blank_dist",
+            "nominal_corr",
+            "power_level_dB",
+        ]:
+            cfg[ky + tag] = lib._collapse(dnow[ky], exclude=collapse_exclude, name=ky)
+
+        for ky in [
+            "c_sound",
+            "temp",
+            "pressure",
+            "heading",
+            "pitch",
+            "roll",
+            "mag",
+            "accel",
+            "batt",
+            "temp_clock",
+            "error",
+            "status",
+            "ensemble",
+        ]:
+            outdat["data_vars"][ky + tag] = dnow[ky]
+            if "ensemble" in ky:
+                outdat["data_vars"][ky + tag] += 1
+                outdat["units"][ky + tag] = "#"
+                outdat["long_name"][ky + tag] = "Ensemble Number"
+                outdat["standard_name"][ky + tag] = "number_of_observations"
+
+        for ky in [
+            "vel",
+            "amp",
+            "corr",
+            "prcnt_gd",
+            "echo",
+            "dist",
+            "orientmat",
+            "angrt",
+            "quaternions",
+            "pressure_alt",
+            "le_dist_alt",
+            "le_quality_alt",
+            "status_alt",
+            "ast_dist_alt",
+            "ast_quality_alt",
+            "ast_offset_time_alt",
+            "nsamp_alt",
+            "dsamp_alt",
+            "samp_alt",
+            "status0",
+            "fom",
+            "temp_press",
+            "press_std",
+            "pitch_std",
+            "roll_std",
+            "heading_std",
+            "xmit_energy",
+        ]:
             if ky in dnow:
-                outdat['data_vars'][ky + tag] = dnow[ky]
+                outdat["data_vars"][ky + tag] = dnow[ky]
 
     # Move 'altimeter raw' data to its own down-sampled structure
     if 26 in dat:
-        ard = outdat['altraw']
-        for ky in list(outdat['data_vars']):
-            if ky.endswith('_ast'):
-                grp = ky.split('.')[0]
-                if '.' in ky and grp not in ard:
-                    ard[grp] = {}
-                ard[ky.rstrip('_ast')] = outdat['data_vars'].pop(ky)
-
-        # Read altimeter status
-        alt_status = lib._alt_status2data(outdat['data_vars']['alt_status'])
-        for ky in alt_status:
-            outdat['attrs'][ky] = lib._collapse(
-                alt_status[ky].astype('uint8'), name=ky)
-        outdat['data_vars'].pop('alt_status')
-
-        # Power level index
-        power = {0: 'high', 1: 'med-high', 2: 'med-low', 3: 'low'}
-        outdat['attrs']['power_level_alt'] = power[outdat['attrs'].pop(
-            'power_level_idx_alt')]
+        _altraw_reorg(outdat)
+    if 31 in dat:
+        _altraw_reorg(outdat, tag="_avg")
 
     # Read status data
-    status0_vars = [x for x in outdat['data_vars'] if 'status0' in x]
+    status0_vars = [x for x in outdat["data_vars"] if "status0" in x]
     # Status data is the same across all tags, and there is always a 'status' and 'status0'
     status0_key = status0_vars[0]
-    status0_data = lib._status02data(outdat['data_vars'][status0_key])
-    status_key = status0_key.replace('0', '')
-    status_data = lib._status2data(outdat['data_vars'][status_key])
+    status0_data = lib._status02data(outdat["data_vars"][status0_key])
+    status_key = status0_key.replace("0", "")
+    status_data = lib._status2data(outdat["data_vars"][status_key])
 
     # Individual status codes
     # Wake up state
-    wake = {0: 'bad power', 1: 'power on', 2: 'break', 3: 'clock'}
-    outdat['attrs']['wakeup_state'] = wake[lib._collapse(
-        status_data.pop('wakeup_state'), name=ky)]
+    wake = {0: "bad power", 1: "power on", 2: "break", 3: "clock"}
+    outdat["attrs"]["wakeup_state"] = wake[
+        lib._collapse(status_data.pop("wakeup_state"), name=ky)
+    ]
 
     # Instrument direction
     # 0: XUP, 1: XDOWN, 2: YUP, 3: YDOWN, 4: ZUP, 5: ZDOWN,
     # 7: AHRS, handle as ZUP
-    nortek_orient = {0: 'horizontal', 1: 'horizontal', 2: 'horizontal',
-                     3: 'horizontal', 4: 'up', 5: 'down', 7: 'AHRS'}
-    outdat['attrs']['orientation'] = nortek_orient[lib._collapse(
-        status_data.pop('orient_up'), name='orientation')]
+    nortek_orient = {
+        0: "horizontal",
+        1: "horizontal",
+        2: "horizontal",
+        3: "horizontal",
+        4: "up",
+        5: "down",
+        7: "AHRS",
+    }
+    outdat["attrs"]["orientation"] = nortek_orient[
+        lib._collapse(status_data.pop("orient_up"), name="orientation")
+    ]
 
     # Orientation detection
-    orient_status = {0: 'fixed', 1: 'auto_UD', 3: 'AHRS-3D'}
-    outdat['attrs']['orient_status'] = orient_status[lib._collapse(
-        status_data.pop('auto_orientation'), name='orient_status')]
+    orient_status = {0: "fixed", 1: "auto_UD", 3: "AHRS-3D"}
+    outdat["attrs"]["orient_status"] = orient_status[
+        lib._collapse(status_data.pop("auto_orientation"), name="orient_status")
+    ]
 
     # Status variables
-    for ky in ['low_volt_skip', 'active_config', 'telemetry_data', 'boost_running']:
-        outdat['data_vars'][ky] = status_data[ky].astype('uint8')
+    for ky in ["low_volt_skip", "active_config", "telemetry_data", "boost_running"]:
+        outdat["data_vars"][ky] = status_data[ky].astype("uint8")
 
     # Processor idle state - need to save as 1/0 per netcdf attribute limitations
     for ky in status0_data:
-        outdat['attrs'][ky] = lib._collapse(
-            status0_data[ky].astype('uint8'), name=ky)
+        outdat["attrs"][ky] = lib._collapse(status0_data[ky].astype("uint8"), name=ky)
 
-    # Remove status0 variables - keep status variables as they useful for finding missing pings
-    [outdat['data_vars'].pop(var) for var in status0_vars]
+    # Remove status0 variables - keep status variables as they are useful for finding missing pings
+    [outdat["data_vars"].pop(var) for var in status0_vars]
 
     # Set coordinate system
     if 21 not in dat:
-        cfg['rotate_vars'] = []
-        cy = cfg['coord_sys_axes_avg']
+        cfg["rotate_vars"] = []
+        cy = cfg["coord_sys_axes_avg"]
     else:
-        cfg['rotate_vars'] = ['vel', ]
-        cy = cfg['coord_sys_axes']
-    outdat['attrs']['coord_sys'] = {'XYZ': 'inst',
-                                    'ENU': 'earth',
-                                    'beam': 'beam'}[cy]
+        cfg["rotate_vars"] = [
+            "vel",
+        ]
+        cy = cfg["coord_sys_axes"]
+    outdat["attrs"]["coord_sys"] = {"XYZ": "inst", "ENU": "earth", "beam": "beam"}[cy]
 
     # Copy appropriate vars to rotate_vars
-    for ky in ['accel', 'angrt', 'mag']:
-        for dky in outdat['data_vars'].keys():
-            if dky == ky or dky.startswith(ky + '_'):
-                outdat['attrs']['rotate_vars'].append(dky)
-    if 'vel_bt' in outdat['data_vars']:
-        outdat['attrs']['rotate_vars'].append('vel_bt')
-    if 'vel_avg' in outdat['data_vars']:
-        outdat['attrs']['rotate_vars'].append('vel_avg')
+    for ky in ["accel", "angrt", "mag"]:
+        for dky in outdat["data_vars"].keys():
+            if dky == ky or dky.startswith(ky + "_"):
+                outdat["attrs"]["rotate_vars"].append(dky)
+    if "vel_bt" in outdat["data_vars"]:
+        outdat["attrs"]["rotate_vars"].append("vel_bt")
+    if "vel_avg" in outdat["data_vars"]:
+        outdat["attrs"]["rotate_vars"].append("vel_avg")
 
     return outdat
 
 
+def _clean_dp_skips(data):
+    """
+    Removes zeros from interwoven measurements taken in a dual profile
+    configuration.
+    """
+
+    for id in data:
+        if id == "filehead_config":
+            continue
+        # Check where 'ver' is zero (should be 1 (for bt) or 3 (everything else))
+        skips = np.where(data[id]["ver"] != 0)
+        for var in data[id]:
+            if var not in ["units", "long_name", "standard_name"]:
+                data[id][var] = np.squeeze(data[id][var][..., skips], axis=-2)
+
+
 def _reduce(data):
-    """This function takes the output from `reorg`, and further simplifies the
+    """
+    This function takes the output from `reorg`, and further simplifies the
     data. Mostly this is combining system, environmental, and orientation data
     --- from different data structures within the same ensemble --- by
     averaging.
     """
-    
-    dv = data['data_vars']
-    dc = data['coords']
-    da = data['attrs']
+
+    dv = data["data_vars"]
+    dc = data["coords"]
+    da = data["attrs"]
 
     # Average these fields
-    for ky in ['c_sound', 'temp', 'pressure',
-               'temp_press', 'temp_clock', 'batt']:
-        lib._reduce_by_average(dv, ky, ky + '_b5')
+    for ky in ["c_sound", "temp", "pressure", "temp_press", "temp_clock", "batt"]:
+        lib._reduce_by_average(dv, ky, ky + "_b5")
 
     # Angle-averaging is treated separately
-    for ky in ['heading', 'pitch', 'roll']:
-        lib._reduce_by_average_angle(dv, ky, ky + '_b5')
+    for ky in ["heading", "pitch", "roll"]:
+        lib._reduce_by_average_angle(dv, ky, ky + "_b5")
 
-    if 'vel' in dv:
-        dc['range'] = ((np.arange(dv['vel'].shape[1])+1) *
-                    da['cell_size'] +
-                    da['blank_dist'])
-        da['fs'] = da['filehead_config']['BURST']['SR']
-        tmat = da['filehead_config']['XFBURST']
-    if 'vel_avg' in dv:
-        dc['range_avg'] = ((np.arange(dv['vel_avg'].shape[1])+1) *
-                    da['cell_size_avg'] +
-                    da['blank_dist_avg'])
-        dv['orientmat'] = dv.pop('orientmat_avg')
-        tmat = da['filehead_config']['XFAVG']
-        da['fs'] = da['filehead_config']['PLAN']['MIAVG']
-        da['avg_interval_sec'] = da['filehead_config']['AVG']['AI']
-        da['bandwidth'] = da['filehead_config']['AVG']['BW']
-    if 'vel_b5' in dv:
-        dc['range_b5'] = ((np.arange(dv['vel_b5'].shape[1])+1) *
-                          da['cell_size_b5'] +
-                          da['blank_dist_b5'])
-    if 'echo_echo' in dv:
-        dv['echo'] = dv.pop('echo_echo')
-        dc['range_echo'] = ((np.arange(dv['echo'].shape[0])+1) *
-                            da['cell_size_echo'] +
-                            da['blank_dist_echo'])
+    if "vel" in dv:
+        dc["range"] = (np.arange(dv["vel"].shape[1]) + 1) * da["cell_size"] + da[
+            "blank_dist"
+        ]
+        da["fs"] = da["filehead_config"]["BURST"]["SR"]
+        tmat = da["filehead_config"]["XFBURST"]
+    if "vel_avg" in dv:
+        dc["range_avg"] = (np.arange(dv["vel_avg"].shape[1]) + 1) * da[
+            "cell_size_avg"
+        ] + da["blank_dist_avg"]
+        if "orientmat" not in dv:
+            dv["orientmat"] = dv.pop("orientmat_avg")
+        tmat = da["filehead_config"]["XFAVG"]
+        da["fs"] = da["filehead_config"]["PLAN"]["MIAVG"]
+        da["avg_interval_sec"] = da["filehead_config"]["AVG"]["AI"]
+        da["bandwidth"] = da["filehead_config"]["AVG"]["BW"]
+    if "vel_b5" in dv:
+        # vel_b5 is sometimes shape 2 and sometimes shape 3
+        dc["range_b5"] = (np.arange(dv["vel_b5"].shape[-2]) + 1) * da[
+            "cell_size_b5"
+        ] + da["blank_dist_b5"]
+    if "echo_echo" in dv:
+        dv["echo"] = dv.pop("echo_echo")
+        dc["range_echo"] = (np.arange(dv["echo"].shape[0]) + 1) * da[
+            "cell_size_echo"
+        ] + da["blank_dist_echo"]
 
-    if 'orientmat' in data['data_vars']:
-        da['has_imu'] = 1  # logical
+    if "orientmat" in data["data_vars"]:
+        da["has_imu"] = 1  # logical
         # Signature AHRS rotation matrix returned in "inst->earth"
         # Change to dolfyn's "earth->inst"
-        dv['orientmat'] = np.rollaxis(dv['orientmat'], 1)
+        dv["orientmat"] = np.rollaxis(dv["orientmat"], 1)
     else:
-        da['has_imu'] = 0
+        da["has_imu"] = 0
 
-    theta = da['filehead_config']['BEAMCFGLIST'][0]
-    if 'THETA=' in theta:
-        da['beam_angle'] = int(theta[13:15])
-    
-    tm = np.zeros((tmat['ROWS'], tmat['COLS']), dtype=np.float32)
-    for irow in range(tmat['ROWS']):
-        for icol in range(tmat['COLS']):
-            tm[irow, icol] = tmat['M' + str(irow + 1) + str(icol + 1)]
-    dv['beam2inst_orientmat'] = tm
+    theta = da["filehead_config"]["BEAMCFGLIST"][0]
+    if "THETA=" in theta:
+        da["beam_angle"] = int(theta[13:15])
+
+    tm = np.zeros((tmat["ROWS"], tmat["COLS"]), dtype=np.float32)
+    for irow in range(tmat["ROWS"]):
+        for icol in range(tmat["COLS"]):
+            tm[irow, icol] = tmat["M" + str(irow + 1) + str(icol + 1)]
+    dv["beam2inst_orientmat"] = tm
 
     # If burst velocity isn't used, need to copy one for 'time'
-    if 'time' not in dc:
+    if "time" not in dc:
         for val in dc:
-            if 'time' in val:
+            if "time" in val:
                 time = val
-        dc['time'] = dc[time]
+        dc["time"] = dc[time]
+
+
+def split_dp_datasets(ds):
+    """
+    Splits a dataset containing dual profiles into individual profiles
+    """
+
+    # Figure out which variables belong to which profile based on length of time variables
+    t_dict = {}
+    for t in ds.coords:
+        if "time" in t:
+            t_dict[t] = ds[t].size
+
+    other_coords = []
+    for key, val in t_dict.items():
+        if val != t_dict["time"]:
+            if key.endswith("altraw"):
+                # altraw goes with burst, altraw_avg goes with avg
+                continue
+            other_coords.append(key)
+    # Fetch variables, coordinates, and attrs for second profiling configuration
+    other_vars = [
+        v for v in ds.data_vars if any(x in ds[v].coords for x in other_coords)
+    ]
+    other_tags = [s.split("_")[-1] for s in other_coords]
+    other_coords += [v for v in ds.coords if any(x in v for x in other_tags)]
+    other_attrs = [s for s in ds.attrs if any(x in s for x in other_tags)]
+    critical_attrs = [
+        "inst_model",
+        "inst_make",
+        "inst_type",
+        "fs",
+        "orientation",
+        "orient_status",
+        "has_imu",
+        "beam_angle",
+    ]
+
+    # Create second dataset
+    ds2 = type(ds)()
+    for a in other_attrs + critical_attrs:
+        ds2.attrs[a] = ds.attrs[a]
+    for v in other_vars:
+        ds2[v] = ds[v]
+    # Set rotate_vars
+    rotate_vars2 = [v for v in ds.attrs["rotate_vars"] if v in other_vars]
+    ds2.attrs["rotate_vars"] = rotate_vars2
+    # Set orientation matricies
+    ds2["beam2inst_orientmat"] = ds["beam2inst_orientmat"]
+    ds2 = ds2.rename({"orientmat_" + other_tags[0]: "orientmat"})
+    # Set original coordinate system
+    cy = ds2.attrs["coord_sys_axes_" + other_tags[0]]
+    ds2.attrs["coord_sys"] = {"XYZ": "inst", "ENU": "earth", "beam": "beam"}[cy]
+    ds2 = _set_coords(ds2, ref_frame=ds2.coord_sys)
+
+    # Clean up first dataset
+    [ds.attrs.pop(ky) for ky in other_attrs]
+    ds = ds.drop_vars(other_vars + other_coords)
+    for itm in rotate_vars2:
+        ds.attrs["rotate_vars"].remove(itm)
+
+    return ds, ds2
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/io/nortek2_lib.py` & `mhkit-0.8.0/mhkit/dolfyn/io/nortek2_lib.py`

 * *Files 15% similar despite different names*

```diff
@@ -22,189 +22,263 @@
     # Average two arrays of angles together, if they both exist.
     if degrees:
         rad_fact = np.pi / 180
     else:
         rad_fact = 1
     if ky1 in data:
         if ky0 in data:
-            data[ky0] = np.angle(
-                np.exp(1j * data.pop(ky0) * rad_fact) +
-                np.exp(1j * data.pop(ky1) * rad_fact)) / rad_fact
+            data[ky0] = (
+                np.angle(
+                    np.exp(1j * data.pop(ky0) * rad_fact)
+                    + np.exp(1j * data.pop(ky1) * rad_fact)
+                )
+                / rad_fact
+            )
         else:
             data[ky0] = data.pop(ky1)
 
 
 # This is the data-type of the index file.
 # This must match what is written-out by the create_index function.
 _index_version = 1
-_hdr = struct.Struct('<BBBBhhh')
+_hdr = struct.Struct("<BBBBhhh")
 _index_dtype = {
-    None:
-    np.dtype([('ens', np.uint64),
-              ('pos', np.uint64),
-              ('ID', np.uint16),
-              ('config', np.uint16),
-              ('beams_cy', np.uint16),
-              ('_blank', np.uint16),
-              ('year', np.uint8),
-              ('month', np.uint8),
-              ('day', np.uint8),
-              ('hour', np.uint8),
-              ('minute', np.uint8),
-              ('second', np.uint8),
-              ('usec100', np.uint16),
-              ]),
-    1:
-    np.dtype([('ens', np.uint64),
-              ('hw_ens', np.uint32),
-              ('pos', np.uint64),
-              ('ID', np.uint16),
-              ('config', np.uint16),
-              ('beams_cy', np.uint16),
-              ('_blank', np.uint16),
-              ('year', np.uint8),
-              ('month', np.uint8),
-              ('day', np.uint8),
-              ('hour', np.uint8),
-              ('minute', np.uint8),
-              ('second', np.uint8),
-              ('usec100', np.uint16),
-              ('d_ver', np.uint8),
-              ])
+    None: np.dtype(
+        [
+            ("ens", np.uint64),
+            ("pos", np.uint64),
+            ("ID", np.uint16),
+            ("config", np.uint16),
+            ("beams_cy", np.uint16),
+            ("_blank", np.uint16),
+            ("year", np.uint8),
+            ("month", np.uint8),
+            ("day", np.uint8),
+            ("hour", np.uint8),
+            ("minute", np.uint8),
+            ("second", np.uint8),
+            ("usec100", np.uint16),
+        ]
+    ),
+    1: np.dtype(
+        [
+            ("ens", np.uint64),
+            ("hw_ens", np.uint32),
+            ("pos", np.uint64),
+            ("ID", np.uint16),
+            ("config", np.uint16),
+            ("beams_cy", np.uint16),
+            ("_blank", np.uint16),
+            ("year", np.uint8),
+            ("month", np.uint8),
+            ("day", np.uint8),
+            ("hour", np.uint8),
+            ("minute", np.uint8),
+            ("second", np.uint8),
+            ("usec100", np.uint16),
+            ("d_ver", np.uint8),
+        ]
+    ),
 }
 
 
 def _calc_time(year, month, day, hour, minute, second, usec, zero_is_bad=True):
-    dt = np.zeros(year.shape, dtype='float')
+    dt = np.zeros(year.shape, dtype="float")
     for idx, (y, mo, d, h, mi, s, u) in enumerate(
-            zip(year, month, day,
-                hour, minute, second, usec)):
-        if (zero_is_bad and mo == 0 and d == 0 and
-                h == 0 and mi == 0 and
-                s == 0 and u == 0):
+        zip(year, month, day, hour, minute, second, usec)
+    ):
+        if (
+            zero_is_bad
+            and mo == 0
+            and d == 0
+            and h == 0
+            and mi == 0
+            and s == 0
+            and u == 0
+        ):
             continue
         try:
             # Note that month is zero-based, seconds since Jan 1 1970
-            dt[idx] = time.date2epoch(
-                time.datetime(y, mo + 1, d, h, mi, s, u))[0]
+            dt[idx] = time.date2epoch(time.datetime(y, mo + 1, d, h, mi, s, u))[0]
         except ValueError:
             # One of the time values is out-of-range (e.g., mi > 60)
             # This probably indicates a corrupted byte, so we just insert None.
             dt[idx] = None
     # None -> NaN in this step
     return dt
 
 
-def _create_index(infile, outfile, N_ens, debug):
+def _create_index(infile, outfile, init_pos, eof, debug):
     logging = getLogger()
-    print("Indexing {}...".format(infile), end='')
-    fin = open(_abspath(infile), 'rb')
-    fout = open(_abspath(outfile), 'wb')
-    fout.write(b'Index Ver:')
-    fout.write(struct.pack('<H', _index_version))
-    ids = [21, 22, 23, 24, 26, 28, 
-           27, 29, 30, 31, 35, 36]  
+    print("Indexing {}...".format(infile), end="")
+    fin = open(_abspath(infile), "rb")
+    fout = open(_abspath(outfile), "wb")
+    fout.write(b"Index Ver:")
+    fout.write(struct.pack("<H", _index_version))
+    ids = [21, 22, 23, 24, 26, 28, 27, 29, 30, 31, 35, 36]
     # Saved: burst, avg, bt, vel_b5, alt_raw, echo
     # Not saved: bt record, DVL, alt record, avg alt_raw record, raw echo, raw echo transmit
     ens = dict.fromkeys(ids, 0)
     N = dict.fromkeys(ids, 0)
     config = 0
     last_ens = dict.fromkeys(ids, -1)
-    seek_2ens = {21:40, 22:40, 23:42, 24:40, 26:40, 28:40, # 23 starts from "42"
-                 27:40, 29:40, 30:40, 31:40, 35:40, 36:40}
-    while N[21] < N_ens:  # Will fail if velocity ping isn't saved first
+    seek_2ens = {
+        21: 40,
+        22: 40,
+        23: 42,
+        24: 40,
+        26: 40,
+        28: 40,  # 23 starts from "42"
+        27: 40,
+        29: 40,
+        30: 40,
+        31: 40,
+        35: 40,
+        36: 40,
+    }
+    pos = 0
+    while pos <= eof:
         pos = fin.tell()
+        if init_pos and not pos:
+            fin.seek(init_pos, 1)
         try:
             dat = _hdr.unpack(fin.read(_hdr.size))
         except:
             break
-        if dat[2] in ids:  
+        if dat[2] in ids:
             idk = dat[2]
-            d_ver, d_off, config = struct.unpack('<BBH', fin.read(4))
+            d_ver, d_off, config = struct.unpack("<BBH", fin.read(4))
+            if d_ver not in [1, 3]:
+                # 1 for bottom track, 3 for all others
+                continue
             fin.seek(4, 1)
-            yr, mo, dy, h, m, s, u = struct.unpack('6BH', fin.read(8))
+            yr, mo, dy, h, m, s, u = struct.unpack("6BH", fin.read(8))
             fin.seek(14, 1)
-            beams_cy = struct.unpack('<H', fin.read(2))[0]
+            beams_cy = struct.unpack("<H", fin.read(2))[0]
             fin.seek(seek_2ens[dat[2]], 1)
-            ens[idk] = struct.unpack('<I', fin.read(4))[0]
+            ens[idk] = struct.unpack("<I", fin.read(4))[0]
 
-            if ens[idk] == 1 and last_ens[idk] > 0:
-                # Covers all id keys saved in "burst mode"
-                ens[idk] = last_ens[idk]+1
+            if last_ens[idk] > 0:
+                if (ens[idk] == 1) or (ens[idk] < last_ens[idk]):
+                    # Covers all id keys saved in "burst mode"
+                    # Covers ID keys not saved in sequential order
+                    ens[idk] = last_ens[idk] + 1
 
             if last_ens[idk] > 0 and last_ens[idk] != ens[idk]:
                 N[idk] += 1
 
-            fout.write(struct.pack('<QIQ4H6BHB', N[idk], ens[idk], pos, idk,
-                                   config, beams_cy, 0,
-                                   yr, mo + 1, dy, h, m, s, u, d_ver))
+            fout.write(
+                struct.pack(
+                    "<QIQ4H6BHB",
+                    N[idk],
+                    ens[idk],
+                    pos,
+                    idk,
+                    config,
+                    beams_cy,
+                    0,
+                    yr,
+                    mo + 1,
+                    dy,
+                    h,
+                    m,
+                    s,
+                    u,
+                    d_ver,
+                )
+            )
             fin.seek(dat[4] - (36 + seek_2ens[idk]), 1)
             last_ens[idk] = ens[idk]
 
-            if debug and N[idk] < 5:
+            if debug:
+                # File Position: Valid ID keys (1A, 10), Hex ID, Length in bytes, Ensemble #, Last Ensemble Found'
                 # hex: [18, 15, 1C, 17] = [vel_b5, vel, echo, bt]
-                logging.info('%10d: %02X, %d, %02X, %d, %d, %d, %d\n' %
-                             (pos, dat[0], dat[1], dat[2], dat[4],
-                              N[idk], ens[idk], last_ens[idk]))
+                logging.info(
+                    "%10d: %02X, %d, %02X, %d, %d, %d, %d\n"
+                    % (
+                        pos,
+                        dat[0],
+                        dat[1],
+                        dat[2],
+                        dat[4],
+                        N[idk],
+                        ens[idk],
+                        last_ens[idk],
+                    )
+                )
         else:
+            if dat[4] < 0:
+                if debug:
+                    logging.info("Invalid skip byte at pos: %10d\n" % (pos))
+                break
             fin.seek(dat[4], 1)
     fin.close()
     fout.close()
     print(" Done.")
 
 
-def _check_index(idx, infile, fix_hw_ens=False):
-    uid = np.unique(idx['ID'])
+def _check_index(idx, infile, fix_hw_ens=False, dp=False):
+    uid = np.unique(idx["ID"])
     if fix_hw_ens:
-        hwe = idx['hw_ens']
+        hwe = idx["hw_ens"]
     else:
-        hwe = idx['hw_ens'].copy()
+        hwe = idx["hw_ens"].copy()
     period = hwe.max()
-    ens = idx['ens']
+    ens = idx["ens"]
     N_id = len(uid)
     FLAG = False
+
+    # Are there better ways to detect dual profile?
+    if (21 in uid) and (22 in uid):
+        warnings.warn("Dual Profile detected... Two datasets will be returned.")
+        dp = True
+
     # This loop fixes 'skips' inside the file
     for id in uid:
         # These are the indices for this ID
-        inds = np.nonzero(idx['ID'] == id)[0]
-
+        inds = np.nonzero(idx["ID"] == id)[0]
         # These are bad steps in the indices for this ID
         ibad = np.nonzero(np.diff(inds) > N_id)[0]
+        # Check if spacing is equal for dual profiling ADCPs
+        if dp:
+            skip_size = np.diff(ibad)
+            n_skip, count = np.unique(skip_size, return_counts=True)
+            # If multiple skips are of the same size, assume okay
+            for n, c in zip(n_skip, count):
+                if c > 1:
+                    skip_size[skip_size == n] = 0
+            # assume last "ibad" element is always good for dp's
+            mask = np.append(skip_size, 0).astype(bool) if any(skip_size) else []
+            ibad = ibad[mask]
         for ib in ibad:
             FLAG = True
             # The ping number reported here may not be quite right if
             # the ensemble count is wrong.
-            warnings.warn("Skipped ping (ID: {}) in file {} at ensemble {}."
-                          .format(id, infile, idx['ens'][inds[ib + 1] - 1]))
-            hwe[inds[(ib + 1):]] += 1
-            ens[inds[(ib + 1):]] += 1
-
-    # This block fixes skips that originate from before this file.
-    delta = max(hwe[:N_id]) - hwe[:N_id]
-    for d, id in zip(delta, idx['ID'][:N_id]):
-        if d != 0:
-            FLAG = True
-            hwe[id == idx['ID']] += d
-            ens[id == idx['ID']] += d
+            warnings.warn(
+                "Skipped ping (ID: {}) in file {} at ensemble {}.".format(
+                    id, infile, idx["ens"][inds[ib + 1] - 1]
+                )
+            )
+            hwe[inds[(ib + 1) :]] += 1
+            ens[inds[(ib + 1) :]] += 1
 
-    if np.any(np.diff(ens) > 1) and FLAG:
-        idx['ens'] = np.unwrap(hwe.astype(np.int64), period=period) - hwe[0]
+    return dp
 
 
 def _boolarray_firstensemble_ping(index):
     """
-    Return a boolean of the index that indicates only the first ping in 
+    Return a boolean of the index that indicates only the first ping in
     each ensemble.
     """
-    dens = np.ones(index['ens'].shape, dtype='bool')
-    dens[1:] = np.diff(index['ens']) != 0
+    dens = np.ones(index["ens"].shape, dtype="bool")
+    dens[1:] = np.diff(index["ens"]) != 0
     return dens
 
 
-def get_index(infile, reload=False, debug=False):
+def get_index(infile, pos=0, eof=2**32, rebuild=False, debug=False, dp=False):
     """
     This function reads ad2cp.index files
 
     Parameters
     ----------
     infile: str
       Path and filename of ad2cp datafile, not including ".index"
@@ -215,29 +289,29 @@
 
     Returns
     -------
     out: tuple
       Tuple containing info held within index file
     """
 
-    index_file = infile + '.index'
-    if not path.isfile(index_file) or reload:
-        _create_index(infile, index_file, 2 ** 32, debug)
-    f = open(_abspath(index_file), 'rb')
+    index_file = infile + ".index"
+    if not path.isfile(index_file) or rebuild or debug:
+        _create_index(infile, index_file, pos, eof, debug)
+    f = open(_abspath(index_file), "rb")
     file_head = f.read(12)
-    if file_head[:10] == b'Index Ver:':
-        index_ver = struct.unpack('<H', file_head[10:])[0]
+    if file_head[:10] == b"Index Ver:":
+        index_ver = struct.unpack("<H", file_head[10:])[0]
     else:
         # This is pre-versioning the index files
         index_ver = None
         f.seek(0, 0)
     out = np.fromfile(f, dtype=_index_dtype[index_ver])
     f.close()
-    _check_index(out, infile)
-    return out
+    dp = _check_index(out, infile, dp=dp)
+    return out, dp
 
 
 def crop_ensembles(infile, outfile, range):
     """
     This function is for cropping certain pings out of an AD2CP
     file to create a new AD2CP file. It properly grabs the header from
     infile.
@@ -251,63 +325,67 @@
       Path of ad2cp filename (with .ad2cp file extension)
     outfile: str
       Path for new, cropped ad2cp file (with .ad2cp file extension)
     range: list
       2 element list of start and end ensemble (or time index)
     """
 
-    idx = get_index(infile)
-    with open(_abspath(infile), 'rb') as fin:
-        with open(_abspath(outfile), 'wb') as fout:
-            fout.write(fin.read(idx['pos'][0]))
-            i0 = np.nonzero(idx['ens'] == range[0])[0][0]
-            ie = np.nonzero(idx['ens'] == range[1])[0][0]
-            pos = idx['pos'][i0]
-            nbyte = idx['pos'][ie] - pos
+    idx, dp = get_index(infile)
+    with open(_abspath(infile), "rb") as fin:
+        with open(_abspath(outfile), "wb") as fout:
+            fout.write(fin.read(idx["pos"][0]))
+            i0 = np.nonzero(idx["ens"] == range[0])[0][0]
+            ie = np.nonzero(idx["ens"] == range[1])[0][0]
+            pos = idx["pos"][i0]
+            nbyte = idx["pos"][ie] - pos
             fin.seek(pos, 0)
             fout.write(fin.read(nbyte))
 
 
-class _BitIndexer():
+class _BitIndexer:
     def __init__(self, data):
         self.data = data
 
     @property
-    def _data_is_array(self, ):
+    def _data_is_array(
+        self,
+    ):
         return isinstance(self.data, np.ndarray)
 
     @property
-    def nbits(self, ):
+    def nbits(
+        self,
+    ):
         if self._data_is_array:
             return self.data.dtype.itemsize * 8
         else:
-            raise ValueError("You must specify the end-range "
-                             "for non-ndarray input data.")
+            raise ValueError(
+                "You must specify the end-range " "for non-ndarray input data."
+            )
 
     def _get_out_type(self, mask):
         # The mask indicates how big this item is.
         if not self._data_is_array:
             return None
         if mask < 2:
             return bool
-        if mask < 2 ** 8:
+        if mask < 2**8:
             return np.uint8
-        elif mask < 2 ** 16:
+        elif mask < 2**16:
             return np.uint16
-        elif mask < 2 ** 32:
+        elif mask < 2**32:
             return np.uint32
         else:
             return np.uint64
 
     def __getitem__(self, slc):
         if isinstance(slc, int):
             slc = slice(slc, slc + 1)
         if slc.step not in [1, None]:
-            raise ValueError("Slice syntax for `_getbits` does "
-                             "not support steps")
+            raise ValueError("Slice syntax for `_getbits` does " "not support steps")
         start = slc.start
         stop = slc.stop
         if start is None:
             start = 0
         if stop is None:
             stop = self.nbits
         mask = 2 ** (stop - start) - 1
@@ -318,42 +396,42 @@
         return out
 
 
 def _getbit(val, n):
     return bool((val >> n) & 1)
 
 
-def _headconfig_int2dict(val, mode='burst'):
+def _headconfig_int2dict(val, mode="burst"):
     """
     Convert the burst Configuration bit-mask to a dict of bools.
 
     mode: {'burst', 'bt'}
        For 'burst' configs, or 'bottom-track' configs.
     """
 
-    if (mode == 'burst') or (mode == 'avg'):
+    if (mode == "burst") or (mode == "avg"):
         return dict(
             press_valid=_getbit(val, 0),
             temp_valid=_getbit(val, 1),
             compass_valid=_getbit(val, 2),
             tilt_valid=_getbit(val, 3),
             # bit 4 is unused
             vel=_getbit(val, 5),
             amp=_getbit(val, 6),
             corr=_getbit(val, 7),
-            alt=_getbit(val, 8),
-            alt_raw=_getbit(val, 9),
+            le=_getbit(val, 8),
+            altraw=_getbit(val, 9),
             ast=_getbit(val, 10),
             echo=_getbit(val, 11),
             ahrs=_getbit(val, 12),
             p_gd=_getbit(val, 13),
             std=_getbit(val, 14),
             # bit 15 is unused
         )
-    elif mode == 'bt':
+    elif mode == "bt":
         return dict(
             press_valid=_getbit(val, 0),
             temp_valid=_getbit(val, 1),
             compass_valid=_getbit(val, 2),
             tilt_valid=_getbit(val, 3),
             # bit 4 is unused
             vel=_getbit(val, 5),
@@ -367,66 +445,67 @@
 
 def _status02data(val):
     # This is detailed in the 6.1.2 of the Nortek Signature
     # Integrators Guide (2017)
     bi = _BitIndexer(val)
     out = {}
     if any(bi[15]):  # 'status0_in_use'
-        out['proc_idle_less_3pct'] = bi[0]
-        out['proc_idle_less_6pct'] = bi[1]
-        out['proc_idle_less_12pct'] = bi[2]
+        out["proc_idle_less_3pct"] = bi[0]
+        out["proc_idle_less_6pct"] = bi[1]
+        out["proc_idle_less_12pct"] = bi[2]
 
     return out
 
 
 def _status2data(val):
     # This is detailed in the 6.1.2 of the Nortek Signature
     # Integrators Guide (2017)
     bi = _BitIndexer(val)
     out = {}
-    out['wakeup_state'] = bi[28:32]
-    out['orient_up'] = bi[25:28]
-    out['auto_orientation'] = bi[22:25]
-    out['previous_wakeup_state'] = bi[18:22]
-    out['low_volt_skip'] = bi[17]
-    out['active_config'] = bi[16]
-    out['echo_index'] = bi[12:16]
-    out['telemetry_data'] = bi[11]
-    out['boost_running'] = bi[10]
-    out['echo_freq_bin'] = bi[5:10]
+    out["wakeup_state"] = bi[28:32]
+    out["orient_up"] = bi[25:28]
+    out["auto_orientation"] = bi[22:25]
+    out["previous_wakeup_state"] = bi[18:22]
+    out["low_volt_skip"] = bi[17]
+    out["active_config"] = bi[16]
+    out["echo_index"] = bi[12:16]
+    out["telemetry_data"] = bi[11]
+    out["boost_running"] = bi[10]
+    out["echo_freq_bin"] = bi[5:10]
     # 2,3,4 unused
-    out['bd_scaling'] = bi[1]  # if True: cm scaling of blanking dist
+    out["bd_scaling"] = bi[1]  # if True: cm scaling of blanking dist
     # 0 unused
     return out
 
 
 def _alt_status2data(val):
     # This is detailed in the 6.1.2 of the Nortek Signature
     # Integrators Guide (2017)
     bi = _BitIndexer(val)
     out = {}
-    out['tilt_over_5deg'] = bi[0]
-    out['tilt_over_10deg'] = bi[1]
-    out['multibeam_alt'] = bi[2]
-    out['n_beams_alt'] = bi[3:7]
-    out['power_level_idx_alt'] = bi[7:10]
+    out["tilt_over_5deg"] = bi[0]
+    out["tilt_over_10deg"] = bi[1]
+    out["multibeam_alt"] = bi[2]
+    out["n_beams_alt"] = bi[3:7]
+    out["power_level_idx_alt"] = bi[7:10]
 
     return out
 
 
 def _beams_cy_int2dict(val, id):
-    """Convert the beams/coordinate-system bytes to a dict of values.
-    """
+    """Convert the beams/coordinate-system bytes to a dict of values."""
     if id == 28:  # 0x1C (echosounder)
         return dict(n_cells=val)
-
+    elif id in [26, 31]:
+        return dict(n_cells=val & (2**10 - 1), cy="beam", n_beams=1)
     return dict(
-        n_cells=val & (2 ** 10 - 1),
-        cy=['ENU', 'XYZ', 'beam', None][val >> 10 & 3],
-        n_beams=val >> 12)
+        n_cells=val & (2**10 - 1),
+        cy=["ENU", "XYZ", "beam", None][val >> 10 & 3],
+        n_beams=val >> 12,
+    )
 
 
 def _isuniform(vec, exclude=[]):
     if len(exclude):
         return len(set(np.unique(vec)) - set(exclude)) <= 1
     return np.all(vec == vec[0])
 
@@ -438,66 +517,76 @@
     """
 
     if _isuniform(vec):
         return vec[0]
     elif _isuniform(vec, exclude=exclude):
         return list(set(np.unique(vec)) - set(exclude))[0]
     else:
-        uniq, idx, counts = np.unique(
-            vec, return_index=True, return_counts=True)
+        uniq, idx, counts = np.unique(vec, return_index=True, return_counts=True)
 
         if all(e == counts[0] for e in counts):
             val = max(vec)  # pings saved out of order, but equal # of pings
         else:
             val = vec[idx[np.argmax(counts)]]
 
         if not set(uniq) == set([0, val]) and set(counts) == set([1, np.max(counts)]):
             # warn when the 'wrong value' is not just a single zero.
-            warnings.warn("The variable {} is expected to be uniform, but it is not.\n"
-                          "Values found: {} (counts: {}).\n"
-                          "Using the most common value: {}".format(
-                              name, list(uniq), list(counts), val))
-    
+            warnings.warn(
+                "The variable {} is expected to be uniform, but it is not.\n"
+                "Values found: {} (counts: {}).\n"
+                "Using the most common value: {}".format(
+                    name, list(uniq), list(counts), val
+                )
+            )
+
         return val
 
 
 def _calc_config(index):
     """
     Calculate the configuration information (e.g., number of pings,
     number of beams, struct types, etc.) from the index data.
 
     Returns
     =======
     config : dict
         A dict containing the key information for initializing arrays.
     """
 
-    ids = np.unique(index['ID'])
+    ids = np.unique(index["ID"])
     config = {}
     for id in ids:
-        if id not in [21, 22, 23, 24, 26, 28]:
+        if id not in [21, 22, 23, 24, 26, 28, 31]:
             continue
         if id == 23:
-            type = 'bt'
-        elif id == 22:
-            type = 'avg'
+            type = "bt"
+        elif (id == 22) or (id == 31):
+            type = "avg"
         else:
-            type = 'burst'
-        inds = index['ID'] == id
-        _config = index['config'][inds]
-        _beams_cy = index['beams_cy'][inds]
+            type = "burst"
+        inds = index["ID"] == id
+        _config = index["config"][inds]
+        _beams_cy = index["beams_cy"][inds]
+
         # Check that these variables are consistent
         if not _isuniform(_config):
-            raise Exception("config are not identical for id: 0x{:X}."
-                            .format(id))
+            raise Exception("config are not identical for id: 0x{:X}.".format(id))
         if not _isuniform(_beams_cy):
-            raise Exception("beams_cy are not identical for id: 0x{:X}."
-                            .format(id))
+            err = True
+            if id == 23:
+                # change in "n_cells" doesn't matter
+                lob = np.unique(_beams_cy)
+                beams = list(map(_beams_cy_int2dict, lob, 23 * np.ones(lob.size)))
+                if all([d["cy"] for d in beams]) and all([d["n_beams"] for d in beams]):
+                    err = False
+            if err:
+                raise Exception("beams_cy are not identical for id: 0x{:X}.".format(id))
+
         # Now that we've confirmed they are the same:
         config[id] = _headconfig_int2dict(_config[0], mode=type)
         config[id].update(_beams_cy_int2dict(_beams_cy[0], id))
-        config[id]['_config'] = _config[0]
-        config[id]['_beams_cy'] = _beams_cy[0]
-        config[id]['type'] = type
-        config[id].pop('cy', None)
+        config[id]["_config"] = _config[0]
+        config[id]["_beams_cy"] = _beams_cy[0]
+        config[id]["type"] = type
+        config[id].pop("cy", None)
 
     return config
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/io/rdi.py` & `mhkit-0.8.0/mhkit/dolfyn/io/rdi.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,120 +10,127 @@
 from .base import _find_userdata, _create_dataset, _abspath
 from .. import time as tmlib
 from ..rotate.rdi import _calc_beam_orientmat, _calc_orientmat
 from ..rotate.base import _set_coords
 from ..rotate.api import set_declination
 
 
-def read_rdi(filename, userdata=None, nens=None, debug_level=-1,
-             vmdas_search=False, winriver=False, **kwargs):
+def read_rdi(
+    filename,
+    userdata=None,
+    nens=None,
+    debug_level=-1,
+    vmdas_search=False,
+    winriver=False,
+    **kwargs,
+):
     """
     Read a TRDI binary data file.
 
     Parameters
     ----------
     filename : string
       Filename of TRDI file to read.
     userdata : True, False, or string of userdata.json filename
       Whether to read the '<base-filename>.userdata.json' file. Default = True
     nens : None, int or 2-element tuple (start, stop)
-      Number of pings or ensembles to read from the file. 
+      Number of pings or ensembles to read from the file.
       Default is None, read entire file
     debug_level : int
       Debug level [0 - 2]. Default = -1
     vmdas_search : bool
       Search from the end of each ensemble for the VMDAS navigation
       block.  The byte offsets are sometimes incorrect. Default = False
     winriver : bool
-      If file is winriver or not. Automatically set by dolfyn, this is helpful 
+      If file is winriver or not. Automatically set by dolfyn, this is helpful
       for debugging. Default = False
 
     Returns
     -------
     ds : xarray.Dataset
       An xarray dataset from the binary instrument data
     """
     # Start debugger logging
     if debug_level >= 0:
         for handler in logging.root.handlers[:]:
             logging.root.removeHandler(handler)
         filepath = Path(filename)
-        logfile = filepath.with_suffix('.dolfyn.log')
-        logging.basicConfig(filename=str(logfile),
-                            filemode='w',
-                            level=logging.NOTSET,
-                            format='%(name)s - %(levelname)s - %(message)s')
+        logfile = filepath.with_suffix(".dolfyn.log")
+        logging.basicConfig(
+            filename=str(logfile),
+            filemode="w",
+            level=logging.NOTSET,
+            format="%(name)s - %(levelname)s - %(message)s",
+        )
 
     # Reads into a dictionary of dictionaries using netcdf naming conventions
     # Should be easier to debug
-    with _RDIReader(filename,
-                    debug_level=debug_level,
-                    vmdas_search=vmdas_search,
-                    winriver=winriver) as ldr:
-        datNB, datBB = ldr.load_data(nens=nens)
+    rdr = _RDIReader(
+        filename, debug_level=debug_level, vmdas_search=vmdas_search, winriver=winriver
+    )
+    datNB, datBB = rdr.load_data(nens=nens)
 
     dats = [dat for dat in [datNB, datBB] if dat is not None]
 
     # Read in userdata
     userdata = _find_userdata(filename, userdata)
     dss = []
     for dat in dats:
         for nm in userdata:
-            dat['attrs'][nm] = userdata[nm]
+            dat["attrs"][nm] = userdata[nm]
 
         # Pass one if only one ds returned
-        if not np.isfinite(dat['coords']['time'][0]):
+        if not np.isfinite(dat["coords"]["time"][0]):
             continue
 
         # GPS data not necessarily sampling at the same rate as ADCP DAQ.
-        if 'time_gps' in dat['coords']:
+        if "time_gps" in dat["coords"]:
             dat = _remove_gps_duplicates(dat)
 
         # Convert time coords to dt64
-        t_coords = [t for t in dat['coords'] if 'time' in t]
+        t_coords = [t for t in dat["coords"] if "time" in t]
         for ky in t_coords:
-            dat['coords'][ky] = tmlib.epoch2dt64(dat['coords'][ky])
+            dat["coords"][ky] = tmlib.epoch2dt64(dat["coords"][ky])
 
         # Convert time vars to dt64
-        t_data = [t for t in dat['data_vars'] if 'time' in t]
+        t_data = [t for t in dat["data_vars"] if "time" in t]
         for ky in t_data:
-            dat['data_vars'][ky] = tmlib.epoch2dt64(dat['data_vars'][ky])
+            dat["data_vars"][ky] = tmlib.epoch2dt64(dat["data_vars"][ky])
 
         # Create xarray dataset from upper level dictionary
         ds = _create_dataset(dat)
         ds = _set_coords(ds, ref_frame=ds.coord_sys)
 
         # Create orientation matrices
-        if 'beam2inst_orientmat' not in ds:
-            ds['beam2inst_orientmat'] = xr.DataArray(
-                _calc_beam_orientmat(ds.beam_angle,
-                                     ds.beam_pattern == 'convex'),
-                coords={'x1': [1, 2, 3, 4],
-                        'x2': [1, 2, 3, 4]},
-                dims=['x1', 'x2'],
-                attrs={'units': '1',
-                       'long_name': 'Rotation Matrix'})
+        if "beam2inst_orientmat" not in ds:
+            ds["beam2inst_orientmat"] = xr.DataArray(
+                _calc_beam_orientmat(ds.beam_angle, ds.beam_pattern == "convex"),
+                coords={"x1": [1, 2, 3, 4], "x2": [1, 2, 3, 4]},
+                dims=["x1", "x2"],
+                attrs={"units": "1", "long_name": "Rotation Matrix"},
+            )
 
-        if 'orientmat' not in ds:
-            ds['orientmat'] = _calc_orientmat(ds)
+        if "orientmat" not in ds:
+            ds["orientmat"] = _calc_orientmat(ds)
 
         # Check magnetic declination if provided via software and/or userdata
         _set_rdi_declination(ds, filename, inplace=True)
 
         # VMDAS applies gps correction on velocity in .ENX files only
-        if filename.rsplit('.')[-1] == 'ENX':
-            ds.attrs['vel_gps_corrected'] = 1
+        if filename.rsplit(".")[-1] == "ENX":
+            ds.attrs["vel_gps_corrected"] = 1
         else:  # (not ENR or ENS) or WinRiver files
-            ds.attrs['vel_gps_corrected'] = 0
+            ds.attrs["vel_gps_corrected"] = 0
 
         dss += [ds]
 
     if len(dss) == 2:
-        warnings.warn("\nTwo profiling configurations retrieved from file"
-                      "\nReturning first.")
+        warnings.warn(
+            "\nTwo profiling configurations retrieved from file" "\nReturning first."
+        )
 
     # Close handler
     if debug_level >= 0:
         for handler in logging.root.handlers[:]:
             logging.root.removeHandler(handler)
             handler.close()
 
@@ -133,1215 +140,1362 @@
 def _remove_gps_duplicates(dat):
     """
     Removes duplicate and nan timestamp values in 'time_gps' coordinate,
     and add hardware (ADCP DAQ) timestamp corresponding to GPS acquisition
     (in addition to the GPS unit's timestamp).
     """
 
-    dat['data_vars']['hdwtime_gps'] = dat['coords']['time']
+    dat["data_vars"]["hdwtime_gps"] = dat["coords"]["time"]
 
     # Remove duplicate timestamp values, if applicable
-    dat['coords']['time_gps'], idx = np.unique(dat['coords']['time_gps'],
-                                               return_index=True)
+    dat["coords"]["time_gps"], idx = np.unique(
+        dat["coords"]["time_gps"], return_index=True
+    )
     # Remove nan values, if applicable
-    nan = np.zeros(dat['coords']['time'].shape, dtype=bool)
-    if any(np.isnan(dat['coords']['time_gps'])):
-        nan = np.isnan(dat['coords']['time_gps'])
-        dat['coords']['time_gps'] = dat['coords']['time_gps'][~nan]
-
-    for key in dat['data_vars']:
-        if ('gps' in key) or ('nmea' in key):
-            dat['data_vars'][key] = dat['data_vars'][key][idx]
+    nan = np.zeros(dat["coords"]["time"].shape, dtype=bool)
+    if any(np.isnan(dat["coords"]["time_gps"])):
+        nan = np.isnan(dat["coords"]["time_gps"])
+        dat["coords"]["time_gps"] = dat["coords"]["time_gps"][~nan]
+
+    for key in dat["data_vars"]:
+        if ("gps" in key) or ("nmea" in key):
+            dat["data_vars"][key] = dat["data_vars"][key][idx]
             if sum(nan) > 0:
-                dat['data_vars'][key] = dat['data_vars'][key][~nan]
+                dat["data_vars"][key] = dat["data_vars"][key][~nan]
 
     return dat
 
 
 def _set_rdi_declination(dat, fname, inplace):
     """
     If magnetic_var_deg is set, this means that the declination is already
     included in the heading and in the velocity data.
     """
 
-    declin = dat.attrs.pop('declination', None)  # userdata declination
+    declin = dat.attrs.pop("declination", None)  # userdata declination
 
-    if dat.attrs['magnetic_var_deg'] != 0:  # from TRDI software if set
-        dat.attrs['declination'] = dat.attrs['magnetic_var_deg']
-        dat.attrs['declination_in_orientmat'] = 1  # logical
+    if dat.attrs["magnetic_var_deg"] != 0:  # from TRDI software if set
+        dat.attrs["declination"] = dat.attrs["magnetic_var_deg"]
+        dat.attrs["declination_in_orientmat"] = 1  # logical
 
-    if dat.attrs['magnetic_var_deg'] != 0 and declin is not None:
+    if dat.attrs["magnetic_var_deg"] != 0 and declin is not None:
         warnings.warn(
             "'magnetic_var_deg' is set to {:.2f} degrees in the binary "
             "file '{}', AND 'declination' is set in the 'userdata.json' "
             "file. DOLfYN WILL USE THE VALUE of {:.2f} degrees in "
             "userdata.json. If you want to use the value in "
             "'magnetic_var_deg', delete the value from userdata.json and "
-            "re-read the file."
-            .format(dat.attrs['magnetic_var_deg'], fname, declin))
-        dat.attrs['declination'] = declin
+            "re-read the file.".format(dat.attrs["magnetic_var_deg"], fname, declin)
+        )
+        dat.attrs["declination"] = declin
 
     if declin is not None:
         set_declination(dat, declin, inplace)
 
 
-class _RDIReader():
-    _pos = 0
-    progress = 0
-    _cfac = 180 / 2 ** 31
-    _source = 0
-    _fixoffset = 0
-    _nbyte = 0
-    _search_num = 30000  # Maximum distance? to search
-    _debug7f79 = None
-
-    def __init__(self, fname, navg=1, debug_level=0, vmdas_search=False, winriver=False):
+class _RDIReader:
+    def __init__(
+        self, fname, navg=1, debug_level=-1, vmdas_search=False, winriver=False
+    ):
         self.fname = _abspath(fname)
-        print('\nReading file {} ...'.format(fname))
+        print("\nReading file {} ...".format(fname))
         self._debug_level = debug_level
         self._vmdas_search = vmdas_search
         self._winrivprob = winriver
-        self.flag = 0
+        self._vm_source = 0
+        self._pos = 0
+        self.progress = 0
+        self._cfac = 180 / 2**31
+        self._fixoffset = 0
+        self._nbyte = 0
+        self.n_cells_diff = 0
+        self.n_cells_sl = 0
+        self.cs_diff = 0
+        self.cs = []
         self.cfg = {}
         self.cfgbb = {}
         self.hdr = {}
         self.f = bin_reader(self.fname)
 
         # Check header, double buffer, and get filesize
         self._filesize = getsize(self.fname)
         space = self.code_spacing()  # '0x7F'
-        self._npings = int(self._filesize / (space + 2))
-        if self._debug_level >= 0:
-            logging.info('Done: {}'.format(self.cfg))
-            logging.info('self._bb {}'.format(self._bb))
-            logging.info(self.cfgbb)
+        self._npings = self._filesize // space
+        if self._debug_level > -1:
+            logging.info("Done: {}".format(self.cfg))
+            logging.info("self._bb {}".format(self._bb))
+            logging.info("self.cfgbb: {}".format(self.cfgbb))
         self.f.seek(self._pos, 0)
         self.n_avg = navg
 
-        self.ensemble = defs._ensemble(self.n_avg, self.cfg['n_cells'])
+        self.ensemble = defs._ensemble(self.n_avg, self.cfg["n_cells"])
         if self._bb:
-            self.ensembleBB = defs._ensemble(self.n_avg, self.cfgbb['n_cells'])
+            self.ensembleBB = defs._ensemble(self.n_avg, self.cfgbb["n_cells"])
 
-        self.vars_read = defs._variable_setlist(['time'])
+        self.vars_read = defs._variable_setlist(["time"])
         if self._bb:
-            self.vars_readBB = defs._variable_setlist(['time'])
-
-        if self._debug_level >= 0:
-            logging.info('  %d pings estimated in this file' % self._npings)
+            self.vars_readBB = defs._variable_setlist(["time"])
 
     def code_spacing(self, iternum=50):
         """
         Returns the average spacing, in bytes, between pings.
         Repeat this * iternum * times(default 50).
         """
         fd = self.f
         p0 = self._pos
         # Get basic header data and check dual profile
         if not self.read_hdr():
-            raise RuntimeError('No header in this file')
+            raise RuntimeError("No header in this file")
         self._bb = self.check_for_double_buffer()
 
         # Turn off debugging to check code spacing
         debug_level = self._debug_level
         self._debug_level = -1
         for i in range(iternum):
             try:
                 self.read_hdr()
             except:
                 break
         # Compute the average of the data size:
-        size = (self._pos - p0) / (i+1) * 0.995
+        size = (self._pos - p0) / (i + 1)
         self.f = fd
         self._pos = p0
         self._debug_level = debug_level
         return size
 
-    def read_hdr(self,):
-        fd = self.f
-        cfgid = list(fd.read_ui8(2))
-        nread = 0
-        if self._debug_level >= 0:
-            logging.info('pos {}'.format(self.f.pos))
-            logging.info('cfgid0: [{:x}, {:x}]'.format(*cfgid))
-        while (cfgid[0] != 127 or cfgid[1] != 127) or not self.checkheader():
-            nextbyte = fd.read_ui8(1)
-            if nextbyte is None:
-                return False
-            pos = fd.tell()
-            nread += 1
-            cfgid[1] = cfgid[0]
-            cfgid[0] = nextbyte
-            if not pos % 1000:
-                if self._debug_level >= 0:
-                    logging.info('  Still looking for valid cfgid at file '
-                                 'position %d ...' % pos)
+    def read_hdr(self):
+        """
+        Scan file until 7f7f is found
+        """
+        if not self.search_buffer():
+            return False
         self._pos = self.f.tell() - 2
         self.read_hdrseg()
         return True
 
-    def check_for_double_buffer(self,):
+    def read_hdrseg(self):
+        fd = self.f
+        hdr = self.hdr
+        hdr["nbyte"] = fd.read_i16(1)
+        spare = fd.read_ui8(1)
+        ndat = fd.read_ui8(1)
+        hdr["dat_offsets"] = fd.read_ui16(ndat)
+        self._nbyte = 4 + ndat * 2
+
+    def check_for_double_buffer(self):
         """
         VMDAS will record two buffers in NB or NB/BB mode, so we need to
         figure out if that is happening here
         """
         found = False
         pos = self.f.pos
-        if self._debug_level >= 0:
+        if self._debug_level > -1:
             logging.info(self.hdr)
-            logging.info('pos {}'.format(pos))
+            logging.info("pos {}".format(pos))
         self.id_positions = {}
-        for offset in self.hdr['dat_offsets']:
-            self.f.seek(offset+pos - self.hdr['dat_offsets'][0], rel=0)
+        for offset in self.hdr["dat_offsets"]:
+            self.f.seek(offset + pos - self.hdr["dat_offsets"][0], rel=0)
             id = self.f.read_ui16(1)
             self.id_positions[id] = offset
-            if self._debug_level >= 0:
-                logging.info('pos {} id {}'.format(offset, id))
+            if self._debug_level > -1:
+                logging.info("id {} offset {}".format(id, offset))
             if id == 1:
                 self.read_fixed(bb=True)
                 found = True
             elif id == 0:
                 self.read_fixed(bb=False)
             elif id == 16:
                 self.read_fixed_sl()  # bb=True
             elif id == 8192:
                 self._vmdas_search = True
         return found
 
-    def mean(self, dat):
-        if self.n_avg == 1:
-            return dat[..., 0]
-        return np.nanmean(dat, axis=-1)
-
     def load_data(self, nens=None):
         if nens is None:
-            self._nens = int(self._npings / self.n_avg)
-        elif (nens.__class__ is tuple or nens.__class__ is list):
+            # Attempt to overshoot WinRiver2 or *Pro filesize
+            if (self.cfg["coord_sys"] == "ship") or (
+                self.cfg["inst_model"]
+                in [
+                    "RiverPro",
+                    "StreamPro",
+                ]
+            ):
+                self._nens = int(self._filesize / self.hdr["nbyte"] / self.n_avg * 1.1)
+            else:
+                # Attempt to overshoot other instrument filesizes
+                self._nens = int(self._npings / self.n_avg)
+        elif nens.__class__ is tuple or nens.__class__ is list:
             raise Exception("    `nens` must be a integer")
         else:
             self._nens = nens
-        if self._debug_level >= 0:
-            logging.info('  taking data from pings 0 - %d' % self._nens)
-            logging.info('  %d ensembles will be produced.\n' % self._nens)
+        if self._debug_level > -1:
+            logging.info("  taking data from pings 0 - %d" % self._nens)
+            logging.info("  %d ensembles will be produced.\n" % self._nens)
         self.init_data()
 
         for iens in range(self._nens):
             if not self.read_buffer():
                 self.remove_end(iens)
                 break
             self.ensemble.clean_data()
             if self._bb:
                 self.ensembleBB.clean_data()
             ens = [self.ensemble]
             vars = [self.vars_read]
             datl = [self.outd]
+            cfgl = [self.cfg]
             if self._bb:
                 ens += [self.ensembleBB]
                 vars += [self.vars_readBB]
                 datl += [self.outdBB]
+                cfgl += [self.cfgbb]
 
             for var, en, dat in zip(vars, ens, datl):
+                for nm in var:
+                    dat = self.save_profiles(dat, nm, en, iens)
+                # reset flag after all variables run
+                self.n_cells_diff = 0
+
+                # Set clock
                 clock = en.rtc[:, :]
                 if clock[0, 0] < 100:
                     clock[0, :] += defs.century
-
-                for nm in var:
-                    # If n_cells has increased (WinRiver transects)
-                    ds = defs._get(dat, nm)
-                    bn = self.mean(en[nm])
-                    # Check that
-                    # 1. n_cells has changed,
-                    # 2. nm is a beam variable
-                    # 3. n_cells is greater than any previous
-                    if self.flag > 0 and len(ds.shape) == 3 and (ds.shape[0] != bn.shape[0]):
-                        # increase the size of original dataset
-                        a = np.empty(
-                            (self.flag, ds.shape[1], ds.shape[2]))*np.nan
-                        ds = np.append(ds, a, axis=0)
-                        defs._setd(dat, nm, ds)
-                    # Copy the ensemble to the dataset.
-                    ds[..., iens] = bn
-                # reset after all variables run
-                self.flag = 0
-
                 try:
                     dates = tmlib.date2epoch(
-                        tmlib.datetime(*clock[:6, 0],
-                                       microsecond=clock[6, 0] * 10000))[0]
+                        tmlib.datetime(*clock[:6, 0], microsecond=clock[6, 0] * 10000)
+                    )[0]
                 except ValueError:
-                    warnings.warn("Invalid time stamp in ping {}.".format(
-                        int(self.ensemble.number[0])))
-                    dat['coords']['time'][iens] = np.NaN
+                    warnings.warn(
+                        "Invalid time stamp in ping {}.".format(
+                            int(self.ensemble.number[0])
+                        )
+                    )
+                    dat["coords"]["time"][iens] = np.NaN
                 else:
-                    dat['coords']['time'][iens] = np.median(dates)
-
-        self.cleanup(self.cfg, self.outd)
-        if self._bb:
-            self.cleanup(self.cfgbb, self.outdBB)
+                    dat["coords"]["time"][iens] = np.median(dates)
 
         # Finalize dataset (runs through both nb and bb)
-        for dat in datl:
-            self.finalize(dat)
-            if 'vel_bt' in dat['data_vars']:
-                dat['attrs']['rotate_vars'].append('vel_bt')
+        for dat, cfg in zip(datl, cfgl):
+            dat, cfg = self.cleanup(dat, cfg)
+            dat = self.finalize(dat)
+            if "vel_bt" in dat["data_vars"]:
+                dat["attrs"]["rotate_vars"].append("vel_bt")
 
-        dat = self.outd
         datbb = self.outdBB if self._bb else None
-        return dat, datbb
+        return self.outd, datbb
 
-    def init_data(self,):
-        outd = {'data_vars': {}, 'coords': {},
-                'attrs': {}, 'units': {}, 'long_name': {},
-                'standard_name': {}, 'sys': {}}
-        outd['attrs']['inst_make'] = 'TRDI'
-        outd['attrs']['inst_type'] = 'ADCP'
-        outd['attrs']['rotate_vars'] = ['vel', ]
+    def init_data(self):
+        outd = {
+            "data_vars": {},
+            "coords": {},
+            "attrs": {},
+            "units": {},
+            "long_name": {},
+            "standard_name": {},
+            "sys": {},
+        }
+        outd["attrs"]["inst_make"] = "TRDI"
+        outd["attrs"]["inst_type"] = "ADCP"
+        outd["attrs"]["rotate_vars"] = [
+            "vel",
+        ]
         # Currently RDI doesn't use IMUs
-        outd['attrs']['has_imu'] = 0
+        outd["attrs"]["has_imu"] = 0
         if self._bb:
-            outdbb = {'data_vars': {}, 'coords': {},
-                      'attrs': {}, 'units': {}, 'long_name': {},
-                      'standard_name': {}, 'sys': {}}
-            outdbb['attrs']['inst_make'] = 'TRDI'
-            outdbb['attrs']['inst_type'] = 'ADCP'
-            outdbb['attrs']['rotate_vars'] = ['vel', ]
-            outdbb['attrs']['has_imu'] = 0
+            outdbb = {
+                "data_vars": {},
+                "coords": {},
+                "attrs": {},
+                "units": {},
+                "long_name": {},
+                "standard_name": {},
+                "sys": {},
+            }
+            outdbb["attrs"]["inst_make"] = "TRDI"
+            outdbb["attrs"]["inst_type"] = "ADCP"
+            outdbb["attrs"]["rotate_vars"] = [
+                "vel",
+            ]
+            outdbb["attrs"]["has_imu"] = 0
 
+        # Preallocate variables and data sizes
         for nm in defs.data_defs:
-            outd = defs._idata(outd, nm,
-                               sz=defs._get_size(nm, self._nens, self.cfg['n_cells']))
+            outd = defs._idata(
+                outd, nm, sz=defs._get_size(nm, self._nens, self.cfg["n_cells"])
+            )
         self.outd = outd
 
         if self._bb:
             for nm in defs.data_defs:
-                outdbb = defs._idata(outdbb, nm,
-                                     sz=defs._get_size(nm, self._nens, self.cfgbb['n_cells']))
+                outdbb = defs._idata(
+                    outdbb, nm, sz=defs._get_size(nm, self._nens, self.cfgbb["n_cells"])
+                )
             self.outdBB = outdbb
             if self._debug_level > 1:
-                logging.info(np.shape(outdbb['data_vars']['vel']))
+                logging.info(np.shape(outdbb["data_vars"]["vel"]))
 
         if self._debug_level > 1:
-            logging.info('{} ncells, not BB'.format(self.cfg['n_cells']))
+            logging.info("{} ncells, not BB".format(self.cfg["n_cells"]))
             if self._bb:
-                logging.info('{} ncells, BB'.format(self.cfgbb['n_cells']))
+                logging.info("{} ncells, BB".format(self.cfgbb["n_cells"]))
 
-    def read_buffer(self,):
+    def read_buffer(self):
         fd = self.f
         self.ensemble.k = -1  # so that k+=1 gives 0 on the first loop.
         if self._bb:
             self.ensembleBB.k = -1  # so that k+=1 gives 0 on the first loop.
         self.print_progress()
         hdr = self.hdr
         while self.ensemble.k < self.ensemble.n_avg - 1:
             if not self.search_buffer():
                 return False
             startpos = fd.tell() - 2
             self.read_hdrseg()
-            if self._debug_level >= 0:
-                logging.info('Read Header', hdr)
+            if self._debug_level > -1:
+                logging.info("Read Header", hdr)
             byte_offset = self._nbyte + 2
             self._read_vmdas = False
-            for n in range(len(hdr['dat_offsets'])):
+            for n in range(len(hdr["dat_offsets"])):
                 id = fd.read_ui16(1)
                 if self._debug_level > 0:
-                    logging.info(f'n {n}: {id} {id:04x}')
+                    logging.info(f"n {n}: {id} {id:04x}")
                 self.print_pos()
                 retval = self.read_dat(id)
 
-                if retval == 'FAIL':
+                if retval == "FAIL":
                     break
                 byte_offset += self._nbyte
-                if n < (len(hdr['dat_offsets']) - 1):
-                    oset = hdr['dat_offsets'][n + 1] - byte_offset
+                if n < (len(hdr["dat_offsets"]) - 1):
+                    oset = hdr["dat_offsets"][n + 1] - byte_offset
                     if oset != 0:
                         if self._debug_level > 0:
-                            logging.debug(
-                                '  %s: Adjust location by %d\n' % (id, oset))
+                            logging.debug("  %s: Adjust location by %d\n" % (id, oset))
                         fd.seek(oset, 1)
-                    byte_offset = hdr['dat_offsets'][n + 1]
+                    byte_offset = hdr["dat_offsets"][n + 1]
                 else:
-                    if hdr['nbyte'] - 2 != byte_offset:
+                    if hdr["nbyte"] - 2 != byte_offset:
                         if not self._winrivprob:
                             if self._debug_level > 0:
-                                logging.debug('  {:d}: Adjust location by {:d}\n'
-                                              .format(id, hdr['nbyte'] - 2 - byte_offset))
-                            self.f.seek(hdr['nbyte'] - 2 - byte_offset, 1)
-                    byte_offset = hdr['nbyte'] - 2
+                                logging.debug(
+                                    "  {:d}: Adjust location by {:d}\n".format(
+                                        id, hdr["nbyte"] - 2 - byte_offset
+                                    )
+                                )
+                            self.f.seek(hdr["nbyte"] - 2 - byte_offset, 1)
+                    byte_offset = hdr["nbyte"] - 2
             # Check for vmdas again because vmdas doesn't set the offsets
             # correctly, and we need this info:
             if not self._read_vmdas and self._vmdas_search:
-                if self._debug_level >= 1:
-                    logging.info(
-                        'Searching for vmdas nav data. Going to next ensemble')
+                if self._debug_level > 0:
+                    logging.info("Searching for vmdas nav data. Going to next ensemble")
                 self.search_buffer()
                 # now go back to where vmdas would be:
                 fd.seek(-98, 1)
                 id = self.f.read_ui16(1)
                 if id is not None:
-                    if self._debug_level >= 1:
-                        logging.info(f'Found {id:04d}')
+                    if self._debug_level > 0:
+                        logging.info(f"Found {id:04d}")
                     if id == 8192:
                         self.read_dat(id)
             readbytes = fd.tell() - startpos
-            offset = hdr['nbyte'] + 2 - readbytes
+            offset = hdr["nbyte"] + 2 - readbytes
             self.check_offset(offset, readbytes)
             self.print_pos(byte_offset=byte_offset)
 
         return True
 
+    def print_progress(self):
+        self.progress = self.f.tell()
+        if self._debug_level > 1:
+            logging.debug(
+                "  pos %0.0fmb/%0.0fmb\n"
+                % (self.f.tell() / 1048576.0, self._filesize / 1048576.0)
+            )
+        if (self.f.tell() - self.progress) < 1048576:
+            return
+
     def search_buffer(self):
         """
         Check to see if the next bytes indicate the beginning of a
         data block.  If not, search for the next data block, up to
         _search_num times.
         """
-        id = self.f.read_ui8(2)
+        fd = self.f
+        id = fd.read_ui8(2)
         if id is None:
             return False
-        id1 = list(id)
+        cfgid = list(id)
+        pos_7f79 = False
         search_cnt = 0
-        fd = self.f
-        if self._debug_level >= 2:
-            logging.info('  -->In search_buffer...')
-        while (search_cnt < self._search_num and
-               ((id1[0] != 127 or id1[1] != 127) or
-                not self.checkheader())):
-            search_cnt += 1
-            nextbyte = fd.read_ui8(1)
-            if nextbyte == None:
-                return False
-            id1[1] = id1[0]
-            id1[0] = nextbyte
-        if search_cnt == self._search_num:
-            raise Exception(
-                'Searched {} entries... Bad data encountered. -> {}'
-                .format(search_cnt, id1))
-        elif search_cnt > 0:
-            if self._debug_level >= 1:
-                logging.info('  Searched {} bytes to find next '
-                             'valid ensemble start [{:x}, {:x}]\n'
-                             .format(search_cnt, *id1))
+
+        if self._debug_level > -1:
+            logging.info("pos {}".format(fd.pos))
+            logging.info("cfgid0: [{:x}, {:x}]".format(*cfgid))
+        # If not [127, 127] or if the file ends in the next ensemble
+        while (cfgid != [127, 127]) or self.check_eof():
+            if cfgid == [127, 121]:
+                # Search for the next header or the end of the file
+                skipbytes = fd.read_i16(1)
+                fd.seek(skipbytes - 2, 1)
+                id = fd.read_ui8(2)
+                if id is None:  # EOF
+                    return False
+                cfgid = list(id)
+                pos_7f79 = True
+            else:
+                # Search til we find something or hit the end of the file
+                search_cnt += 1
+                nextbyte = fd.read_ui8(1)
+                if nextbyte is None:  # EOF
+                    return False
+                cfgid[0] = cfgid[1]
+                cfgid[1] = nextbyte
+
+        if pos_7f79 and self._debug_level > -1:
+            logging.info("Skipped junk data: [{:x}, {:x}]".format(*[127, 121]))
+
+        if search_cnt > 0:
+            if self._debug_level > 0:
+                logging.info(
+                    "  Searched {} bytes to find next "
+                    "valid ensemble start [{:x}, {:x}]\n".format(search_cnt, *cfgid)
+                )
+
         return True
 
-    def checkheader(self,):
-        if self._debug_level > 1:
-            logging.info("  ###In checkheader.")
+    def check_eof(self):
+        """
+        Returns True if next header is bad or at end of file.
+        """
         fd = self.f
-        valid = False
-        if self._debug_level >= 0:
-            logging.info('pos {}'.format(self.f.pos))
+        out = True
         numbytes = fd.read_i16(1)
+        # Search for next config id
         if numbytes > 0:
             fd.seek(numbytes - 2, 1)
             cfgid = fd.read_ui8(2)
             if cfgid is None:
                 if self._debug_level > 1:
-                    logging.info('EOF')
-                return False
+                    logging.info("EOF")
+                return True
+            # Make sure one is found, either 7f7f or 7f79
             if len(cfgid) == 2:
                 fd.seek(-numbytes - 2, 1)
                 if cfgid[0] == 127 and cfgid[1] in [127, 121]:
-                    if cfgid[1] == 121 and self._debug7f79 is None:
-                        self._debug7f79 = True
-                        if self._debug_level > 1:
-                            logging.warning('7f79!!!')
-                    valid = True
+                    out = False
         else:
             fd.seek(-2, 1)
-        if self._debug_level > 1:
-            logging.info("  ###Leaving checkheader.")
-        return valid
-
-    def read_hdrseg(self,):
-        fd = self.f
-        hdr = self.hdr
-        hdr['nbyte'] = fd.read_i16(1)
-        spare = fd.read_ui8(1)
-        ndat = fd.read_ui8(1)
-        hdr['dat_offsets'] = fd.read_ui16(ndat)
-        self._nbyte = 4 + ndat * 2
-
-    def print_progress(self,):
-        self.progress = self.f.tell()
-        if self._debug_level > 1:
-            logging.debug('  pos %0.0fmb/%0.0fmb\n' %
-                          (self.f.tell() / 1048576., self._filesize / 1048576.))
-        if (self.f.tell() - self.progress) < 1048576:
-            return
+        return out
 
     def print_pos(self, byte_offset=-1):
-        """Print the position in the file, used for debugging.
-        """
-        if self._debug_level >= 2:
-            if hasattr(self, 'ensemble'):
+        """Print the position in the file, used for debugging."""
+        if self._debug_level > 1:
+            if hasattr(self, "ensemble"):
                 k = self.ensemble.k
             else:
                 k = 0
             logging.debug(
-                f'  pos: {self.f.tell()}, pos_: {self._pos}, nbyte: {self._nbyte}, k: {k}, byte_offset: {byte_offset}')
-
-    def check_offset(self, offset, readbytes):
-        fd = self.f
-        if offset != 4 and self._fixoffset == 0:
-            if self._debug_level > 0:
-                if fd.tell() == self._filesize:
-                    logging.error(
-                        ' EOF reached unexpectedly - discarding this last ensemble\n')
-                else:
-                    logging.debug("  Adjust location by {:d} (readbytes={:d},hdr['nbyte']={:d})\n"
-                                  .format(offset, readbytes, self.hdr['nbyte']))
-            self._fixoffset = offset - 4
-        fd.seek(4 + self._fixoffset, 1)
-
-    def remove_end(self, iens):
-        dat = self.outd
-        if self._debug_level > 0:
-            logging.info('  Encountered end of file.  Cleaning up data.')
-        for nm in self.vars_read:
-            defs._setd(dat, nm, defs._get(dat, nm)[..., :iens])
+                f"  pos: {self.f.tell()}, pos_: {self._pos}, nbyte: {self._nbyte}, k: {k}, byte_offset: {byte_offset}"
+            )
 
     def read_dat(self, id):
-        function_map = {0: (self.read_fixed, []),   # 0000 1st profile fixed leader
-                        1:  (self.read_fixed, [True]),  # 0001
-                        # 0010 Surface layer fixed leader (RiverPro & StreamPro)
-                        16: (self.read_fixed_sl, []),
-                        # 0080 1st profile variable leader
-                        128: (self.read_var, [0]),
-                        # 0081 2nd profile variable leader
-                        129: (self.read_var, [1]),
-                        # 0100 1st profile velocity
-                        256: (self.read_vel, [0]),
-                        # 0101 2nd profile velocity
-                        257: (self.read_vel, [1]),
-                        # 0103 Waves first leader
-                        259: (self.skip_Nbyte, [74]),
-                        # 0110 Surface layer velocity (RiverPro & StreamPro)
-                        272: (self.read_vel, [2]),
-                        # 0200 1st profile correlation
-                        512: (self.read_corr, [0]),
-                        # 0201 2nd profile correlation
-                        513: (self.read_corr, [1]),
-                        # 0203 Waves data
-                        515: (self.skip_Nbyte, [186]),
-                        # 020C Ambient sound profile
-                        524: (self.skip_Nbyte, [4]),
-                        # 0210 Surface layer correlation (RiverPro & StreamPro)
-                        528: (self.read_corr, [2]),
-                        # 0300 1st profile amplitude
-                        768: (self.read_amp, [0]),
-                        # 0301 2nd profile amplitude
-                        769: (self.read_amp, [1]),
-                        # 0302 Beam 5 Sum of squared velocities
-                        770: (self.skip_Ncol, []),
-                        # 0303 Waves last leader
-                        771: (self.skip_Ncol, [18]),
-                        # 0310 Surface layer amplitude (RiverPro & StreamPro)
-                        784: (self.read_amp, [2]),
-                        # 0400 1st profile % good
-                        1024: (self.read_prcnt_gd, [0]),
-                        # 0401 2nd profile pct good
-                        1025: (self.read_prcnt_gd, [1]),
-                        # 0403 Waves HPR data
-                        1027: (self.skip_Nbyte, [6]),
-                        # 0410 Surface layer pct good (RiverPro & StreamPro)
-                        1040: (self.read_prcnt_gd, [2]),
-                        # 0500 1st profile status
-                        1280: (self.read_status, [0]),
-                        # 0501 2nd profile status
-                        1281: (self.read_status, [1]),
-                        # 0510 Surface layer status (RiverPro & StreamPro)
-                        1296: (self.read_status, [2]),
-                        1536: (self.read_bottom, []),  # 0600 bottom tracking
-                        1793: (self.skip_Ncol, [4]),  # 0701 number of pings
-                        1794: (self.skip_Ncol, [4]),  # 0702 sum of squared vel
-                        1795: (self.skip_Ncol, [4]),  # 0703 sum of velocities
-                        2560: (self.skip_Ncol, []),  # 0A00 Beam 5 velocity
-                        2816: (self.skip_Ncol, []),  # 0B00 Beam 5 correlation
-                        3072: (self.skip_Ncol, []),  # 0C00 Beam 5 amplitude
-                        3328: (self.skip_Ncol, []),  # 0D00 Beam 5 pct_good
-                        # Fixed attitude data format for Ocean Surveyor ADCPs
-                        3000: (self.skip_Nbyte, [32]),
-                        3841: (self.skip_Nbyte, [38]),  # 0F01 Beam 5 leader
-                        8192: (self.read_vmdas, []),   # 2000
-                        # 2013 Navigation parameter data
-                        8211: (self.skip_Nbyte, [83]),
-                        8226: (self.read_winriver2, []),  # 2022
-                        8448: (self.read_winriver, [38]),  # 2100
-                        8449: (self.read_winriver, [97]),  # 2101
-                        8450: (self.read_winriver, [45]),  # 2102
-                        8451: (self.read_winriver, [60]),  # 2103
-                        8452: (self.read_winriver, [38]),  # 2104
-                        # 3200 Transformation matrix
-                        12800: (self.skip_Nbyte, [32]),
-                        # 3000 Fixed attitude data format for Ocean Surveyor ADCPs
-                        12288: (self.skip_Nbyte, [32]),
-                        12496: (self.skip_Nbyte, [24]),  # 30D0
-                        12504: (self.skip_Nbyte, [48]),  # 30D8
-                        # 4100 beam 5 range
-                        16640: (self.read_alt, []),
-                        # 4400 Firmware status data (RiverPro & StreamPro)
-                        17408: (self.skip_Nbyte, [28]),
-                        # 4401 Auto mode setup (RiverPro & StreamPro)
-                        17409: (self.skip_Nbyte, [82]),
-                        # 5803 High resolution bottom track velocity
-                        22531: (self.skip_Nbyte, [68]),
-                        # 5804 Bottom track range
-                        22532: (self.skip_Nbyte, [21]),
-                        # 5901 ISM (IMU) data
-                        22785: (self.skip_Nbyte, [65]),
-                        # 5902 Ping attitude
-                        22786: (self.skip_Nbyte, [105]),
-                        # 7001 ADC data
-                        28673: (self.skip_Nbyte, [14]),
-                        }
+        function_map = {
+            # 0000 1st profile fixed leader
+            0: (self.read_fixed, []),
+            # 0001 2nd profile fixed leader
+            1: (self.read_fixed, [True]),
+            # 0010 Surface layer fixed leader (RiverPro & StreamPro)
+            16: (self.read_fixed_sl, []),
+            # 0080 1st profile variable leader
+            128: (self.read_var, [0]),
+            # 0081 2nd profile variable leader
+            129: (self.read_var, [1]),
+            # 0100 1st profile velocity
+            256: (self.read_vel, [0]),
+            # 0101 2nd profile velocity
+            257: (self.read_vel, [1]),
+            # 0103 Waves first leader
+            259: (self.skip_Nbyte, [74]),
+            # 0110 Surface layer velocity (RiverPro & StreamPro)
+            272: (self.read_vel, [2]),
+            # 0200 1st profile correlation
+            512: (self.read_corr, [0]),
+            # 0201 2nd profile correlation
+            513: (self.read_corr, [1]),
+            # 0203 Waves data
+            515: (self.skip_Nbyte, [186]),
+            # 020C Ambient sound profile
+            524: (self.skip_Nbyte, [4]),
+            # 0210 Surface layer correlation (RiverPro & StreamPro)
+            528: (self.read_corr, [2]),
+            # 0300 1st profile amplitude
+            768: (self.read_amp, [0]),
+            # 0301 2nd profile amplitude
+            769: (self.read_amp, [1]),
+            # 0302 Beam 5 Sum of squared velocities
+            770: (self.skip_Ncol, []),
+            # 0303 Waves last leader
+            771: (self.skip_Ncol, [18]),
+            # 0310 Surface layer amplitude (RiverPro & StreamPro)
+            784: (self.read_amp, [2]),
+            # 0400 1st profile % good
+            1024: (self.read_prcnt_gd, [0]),
+            # 0401 2nd profile pct good
+            1025: (self.read_prcnt_gd, [1]),
+            # 0403 Waves HPR data
+            1027: (self.skip_Nbyte, [6]),
+            # 0410 Surface layer pct good (RiverPro & StreamPro)
+            1040: (self.read_prcnt_gd, [2]),
+            # 0500 1st profile status
+            1280: (self.read_status, [0]),
+            # 0501 2nd profile status
+            1281: (self.read_status, [1]),
+            # 0510 Surface layer status (RiverPro & StreamPro)
+            1296: (self.read_status, [2]),
+            1536: (self.read_bottom, []),  # 0600 bottom tracking
+            1793: (self.skip_Ncol, [4]),  # 0701 number of pings
+            1794: (self.skip_Ncol, [4]),  # 0702 sum of squared vel
+            1795: (self.skip_Ncol, [4]),  # 0703 sum of velocities
+            2560: (self.skip_Ncol, []),  # 0A00 Beam 5 velocity
+            2816: (self.skip_Ncol, []),  # 0B00 Beam 5 correlation
+            3072: (self.skip_Ncol, []),  # 0C00 Beam 5 amplitude
+            3328: (self.skip_Ncol, []),  # 0D00 Beam 5 pct_good
+            # Fixed attitude data format for Ocean Surveyor ADCPs
+            3000: (self.skip_Nbyte, [32]),
+            3841: (self.skip_Nbyte, [38]),  # 0F01 Beam 5 leader
+            8192: (self.read_vmdas, []),  # 2000
+            # 2013 Navigation parameter data
+            8211: (self.skip_Nbyte, [83]),
+            8226: (self.read_winriver2, []),  # 2022
+            8448: (self.read_winriver, [38]),  # 2100
+            8449: (self.read_winriver, [97]),  # 2101
+            8450: (self.read_winriver, [45]),  # 2102
+            8451: (self.read_winriver, [60]),  # 2103
+            8452: (self.read_winriver, [38]),  # 2104
+            # 3200 Transformation matrix
+            12800: (self.skip_Nbyte, [32]),
+            # 3000 Fixed attitude data format for Ocean Surveyor ADCPs
+            12288: (self.skip_Nbyte, [32]),
+            12496: (self.skip_Nbyte, [24]),  # 30D0
+            12504: (self.skip_Nbyte, [48]),  # 30D8
+            # 4100 beam 5 range
+            16640: (self.read_alt, []),
+            # 4400 Firmware status data (RiverPro & StreamPro)
+            17408: (self.skip_Nbyte, [28]),
+            # 4401 Auto mode setup (RiverPro & StreamPro)
+            17409: (self.skip_Nbyte, [82]),
+            # 5803 High resolution bottom track velocity
+            22531: (self.skip_Nbyte, [68]),
+            # 5804 Bottom track range
+            22532: (self.skip_Nbyte, [21]),
+            # 5901 ISM (IMU) data
+            22785: (self.skip_Nbyte, [65]),
+            # 5902 Ping attitude
+            22786: (self.skip_Nbyte, [105]),
+            # 7001 ADC data
+            28673: (self.skip_Nbyte, [14]),
+        }
         # Call the correct function:
-        if self._debug_level >= 2:
-            logging.debug(f'Trying to Read {id}')
+        if self._debug_level > 1:
+            logging.debug(f"Trying to Read {id}")
         if id in function_map:
             if self._debug_level > 1:
-                logging.info('  Reading code {}...'.format(hex(id)))
+                logging.info("  Reading code {}...".format(hex(id)))
             retval = function_map.get(id)[0](*function_map[id][1])
             if retval:
                 return retval
             if self._debug_level > 1:
-                logging.info('    success!')
+                logging.info("    success!")
         else:
             self.read_nocode(id)
 
     def read_fixed(self, bb=False):
         self.read_cfgseg(bb=bb)
         self._nbyte += 2
-        if self._debug_level >= 0:
-            logging.info('Read Fixed')
+        if self._debug_level > -1:
+            logging.info("Read Fixed")
 
-        # Check if n_cells changed (for winriver transect files)
-        if hasattr(self, 'ensemble') and (self.ensemble['n_cells'] != self.cfg['n_cells']):
-            diff = self.cfg['n_cells'] - self.ensemble['n_cells']
-            if diff > 0:
-                self.flag = diff
-                self.ensemble = defs._ensemble(self.n_avg, self.cfg['n_cells'])
-                # Not concerned if # of cells decreases
-                if self._debug_level >= 1:
-                    logging.warning('Number of cells changed to {}'
-                                    .format(self.cfg['n_cells']))
+        # Check if n_cells has increased (for winriver transect files)
+        if hasattr(self, "ensemble"):
+            self.n_cells_diff = self.cfg["n_cells"] - self.ensemble["n_cells"]
+            # Increase n_cells if greater than 0
+            if self.n_cells_diff > 0:
+                self.ensemble = defs._ensemble(self.n_avg, self.cfg["n_cells"])
+                if self._debug_level > 0:
+                    logging.warning(
+                        f"Maximum number of cells increased to {self.cfg['n_cells']}"
+                    )
 
-    def read_fixed_sl(self,):
+    def read_fixed_sl(self):
         # Surface layer profile
         cfg = self.cfg
-        cfg['surface_layer'] = 1
-        cfg['n_cells_sl'] = self.f.read_ui8(1)
-        cfg['cell_size_sl'] = self.f.read_ui16(1) * .01
-        cfg['bin1_dist_m_sl'] = round(self.f.read_ui16(1) * .01, 4)
+        cfg["surface_layer"] = 1
+        n_cells = self.f.read_ui8(1)
+        # Check if n_cells is greater than what was used in prior profiles
+        if n_cells > self.n_cells_sl:
+            self.n_cells_sl = n_cells
+            if self._debug_level > 0:
+                logging.warning(
+                    f"Maximum number of surface layer cells increased to {n_cells}"
+                )
+        cfg["n_cells_sl"] = n_cells
+        # Assuming surface layer profile cell size never changes
+        cfg["cell_size_sl"] = self.f.read_ui16(1) * 0.01
+        cfg["bin1_dist_m_sl"] = round(self.f.read_ui16(1) * 0.01, 4)
 
-        if self._debug_level >= 0:
-            logging.info('Read Surface Layer Config')
+        if self._debug_level > -1:
+            logging.info("Read Surface Layer Config")
         self._nbyte = 2 + 5
 
     def read_cfgseg(self, bb=False):
         cfgstart = self.f.tell()
 
         if bb:
             cfg = self.cfgbb
         else:
             cfg = self.cfg
         fd = self.f
         tmp = fd.read_ui8(5)
         prog_ver0 = tmp[0]
-        cfg['prog_ver'] = tmp[0] + tmp[1] / 100.
-        cfg['inst_model'] = defs.adcp_type.get(tmp[0],
-                                               'unrecognized firmware version')
+        cfg["prog_ver"] = tmp[0] + tmp[1] / 100.0
+        cfg["inst_model"] = defs.adcp_type.get(tmp[0], "unrecognized firmware version")
         config = tmp[2:4]
-        cfg['beam_angle'] = [15, 20, 30][(config[1] & 3)]
+        cfg["beam_angle"] = [15, 20, 30][(config[1] & 3)]
         beam5 = [0, 1][int((config[1] & 16) == 16)]
-        cfg['freq'] = ([75, 150, 300, 600, 1200, 2400, 38][(config[0] & 7)])
-        cfg['beam_pattern'] = (['concave',
-                                'convex'][int((config[0] & 8) == 8)])
-        cfg['orientation'] = ['down', 'up'][int((config[0] & 128) == 128)]
-        simflag = ['real', 'simulated'][tmp[4]]
+        cfg["freq"] = [75, 150, 300, 600, 1200, 2400, 38][(config[0] & 7)]
+        cfg["beam_pattern"] = ["concave", "convex"][int((config[0] & 8) == 8)]
+        cfg["orientation"] = ["down", "up"][int((config[0] & 128) == 128)]
+        simflag = ["real", "simulated"][tmp[4]]
         fd.seek(1, 1)
-        cfg['n_beams'] = fd.read_ui8(1) + beam5
-        cfg['n_cells'] = fd.read_ui8(1)
-        cfg['pings_per_ensemble'] = fd.read_ui16(1)
-        cfg['cell_size'] = fd.read_ui16(1) * .01
-        cfg['blank_dist'] = fd.read_ui16(1) * .01
-        cfg['profiling_mode'] = fd.read_ui8(1)
-        cfg['min_corr_threshold'] = fd.read_ui8(1)
-        cfg['n_code_reps'] = fd.read_ui8(1)
-        cfg['min_prcnt_gd'] = fd.read_ui8(1)
-        cfg['max_error_vel'] = fd.read_ui16(1) / 1000
-        cfg['sec_between_ping_groups'] = (
-            np.sum(np.array(fd.read_ui8(3)) *
-                   np.array([60., 1., .01])))
+        cfg["n_beams"] = fd.read_ui8(1) + beam5
+        # Check if number of cells has changed
+        n_cells = fd.read_ui8(1)
+        if ("n_cells" not in cfg) or (n_cells != cfg["n_cells"]):
+            cfg["n_cells"] = n_cells
+            if self._debug_level > 0:
+                logging.info(f"Number of cells set to {cfg['n_cells']}")
+        cfg["pings_per_ensemble"] = fd.read_ui16(1)
+        # Check if cell size has changed
+        cs = fd.read_ui16(1) * 0.01
+        if ("cell_size" not in cfg) or (cs != cfg["cell_size"]):
+            self.cs_diff = cs if "cell_size" not in cfg else (cs - cfg["cell_size"])
+            cfg["cell_size"] = cs
+            if self._debug_level > 0:
+                logging.info(f"Cell size set to {cfg['cell_size']}")
+        cfg["blank_dist"] = fd.read_ui16(1) * 0.01
+        cfg["profiling_mode"] = fd.read_ui8(1)
+        cfg["min_corr_threshold"] = fd.read_ui8(1)
+        cfg["n_code_reps"] = fd.read_ui8(1)
+        cfg["min_prcnt_gd"] = fd.read_ui8(1)
+        cfg["max_error_vel"] = fd.read_ui16(1) / 1000
+        cfg["sec_between_ping_groups"] = np.sum(
+            np.array(fd.read_ui8(3)) * np.array([60.0, 1.0, 0.01])
+        )
         coord_sys = fd.read_ui8(1)
-        cfg['coord_sys'] = (['beam', 'inst',
-                             'ship', 'earth'][((coord_sys >> 3) & 3)])
-        cfg['use_pitchroll'] = ['no', 'yes'][(coord_sys & 4) == 4]
-        cfg['use_3beam'] = ['no', 'yes'][(coord_sys & 2) == 2]
-        cfg['bin_mapping'] = ['no', 'yes'][(coord_sys & 1) == 1]
-        cfg['heading_misalign_deg'] = fd.read_i16(1) * .01
-        cfg['magnetic_var_deg'] = fd.read_i16(1) * .01
-        cfg['sensors_src'] = np.binary_repr(fd.read_ui8(1), 8)
-        cfg['sensors_avail'] = np.binary_repr(fd.read_ui8(1), 8)
-        cfg['bin1_dist_m'] = round(fd.read_ui16(1) * .01, 4)
-        cfg['transmit_pulse_m'] = fd.read_ui16(1) * .01
-        cfg['water_ref_cells'] = list(fd.read_ui8(2))  # list for attrs
-        cfg['false_target_threshold'] = fd.read_ui8(1)
+        cfg["coord_sys"] = ["beam", "inst", "ship", "earth"][((coord_sys >> 3) & 3)]
+        cfg["use_pitchroll"] = ["no", "yes"][(coord_sys & 4) == 4]
+        cfg["use_3beam"] = ["no", "yes"][(coord_sys & 2) == 2]
+        cfg["bin_mapping"] = ["no", "yes"][(coord_sys & 1) == 1]
+        cfg["heading_misalign_deg"] = fd.read_i16(1) * 0.01
+        cfg["magnetic_var_deg"] = fd.read_i16(1) * 0.01
+        cfg["sensors_src"] = np.binary_repr(fd.read_ui8(1), 8)
+        cfg["sensors_avail"] = np.binary_repr(fd.read_ui8(1), 8)
+        cfg["bin1_dist_m"] = round(fd.read_ui16(1) * 0.01, 4)
+        cfg["transmit_pulse_m"] = fd.read_ui16(1) * 0.01
+        cfg["water_ref_cells"] = list(fd.read_ui8(2))  # list for attrs
+        cfg["false_target_threshold"] = fd.read_ui8(1)
         fd.seek(1, 1)
-        cfg['transmit_lag_m'] = fd.read_ui16(1) * .01
+        cfg["transmit_lag_m"] = fd.read_ui16(1) * 0.01
         self._nbyte = 40
 
-        if cfg['prog_ver'] >= 8.14:
+        if cfg["prog_ver"] >= 8.14:
             cpu_serialnum = fd.read_ui8(8)
             self._nbyte += 8
-        if cfg['prog_ver'] >= 8.24:
-            cfg['bandwidth'] = fd.read_ui16(1)
+        if cfg["prog_ver"] >= 8.24:
+            cfg["bandwidth"] = fd.read_ui16(1)
             self._nbyte += 2
-        if cfg['prog_ver'] >= 16.05:
-            cfg['power_level'] = fd.read_ui8(1)
+        if cfg["prog_ver"] >= 16.05:
+            cfg["power_level"] = fd.read_ui8(1)
             self._nbyte += 1
-        if cfg['prog_ver'] >= 16.27:
+        if cfg["prog_ver"] >= 16.27:
             # cfg['navigator_basefreqindex'] = fd.read_ui8(1)
             fd.seek(1, 1)
-            cfg['serialnum'] = fd.read_ui32(1)
-            cfg['beam_angle'] = fd.read_ui8(1)
+            cfg["serialnum"] = fd.read_ui32(1)
+            cfg["beam_angle"] = fd.read_ui8(1)
             self._nbyte += 6
 
         self.configsize = self.f.tell() - cfgstart
-        if self._debug_level >= 0:
-            logging.info('Read Config')
+        if self._debug_level > -1:
+            logging.info("Read Config")
 
     def read_var(self, bb=False):
-        """ Read variable leader """
+        """Read variable leader"""
         fd = self.f
         if bb:
             ens = self.ensembleBB
         else:
             ens = self.ensemble
         ens.k += 1
         ens = self.ensemble
         k = ens.k
-        self.vars_read += ['number',
-                           'rtc',
-                           'number',
-                           'builtin_test_fail',
-                           'c_sound',
-                           'depth',
-                           'heading',
-                           'pitch',
-                           'roll',
-                           'salinity',
-                           'temp',
-                           'min_preping_wait',
-                           'heading_std',
-                           'pitch_std',
-                           'roll_std',
-                           'adc']
+        self.vars_read += [
+            "number",
+            "rtc",
+            "number",
+            "builtin_test_fail",
+            "c_sound",
+            "depth",
+            "heading",
+            "pitch",
+            "roll",
+            "salinity",
+            "temp",
+            "min_preping_wait",
+            "heading_std",
+            "pitch_std",
+            "roll_std",
+            "adc",
+        ]
         ens.number[k] = fd.read_ui16(1)
         ens.rtc[:, k] = fd.read_ui8(7)
         ens.number[k] += 65535 * fd.read_ui8(1)
         ens.builtin_test_fail[k] = fd.read_ui16(1)
         ens.c_sound[k] = fd.read_ui16(1)
         ens.depth[k] = fd.read_ui16(1) * 0.1
         ens.heading[k] = fd.read_ui16(1) * 0.01
         ens.pitch[k] = fd.read_i16(1) * 0.01
         ens.roll[k] = fd.read_i16(1) * 0.01
         ens.salinity[k] = fd.read_i16(1)
         ens.temp[k] = fd.read_i16(1) * 0.01
-        ens.min_preping_wait[k] = (fd.read_ui8(
-            3) * np.array([60, 1, .01])).sum()
+        ens.min_preping_wait[k] = (fd.read_ui8(3) * np.array([60, 1, 0.01])).sum()
         ens.heading_std[k] = fd.read_ui8(1)
         ens.pitch_std[k] = fd.read_ui8(1) * 0.1
         ens.roll_std[k] = fd.read_ui8(1) * 0.1
         ens.adc[:, k] = fd.read_i8(8)
         self._nbyte = 2 + 40
 
         cfg = self.cfg
-        if cfg['inst_model'].lower() == 'broadband':
-            if cfg['prog_ver'] >= 5.55:
+        if cfg["inst_model"].lower() == "broadband":
+            if cfg["prog_ver"] >= 5.55:
                 fd.seek(15, 1)
                 cent = fd.read_ui8(1)
                 ens.rtc[:, k] = fd.read_ui8(7)
                 ens.rtc[0, k] = ens.rtc[0, k] + cent * 100
                 self._nbyte += 23
-        elif cfg['inst_model'].lower() == 'ocean surveyor':
+        elif cfg["inst_model"].lower() == "ocean surveyor":
             fd.seek(16, 1)  # 30 bytes all set to zero, 14 read above
             self._nbyte += 16
-            if cfg['prog_ver'] > 23:
+            if cfg["prog_ver"] > 23:
                 fd.seek(2, 1)
                 self._nbyte += 2
         else:
             ens.error_status[k] = np.binary_repr(fd.read_ui32(1), 32)
-            self.vars_read += ['pressure', 'pressure_std']
+            self.vars_read += ["pressure", "pressure_std"]
             self._nbyte += 4
-            if cfg['prog_ver'] >= 8.13:
+            if cfg["prog_ver"] >= 8.13:
                 # Added pressure sensor stuff in 8.13
                 fd.seek(2, 1)
                 ens.pressure[k] = fd.read_ui32(1) / 1000  # dPa to dbar
                 ens.pressure_std[k] = fd.read_ui32(1) / 1000
                 self._nbyte += 10
-            if cfg['prog_ver'] >= 8.24:
+            if cfg["prog_ver"] >= 8.24:
                 # Spare byte added 8.24
                 fd.seek(1, 1)
                 self._nbyte += 1
-            if cfg['prog_ver'] >= 16.05:
+            if cfg["prog_ver"] >= 16.05:
                 # Added more fields with century in clock
                 cent = fd.read_ui8(1)
                 ens.rtc[:, k] = fd.read_ui8(7)
                 ens.rtc[0, k] = ens.rtc[0, k] + cent * 100
                 self._nbyte += 8
-            if cfg['prog_ver'] >= 56:
+            if cfg["prog_ver"] >= 56:
                 fd.seek(1)  # lag near bottom flag
                 self._nbyte += 1
 
-        if self._debug_level >= 0:
-            logging.info('Read Var')
+        if self._debug_level > -1:
+            logging.info("Read Var")
 
     def switch_profile(self, bb):
         if bb == 1:
             ens = self.ensembleBB
             cfg = self.cfgbb
             # Placeholder for dual profile mode
             # Solution for vmdas profile in bb spot (vs nb)
-            tag = ''
+            tag = ""
         elif bb == 2:
             ens = self.ensemble
             cfg = self.cfg
-            tag = '_sl'
+            tag = "_sl"
         else:
             ens = self.ensemble
             cfg = self.cfg
-            tag = ''
+            tag = ""
 
         return ens, cfg, tag
 
     def read_vel(self, bb=0):
         ens, cfg, tg = self.switch_profile(bb)
-        self.vars_read += ['vel'+tg]
-        n_cells = cfg['n_cells'+tg]
+        self.vars_read += ["vel" + tg]
+        n_cells = cfg["n_cells" + tg]
 
         k = ens.k
-        vel = np.array(
-            self.f.read_i16(4 * n_cells)
-        ).reshape((n_cells, 4)) * .001
-        ens['vel'+tg][:n_cells, :, k] = vel
+        vel = np.array(self.f.read_i16(4 * n_cells)).reshape((n_cells, 4)) * 0.001
+        ens["vel" + tg][:n_cells, :, k] = vel
         self._nbyte = 2 + 4 * n_cells * 2
-        if self._debug_level >= 0:
-            logging.info('Read Vel')
+        if self._debug_level > -1:
+            logging.info("Read Vel")
 
     def read_corr(self, bb=0):
         ens, cfg, tg = self.switch_profile(bb)
-        self.vars_read += ['corr'+tg]
-        n_cells = cfg['n_cells'+tg]
+        self.vars_read += ["corr" + tg]
+        n_cells = cfg["n_cells" + tg]
 
         k = ens.k
-        ens['corr'+tg][:n_cells, :, k] = np.array(
+        ens["corr" + tg][:n_cells, :, k] = np.array(
             self.f.read_ui8(4 * n_cells)
         ).reshape((n_cells, 4))
         self._nbyte = 2 + 4 * n_cells
-        if self._debug_level >= 0:
-            logging.info('Read Corr')
+        if self._debug_level > -1:
+            logging.info("Read Corr")
 
     def read_amp(self, bb=0):
         ens, cfg, tg = self.switch_profile(bb)
-        self.vars_read += ['amp'+tg]
-        n_cells = cfg['n_cells'+tg]
+        self.vars_read += ["amp" + tg]
+        n_cells = cfg["n_cells" + tg]
 
         k = ens.k
-        ens['amp'+tg][:n_cells, :, k] = np.array(
+        ens["amp" + tg][:n_cells, :, k] = np.array(
             self.f.read_ui8(4 * n_cells)
         ).reshape((n_cells, 4))
         self._nbyte = 2 + 4 * n_cells
-        if self._debug_level >= 0:
-            logging.info('Read Amp')
+        if self._debug_level > -1:
+            logging.info("Read Amp")
 
     def read_prcnt_gd(self, bb=0):
         ens, cfg, tg = self.switch_profile(bb)
-        self.vars_read += ['prcnt_gd'+tg]
-        n_cells = cfg['n_cells'+tg]
+        self.vars_read += ["prcnt_gd" + tg]
+        n_cells = cfg["n_cells" + tg]
 
-        ens['prcnt_gd'+tg][:n_cells, :, ens.k] = np.array(
+        ens["prcnt_gd" + tg][:n_cells, :, ens.k] = np.array(
             self.f.read_ui8(4 * n_cells)
         ).reshape((n_cells, 4))
         self._nbyte = 2 + 4 * n_cells
-        if self._debug_level >= 0:
-            logging.info('Read PG')
+        if self._debug_level > -1:
+            logging.info("Read PG")
 
     def read_status(self, bb=0):
         ens, cfg, tg = self.switch_profile(bb)
-        self.vars_read += ['status'+tg]
-        n_cells = cfg['n_cells'+tg]
+        self.vars_read += ["status" + tg]
+        n_cells = cfg["n_cells" + tg]
 
-        ens['status'+tg][:n_cells, :, ens.k] = np.array(
+        ens["status" + tg][:n_cells, :, ens.k] = np.array(
             self.f.read_ui8(4 * n_cells)
         ).reshape((n_cells, 4))
         self._nbyte = 2 + 4 * n_cells
-        if self._debug_level >= 0:
-            logging.info('Read Status')
+        if self._debug_level > -1:
+            logging.info("Read Status")
 
-    def read_bottom(self,):
-        self.vars_read += ['dist_bt', 'vel_bt', 'corr_bt', 'amp_bt',
-                           'prcnt_gd_bt']
+    def read_bottom(self):
+        self.vars_read += ["dist_bt", "vel_bt", "corr_bt", "amp_bt", "prcnt_gd_bt"]
         fd = self.f
         ens = self.ensemble
         k = ens.k
         cfg = self.cfg
-        if self._source == 2:
-            self.vars_read += ['latitude_gps', 'longitude_gps']
+        if self._vm_source == 2:
+            self.vars_read += ["latitude_gps", "longitude_gps"]
             fd.seek(2, 1)
             long1 = fd.read_ui16(1)
             fd.seek(6, 1)
             ens.latitude_gps[k] = fd.read_i32(1) * self._cfac
             if ens.latitude_gps[k] == 0:
                 ens.latitude_gps[k] = np.NaN
         else:
             fd.seek(14, 1)
         ens.dist_bt[:, k] = fd.read_ui16(4) * 0.01
         ens.vel_bt[:, k] = fd.read_i16(4) * 0.001
         ens.corr_bt[:, k] = fd.read_ui8(4)
         ens.amp_bt[:, k] = fd.read_ui8(4)
         ens.prcnt_gd_bt[:, k] = fd.read_ui8(4)
-        if self._source == 2:
+        if self._vm_source == 2:
             fd.seek(2, 1)
-            ens.longitude_gps[k] = (
-                long1 + 65536 * fd.read_ui16(1)) * self._cfac
+            ens.longitude_gps[k] = (long1 + 65536 * fd.read_ui16(1)) * self._cfac
             if ens.longitude_gps[k] > 180:
                 ens.longitude_gps[k] = ens.longitude_gps[k] - 360
             if ens.longitude_gps[k] == 0:
                 ens.longitude_gps[k] = np.NaN
             fd.seek(16, 1)
             qual = fd.read_ui8(1)
             if qual == 0:
                 if self._debug_level > 0:
-                    logging.info('  qual==%d,%f %f' % (qual,
-                                                       ens.latitude_gps[k],
-                                                       ens.longitude_gps[k]))
+                    logging.info(
+                        "  qual==%d,%f %f"
+                        % (qual, ens.latitude_gps[k], ens.longitude_gps[k])
+                    )
                 ens.latitude_gps[k] = np.NaN
                 ens.longitude_gps[k] = np.NaN
             fd.seek(71 - 45 - 16 - 17, 1)
             self._nbyte = 2 + 68
         else:
             # Skip reference layer data
             fd.seek(26, 1)
             self._nbyte = 2 + 68
-        if cfg['prog_ver'] >= 5.3:
+        if cfg["prog_ver"] >= 5.3:
             fd.seek(7, 1)  # skip to rangeMsb bytes
             ens.dist_bt[:, k] = ens.dist_bt[:, k] + fd.read_ui8(4) * 655.36
             self._nbyte += 11
-        if cfg['prog_ver'] >= 16.2 and (cfg.get('sourceprog') != 'WINRIVER'):
+        if cfg["prog_ver"] >= 16.2 and (cfg.get("sourceprog") != "WINRIVER"):
             fd.seek(4, 1)  # not documented
             self._nbyte += 4
-        if cfg['prog_ver'] >= 56.1:
+        if cfg["prog_ver"] >= 56.1:
             fd.seek(4, 1)  # not documented
             self._nbyte += 4
 
-        if self._debug_level >= 0:
-            logging.info('Read Bottom Track')
+        if self._debug_level > -1:
+            logging.info("Read Bottom Track")
 
-    def read_alt(self,):
-        """Read altimeter (vertical beam range) """
+    def read_alt(self):
+        """Read altimeter (vertical beam range)"""
         fd = self.f
         ens = self.ensemble
         k = ens.k
-        self.vars_read += ['alt_dist', 'alt_rssi', 'alt_eval', 'alt_status']
+        self.vars_read += ["alt_dist", "alt_rssi", "alt_eval", "alt_status"]
         ens.alt_eval[k] = fd.read_ui8(1)  # evaluation amplitude
         ens.alt_rssi[k] = fd.read_ui8(1)  # RSSI amplitude
         ens.alt_dist[k] = fd.read_ui32(1) / 1000  # range to surface/seafloor
         ens.alt_status[k] = fd.read_ui8(1)  # status bit flags
         self._nbyte = 7 + 2
-        if self._debug_level >= 0:
-            logging.info('Read Altimeter')
+        if self._debug_level > -1:
+            logging.info("Read Altimeter")
 
-    def read_vmdas(self,):
+    def read_vmdas(self):
         """Read VMDAS Navigation block"""
         fd = self.f
-        self.cfg['sourceprog'] = 'VMDAS'
+        self.cfg["sourceprog"] = "VMDAS"
         ens = self.ensemble
         k = ens.k
-        if self._source != 1 and self._debug_level >= 0:
-            logging.info('  \n***** Apparently a VMDAS file \n\n')
-        self._source = 1
-        self.vars_read += ['time_gps',
-                           'clock_offset_UTC_gps',
-                           'latitude_gps',
-                           'longitude_gps',
-                           'avg_speed_gps',
-                           'avg_dir_gps',
-                           'speed_made_good_gps',
-                           'dir_made_good_gps',
-                           'flags_gps',
-                           'pitch_gps',
-                           'roll_gps',
-                           'heading_gps',
-                           ]
+        if self._vm_source != 1 and self._debug_level > -1:
+            logging.info("  \n***** Apparently a VMDAS file \n\n")
+        self._vm_source = 1
+        self.vars_read += [
+            "time_gps",
+            "clock_offset_UTC_gps",
+            "latitude_gps",
+            "longitude_gps",
+            "avg_speed_gps",
+            "avg_dir_gps",
+            "speed_made_good_gps",
+            "dir_made_good_gps",
+            "flags_gps",
+            "pitch_gps",
+            "roll_gps",
+            "heading_gps",
+        ]
         # UTC date time
         utim = fd.read_ui8(4)
         date_utc = tmlib.datetime(utim[2] + utim[3] * 256, utim[1], utim[0])
 
         # 1st lat/lon position after previous ADCP ping
         # This byte is in hundredths of seconds (10s of milliseconds):
-        utc_time_first_fix = tmlib.timedelta(
-            milliseconds=(int(fd.read_ui32(1) / 10)))
-        ens.clock_offset_UTC_gps[k] = fd.read_i32(
-            1) / 1000  # "PC clock offset from UTC" in ms
+        utc_time_first_fix = tmlib.timedelta(milliseconds=(int(fd.read_ui32(1) / 10)))
+        ens.clock_offset_UTC_gps[k] = (
+            fd.read_i32(1) / 1000
+        )  # "PC clock offset from UTC" in ms
         latitude_first_gps = fd.read_i32(1) * self._cfac
         longitude_first_gps = fd.read_i32(1) * self._cfac
 
         # Last lat/lon position prior to current ADCP ping
-        utc_time_fix = tmlib.timedelta(
-            milliseconds=(int(fd.read_ui32(1) / 10)))
+        utc_time_fix = tmlib.timedelta(milliseconds=(int(fd.read_ui32(1) / 10)))
         ens.time_gps[k] = tmlib.date2epoch(date_utc + utc_time_fix)[0]
         ens.latitude_gps[k] = fd.read_i32(1) * self._cfac
         ens.longitude_gps[k] = fd.read_i32(1) * self._cfac
 
         ens.avg_speed_gps[k] = fd.read_ui16(1) / 1000
-        ens.avg_dir_gps[k] = fd.read_ui16(1) * 180 / 2 ** 15  # avg true track
+        ens.avg_dir_gps[k] = fd.read_ui16(1) * 180 / 2**15  # avg true track
         fd.seek(2, 1)  # avg magnetic track
         ens.speed_made_good_gps[k] = fd.read_ui16(1) / 1000
-        ens.dir_made_good_gps[k] = fd.read_ui16(1) * 180 / 2 ** 15
+        ens.dir_made_good_gps[k] = fd.read_ui16(1) * 180 / 2**15
         fd.seek(2, 1)  # reserved
         ens.flags_gps[k] = int(np.binary_repr(fd.read_ui16(1)))
         fd.seek(6, 1)  # reserved, ADCP ensemble #
 
         # ADCP date time
         utim = fd.read_ui8(4)
         date_adcp = tmlib.datetime(utim[0] + utim[1] * 256, utim[3], utim[2])
-        time_adcp = tmlib.timedelta(
-            milliseconds=(int(fd.read_ui32(1) / 10)))
+        time_adcp = tmlib.timedelta(milliseconds=(int(fd.read_ui32(1) / 10)))
 
-        ens.pitch_gps[k] = fd.read_ui16(1) * 180 / 2 ** 15
-        ens.roll_gps[k] = fd.read_ui16(1) * 180 / 2 ** 15
-        ens.heading_gps[k] = fd.read_ui16(1) * 180 / 2 ** 15
+        ens.pitch_gps[k] = fd.read_ui16(1) * 180 / 2**15
+        ens.roll_gps[k] = fd.read_ui16(1) * 180 / 2**15
+        ens.heading_gps[k] = fd.read_ui16(1) * 180 / 2**15
 
         fd.seek(10, 1)
         self._nbyte = 2 + 76
 
-        if self._debug_level >= 0:
-            logging.info('Read VMDAS')
+        if self._debug_level > -1:
+            logging.info("Read VMDAS")
         self._read_vmdas = True
 
-    def read_winriver2(self, ):
+    def read_winriver2(self):
         startpos = self.f.tell()
         self._winrivprob = True
-        self.cfg['sourceprog'] = 'WinRiver2'
+        self.cfg["sourceprog"] = "WinRiver2"
         ens = self.ensemble
         k = ens.k
-        if self._debug_level >= 0:
-            logging.info('Read WinRiver2')
-        self._source = 3
+        if self._debug_level > -1:
+            logging.info("Read WinRiver2")
+        self._vm_source = 3
 
         spid = self.f.read_ui16(1)  # NMEA specific IDs
         if spid in [4, 104]:  # GGA
             sz = self.f.read_ui16(1)
             dtime = self.f.read_f64(1)
             if sz <= 43:  # If no sentence, data is still stored in nmea format
-                empty_gps = self.f.reads(sz-2)
+                empty_gps = self.f.reads(sz - 2)
                 self.f.seek(2, 1)
             else:  # TRDI rewrites the nmea string into their format if one is found
                 start_string = self.f.reads(6)
-                if type(start_string) != str:
-                    if self._debug_level >= 1:
-                        logging.warning(f'Invalid GGA string found in ensemble {k},'
-                                        ' skipping...')
-                    return 'FAIL'
+                if not isinstance(start_string, str):
+                    if self._debug_level > 0:
+                        logging.warning(
+                            f"Invalid GGA string found in ensemble {k}," " skipping..."
+                        )
+                    return "FAIL"
                 self.f.seek(1, 1)
                 gga_time = self.f.reads(9)
-                time = tmlib.timedelta(hours=int(gga_time[0:2]),
-                                       minutes=int(gga_time[2:4]),
-                                       seconds=int(gga_time[4:6]),
-                                       milliseconds=int(float(gga_time[6:])*1000))
+                time = tmlib.timedelta(
+                    hours=int(gga_time[0:2]),
+                    minutes=int(gga_time[2:4]),
+                    seconds=int(gga_time[4:6]),
+                    milliseconds=int(float(gga_time[6:]) * 1000),
+                )
                 clock = self.ensemble.rtc[:, :]
                 if clock[0, 0] < 100:
                     clock[0, :] += defs.century
                 date = tmlib.datetime(*clock[:3, 0]) + time
                 ens.time_gps[k] = tmlib.date2epoch(date)[0]
                 self.f.seek(1, 1)
                 ens.latitude_gps[k] = self.f.read_f64(1)
                 tcNS = self.f.reads(1)  # 'N' or 'S'
-                if tcNS == 'S':
+                if tcNS == "S":
                     ens.latitude_gps[k] *= -1
                 ens.longitude_gps[k] = self.f.read_f64(1)
                 tcEW = self.f.reads(1)  # 'E' or 'W'
-                if tcEW == 'W':
+                if tcEW == "W":
                     ens.longitude_gps[k] *= -1
                 ens.fix_gps[k] = self.f.read_ui8(1)  # gps fix type/quality
                 ens.n_sat_gps[k] = self.f.read_ui8(1)  # of satellites
                 # horizontal dilution of precision
-                ens.hdop_gps[k] = self.f.read_float(1)
-                ens.elevation_gps[k] = self.f.read_float(1)  # altitude
+                ens.hdop_gps[k] = self.f.read_f32(1)
+                ens.elevation_gps[k] = self.f.read_f32(1)  # altitude
                 m = self.f.reads(1)  # altitude unit, 'm'
-                h_geoid = self.f.read_float(1)  # height of geoid
+                h_geoid = self.f.read_f32(1)  # height of geoid
                 m2 = self.f.reads(1)  # geoid unit, 'm'
-                ens.rtk_age_gps[k] = self.f.read_float(1)
+                ens.rtk_age_gps[k] = self.f.read_f32(1)
                 station_id = self.f.read_ui16(1)
-            self.vars_read += ['time_gps', 'longitude_gps', 'latitude_gps', 'fix_gps',
-                               'n_sat_gps', 'hdop_gps', 'elevation_gps', 'rtk_age_gps']
+            self.vars_read += [
+                "time_gps",
+                "longitude_gps",
+                "latitude_gps",
+                "fix_gps",
+                "n_sat_gps",
+                "hdop_gps",
+                "elevation_gps",
+                "rtk_age_gps",
+            ]
             self._nbyte = self.f.tell() - startpos + 2
 
         elif spid in [5, 105]:  # VTG
             sz = self.f.read_ui16(1)
             dtime = self.f.read_f64(1)
             if sz <= 22:  # if no data
-                empty_gps = self.f.reads(sz-2)
+                empty_gps = self.f.reads(sz - 2)
                 self.f.seek(2, 1)
             else:
                 start_string = self.f.reads(6)
-                if type(start_string) != str:
-                    if self._debug_level >= 1:
-                        logging.warning(f'Invalid VTG string found in ensemble {k},'
-                                        ' skipping...')
-                    return 'FAIL'
+                if not isinstance(start_string, str):
+                    if self._debug_level > 0:
+                        logging.warning(
+                            f"Invalid VTG string found in ensemble {k}," " skipping..."
+                        )
+                    return "FAIL"
                 self.f.seek(1, 1)
-                true_track = self.f.read_float(1)
+                true_track = self.f.read_f32(1)
                 t = self.f.reads(1)  # 'T'
-                magn_track = self.f.read_float(1)
+                magn_track = self.f.read_f32(1)
                 m = self.f.reads(1)  # 'M'
-                speed_knot = self.f.read_float(1)
+                speed_knot = self.f.read_f32(1)
                 kts = self.f.reads(1)  # 'N'
-                speed_kph = self.f.read_float(1)
+                speed_kph = self.f.read_f32(1)
                 kph = self.f.reads(1)  # 'K'
                 mode = self.f.reads(1)
                 # knots -> m/s
                 ens.speed_over_grnd_gps[k] = speed_knot / 1.944
                 ens.dir_over_grnd_gps[k] = true_track
-            self.vars_read += ['speed_over_grnd_gps',
-                               'dir_over_grnd_gps']
+            self.vars_read += ["speed_over_grnd_gps", "dir_over_grnd_gps"]
             self._nbyte = self.f.tell() - startpos + 2
 
         elif spid in [6, 106]:  # 'DBT' depth sounder
             sz = self.f.read_ui16(1)
             dtime = self.f.read_f64(1)
             if sz <= 20:
-                empty_gps = self.f.reads(sz-2)
+                empty_gps = self.f.reads(sz - 2)
                 self.f.seek(2, 1)
             else:
                 start_string = self.f.reads(6)
-                if type(start_string) != str:
-                    if self._debug_level >= 1:
-                        logging.warning(f'Invalid DBT string found in ensemble {k},'
-                                        ' skipping...')
-                    return 'FAIL'
+                if not isinstance(start_string, str):
+                    if self._debug_level > 0:
+                        logging.warning(
+                            f"Invalid DBT string found in ensemble {k}," " skipping..."
+                        )
+                    return "FAIL"
                 self.f.seek(1, 1)
-                depth_ft = self.f.read_float(1)
+                depth_ft = self.f.read_f32(1)
                 ft = self.f.reads(1)  # 'f'
-                depth_m = self.f.read_float(1)
+                depth_m = self.f.read_f32(1)
                 m = self.f.reads(1)  # 'm'
-                depth_fathom = self.f.read_float(1)
+                depth_fathom = self.f.read_f32(1)
                 f = self.f.reads(1)  # 'F'
                 ens.dist_nmea[k] = depth_m
-            self.vars_read += ['dist_nmea']
+            self.vars_read += ["dist_nmea"]
             self._nbyte = self.f.tell() - startpos + 2
 
         elif spid in [7, 107]:  # 'HDT'
             sz = self.f.read_ui16(1)
             dtime = self.f.read_f64(1)
             if sz <= 14:
-                empty_gps = self.f.reads(sz-2)
+                empty_gps = self.f.reads(sz - 2)
                 self.f.seek(2, 1)
             else:
                 start_string = self.f.reads(6)
-                if type(start_string) != str:
-                    if self._debug_level >= 1:
-                        logging.warning(f'Invalid HDT string found in ensemble {k},'
-                                        ' skipping...')
-                    return 'FAIL'
+                if not isinstance(start_string, str):
+                    if self._debug_level > 0:
+                        logging.warning(
+                            f"Invalid HDT string found in ensemble {k}," " skipping..."
+                        )
+                    return "FAIL"
                 self.f.seek(1, 1)
                 ens.heading_gps[k] = self.f.read_f64(1)
                 tt = self.f.reads(1)
-            self.vars_read += ['heading_gps']
+            self.vars_read += ["heading_gps"]
             self._nbyte = self.f.tell() - startpos + 2
 
     def read_winriver(self, nbt):
         self._winrivprob = True
-        self.cfg['sourceprog'] = 'WINRIVER'
-        if self._source not in [2, 3]:
-            if self._debug_level >= 0:
-                logging.warning('\n***** Apparently a WINRIVER file - '
-                                'Raw NMEA data handler not yet implemented\n')
-            self._source = 2
+        self.cfg["sourceprog"] = "WINRIVER"
+        if self._vm_source not in [2, 3]:
+            if self._debug_level > -1:
+                logging.warning(
+                    "\n***** Apparently a WINRIVER file - "
+                    "Raw NMEA data handler not yet implemented\n"
+                )
+            self._vm_source = 2
         startpos = self.f.tell()
         sz = self.f.read_ui16(1)
-        tmp = self.f.reads(sz-2)
+        tmp = self.f.reads(sz - 2)
         self._nbyte = self.f.tell() - startpos + 2
 
     def skip_Ncol(self, n_skip=1):
-        self.f.seek(n_skip * self.cfg['n_cells'], 1)
-        self._nbyte = 2 + n_skip * self.cfg['n_cells']
+        self.f.seek(n_skip * self.cfg["n_cells"], 1)
+        self._nbyte = 2 + n_skip * self.cfg["n_cells"]
 
     def skip_Nbyte(self, n_skip):
         self.f.seek(n_skip, 1)
         self._nbyte = 2 + n_skip
 
     def read_nocode(self, id):
         # Skipping bytes from codes 0340-30FC, commented if needed
         hxid = hex(id)
-        if hxid[2:4] == '30':
+        if hxid[2:4] == "30":
             logging.warning("Skipping bytes from codes 0340-30FC")
             # I want to count the number of 1s in the middle 4 bits
             # of the 2nd two bytes.
             # 60 is a 0b00111100 mask
-            nflds = (bin(int(hxid[3]) & 60).count('1') +
-                     bin(int(hxid[4]) & 60).count('1'))
+            nflds = bin(int(hxid[3]) & 60).count("1") + bin(int(hxid[4]) & 60).count(
+                "1"
+            )
             # I want to count the number of 1s in the highest
             # 2 bits of byte 3
             # 3 is a 0b00000011 mask:
-            dfac = bin(int(hxid[3], 0) & 3).count('1')
+            dfac = bin(int(hxid[3], 0) & 3).count("1")
             self.skip_Nbyte(12 * nflds * dfac)
         else:
-            if self._debug_level >= 0:
-                logging.warning('  Unrecognized ID code: %0.4X' % id)
+            if self._debug_level > -1:
+                logging.warning("  Unrecognized ID code: %0.4X" % id)
             self.skip_nocode(id)
 
     def skip_nocode(self, id):
         # Skipping bytes if ID isn't known
         offsets = list(self.id_positions.values())
         idx = np.where(offsets == self.id_positions[id])[0][0]
-        byte_len = offsets[idx+1] - offsets[idx] - 2
+        byte_len = offsets[idx + 1] - offsets[idx] - 2
 
         self.skip_Nbyte(byte_len)
-        if self._debug_level >= 0:
+        if self._debug_level > -1:
             logging.debug(f"Skipping ID code {id}\n")
 
-    def cleanup(self, cfg, dat):
-        dat['coords']['range'] = (cfg['bin1_dist_m'] +
-                                  np.arange(self.ensemble['n_cells']) *
-                                  cfg['cell_size'])
+    def check_offset(self, offset, readbytes):
+        fd = self.f
+        if offset != 4 and self._fixoffset == 0:
+            if self._debug_level > 0:
+                if fd.tell() == self._filesize:
+                    logging.error(
+                        " EOF reached unexpectedly - discarding this last ensemble\n"
+                    )
+                else:
+                    logging.debug(
+                        "  Adjust location by {:d} (readbytes={:d},hdr['nbyte']={:d})\n".format(
+                            offset, readbytes, self.hdr["nbyte"]
+                        )
+                    )
+            self._fixoffset = offset - 4
+        fd.seek(4 + self._fixoffset, 1)
+
+    def remove_end(self, iens):
+        dat = self.outd
+        if self._debug_level > 0:
+            logging.info("  Encountered end of file.  Cleaning up data.")
+        for nm in self.vars_read:
+            defs._setd(dat, nm, defs._get(dat, nm)[..., :iens])
+
+    def save_profiles(self, dat, nm, en, iens):
+        ds = defs._get(dat, nm)
+        if self.n_avg == 1:
+            bn = en[nm][..., 0]
+        else:
+            bn = np.nanmean(en[nm], axis=-1)
 
+        # If n_cells has changed (RiverPro/StreamPro WinRiver transects)
+        if len(ds.shape) == 3:
+            if "_sl" in nm:
+                # This works here b/c the max number of surface layer cells
+                # is smaller than the min number of normal profile cells used.
+                # Extra nan cells created after this if-statement
+                # are trimmed off in self.cleanup.
+                bn = bn[: self.cfg["n_cells_sl"]]
+            else:
+                # Set bn to current ping size
+                bn = bn[: self.cfg["n_cells"]]
+                # If n_cells has increased, we also need to increment defs
+                if self.n_cells_diff > 0:
+                    a = np.empty((self.n_cells_diff, ds.shape[1], ds.shape[2])) * np.nan
+                    ds = np.append(ds, a.astype(ds.dtype), axis=0)
+                    defs._setd(dat, nm, ds)
+            # If the number of cells decreases, set extra cells to nan instead of
+            # whatever is stuck in memory
+            if ds.shape[0] != bn.shape[0]:
+                n_cells = ds.shape[0] - bn.shape[0]
+                a = np.empty((n_cells, bn.shape[1])) * np.nan
+                bn = np.append(bn, a.astype(ds.dtype), axis=0)
+
+        # Keep track of when the cell size changes
+        if self.cs_diff:
+            self.cs.append([iens, self.cfg["cell_size"]])
+            self.cs_diff = 0
+
+        # Then copy the ensemble to the dataset.
+        ds[..., iens] = bn
+        defs._setd(dat, nm, ds)
+
+        return dat
+
+    def cleanup(self, dat, cfg):
+        # Clean up changing cell size, if necessary
+        cs = np.array(self.cs)
+        cell_sizes = cs[:, 1]
+
+        # If cell sizes change, depth-bin average the smaller cell sizes
+        if len(self.cs) > 1:
+            bins_to_merge = cell_sizes.max() / cell_sizes
+            idx_start = cs[:, 0].astype(int)
+            idx_end = np.append(cs[1:, 0], self._nens).astype(int)
+
+            dv = dat["data_vars"]
+            for var in dv:
+                if (len(dv[var].shape) == 3) and ("_sl" not in var):
+                    # Create a new NaN var to save data in
+                    new_var = (np.zeros(dv[var].shape) * np.nan).astype(dv[var].dtype)
+                    # For each cell size change, reshape and bin-average
+                    for id1, id2, b in zip(idx_start, idx_end, bins_to_merge):
+                        array = np.transpose(dv[var][..., id1:id2])
+                        bin_arr = np.transpose(np.mean(self.reshape(array, b), axis=-1))
+                        new_var[: len(bin_arr), :, id1:id2] = bin_arr
+                    # Reset data. This often leaves nan data at farther ranges
+                    dv[var] = new_var
+
+        # Set cell size and range
+        cfg["n_cells"] = self.ensemble["n_cells"]
+        cfg["cell_size"] = cell_sizes.max()
+        dat["coords"]["range"] = (
+            cfg["bin1_dist_m"] + np.arange(cfg["n_cells"]) * cfg["cell_size"]
+        )
+
+        # Save configuration data as attributes
         for nm in cfg:
-            dat['attrs'][nm] = cfg[nm]
+            dat["attrs"][nm] = cfg[nm]
 
-        if 'surface_layer' in cfg:  # RiverPro/StreamPro
-            dat['coords']['range_sl'] = (cfg['bin1_dist_m_sl'] +
-                                         np.arange(self.cfg['n_cells_sl']) *
-                                         cfg['cell_size_sl'])
-            # Trim surface layer profile to length
-            dv = dat['data_vars']
+        # Clean up surface layer profiles
+        if "surface_layer" in cfg:  # RiverPro/StreamPro
+            dat["coords"]["range_sl"] = (
+                cfg["bin1_dist_m_sl"]
+                + np.arange(0, self.n_cells_sl) * cfg["cell_size_sl"]
+            )
+            # Trim off extra nan data
+            dv = dat["data_vars"]
             for var in dv:
-                if 'sl' in var:
-                    dv[var] = dv[var][:cfg['n_cells_sl']]
-            dat['attrs']['rotate_vars'].append('vel_sl')
+                if "sl" in var:
+                    dv[var] = dv[var][: self.n_cells_sl]
+            dat["attrs"]["rotate_vars"].append("vel_sl")
+
+        return dat, cfg
+
+    def reshape(self, arr, n_bin=None):
+        """
+        Reshape the array `arr` to shape (...,n,n_bin).
+        """
+
+        out = np.zeros(
+            list(arr.shape[:-1]) + [int(arr.shape[-1] // n_bin), int(n_bin)],
+            dtype=arr.dtype,
+        )
+        shp = out.shape
+        if np.mod(n_bin, 1) == 0:
+            # n_bin needs to be int
+            n_bin = int(n_bin)
+            # If n_bin is an integer, we can do this simply.
+            out[..., :n_bin] = (arr[..., : (shp[-2] * shp[-1])]).reshape(shp, order="C")
+        else:
+            inds = (np.arange(np.prod(shp[-2:])) * n_bin // int(n_bin)).astype(int)
+            # If there are too many indices, drop one bin
+            if inds[-1] >= arr.shape[-1]:
+                inds = inds[: -int(n_bin)]
+                shp[-2] -= 1
+                out = out[..., 1:, :]
+            n_bin = int(n_bin)
+            out[..., :n_bin] = (arr[..., inds]).reshape(shp, order="C")
+            n_bin = int(n_bin)
+
+        return out
 
     def finalize(self, dat):
-        """Remove the attributes from the data that were never loaded.
         """
+        Remove the attributes from the data that were never loaded.
+        """
+
         for nm in set(defs.data_defs.keys()) - self.vars_read:
             defs._pop(dat, nm)
         for nm in self.cfg:
-            dat['attrs'][nm] = self.cfg[nm]
+            dat["attrs"][nm] = self.cfg[nm]
 
         # VMDAS and WinRiver have different set sampling frequency
-        da = dat['attrs']
-        if hasattr(da, 'sourceprog') and (da['sourceprog'].lower() in ['vmdas', 'winriver', 'winriver2']):
-            da['fs'] = round(np.diff(dat['coords']['time']).mean() ** -1, 2)
+        da = dat["attrs"]
+        if ("sourceprog" in da) and (
+            da["sourceprog"].lower() in ["vmdas", "winriver", "winriver2"]
+        ):
+            da["fs"] = round(1 / np.median(np.diff(dat["coords"]["time"])), 2)
         else:
-            da['fs'] = (da['sec_between_ping_groups'] *
-                        da['pings_per_ensemble']) ** (-1)
-        da['n_cells'] = self.ensemble['n_cells']
+            da["fs"] = 1 / (da["sec_between_ping_groups"] * da["pings_per_ensemble"])
 
         for nm in defs.data_defs:
             shp = defs.data_defs[nm][0]
-            if len(shp) and shp[0] == 'nc' and defs._in_group(dat, nm):
+            if len(shp) and shp[0] == "nc" and defs._in_group(dat, nm):
                 defs._setd(dat, nm, np.swapaxes(defs._get(dat, nm), 0, 1))
 
-    def __enter__(self,):
-        return self
-
-    def __exit__(self, type, value, traceback):
-        self.f.close()
+        return dat
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/io/rdi_lib.py` & `mhkit-0.8.0/mhkit/dolfyn/io/rdi_lib.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,80 +1,80 @@
 import numpy as np
 from struct import unpack
 from os.path import expanduser
 
 
-class bin_reader():
+class bin_reader:
     """
     Reads binary data files. It is mostly for development purposes, to
     simplify learning a data file's format. Reading binary data files should
     minimize the number of calls to struct.unpack and file.read because many
     calls to these functions (i.e. using the code in this module) are slow.
     """
-    _size_factor = {'B': 1, 'b': 1, 'H': 2,
-                    'h': 2, 'L': 4, 'l': 4, 'f': 4, 'd': 8}
-    _frmt = {np.uint8: 'B', np.int8: 'b',
-             np.uint16: 'H', np.int16: 'h',
-             np.uint32: 'L', np.int32: 'l',
-             float: 'f', np.float32: 'f',
-             np.double: 'd', np.float64: 'd',
-             }
+
+    _size_factor = {"B": 1, "b": 1, "H": 2, "h": 2, "L": 4, "l": 4, "f": 4, "d": 8}
+    _frmt = {
+        np.uint8: "B",
+        np.int8: "b",
+        np.uint16: "H",
+        np.int16: "h",
+        np.uint32: "L",
+        np.int32: "l",
+        float: "f",
+        np.float32: "f",
+        np.double: "d",
+        np.float64: "d",
+    }
 
     @property
-    def pos(self,):
+    def pos(self):
         return self.f.tell()
 
-    def __enter__(self,):
-        return self
-
-    def __exit__(self,):
-        self.close()
-
-    def __init__(self, fname, endian='<', checksum_size=None, debug_level=0):
+    def __init__(self, fname, endian="<", checksum_size=None, debug_level=0):
         """
         Default to little-endian '<'...
         *checksum_size* is in bytes, if it is None or False, this
          function does not perform checksums.
         """
         self.endian = endian
-        self.f = open(expanduser(fname), 'rb')
+        self.f = open(expanduser(fname), "rb")
         self.f.seek(0, 2)
         self.fsize = self.tell()
         self.f.seek(0, 0)
         self.close = self.f.close
         if checksum_size:
             self.cs = self.checksum(0, checksum_size)
         else:
             self.cs = checksum_size
         self.debug_level = debug_level
 
-    def checksum(self,):
+    def checksum(self):
         """
         The next byte(s) are the expected checksum.  Perform the checksum.
         """
         if self.cs:
             cs = self.read(1, self.cs._frmt)
             self.cs(cs, True)
         else:
-            raise Exception('CheckSum not requested for this file')
+            raise Exception("CheckSum not requested for this file")
 
-    def tell(self,):
+    def tell(self):
         return self.f.tell()
 
     def seek(self, pos, rel=1):
         return self.f.seek(pos, rel)
 
     def reads(self, n):
         """
         Read a string of n characters.
         """
         val = self.f.read(n)
         self.cs and self.cs.add(val)
         try:
-            val = val.decode('utf-8')
+            val = val.decode("utf-8")
         except:
             if self.debug_level > 5:
                 print("ERROR DECODING: {}".format(val))
             pass
         return val
 
     def read(self, n, frmt):
@@ -84,32 +84,29 @@
         self.cs and self.cs.add(val)
         if n == 1:
             return unpack(self.endian + frmt * n, val)[0]
         else:
             return np.array(unpack(self.endian + frmt * n, val))
 
     def read_ui8(self, n):
-        return self.read(n, 'B')
-
-    def read_float(self, n):
-        return self.read(n, 'f')
+        return self.read(n, "B")
 
-    def read_double(self, n):
-        return self.read(n, 'd')
+    def read_f32(self, n):
+        return self.read(n, "f")
 
-    read_f32 = read_float
-    read_f64 = read_double
+    def read_f64(self, n):
+        return self.read(n, "d")
 
     def read_i8(self, n):
-        return self.read(n, 'b')
+        return self.read(n, "b")
 
     def read_ui16(self, n):
-        return self.read(n, 'H')
+        return self.read(n, "H")
 
     def read_i16(self, n):
-        return self.read(n, 'h')
+        return self.read(n, "h")
 
     def read_ui32(self, n):
-        return self.read(n, 'L')
+        return self.read(n, "L")
 
     def read_i32(self, n):
-        return self.read(n, 'l')
+        return self.read(n, "l")
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/rotate/api.py` & `mhkit-0.8.0/mhkit/dolfyn/rotate/api.py`

 * *Files 17% similar despite different names*

```diff
@@ -5,28 +5,28 @@
 from .base import _make_model
 import numpy as np
 import xarray as xr
 import warnings
 
 
 # The 'rotation chain'
-rc = ['beam', 'inst', 'earth', 'principal']
+rc = ["beam", "inst", "earth", "principal"]
 
 rot_module_dict = {
     # Nortek instruments
-    'vector': r_vec,
-    'awac': r_awac,
-    'signature': r_sig,
-    'ad2cp': r_sig,
-
+    "vector": r_vec,
+    "awac": r_awac,
+    "signature": r_sig,
+    "ad2cp": r_sig,
     # TRDI instruments
-    'rdi': r_rdi}
+    "rdi": r_rdi,
+}
 
 
-def rotate2(ds, out_frame='earth', inplace=True):
+def rotate2(ds, out_frame="earth", inplace=True):
     """
     Rotate a dataset to a new coordinate system.
 
     Parameters
     ----------
     ds : xr.Dataset
       The dolfyn dataset (ADV or ADCP) to rotate.
@@ -42,83 +42,88 @@
       Returns a new rotated dataset **when ``inplace=False``**, otherwise
       returns None.
 
     Notes
     -----
     - This function rotates all variables in ``ds.attrs['rotate_vars']``.
 
-    - In order to rotate to the 'principal' frame, a value should exist for 
-      ``ds.attrs['principal_heading']``. The function 
+    - In order to rotate to the 'principal' frame, a value should exist for
+      ``ds.attrs['principal_heading']``. The function
       :func:`calc_principal_heading <dolfyn.calc_principal_heading>`
       is recommended for this purpose, e.g.:
 
           ds.attrs['principal_heading'] = dolfyn.calc_principal_heading(ds['vel'].mean(range))
 
       where here we are using the depth-averaged velocity to calculate
       the principal direction.
     """
 
     # Create and return deep copy if not writing "in place"
     if not inplace:
         ds = ds.copy(deep=True)
 
     csin = ds.coord_sys.lower()
-    if csin == 'ship':
-        csin = 'inst'
+    if csin == "ship":
+        csin = "inst"
 
     # Returns True/False if head2inst_rotmat has been set/not-set.
     r_vec._check_inst2head_rotmat(ds)
 
-    if out_frame == 'principal' and csin != 'earth':
+    if out_frame == "principal" and csin != "earth":
         warnings.warn(
             "You are attempting to rotate into the 'principal' "
             "coordinate system, but the dataset is in the {} "
             "coordinate system. Be sure that 'principal_heading' is "
-            "defined based on the earth coordinate system.".format(csin))
+            "defined based on the earth coordinate system.".format(csin)
+        )
 
     rmod = None
     for ky in rot_module_dict:
         if ky in _make_model(ds):
             rmod = rot_module_dict[ky]
             break
     if rmod is None:
-        raise ValueError("Rotations are not defined for "
-                         "instrument '{}'.".format(_make_model(ds)))
+        raise ValueError(
+            "Rotations are not defined for " "instrument '{}'.".format(_make_model(ds))
+        )
 
     # Get the 'indices' of the rotation chain
     try:
         iframe_in = rc.index(csin)
     except ValueError:
-        raise Exception("The coordinate system of the input "
-                        "dataset, '{}', is invalid."
-                        .format(ds.coord_sys))
+        raise Exception(
+            "The coordinate system of the input "
+            "dataset, '{}', is invalid.".format(ds.coord_sys)
+        )
     try:
         iframe_out = rc.index(out_frame.lower())
     except ValueError:
-        raise Exception("The specified output coordinate system "
-                        "is invalid, please select one of: 'beam', 'inst', "
-                        "'earth', 'principal'.")
+        raise Exception(
+            "The specified output coordinate system "
+            "is invalid, please select one of: 'beam', 'inst', "
+            "'earth', 'principal'."
+        )
 
     if iframe_out == iframe_in:
         print("Data is already in the {} coordinate system".format(out_frame))
 
     if iframe_out > iframe_in:
         reverse = False
     else:
         reverse = True
 
     while ds.coord_sys.lower() != out_frame.lower():
         csin = ds.coord_sys
-        if csin == 'ship':
-            csin = 'inst'
+        if csin == "ship":
+            csin = "inst"
         inow = rc.index(csin)
         if reverse:
-            func = getattr(rmod, '_' + rc[inow - 1] + '2' + rc[inow])
+            func = getattr(rmod, "_" + rc[inow - 1] + "2" + rc[inow])
         else:
-            func = getattr(rmod, '_' + rc[inow] + '2' + rc[inow + 1])
+            func = getattr(rmod, "_" + rc[inow] + "2" + rc[inow + 1])
         ds = func(ds, reverse=reverse)
 
     if not inplace:
         return ds
 
 
 def calc_principal_heading(vel, tidal_mode=True):
@@ -126,15 +131,15 @@
     Compute the principal angle of the horizontal velocity.
 
     Parameters
     ----------
     vel : np.ndarray (2,...,Nt), or (3,...,Nt)
       The 2D or 3D velocity array (3rd-dim is ignored in this calculation)
     tidal_mode : bool
-      If true, range is set from 0 to +/-180 degrees. If false, range is 0 to 
+      If true, range is set from 0 to +/-180 degrees. If false, range is 0 to
       360 degrees. Default = True
 
     Returns
     -------
     p_heading : float or ndarray
       The principal heading in degrees clockwise from North.
 
@@ -161,16 +166,15 @@
         dt[dt.imag <= 0] *= -1
         # Now double the angle, so that angles near pi and 0 get averaged
         # together correctly:
         dt *= np.exp(1j * np.angle(dt))
         dt = np.ma.masked_invalid(dt)
         # Divide the angle by 2 to remove the doubling done on the previous
         # line.
-        pang = np.angle(
-            np.nanmean(dt, -1, dtype=np.complex128)) / 2
+        pang = np.angle(np.nanmean(dt, -1, dtype=np.complex128)) / 2
     else:
         pang = np.angle(np.nanmean(dt, -1))
 
     return np.round((90 - np.rad2deg(pang)), decimals=4)
 
 
 def set_declination(ds, declin, inplace=True):
@@ -221,45 +225,45 @@
       'True' earth coordinate system)
     """
 
     # Create and return deep copy if not writing "in place"
     if not inplace:
         ds = ds.copy(deep=True)
 
-    if 'declination' in ds.attrs:
-        angle = declin - ds.attrs.pop('declination')
+    if "declination" in ds.attrs:
+        angle = declin - ds.attrs.pop("declination")
     else:
         angle = declin
     cd = np.cos(-np.deg2rad(angle))
     sd = np.sin(-np.deg2rad(angle))
 
     # The ordering is funny here because orientmat is the
     # transpose of the inst->earth rotation matrix:
-    Rdec = np.array([[cd, -sd, 0],
-                     [sd, cd, 0],
-                     [0, 0, 1]])
+    Rdec = np.array([[cd, -sd, 0], [sd, cd, 0], [0, 0, 1]])
 
-    if ds.coord_sys == 'earth':
+    if ds.coord_sys == "earth":
         rotate2earth = True
-        rotate2(ds, 'inst', inplace=True)
+        rotate2(ds, "inst", inplace=True)
     else:
         rotate2earth = False
 
-    ds['orientmat'].values = np.einsum('kj...,ij->ki...',
-                                       ds['orientmat'].values,
-                                       Rdec, )
-    if 'heading' in ds:
-        ds['heading'] += angle
+    ds["orientmat"].values = np.einsum(
+        "kj...,ij->ki...",
+        ds["orientmat"].values,
+        Rdec,
+    )
+    if "heading" in ds:
+        ds["heading"] += angle
     if rotate2earth:
-        rotate2(ds, 'earth', inplace=True)
-    if 'principal_heading' in ds.attrs:
-        ds.attrs['principal_heading'] += angle
+        rotate2(ds, "earth", inplace=True)
+    if "principal_heading" in ds.attrs:
+        ds.attrs["principal_heading"] += angle
 
-    ds.attrs['declination'] = declin
-    ds.attrs['declination_in_orientmat'] = 1  # logical
+    ds.attrs["declination"] = declin
+    ds.attrs["declination_in_orientmat"] = 1  # logical
 
     if not inplace:
         return ds
 
 
 def set_inst2head_rotmat(ds, rotmat, inplace=True):
     """
@@ -291,36 +295,37 @@
     coordinate system).
     """
 
     # Create and return deep copy if not writing "in place"
     if not inplace:
         ds = ds.copy(deep=True)
 
-    if not ds.inst_model.lower() == 'vector':
-        raise Exception("Setting 'inst2head_rotmat' is only supported "
-                        "for Nortek Vector ADVs.")
-    if ds.get('inst2head_rotmat', None) is not None:
+    if not ds.inst_model.lower() == "vector":
+        raise Exception(
+            "Setting 'inst2head_rotmat' is only supported " "for Nortek Vector ADVs."
+        )
+    if ds.get("inst2head_rotmat", None) is not None:
         raise Exception(
             "You are setting 'inst2head_rotmat' after it has already "
-            "been set. You can only set it once.")
+            "been set. You can only set it once."
+        )
 
     csin = ds.coord_sys
-    if csin not in ['inst', 'beam']:
-        rotate2(ds, 'inst', inplace=True)
+    if csin not in ["inst", "beam"]:
+        rotate2(ds, "inst", inplace=True)
 
-    ds['inst2head_rotmat'] = xr.DataArray(np.array(rotmat),
-                                          dims=['x1', 'x2'],
-                                          coords={'x1': [1, 2, 3],
-                                                  'x2': [1, 2, 3]})
+    ds["inst2head_rotmat"] = xr.DataArray(
+        np.array(rotmat), dims=["x1", "x2"], coords={"x1": [1, 2, 3], "x2": [1, 2, 3]}
+    )
 
-    ds.attrs['inst2head_rotmat_was_set'] = 1  # logical
+    ds.attrs["inst2head_rotmat_was_set"] = 1  # logical
     # Note that there is no validation that the user doesn't
     # change `ds.attrs['inst2head_rotmat']` after calling this
     # function.
 
-    if not csin == 'beam':  # csin not 'beam', then we're in inst
+    if not csin == "beam":  # csin not 'beam', then we're in inst
         ds = r_vec._rotate_inst2head(ds)
-    if csin not in ['inst', 'beam']:
+    if csin not in ["inst", "beam"]:
         rotate2(ds, csin, inplace=True)
 
     if not inplace:
         return ds
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/rotate/base.py` & `mhkit-0.8.0/mhkit/dolfyn/rotate/base.py`

 * *Files 18% similar despite different names*

```diff
@@ -6,16 +6,15 @@
 
 
 def _make_model(ds):
     """
     The make and model of the instrument that collected the data
     in this data object.
     """
-    return '{} {}'.format(ds.attrs['inst_make'],
-                          ds.attrs['inst_model']).lower()
+    return "{} {}".format(ds.attrs["inst_make"], ds.attrs["inst_model"]).lower()
 
 
 def _check_rotmat_det(rotmat, thresh=1e-3):
     """
     Check that the absolute error of the determinant is small.
 
           abs(det(rotmat) - 1) < thresh
@@ -26,80 +25,89 @@
     if rotmat.ndim > 2:
         rotmat = np.transpose(rotmat)
     return np.abs(det(rotmat) - 1) < thresh
 
 
 def _check_rotate_vars(ds, rotate_vars):
     if rotate_vars is None:
-        if 'rotate_vars' in ds.attrs:
+        if "rotate_vars" in ds.attrs:
             rotate_vars = ds.rotate_vars
         else:
-            warnings.warn("    'rotate_vars' attribute not found."
-                          "Rotating `vel`.")
-            rotate_vars = ['vel']
+            warnings.warn("    'rotate_vars' attribute not found." "Rotating `vel`.")
+            rotate_vars = ["vel"]
 
     return rotate_vars
 
 
 def _set_coords(ds, ref_frame, forced=False):
     """
-    Checks the current reference frame and adjusts xarray coords/dims 
+    Checks the current reference frame and adjusts xarray coords/dims
     as necessary.
     Makes sure assigned dataarray coordinates match what DOLfYN is reading in.
     """
 
     make = _make_model(ds)
 
-    XYZ = ['X', 'Y', 'Z']
-    ENU = ['E', 'N', 'U']
+    XYZ = ["X", "Y", "Z"]
+    ENU = ["E", "N", "U"]
     beam = ds.beam.values
-    principal = ['streamwise', 'x-stream', 'vert']
+    principal = ["streamwise", "x-stream", "vert"]
 
     # check make/model
-    if 'rdi' in make:
-        inst = ['X', 'Y', 'Z', 'err']
-        earth = ['E', 'N', 'U', 'err']
-        princ = ['streamwise', 'x-stream', 'vert', 'err']
-
-    elif 'nortek' in make:
-        if 'signature' in make or 'ad2cp' in make:
-            inst = ['X', 'Y', 'Z1', 'Z2']
-            earth = ['E', 'N', 'U1', 'U2']
-            princ = ['streamwise', 'x-stream', 'vert1', 'vert2']
+    if "rdi" in make:
+        inst = ["X", "Y", "Z", "err"]
+        earth = ["E", "N", "U", "err"]
+        princ = ["streamwise", "x-stream", "vert", "err"]
+
+    elif "nortek" in make:
+        if "signature" in make or "ad2cp" in make:
+            inst = ["X", "Y", "Z1", "Z2"]
+            earth = ["E", "N", "U1", "U2"]
+            princ = ["streamwise", "x-stream", "vert1", "vert2"]
 
         else:  # AWAC or Vector
             inst = XYZ
             earth = ENU
             princ = principal
 
-    orient = {'beam': beam, 'inst': inst, 'ship': inst, 'earth': earth,
-              'principal': princ}
-    orientIMU = {'beam': XYZ, 'inst': XYZ, 'ship': XYZ, 'earth': ENU,
-                 'principal': principal}
+    orient = {
+        "beam": beam,
+        "inst": inst,
+        "ship": inst,
+        "earth": earth,
+        "principal": princ,
+    }
+    orientIMU = {
+        "beam": XYZ,
+        "inst": XYZ,
+        "ship": XYZ,
+        "earth": ENU,
+        "principal": principal,
+    }
 
     if forced:
-        ref_frame += '-forced'
+        ref_frame += "-forced"
 
     # Update 'dir' and 'dirIMU' dimensions
-    attrs = ds['dir'].attrs
-    attrs.update({'ref_frame': ref_frame})
+    attrs = ds["dir"].attrs
+    attrs.update({"ref_frame": ref_frame})
 
-    ds['dir'] = orient[ref_frame]
-    ds['dir'].attrs = attrs
-    if hasattr(ds, 'dirIMU'):
-        ds['dirIMU'] = orientIMU[ref_frame]
-        ds['dirIMU'].attrs = attrs
+    ds["dir"] = orient[ref_frame]
+    ds["dir"].attrs = attrs
+    if hasattr(ds, "dirIMU"):
+        ds["dirIMU"] = orientIMU[ref_frame]
+        ds["dirIMU"].attrs = attrs
 
-    ds.attrs['coord_sys'] = ref_frame
+    ds.attrs["coord_sys"] = ref_frame
 
     # These are essentially one extra line to scroll through
-    tag = ['', '_echo', '_bt']
+    tag = ["", "_echo", "_bt"]
     for tg in tag:
-        if hasattr(ds, 'coord_sys_axes'+tg):
-            ds.attrs.pop('coord_sys_axes'+tg)
+        if hasattr(ds, "coord_sys_axes" + tg):
+            ds.attrs.pop("coord_sys_axes" + tg)
 
     return ds
 
 
 def _beam2inst(dat, reverse=False, force=False):
     """
     Rotate velocities from beam to instrument coordinates.
@@ -118,57 +126,58 @@
       dat.props['coord_sys'] string. If force is a list, it contains
       a list of variables that should be rotated (rather than the
       default values in adpo.rotate_vars).
       Default = False
     """
 
     if not force:
-        if not reverse and dat.coord_sys.lower() != 'beam':
-            raise ValueError('The input must be in beam coordinates.')
-        if reverse and dat.coord_sys != 'inst':
-            raise ValueError('The input must be in inst coordinates.')
+        if not reverse and dat.coord_sys.lower() != "beam":
+            raise ValueError("The input must be in beam coordinates.")
+        if reverse and dat.coord_sys != "inst":
+            raise ValueError("The input must be in inst coordinates.")
 
-    rotmat = dat['beam2inst_orientmat']
+    rotmat = dat["beam2inst_orientmat"]
 
     if isinstance(force, (list, set, tuple)):
         # You can force a distinct set of variables to be rotated by
         # specifying it here.
         rotate_vars = force
     else:
         rotate_vars = [
-            ky for ky in dat.rotate_vars if dat[ky].shape[0] == rotmat.shape[0]]
+            ky for ky in dat.rotate_vars if dat[ky].shape[0] == rotmat.shape[0]
+        ]
 
-    cs = 'inst'
+    cs = "inst"
     if reverse:
         # Can't use transpose because rotation is not between
         # orthogonal coordinate systems
         rotmat = inv(rotmat)
-        cs = 'beam'
+        cs = "beam"
     for ky in rotate_vars:
-        dat[ky].values = np.einsum('ij,j...->i...', rotmat, dat[ky].values)
+        dat[ky].values = np.einsum("ij,j...->i...", rotmat, dat[ky].values)
 
     if force:
         dat = _set_coords(dat, cs, forced=True)
     else:
         dat = _set_coords(dat, cs)
 
     return dat
 
 
-def euler2orient(heading, pitch, roll, units='degrees'):
+def euler2orient(heading, pitch, roll, units="degrees"):
     """
     Calculate the orientation matrix from DOLfYN-defined euler angles.
 
     This function is not likely to be called during data processing since it requires
     DOLfYN-defined euler angles. It is intended for testing DOLfYN.
 
     The matrices H, P, R are the transpose of the matrices for rotation about z, y, x
     as shown here https://en.wikipedia.org/wiki/Rotation_matrix. The transpose is used
-    because in DOLfYN the orientation matrix is organized for 
-    rotation from EARTH --> INST, while the wiki's matrices are organized for 
+    because in DOLfYN the orientation matrix is organized for
+    rotation from EARTH --> INST, while the wiki's matrices are organized for
     rotation from INST --> EARTH.
 
     Parameters
     ----------
     heading : numpy.ndarray
       The heading angle.
     pitch : numpy.ndarray
@@ -183,33 +192,33 @@
     omat : numpy.ndarray (3x3xtime)
       The orientation matrix of the data. The returned orientation
       matrix obeys the following conventions:
 
        - a "ZYX" rotation order. That is, these variables are computed
          assuming that rotation from the earth -> instrument frame happens
          by rotating around the z-axis first (heading), then rotating
-         around the y-axis (pitch), then rotating around the x-axis (roll). 
+         around the y-axis (pitch), then rotating around the x-axis (roll).
          Note this requires matrix multiplication in the reverse order.
 
        - heading is defined as the direction the x-axis points, positive
          clockwise from North (this is *opposite* the right-hand-rule
          around the Z-axis), range 0-360 degrees.
 
        - pitch is positive when the x-axis pitches up (this is *opposite* the
          right-hand-rule around the Y-axis)
 
        - roll is positive according to the right-hand-rule around the
          instrument's x-axis
     """
 
-    if units.lower() == 'degrees':
+    if units.lower() == "degrees":
         pitch = np.deg2rad(pitch)
         roll = np.deg2rad(roll)
         heading = np.deg2rad(heading)
-    elif units.lower() == 'radians':
+    elif units.lower() == "radians":
         pass
     else:
         raise Exception("Invalid units")
 
     # Converts the DOLfYN-defined heading to one that follows the right-hand-rule
     # reports heading as rotation of the y-axis positive counterclockwise from North
     heading = np.pi / 2 - heading
@@ -223,27 +232,36 @@
     sp = np.sin(pitch)
     cr = np.cos(roll)
     sr = np.sin(roll)
     zero = np.zeros_like(sr)
     one = np.ones_like(sr)
 
     H = np.array(
-        [[ch, sh, zero],
-         [-sh, ch, zero],
-         [zero, zero, one], ])
+        [
+            [ch, sh, zero],
+            [-sh, ch, zero],
+            [zero, zero, one],
+        ]
+    )
     P = np.array(
-        [[cp, zero, -sp],
-         [zero, one, zero],
-         [sp, zero, cp], ])
+        [
+            [cp, zero, -sp],
+            [zero, one, zero],
+            [sp, zero, cp],
+        ]
+    )
     R = np.array(
-        [[one, zero, zero],
-         [zero, cr, sr],
-         [zero, -sr, cr], ])
+        [
+            [one, zero, zero],
+            [zero, cr, sr],
+            [zero, -sr, cr],
+        ]
+    )
 
-    return np.einsum('ij...,jk...,kl...->il...', R, P, H)
+    return np.einsum("ij...,jk...,kl...->il...", R, P, H)
 
 
 def orient2euler(omat):
     """
     Calculate DOLfYN-defined euler angles from the orientation matrix.
 
     Parameters
@@ -254,26 +272,25 @@
     Returns
     -------
     heading : numpy.ndarray
       The heading angle. Heading is defined as the direction the x-axis points,
       positive clockwise from North (this is *opposite* the right-hand-rule
       around the Z-axis), range 0-360 degrees.
     pitch : np.ndarray
-      The pitch angle (degrees). Pitch is positive when the x-axis 
+      The pitch angle (degrees). Pitch is positive when the x-axis
       pitches up (this is *opposite* the right-hand-rule around the Y-axis).
     roll : np.ndarray
-      The roll angle (degrees). Roll is positive according to the 
+      The roll angle (degrees). Roll is positive according to the
       right-hand-rule around the instrument's x-axis.
     """
 
-    if isinstance(omat, np.ndarray) and \
-            omat.shape[:2] == (3, 3):
+    if isinstance(omat, np.ndarray) and omat.shape[:2] == (3, 3):
         pass
-    elif hasattr(omat, 'orientmat'):
-        omat = omat['orientmat'].values
+    elif hasattr(omat, "orientmat"):
+        omat = omat["orientmat"].values
 
     # Note: orientation matrix is earth->inst unless supplied by an external IMU
     hh = np.rad2deg(np.arctan2(omat[0, 0], omat[0, 1]))
     hh %= 360
     return (
         # heading
         hh,
@@ -282,15 +299,15 @@
         # roll
         np.rad2deg(np.arctan2(omat[1, 2], omat[2, 2])),
     )
 
 
 def quaternion2orient(quaternions):
     """
-    Calculate orientation from Nortek AHRS quaternions, where q = [W, X, Y, Z] 
+    Calculate orientation from Nortek AHRS quaternions, where q = [W, X, Y, Z]
     instead of the standard q = [X, Y, Z, W] = [q1, q2, q3, q4]
 
     Parameters
     ----------
     quaternions : xarray.DataArray
       Quaternion dataArray from the raw dataset
 
@@ -301,53 +318,73 @@
 
     See Also
     --------
     scipy.spatial.transform.Rotation
     """
 
     omat = type(quaternions)(np.empty((3, 3, quaternions.time.size)))
-    omat = omat.rename({'dim_0': 'earth', 'dim_1': 'inst', 'dim_2': 'time'})
+    omat = omat.rename({"dim_0": "earth", "dim_1": "inst", "dim_2": "time"})
 
     for i in range(quaternions.time.size):
-        r = R.from_quat([quaternions.isel(q=1, time=i),
-                         quaternions.isel(q=2, time=i),
-                         quaternions.isel(q=3, time=i),
-                         quaternions.isel(q=0, time=i)])
+        r = R.from_quat(
+            [
+                quaternions.isel(q=1, time=i),
+                quaternions.isel(q=2, time=i),
+                quaternions.isel(q=3, time=i),
+                quaternions.isel(q=0, time=i),
+            ]
+        )
         omat[..., i] = r.as_matrix()
 
     # quaternions in inst2earth reference frame, need to rotate to earth2inst
     omat.values = np.rollaxis(omat.values, 1)
 
-    earth = xr.DataArray(['E', 'N', 'U'], dims=['earth'], name='earth', attrs={
-        'units': '1', 'long_name': 'Earth Reference Frame', 'coverage_content_type': 'coordinate'})
-    inst = xr.DataArray(['X', 'Y', 'Z'], dims=['inst'], name='inst', attrs={
-        'units': '1', 'long_name': 'Instrument Reference Frame', 'coverage_content_type': 'coordinate'})
-    return omat.assign_coords({'earth': earth, 'inst': inst, 'time': quaternions.time})
+    earth = xr.DataArray(
+        ["E", "N", "U"],
+        dims=["earth"],
+        name="earth",
+        attrs={
+            "units": "1",
+            "long_name": "Earth Reference Frame",
+            "coverage_content_type": "coordinate",
+        },
+    )
+    inst = xr.DataArray(
+        ["X", "Y", "Z"],
+        dims=["inst"],
+        name="inst",
+        attrs={
+            "units": "1",
+            "long_name": "Instrument Reference Frame",
+            "coverage_content_type": "coordinate",
+        },
+    )
+    return omat.assign_coords({"earth": earth, "inst": inst, "time": quaternions.time})
 
 
 def calc_tilt(pitch, roll):
     """
     Calculate "tilt", the vertical inclination, from pitch and roll.
 
     Parameters
     ----------
     roll : numpy.ndarray or xarray.DataArray
       Instrument roll in degrees
     pitch : numpy.ndarray or xarray.DataArray
       Instrument pitch in degrees
-      
+
     Returns
     -------
     tilt : numpy.ndarray
       Vertical inclination of the instrument in degrees
     """
 
-    if 'xarray' in type(pitch).__module__:
+    if "xarray" in type(pitch).__module__:
         pitch = pitch.values
-    if 'xarray' in type(roll).__module__:
+    if "xarray" in type(roll).__module__:
         roll = roll.values
 
     tilt = np.arctan(
         np.sqrt(np.tan(np.deg2rad(roll)) ** 2 + np.tan(np.deg2rad(pitch)) ** 2)
     )
 
     return np.rad2deg(tilt)
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/rotate/rdi.py` & `mhkit-0.8.0/mhkit/dolfyn/rotate/rdi.py`

 * *Files 16% similar despite different names*

```diff
@@ -27,40 +27,41 @@
     Notes
     -----
     The rotation matrix is taken from the Teledyne RDI ADCP Coordinate
     Transformation manual January 2008
     """
 
     csin = adcpo.coord_sys.lower()
-    cs_allowed = ['inst', 'ship']
+    cs_allowed = ["inst", "ship"]
     if reverse:
-        cs_allowed = ['earth']
+        cs_allowed = ["earth"]
     if not force and csin not in cs_allowed:
-        raise ValueError("Invalid rotation for data in {}-frame "
-                         "coordinate system.".format(csin))
+        raise ValueError(
+            "Invalid rotation for data in {}-frame " "coordinate system.".format(csin)
+        )
 
-    if 'orientmat' in adcpo:
-        omat = adcpo['orientmat']
+    if "orientmat" in adcpo:
+        omat = adcpo["orientmat"]
     else:
         omat = _calc_orientmat(adcpo)
 
     rotate_vars = _check_rotate_vars(adcpo, rotate_vars)
 
     # rollaxis gives transpose of orientation matrix.
     # The 'rotation matrix' is the transpose of the 'orientation matrix'
     # NOTE: the double 'rollaxis' within this function, and here, has
     # minimal computational impact because np.rollaxis returns a
     # view (not a new array)
     rotmat = np.rollaxis(omat.data, 1)
     if reverse:
-        cs_new = 'inst'
-        sumstr = 'jik,j...k->i...k'
+        cs_new = "inst"
+        sumstr = "jik,j...k->i...k"
     else:
-        cs_new = 'earth'
-        sumstr = 'ijk,j...k->i...k'
+        cs_new = "earth"
+        sumstr = "ijk,j...k->i...k"
 
     # Only operate on the first 3-components, b/c the 4th is err_vel
     for nm in rotate_vars:
         dat = adcpo[nm].values
         dat[:3] = np.einsum(sumstr, rotmat, dat[:3])
         adcpo[nm].values = dat.copy()
 
@@ -87,26 +88,25 @@
 
     if degrees:
         theta = np.deg2rad(theta)
     if convex == 0 or convex == -1:
         c = -1
     else:
         c = 1
-    a = 1 / (2. * np.sin(theta))
-    b = 1 / (4. * np.cos(theta))
-    d = a / (2. ** 0.5)
-    return np.array([[c * a, -c * a, 0, 0],
-                     [0, 0, -c * a, c * a],
-                     [b, b, b, b],
-                     [d, d, -d, -d]])
+    a = 1 / (2.0 * np.sin(theta))
+    b = 1 / (4.0 * np.cos(theta))
+    d = a / (2.0**0.5)
+    return np.array(
+        [[c * a, -c * a, 0, 0], [0, 0, -c * a, c * a], [b, b, b, b], [d, d, -d, -d]]
+    )
 
 
 def _calc_orientmat(adcpo):
     """
-    Calculate the orientation matrix using the raw 
+    Calculate the orientation matrix using the raw
     heading, pitch, roll values from the RDI binary file.
 
     Parameters
     ----------
     adcpo : xarray.Dataset
       The adcp dataset containing the data.
 
@@ -119,31 +119,31 @@
         P = arctan[tan(Tilt1)*cos(Tilt2)]    (Equation 18)
 
     Where: Tilt1 is the measured pitch from the internal sensor, and
     Tilt2 is the measured roll from the internal sensor The raw pitch
     (Tilt 1) is recorded in the variable leader. P is set to 0 if the
     "use tilt" bit of the EX command is not set."""
 
-    r = np.deg2rad(adcpo['roll'].values)
-    p = np.arctan(np.tan(np.deg2rad(adcpo['pitch'].values)) * np.cos(r))
-    h = np.deg2rad(adcpo['heading'].values)
+    r = np.deg2rad(adcpo["roll"].values)
+    p = np.arctan(np.tan(np.deg2rad(adcpo["pitch"].values)) * np.cos(r))
+    h = np.deg2rad(adcpo["heading"].values)
 
-    if 'rdi' in adcpo.inst_make.lower():
-        if adcpo.orientation == 'up':
+    if "rdi" in adcpo.inst_make.lower():
+        if adcpo.orientation == "up":
             """
             ## RDI-ADCP-MANUAL (Jan 08, section 5.6 page 18)
             Since the roll describes the ship axes rather than the
             instrument axes, in the case of upward-looking
             orientation, 180 degrees must be added to the measured
             roll before it is used to calculate M. This is equivalent
             to negating the first and third columns of M. R is set
             to 0 if the "use tilt" bit of the EX command is not set.
             """
             r += np.pi
-        if (adcpo.coord_sys == 'ship' and adcpo.use_pitchroll == 'yes'):
+        if adcpo.coord_sys == "ship" and adcpo.use_pitchroll == "yes":
             r[:] = 0
             p[:] = 0
 
     ch = np.cos(h)
     sh = np.sin(h)
     cr = np.cos(r)
     sr = np.sin(r)
@@ -159,18 +159,33 @@
     rotmat[2, 0, :] = -cp * sr
     rotmat[2, 1, :] = sp
     rotmat[2, 2, :] = cp * cr
 
     # The 'orientation matrix' is the transpose of the 'rotation matrix'.
     omat = np.rollaxis(rotmat, 1)
 
-    earth = xr.DataArray(['E', 'N', 'U'], dims=['earth'], name='earth', attrs={
-        'units': '1', 'long_name': 'Earth Reference Frame', 'coverage_content_type': 'coordinate'})
-    inst = xr.DataArray(['X', 'Y', 'Z'], dims=['inst'], name='inst', attrs={
-        'units': '1', 'long_name': 'Instrument Reference Frame', 'coverage_content_type': 'coordinate'})
-    return xr.DataArray(omat,
-                        coords={'earth': earth,
-                                'inst': inst,
-                                'time': adcpo.time},
-                        dims=['earth', 'inst', 'time'],
-                        attrs={'units': '1',
-                               'long_name': 'Orientation Matrix'})
+    earth = xr.DataArray(
+        ["E", "N", "U"],
+        dims=["earth"],
+        name="earth",
+        attrs={
+            "units": "1",
+            "long_name": "Earth Reference Frame",
+            "coverage_content_type": "coordinate",
+        },
+    )
+    inst = xr.DataArray(
+        ["X", "Y", "Z"],
+        dims=["inst"],
+        name="inst",
+        attrs={
+            "units": "1",
+            "long_name": "Instrument Reference Frame",
+            "coverage_content_type": "coordinate",
+        },
+    )
+    return xr.DataArray(
+        omat,
+        coords={"earth": earth, "inst": inst, "time": adcpo.time},
+        dims=["earth", "inst", "time"],
+        attrs={"units": "1", "long_name": "Orientation Matrix"},
+    )
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/rotate/signature.py` & `mhkit-0.8.0/mhkit/dolfyn/rotate/signature.py`

 * *Files 7% similar despite different names*

```diff
@@ -18,65 +18,75 @@
     reverse : bool
       If True, this function performs the inverse rotation (earth->inst).
       Default = False
     rotate_vars : iterable
       The list of variables to rotate. By default this is taken from
       adcpo.rotate_vars.
     force : bool
-      Do not check which frame the data is in prior to performing 
+      Do not check which frame the data is in prior to performing
       this rotation. Default = False
     """
 
     if reverse:
         # The transpose of the rotation matrix gives the inverse
         # rotation, so we simply reverse the order of the einsum:
-        sumstr = 'jik,j...k->i...k'
-        cs_now = 'earth'
-        cs_new = 'inst'
+        sumstr = "jik,j...k->i...k"
+        cs_now = "earth"
+        cs_new = "inst"
     else:
-        sumstr = 'ijk,j...k->i...k'
-        cs_now = 'inst'
-        cs_new = 'earth'
+        sumstr = "ijk,j...k->i...k"
+        cs_now = "inst"
+        cs_new = "earth"
 
     # if ADCP is upside down
-    if adcpo.orientation == 'down':
+    if adcpo.orientation == "down":
         down = True
     else:  # orientation = 'up' or 'AHRS'
         down = False
 
     rotate_vars = rotb._check_rotate_vars(adcpo, rotate_vars)
 
     cs = adcpo.coord_sys.lower()
     if not force:
         if cs == cs_new:
             print("Data is already in the '%s' coordinate system" % cs_new)
             return
         elif cs != cs_now:
             raise ValueError(
-                "Data must be in the '%s' frame when using this function" %
-                cs_now)
+                "Data must be in the '%s' frame when using this function" % cs_now
+            )
 
-    if 'orientmat' in adcpo:
-        omat = adcpo['orientmat']
+    if "orientmat" in adcpo:
+        omat = adcpo["orientmat"]
     else:
-        omat = _euler2orient(adcpo['time'], adcpo['heading'].values, adcpo['pitch'].values,
-                             adcpo['roll'].values)
+        omat = _euler2orient(
+            adcpo["time"],
+            adcpo["heading"].values,
+            adcpo["pitch"].values,
+            adcpo["roll"].values,
+        )
 
     # Take the transpose of the orientation to get the inst->earth rotation
     # matrix.
     rmat = np.rollaxis(omat.data, 1)
 
     _dcheck = rotb._check_rotmat_det(rmat)
     if not _dcheck.all():
-        warnings.warn("Invalid orientation matrix (determinant != 1) at indices: {}. "
-                      "If rotated, data at these indices will be erroneous."
-                      .format(np.nonzero(~_dcheck)[0]), UserWarning)
+        warnings.warn(
+            "Invalid orientation matrix (determinant != 1) at indices: {}. "
+            "If rotated, data at these indices will be erroneous.".format(
+                np.nonzero(~_dcheck)[0]
+            ),
+            UserWarning,
+        )
 
     # The dictionary of rotation matrices for different sized arrays.
-    rmd = {3: rmat, }
+    rmd = {
+        3: rmat,
+    }
 
     # The 4-row rotation matrix assume that rows 0,1 are u,v,
     # and 2,3 are independent estimates of w.
     tmp = rmd[4] = np.zeros((4, 4, rmat.shape[-1]), dtype=np.float64)
     tmp[:3, :3] = rmat
     # Copy row 2 to 3
     tmp[3, :2] = rmat[2, :2]
@@ -95,36 +105,41 @@
         # Nortek documents sign change for upside-down instruments
         if down:
             # This is equivalent to adding 180 degrees to roll axis in _calc_omat()
             sign = np.array([1, -1, -1, -1], ndmin=dat.ndim).T
             signIMU = np.array([1, -1, -1], ndmin=dat.ndim).T
             if not reverse:
                 if n == 3:
-                    dat = np.einsum(sumstr, rmd[3], signIMU*dat)
+                    dat = np.einsum(sumstr, rmd[3], signIMU * dat)
                 elif n == 4:
-                    dat = np.einsum('ijk,j...k->i...k', rmd[4], sign*dat)
+                    dat = np.einsum("ijk,j...k->i...k", rmd[4], sign * dat)
                 else:
-                    raise Exception("The entry {} is not a vector, it cannot"
-                                    "be rotated.".format(nm))
+                    raise Exception(
+                        "The entry {} is not a vector, it cannot"
+                        "be rotated.".format(nm)
+                    )
 
             elif reverse:
                 if n == 3:
-                    dat = signIMU*np.einsum(sumstr, rmd[3], dat)
+                    dat = signIMU * np.einsum(sumstr, rmd[3], dat)
                 elif n == 4:
-                    dat = sign*np.einsum('ijk,j...k->i...k', rmd[4], dat)
+                    dat = sign * np.einsum("ijk,j...k->i...k", rmd[4], dat)
                 else:
-                    raise Exception("The entry {} is not a vector, it cannot"
-                                    "be rotated.".format(nm))
+                    raise Exception(
+                        "The entry {} is not a vector, it cannot"
+                        "be rotated.".format(nm)
+                    )
 
         else:  # 'up' and AHRS
             if n == 3:
                 dat = np.einsum(sumstr, rmd[3], dat)
             elif n == 4:
-                dat = np.einsum('ijk,j...k->i...k', rmd[4], dat)
+                dat = np.einsum("ijk,j...k->i...k", rmd[4], dat)
             else:
-                raise Exception("The entry {} is not a vector, it cannot"
-                                "be rotated.".format(nm))
+                raise Exception(
+                    "The entry {} is not a vector, it cannot" "be rotated.".format(nm)
+                )
         adcpo[nm].values = dat.copy()
 
     adcpo = rotb._set_coords(adcpo, cs_new)
 
     return adcpo
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/time.py` & `mhkit-0.8.0/mhkit/dolfyn/time.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,51 +8,51 @@
         return year
     year += 1900 + 100 * (year < 90)
     return year
 
 
 def epoch2dt64(ep_time):
     """
-    Convert from epoch time (seconds since 1/1/1970 00:00:00) to 
+    Convert from epoch time (seconds since 1/1/1970 00:00:00) to
     numpy.datetime64 array
 
     Parameters
     ----------
     ep_time : xarray.DataArray
       Time coordinate data-array or single time element
 
     Returns
     -------
     time : numpy.datetime64
       The converted datetime64 array
     """
 
     # assumes t0=1970-01-01 00:00:00
-    out = np.array(ep_time.astype('int')).astype('datetime64[s]')
-    out = out + ((ep_time % 1) * 1e9).astype('timedelta64[ns]')
+    out = np.array(ep_time.astype("int")).astype("datetime64[s]")
+    out = out + ((ep_time % 1) * 1e9).astype("timedelta64[ns]")
     return out
 
 
 def dt642epoch(dt64):
     """
-    Convert numpy.datetime64 array to epoch time 
+    Convert numpy.datetime64 array to epoch time
     (seconds since 1/1/1970 00:00:00)
 
     Parameters
     ----------
     dt64 : numpy.datetime64
       Single or array of datetime64 object(s)
 
     Returns
     -------
     time : float
       Epoch time (seconds since 1/1/1970 00:00:00)
     """
 
-    return dt64.astype('datetime64[ns]').astype('float') / 1e9
+    return dt64.astype("datetime64[ns]").astype("float") / 1e9
 
 
 def date2dt64(dt):
     """
     Convert numpy.datetime64 array to list of datetime objects
 
     Parameters
@@ -62,15 +62,15 @@
 
     Returns
     -------
     dt64 : numpy.datetime64
       Single or array of datetime64 object(s)
     """
 
-    return np.array(dt).astype('datetime64[ns]')
+    return np.array(dt).astype("datetime64[ns]")
 
 
 def dt642date(dt64):
     """
     Convert numpy.datetime64 array to list of datetime objects
 
     Parameters
@@ -85,36 +85,36 @@
     """
 
     return epoch2date(dt642epoch(dt64))
 
 
 def epoch2date(ep_time, offset_hr=0, to_str=False):
     """
-    Convert from epoch time (seconds since 1/1/1970 00:00:00) to a list 
+    Convert from epoch time (seconds since 1/1/1970 00:00:00) to a list
     of datetime objects
 
     Parameters
     ----------
     ep_time : xarray.DataArray
       Time coordinate data-array or single time element
     offset_hr : int
       Number of hours to offset time by (e.g. UTC -7 hours = PDT)
     to_str : logical
       Converts datetime object to a readable string
 
     Returns
     -------
     time : datetime.datetime
-      The converted datetime object or list(strings) 
+      The converted datetime object or list(strings)
 
     Notes
     -----
     The specific time instance is set during deployment, usually sync'd to the
-    deployment computer. The time seen by |dlfn| is in the timezone of the 
-    deployment computer, which is unknown to |dlfn|.
+    deployment computer. The time seen by DOLfYN is in the timezone of the
+    deployment computer, which is unknown to DOLfYN.
     """
 
     try:
         ep_time = ep_time.values
     except AttributeError:
         pass
 
@@ -157,15 +157,15 @@
     Returns
     -------
     time : string
       Converted timestamps
     """
 
     if format_str is None:
-        format_str = '%Y-%m-%d %H:%M:%S.%f'
+        format_str = "%Y-%m-%d %H:%M:%S.%f"
 
     if not isinstance(dt, list):
         dt = [dt]
 
     return [t.strftime(format_str) for t in dt]
 
 
@@ -204,17 +204,18 @@
     time : float
       List of timestamps in MATLAB datnum format
     """
 
     time = list()
     for i in range(len(dt)):
         mdn = dt[i] + timedelta(days=366)
-        frac_seconds = (dt[i]-datetime(dt[i].year, dt[i].month,
-                        dt[i].day, 0, 0, 0)).seconds / (24*60*60)
-        frac_microseconds = dt[i].microsecond / (24*60*60*1000000)
+        frac_seconds = (
+            dt[i] - datetime(dt[i].year, dt[i].month, dt[i].day, 0, 0, 0)
+        ).seconds / (24 * 60 * 60)
+        frac_microseconds = dt[i].microsecond / (24 * 60 * 60 * 1000000)
         time.append(mdn.toordinal() + frac_seconds + frac_microseconds)
 
     return time
 
 
 def matlab2date(matlab_dn):
     """
@@ -234,35 +235,36 @@
     time = list()
     for i in range(len(matlab_dn)):
         day = datetime.fromordinal(int(matlab_dn[i]))
         dayfrac = timedelta(days=matlab_dn[i] % 1) - timedelta(days=366)
         time.append(day + dayfrac)
 
         # Datenum is precise down to 100 microseconds - add difference to round
-        us = int(round(time[i].microsecond/100, 0))*100
-        time[i] = time[i].replace(microsecond=time[i].microsecond) + \
-            timedelta(microseconds=us-time[i].microsecond)
+        us = int(round(time[i].microsecond / 100, 0)) * 100
+        time[i] = time[i].replace(microsecond=time[i].microsecond) + timedelta(
+            microseconds=us - time[i].microsecond
+        )
 
     return time
 
 
 def _fill_time_gaps(epoch, sample_rate_hz):
     """
     Fill gaps (NaN values) in the timeseries by simple linear
     interpolation.  The ends are extrapolated by stepping
     forward/backward by 1/sample_rate_hz.
     """
 
     # epoch is seconds since 1970
-    dt = 1. / sample_rate_hz
+    dt = 1.0 / sample_rate_hz
     epoch = fillgaps(epoch)
     if np.isnan(epoch[0]):
         i0 = np.nonzero(~np.isnan(epoch))[0][0]
         delta = np.arange(-i0, 0, 1) * dt
         epoch[:i0] = epoch[i0] + delta
     if np.isnan(epoch[-1]):
         # Search backward through the array to get the 'negative index'
         ie = -np.nonzero(~np.isnan(epoch[::-1]))[0][0] - 1
         delta = np.arange(1, -ie, 1) * dt
-        epoch[(ie + 1):] = epoch[ie] + delta
+        epoch[(ie + 1) :] = epoch[ie] + delta
 
     return epoch
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/tools/fft.py` & `mhkit-0.8.0/mhkit/dolfyn/tools/fft.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import numpy as np
 from .misc import detrend_array
+
 fft = np.fft.fft
 
 
 def fft_frequency(nfft, fs, full=False):
     """
     Compute the frequency vector for a given `nfft` and `fs`.
 
@@ -24,24 +25,35 @@
     """
 
     fs = np.float64(fs)
     f = np.fft.fftfreq(int(nfft), 1 / fs)
     if full:
         return f
     else:
-        return np.abs(f[1:int(nfft / 2. + 1)])
+        return np.abs(f[1 : int(nfft / 2.0 + 1)])
 
 
 def _getwindow(window, nfft):
-    if window == 'hann':
-        window = np.hanning(nfft)
-    elif window == 'hamm':
-        window = np.hamming(nfft)
-    elif window is None or window == 1:
+    if window is None:
+        window = np.ones(nfft)
+    elif isinstance(window, (int, float)) and window == 1:
         window = np.ones(nfft)
+    elif isinstance(window, str):
+        if "hann" in window:
+            window = np.hanning(nfft)
+        elif "hamm" in window:
+            window = np.hamming(nfft)
+        else:
+            raise ValueError("Unsupported window type: {}".format(window))
+    elif isinstance(window, np.ndarray):
+        if len(window) != nfft:
+            raise ValueError("Custom window length must be equal to nfft")
+    else:
+        raise ValueError("Invalid window parameter")
+
     return window
 
 
 def _stepsize(l, nfft, nens=None, step=None):
     """
     Calculates the fft-step size for a length *l* array.
 
@@ -64,25 +76,25 @@
     """
 
     if l < nfft:
         nfft = l
     if nens is None and step is None:
         if l == nfft:
             return 0, 1, int(nfft)
-        nens = int(2. * l / nfft)
+        nens = int(2.0 * l / nfft)
         return int((l - nfft) / (nens - 1)), nens, int(nfft)
     elif nens is None:
         return int(step), int((l - nfft) / step + 1), int(nfft)
     else:
         if nens == 1:
             return 0, 1, int(nfft)
         return int((l - nfft) / (nens - 1)), int(nens), int(nfft)
 
 
-def cpsd_quasisync_1D(a, b, nfft, fs, window='hann'):
+def cpsd_quasisync_1D(a, b, nfft, fs, window="hann"):
     """
     Compute the cross power spectral density (CPSD) of the signals `a` and `b`.
 
     Parameters
     ----------
     a : numpy.ndarray
       The first signal.
@@ -144,29 +156,32 @@
         a, b = b, a
         l = l[::-1]
     step = [0, 0]
     step[0], nens, nfft = _stepsize(l[0], nfft)
     step[1], nens, nfft = _stepsize(l[1], nfft, nens=nens)
     fs = np.float64(fs)
     window = _getwindow(window, nfft)
-    fft_inds = slice(1, int(nfft / 2. + 1))
-    wght = 2. / (window ** 2).sum()
-    pwr = fft(detrend_array(a[0:nfft]) * window)[fft_inds] * \
-        np.conj(fft(detrend_array(b[0:nfft]) * window)[fft_inds])
+    fft_inds = slice(1, int(nfft / 2.0 + 1))
+    wght = 2.0 / (window**2).sum()
+    pwr = fft(detrend_array(a[0:nfft]) * window)[fft_inds] * np.conj(
+        fft(detrend_array(b[0:nfft]) * window)[fft_inds]
+    )
     if nens - 1:
-        for i1, i2 in zip(range(step[0], l[0] - nfft + 1, step[0]),
-                          range(step[1], l[1] - nfft + 1, step[1])):
-            pwr += fft(detrend_array(a[i1:(i1 + nfft)]) * window)[fft_inds] * \
-                np.conj(
-                    fft(detrend_array(b[i2:(i2 + nfft)]) * window)[fft_inds])
+        for i1, i2 in zip(
+            range(step[0], l[0] - nfft + 1, step[0]),
+            range(step[1], l[1] - nfft + 1, step[1]),
+        ):
+            pwr += fft(detrend_array(a[i1 : (i1 + nfft)]) * window)[fft_inds] * np.conj(
+                fft(detrend_array(b[i2 : (i2 + nfft)]) * window)[fft_inds]
+            )
     pwr *= wght / nens / fs
     return pwr
 
 
-def cpsd_1D(a, b, nfft, fs, window='hann', step=None):
+def cpsd_1D(a, b, nfft, fs, window="hann", step=None):
     """
     Compute the cross power spectral density (CPSD) of the signals `a` and `b`.
 
     Parameters
     ----------
     a : numpy.ndarray
       The first signal.
@@ -225,35 +240,35 @@
     auto_psd = False
     if a is b:
         auto_psd = True
     l = len(a)
     step, nens, nfft = _stepsize(l, nfft, step=step)
     fs = np.float64(fs)
     window = _getwindow(window, nfft)
-    fft_inds = slice(1, int(nfft / 2. + 1))
-    wght = 2. / (window ** 2).sum()
+    fft_inds = slice(1, int(nfft / 2.0 + 1))
+    wght = 2.0 / (window**2).sum()
     s1 = fft(detrend_array(a[0:nfft]) * window)[fft_inds]
     if auto_psd:
         pwr = np.abs(s1) ** 2
     else:
         pwr = s1 * np.conj(fft(detrend_array(b[0:nfft]) * window)[fft_inds])
     if nens - 1:
         for i in range(step, l - nfft + 1, step):
-            s1 = fft(detrend_array(a[i:(i + nfft)]) * window)[fft_inds]
+            s1 = fft(detrend_array(a[i : (i + nfft)]) * window)[fft_inds]
             if auto_psd:
                 pwr += np.abs(s1) ** 2
             else:
-                pwr += s1 * \
-                    np.conj(
-                        fft(detrend_array(b[i:(i + nfft)]) * window)[fft_inds])
+                pwr += s1 * np.conj(
+                    fft(detrend_array(b[i : (i + nfft)]) * window)[fft_inds]
+                )
     pwr *= wght / nens / fs
     return pwr
 
 
-def psd_1D(a, nfft, fs, window='hann', step=None):
+def psd_1D(a, nfft, fs, window="hann", step=None):
     """
     Compute the power spectral density (PSD).
 
     This function computes the one-dimensional `n`-point PSD.
 
     The units of the spectra is the product of the units of `a` and
     `b`, divided by the units of fs.
@@ -282,15 +297,15 @@
     Returns
     -------
     cpsd : numpy.ndarray
       The cross-spectral density of `a` and `b`.
 
     Notes
     -----
-    Credit: This function's line of code was copied from JN's fast_psd.m 
+    Credit: This function's line of code was copied from JN's fast_psd.m
     routine.
 
     See Also
     --------
     :func:`cpsd_1D`
     :func:`coherence_1D`
     `numpy.fft`
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/tools/misc.py` & `mhkit-0.8.0/mhkit/dolfyn/tools/misc.py`

 * *Files 3% similar despite different names*

```diff
@@ -46,16 +46,17 @@
     if not in_place:
         arr = arr.copy()
     sz = np.ones(arr.ndim, dtype=int)
     sz[axis] = arr.shape[axis]
     x = np.arange(sz[axis], dtype=np.float_).reshape(sz)
     x -= np.nanmean(x, axis=axis, keepdims=True)
     arr -= np.nanmean(arr, axis=axis, keepdims=True)
-    b = np.nanmean((x * arr), axis=axis, keepdims=True) / \
-        np.nanmean((x ** 2), axis=axis, keepdims=True)
+    b = np.nanmean((x * arr), axis=axis, keepdims=True) / np.nanmean(
+        (x**2), axis=axis, keepdims=True
+    )
     arr -= b * x
     return arr
 
 
 def group(bl, min_length=0):
     """
     Find continuous segments in a boolean array.
@@ -78,28 +79,28 @@
     -----
     This function has funny behavior for single points.  It will
     return the same two indices for the beginning and end.
     """
 
     if not any(bl):
         return np.empty(0)
-    vl = np.diff(bl.astype('int'))
+    vl = np.diff(bl.astype("int"))
     ups = np.nonzero(vl == 1)[0] + 1
     dns = np.nonzero(vl == -1)[0] + 1
     if bl[0]:
         if len(ups) == 0:
             ups = np.array([0])
         else:
             ups = np.concatenate((np.array([0]), [len(ups)]))
     if bl[-1]:
         if len(dns) == 0:
             dns = np.array([len(bl)])
         else:
             dns = np.concatenate((dns, [len(bl)]))
-    out = np.empty(len(dns), dtype='O')
+    out = np.empty(len(dns), dtype="O")
     idx = 0
     for u, d in zip(ups, dns):
         if d - u < min_length:
             continue
         out[idx] = slice(u, d)
         idx += 1
     return out[:idx]
@@ -130,20 +131,20 @@
     >>     out[slc]=my_1d_function(arr[slc])
     """
 
     nd = len(arr_shape)
     if axis < 0:
         axis += nd
     ind = [0] * (nd - 1)
-    i = np.zeros(nd, 'O')
+    i = np.zeros(nd, "O")
     indlist = list(range(nd))
     indlist.remove(axis)
     i[axis] = slice(None)
     itr_dims = np.asarray(arr_shape).take(indlist)
-    Ntot = np.product(itr_dims)
+    Ntot = np.prod(itr_dims)
     i.put(indlist, ind)
     k = 0
     while k < Ntot:
         # increment the index
         n = -1
         while (ind[n] >= itr_dims[n]) and (n > (1 - nd)):
             ind[n - 1] += 1
@@ -161,26 +162,26 @@
     the 'polar angle' in (degrees clockwise from North)
 
     Parameters
     ----------
     deg: float or array-like
       Number or array in 'degrees CCW from East' or 'degrees CW from North'
     tidal_mode : bool
-      If true, range is set from 0 to +/-180 degrees. If false, range is 0 to 
+      If true, range is set from 0 to +/-180 degrees. If false, range is 0 to
       360 degrees. Default = True
 
     Returns
     -------
     out : float or array-like
-      Input data transformed to 'degrees CW from North' or 
+      Input data transformed to 'degrees CW from North' or
       'degrees CCW from East', respectively (based on `deg`)
 
     Notes
     -----
-    The same algorithm is used to convert back and forth between 'CCW from E' 
+    The same algorithm is used to convert back and forth between 'CCW from E'
     and 'CW from N'
     """
 
     out = -(deg - 90) % 360
     if tidal_mode:
         out[out > 180] -= 360
     return out
@@ -219,41 +220,43 @@
             fillgaps(a[inds], maxgap, 0, extrapFlg)
         return
 
     a = np.asarray(a)
     nd = a.ndim
     if dim < 0:
         dim += nd
-    if (dim >= nd):
-        raise ValueError("dim must be less than a.ndim; dim=%d, rank=%d."
-                         % (dim, nd))
+    if dim >= nd:
+        raise ValueError("dim must be less than a.ndim; dim=%d, rank=%d." % (dim, nd))
     ind = [0] * (nd - 1)
-    i = np.zeros(nd, 'O')
+    i = np.zeros(nd, "O")
     indlist = list(range(nd))
     indlist.remove(dim)
     i[dim] = slice(None, None)
     i.put(indlist, ind)
 
     gd = np.nonzero(~np.isnan(a))[0]
 
     # Here we extrapolate the ends, if necessary:
     if extrapFlg and gd.__len__() > 0:
         if gd[0] != 0 and gd[0] <= maxgap:
-            a[:gd[0]] = a[gd[0]]
+            a[: gd[0]] = a[gd[0]]
         if gd[-1] != a.__len__() and (a.__len__() - (gd[-1] + 1)) <= maxgap:
-            a[gd[-1]:] = a[gd[-1]]
+            a[gd[-1] :] = a[gd[-1]]
 
     # Here is the main loop
     if gd.__len__() > 1:
         inds = np.nonzero((1 < np.diff(gd)) & (np.diff(gd) <= maxgap + 1))[0]
         for i2 in range(0, inds.__len__()):
             ii = list(range(gd[inds[i2]] + 1, gd[inds[i2] + 1]))
-            a[ii] = (np.diff(a[gd[[inds[i2], inds[i2] + 1]]]) *
-                     (np.arange(0, ii.__len__()) + 1) /
-                     (ii.__len__() + 1) + a[gd[inds[i2]]]).astype(a.dtype)
+            a[ii] = (
+                np.diff(a[gd[[inds[i2], inds[i2] + 1]]])
+                * (np.arange(0, ii.__len__()) + 1)
+                / (ii.__len__() + 1)
+                + a[gd[inds[i2]]]
+            ).astype(a.dtype)
 
     return a
 
 
 def interpgaps(a, t, maxgap=np.inf, dim=0, extrapFlg=False):
     """
     Fill gaps (NaN values) in ``a`` by linear interpolation along
@@ -285,45 +288,46 @@
         return
 
     gd = _find(~np.isnan(a))
 
     # Here we extrapolate the ends, if necessary:
     if extrapFlg and gd.__len__() > 0:
         if gd[0] != 0 and gd[0] <= maxgap:
-            a[:gd[0]] = a[gd[0]]
+            a[: gd[0]] = a[gd[0]]
         if gd[-1] != a.__len__() and (a.__len__() - (gd[-1] + 1)) <= maxgap:
-            a[gd[-1]:] = a[gd[-1]]
+            a[gd[-1] :] = a[gd[-1]]
 
     # Here is the main loop
     if gd.__len__() > 1:
-        inds = _find((1 < np.diff(gd)) &
-                     (np.diff(gd) <= maxgap + 1))
+        inds = _find((1 < np.diff(gd)) & (np.diff(gd) <= maxgap + 1))
         for i2 in range(0, inds.__len__()):
             ii = np.arange(gd[inds[i2]] + 1, gd[inds[i2] + 1])
-            ti = (t[ii] - t[gd[inds[i2]]]) / np.diff(t[[gd[inds[i2]],
-                                                        gd[inds[i2] + 1]]])
-            a[ii] = (np.diff(a[gd[[inds[i2], inds[i2] + 1]]]) * ti +
-                     a[gd[inds[i2]]]).astype(a.dtype)
+            ti = (t[ii] - t[gd[inds[i2]]]) / np.diff(
+                t[[gd[inds[i2]], gd[inds[i2] + 1]]]
+            )
+            a[ii] = (
+                np.diff(a[gd[[inds[i2], inds[i2] + 1]]]) * ti + a[gd[inds[i2]]]
+            ).astype(a.dtype)
 
     return a
 
 
 def medfiltnan(a, kernel, thresh=0):
     """
-    Do a running median filter of the data. Regions where more than 
+    Do a running median filter of the data. Regions where more than
     ``thresh`` fraction of the points are NaN are set to NaN.
 
     Parameters
     ----------
     a : numpy.ndarray
       2D array containing data to be filtered.
     kernel_size : numpy.ndarray or list, optional
-      A scalar or a list of length 2, giving the size of the median 
-      filter window in each dimension. Elements of kernel_size should 
-      be odd. If kernel_size is a scalar, then this scalar is used as 
+      A scalar or a list of length 2, giving the size of the median
+      filter window in each dimension. Elements of kernel_size should
+      be odd. If kernel_size is a scalar, then this scalar is used as
       the size in each dimension.
     thresh : int
       Maximum gap in *a* to filter over
 
     Returns
     -------
     out : numpy.ndarray
@@ -340,13 +344,13 @@
         flag_1D = True
     try:
         len(kernel)
     except:
         kernel = [1, kernel]
     out = medfilt2d(a, kernel)
     if thresh > 0:
-        out[convolve2d(np.isnan(a),
-                       np.ones(kernel) / np.prod(kernel),
-                       'same') > thresh] = np.NaN
+        out[
+            convolve2d(np.isnan(a), np.ones(kernel) / np.prod(kernel), "same") > thresh
+        ] = np.NaN
     if flag_1D:
         return out[0]
     return out
```

### Comparing `mhkit-0.7.0/mhkit/dolfyn/velocity.py` & `mhkit-0.8.0/mhkit/dolfyn/velocity.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,35 +3,35 @@
 from .binned import TimeBinner
 from .time import dt642epoch, dt642date
 from .rotate.api import rotate2, set_declination, set_inst2head_rotmat
 from .io.api import save
 from .tools.misc import slice1d_along_axis, convert_degrees
 
 
-@xr.register_dataset_accessor('velds')  # 'vel dataset'
-class Velocity():
+@xr.register_dataset_accessor("velds")  # 'vel dataset'
+class Velocity:
     """
     All ADCP and ADV xarray datasets wrap this base class.
 
-    The turbulence-related attributes defined within this class 
-    assume that the  ``'tke_vec'`` and ``'stress_vec'`` data entries are 
+    The turbulence-related attributes defined within this class
+    assume that the  ``'tke_vec'`` and ``'stress_vec'`` data entries are
     included in the dataset. These are typically calculated using a
     :class:`VelBinner` tool, but the method for calculating these
     variables can depend on the details of the measurement
     (instrument, it's configuration, orientation, etc.).
 
     See Also
     ========
     :class:`VelBinner`
     """
 
     ########
     # Major components of the dolfyn-API
 
-    def rotate2(self, out_frame='earth', inplace=True):
+    def rotate2(self, out_frame="earth", inplace=True):
         """
         Rotate the dataset to a new coordinate system.
 
         Parameters
         ----------
         out_frame : string {'beam', 'inst', 'earth', 'principal'}
           The coordinate system to rotate the data into.
@@ -169,302 +169,365 @@
 
     def __getitem__(self, key):
         return self.ds[key]
 
     def __contains__(self, val):
         return val in self.ds
 
-    def __repr__(self, ):
-        time_string = '{:.2f} {} (started: {})'
-        if ('time' not in self or dt642epoch(self['time'][0]) < 1):
-            time_string = '-->No Time Information!<--'
+    def __repr__(
+        self,
+    ):
+        time_string = "{:.2f} {} (started: {})"
+        if "time" not in self or dt642epoch(self["time"][0]) < 1:
+            time_string = "-->No Time Information!<--"
         else:
-            tm = self['time'][[0, -1]].values
+            tm = self["time"][[0, -1]].values
             dt = dt642date(tm[0])[0]
-            delta = (dt642epoch(tm[-1]) -
-                     dt642epoch(tm[0])) / (3600 * 24)  # days
+            delta = (dt642epoch(tm[-1]) - dt642epoch(tm[0])) / (3600 * 24)  # days
             if delta > 1:
-                units = 'days'
+                units = "days"
             elif delta * 24 > 1:
-                units = 'hours'
+                units = "hours"
                 delta *= 24
             elif delta * 24 * 60 > 1:
                 delta *= 24 * 60
-                units = 'minutes'
+                units = "minutes"
             else:
                 delta *= 24 * 3600
-                units = 'seconds'
+                units = "seconds"
             try:
-                time_string = time_string.format(delta, units,
-                                                 dt.strftime('%b %d, %Y %H:%M'))
+                time_string = time_string.format(
+                    delta, units, dt.strftime("%b %d, %Y %H:%M")
+                )
             except AttributeError:
-                time_string = '-->Error in time info<--'
+                time_string = "-->Error in time info<--"
 
         p = self.ds.attrs
-        t_shape = self['time'].shape
+        t_shape = self["time"].shape
         if len(t_shape) > 1:
-            shape_string = '({} bins, {} pings @ {}Hz)'.format(
-                t_shape[0], t_shape, p.get('fs'))
+            shape_string = "({} bins, {} pings @ {}Hz)".format(
+                t_shape[0], t_shape, p.get("fs")
+            )
         else:
-            shape_string = '({} pings @ {}Hz)'.format(
-                t_shape[0], p.get('fs', '??'))
-        _header = ("<%s data object>: "
-                   " %s %s\n"
-                   "  . %s\n"
-                   "  . %s-frame\n"
-                   "  . %s\n" %
-                   (p.get('inst_type'),
-                    self.ds.attrs['inst_make'], self.ds.attrs['inst_model'],
-                    time_string,
-                    p.get('coord_sys'),
-                    shape_string))
-        _vars = '  Variables:\n'
+            shape_string = "({} pings @ {}Hz)".format(t_shape[0], p.get("fs", "??"))
+        _header = (
+            "<%s data object>: "
+            " %s %s\n"
+            "  . %s\n"
+            "  . %s-frame\n"
+            "  . %s\n"
+            % (
+                p.get("inst_type"),
+                self.ds.attrs["inst_make"],
+                self.ds.attrs["inst_model"],
+                time_string,
+                p.get("coord_sys"),
+                shape_string,
+            )
+        )
+        _vars = "  Variables:\n"
 
         # Specify which variable show up in this view here.
         # * indicates a wildcard
         # This list also sets the display order.
         # Only the first 12 matches are displayed.
-        show_vars = ['time*', 'vel*', 'range', 'range_echo',
-                     'orientmat', 'heading', 'pitch', 'roll',
-                     'temp', 'press*', 'amp*', 'corr*',
-                     'accel', 'angrt', 'mag', 'echo',
-                     ]
+        show_vars = [
+            "time*",
+            "vel*",
+            "range",
+            "range_echo",
+            "orientmat",
+            "heading",
+            "pitch",
+            "roll",
+            "temp",
+            "press*",
+            "amp*",
+            "corr*",
+            "accel",
+            "angrt",
+            "mag",
+            "echo",
+        ]
         n = 0
         for v in show_vars:
             if n > 12:
                 break
-            if v.endswith('*'):
+            if v.endswith("*"):
                 v = v[:-1]  # Drop the '*'
                 for nm in self.variables:
                     if n > 12:
                         break
                     if nm.startswith(v):
                         n += 1
-                        _vars += '  - {} {}\n'.format(nm, self.ds[nm].dims)
+                        _vars += "  - {} {}\n".format(nm, self.ds[nm].dims)
             elif v in self.ds:
-                _vars += '  - {} {}\n'.format(v, self.ds[v].dims)
+                _vars += "  - {} {}\n".format(v, self.ds[v].dims)
         if n < len(self.variables):
-            _vars += '  ... and others (see `<obj>.variables`)\n'
+            _vars += "  ... and others (see `<obj>.variables`)\n"
         return _header + _vars
 
     ######
     # Duplicate valuable xarray properties here.
     @property
-    def variables(self, ):
+    def variables(
+        self,
+    ):
         """A sorted list of the variable names in the dataset."""
         return sorted(self.ds.variables)
 
     @property
-    def attrs(self, ):
+    def attrs(
+        self,
+    ):
         """The attributes in the dataset."""
         return self.ds.attrs
 
     @property
-    def coords(self, ):
+    def coords(
+        self,
+    ):
         """The coordinates in the dataset."""
         return self.ds.coords
 
     ######
     # A bunch of DOLfYN specific properties
     @property
-    def u(self,):
+    def u(
+        self,
+    ):
         """
         The first velocity component.
 
         This is simply a shortcut to self['vel'][0]. Therefore,
         depending on the coordinate system of the data object
         (self.attrs['coord_sys']), it is:
 
         - beam:      beam1
         - inst:      x
         - earth:     east
         - principal: streamwise
         """
-        return self.ds['vel'][0].drop('dir')
+        return self.ds["vel"][0].drop("dir")
 
     @property
-    def v(self,):
+    def v(
+        self,
+    ):
         """
         The second velocity component.
 
         This is simply a shortcut to self['vel'][1]. Therefore,
         depending on the coordinate system of the data object
         (self.attrs['coord_sys']), it is:
 
         - beam:      beam2
         - inst:      y
         - earth:     north
         - principal: cross-stream
         """
-        return self.ds['vel'][1].drop('dir')
+        return self.ds["vel"][1].drop("dir")
 
     @property
-    def w(self,):
+    def w(
+        self,
+    ):
         """
         The third velocity component.
 
         This is simply a shortcut to self['vel'][2]. Therefore,
         depending on the coordinate system of the data object
         (self.attrs['coord_sys']), it is:
 
         - beam:      beam3
         - inst:      z
         - earth:     up
         - principal: up
         """
-        return self.ds['vel'][2].drop('dir')
+        return self.ds["vel"][2].drop("dir")
 
     @property
-    def U(self,):
+    def U(
+        self,
+    ):
         """Horizontal velocity as a complex quantity"""
 
         return xr.DataArray(
-            (self.u + self.v * 1j).astype('complex64'),
-            attrs={'units': 'm s-1',
-                   'long_name': 'Horizontal Water Velocity'})
-    
+            (self.u + self.v * 1j).astype("complex64"),
+            attrs={"units": "m s-1", "long_name": "Horizontal Water Velocity"},
+        )
+
     @property
-    def U_mag(self,):
+    def U_mag(
+        self,
+    ):
         """Horizontal velocity magnitude"""
 
         return xr.DataArray(
-            np.abs(self.U).astype('float32'),
-            attrs={'units': 'm s-1',
-                   'long_name': 'Water Speed',
-                   'standard_name': 'sea_water_speed'})
+            np.abs(self.U).astype("float32"),
+            attrs={
+                "units": "m s-1",
+                "long_name": "Water Speed",
+                "standard_name": "sea_water_speed",
+            },
+        )
 
     @property
-    def U_dir(self,):
-        """
-        Angle of horizontal velocity vector. Direction is 'to', 
-        as opposed to 'from'. This function calculates angle as 
-        "degrees CCW from X/East/streamwise" and then converts it to 
+    def U_dir(
+        self,
+    ):
+        """
+        Angle of horizontal velocity vector. Direction is 'to',
+        as opposed to 'from'. This function calculates angle as
+        "degrees CCW from X/East/streamwise" and then converts it to
         "degrees CW from X/North/streamwise".
         """
+
         def convert_to_CW(angle):
-            if self.ds.coord_sys == 'earth':
+            if self.ds.coord_sys == "earth":
                 # Convert "deg CCW from East" to "deg CW from North" [0, 360]
                 angle = convert_degrees(angle, tidal_mode=False)
                 relative_to = self.ds.dir[1].values
             else:
                 # Switch to clockwise and from [-180, 180] to [0, 360]
                 angle *= -1
                 angle[angle < 0] += 360
                 relative_to = self.ds.dir[0].values
             return angle, relative_to
 
         # Convert from radians to degrees
-        angle, rel = convert_to_CW(np.angle(self.U)*(180/np.pi))
+        angle, rel = convert_to_CW(np.angle(self.U) * (180 / np.pi))
 
         return xr.DataArray(
-            angle.astype('float32'),
+            angle.astype("float32"),
             dims=self.U.dims,
             coords=self.U.coords,
-            attrs={'units': 'degrees_CW_from_' + str(rel),
-                   'long_name': 'Water Direction',
-                   'standard_name': 'sea_water_to_direction'})
+            attrs={
+                "units": "degrees_CW_from_" + str(rel),
+                "long_name": "Water Direction",
+                "standard_name": "sea_water_to_direction",
+            },
+        )
 
     @property
-    def E_coh(self,):
+    def E_coh(
+        self,
+    ):
         """
         Coherent turbulent energy
 
         Niel Kelley's 'coherent turbulence energy', which is the RMS
         of the Reynold's stresses.
 
         See: NREL Technical Report TP-500-52353
         """
         E_coh = (self.upwp_**2 + self.upvp_**2 + self.vpwp_**2) ** (0.5)
 
         return xr.DataArray(
-            E_coh.astype('float32'),
-            coords={'time': self.ds['stress_vec'].time},
-            dims=['time'],
-            attrs={'units': self.ds['stress_vec'].units,
-                   'long_name': 'Coherent Turbulence Energy'})
+            E_coh.astype("float32"),
+            coords={"time": self.ds["stress_vec"].time},
+            dims=["time"],
+            attrs={
+                "units": self.ds["stress_vec"].units,
+                "long_name": "Coherent Turbulence Energy",
+            },
+        )
 
     @property
     def I_tke(self, thresh=0):
         """
         Turbulent kinetic energy intensity.
 
         Ratio of sqrt(tke) to horizontal velocity magnitude.
         """
-        I_tke = np.ma.masked_where(self.U_mag < thresh,
-                                   np.sqrt(2 * self.tke) / self.U_mag)
+        I_tke = np.ma.masked_where(
+            self.U_mag < thresh, np.sqrt(2 * self.tke) / self.U_mag
+        )
         return xr.DataArray(
-            I_tke.data.astype('float32'),
+            I_tke.data.astype("float32"),
             coords=self.U_mag.coords,
             dims=self.U_mag.dims,
-            attrs={'units': '% [0,1]',
-                   'long_name': 'TKE Intensity'})
+            attrs={"units": "% [0,1]", "long_name": "TKE Intensity"},
+        )
 
     @property
     def I(self, thresh=0):
         """
         Turbulence intensity.
 
         Ratio of standard deviation of horizontal velocity
         to horizontal velocity magnitude.
         """
-        I = np.ma.masked_where(self.U_mag < thresh,
-                               self.ds['U_std'] / self.U_mag)
+        I = np.ma.masked_where(self.U_mag < thresh, self.ds["U_std"] / self.U_mag)
         return xr.DataArray(
-            I.data.astype('float32'),
+            I.data.astype("float32"),
             coords=self.U_mag.coords,
             dims=self.U_mag.dims,
-            attrs={'units': '% [0,1]',
-                   'long_name': 'Turbulence Intensity'})
+            attrs={"units": "% [0,1]", "long_name": "Turbulence Intensity"},
+        )
 
     @property
-    def tke(self,):
-        """Turbulent kinetic energy (sum of the three components)
-        """
-        tke = self.ds['tke_vec'].sum('tke') / 2
-        tke.name = 'TKE'
-        tke.attrs['units'] = self.ds['tke_vec'].units
-        tke.attrs['long_name'] = 'TKE'
-        tke.attrs['standard_name'] = 'specific_turbulent_kinetic_energy_of_sea_water'
+    def tke(
+        self,
+    ):
+        """Turbulent kinetic energy (sum of the three components)"""
+        tke = self.ds["tke_vec"].sum("tke") / 2
+        tke.name = "TKE"
+        tke.attrs["units"] = self.ds["tke_vec"].units
+        tke.attrs["long_name"] = "TKE"
+        tke.attrs["standard_name"] = "specific_turbulent_kinetic_energy_of_sea_water"
         return tke
 
     @property
-    def upvp_(self,):
+    def upvp_(
+        self,
+    ):
         """u'v'bar Reynolds stress"""
 
-        return self.ds['stress_vec'].sel(tau="upvp_").drop('tau')
+        return self.ds["stress_vec"].sel(tau="upvp_").drop("tau")
 
     @property
-    def upwp_(self,):
+    def upwp_(
+        self,
+    ):
         """u'w'bar Reynolds stress"""
 
-        return self.ds['stress_vec'].sel(tau="upwp_").drop('tau')
+        return self.ds["stress_vec"].sel(tau="upwp_").drop("tau")
 
     @property
-    def vpwp_(self,):
+    def vpwp_(
+        self,
+    ):
         """v'w'bar Reynolds stress"""
 
-        return self.ds['stress_vec'].sel(tau="vpwp_").drop('tau')
+        return self.ds["stress_vec"].sel(tau="vpwp_").drop("tau")
 
     @property
-    def upup_(self,):
+    def upup_(
+        self,
+    ):
         """u'u'bar component of the tke"""
 
-        return self.ds['tke_vec'].sel(tke="upup_").drop('tke')
+        return self.ds["tke_vec"].sel(tke="upup_").drop("tke")
 
     @property
-    def vpvp_(self,):
+    def vpvp_(
+        self,
+    ):
         """v'v'bar component of the tke"""
 
-        return self.ds['tke_vec'].sel(tke="vpvp_").drop('tke')
+        return self.ds["tke_vec"].sel(tke="vpvp_").drop("tke")
 
     @property
-    def wpwp_(self,):
+    def wpwp_(
+        self,
+    ):
         """w'w'bar component of the tke"""
 
-        return self.ds['tke_vec'].sel(tke="wpwp_").drop('tke')
+        return self.ds["tke_vec"].sel(tke="wpwp_").drop("tke")
 
 
 class VelBinner(TimeBinner):
     """
     This is the base binning (averaging) tool.
     All DOLfYN binning tools derive from this base class.
 
@@ -483,46 +546,61 @@
         # This computes the basic averages
         avg = binner.bin_average(rawdat)
     """
 
     # This defines how cross-spectra and stresses are computed.
     _cross_pairs = [(0, 1), (0, 2), (1, 2)]
 
-    tke = xr.DataArray(["upup_", "vpvp_", "wpwp_"],
-                       dims=['tke'],
-                       name='tke',
-                       attrs={'units': '1',
-                              'long_name': 'Turbulent Kinetic Energy Vector Components',
-                              'coverage_content_type': 'coordinate'})
-
-    tau = xr.DataArray(["upvp_", "upwp_", "vpwp_"],
-                       dims=['tau'],
-                       name='tau',
-                       attrs={'units': '1',
-                              'long_name': 'Reynolds Stress Vector Components',
-                              'coverage_content_type': 'coordinate'})
-
-    S = xr.DataArray(['Sxx', 'Syy', 'Szz'],
-                     dims=['S'],
-                     name='S',
-                     attrs={'units': '1',
-                            'long_name': 'Power Spectral Density Vector Components',
-                            'coverage_content_type': 'coordinate'})
-
-    C = xr.DataArray(['Cxy', 'Cxz', 'Cyz'],
-                     dims=['C'],
-                     name='C',
-                     attrs={'units': '1',
-                            'long_name': 'Cross-Spectral Density Vector Components',
-                            'coverage_content_type': 'coordinate'})
-
+    tke = xr.DataArray(
+        ["upup_", "vpvp_", "wpwp_"],
+        dims=["tke"],
+        name="tke",
+        attrs={
+            "units": "1",
+            "long_name": "Turbulent Kinetic Energy Vector Components",
+            "coverage_content_type": "coordinate",
+        },
+    )
+
+    tau = xr.DataArray(
+        ["upvp_", "upwp_", "vpwp_"],
+        dims=["tau"],
+        name="tau",
+        attrs={
+            "units": "1",
+            "long_name": "Reynolds Stress Vector Components",
+            "coverage_content_type": "coordinate",
+        },
+    )
+
+    S = xr.DataArray(
+        ["Sxx", "Syy", "Szz"],
+        dims=["S"],
+        name="S",
+        attrs={
+            "units": "1",
+            "long_name": "Power Spectral Density Vector Components",
+            "coverage_content_type": "coordinate",
+        },
+    )
+
+    C = xr.DataArray(
+        ["Cxy", "Cxz", "Cyz"],
+        dims=["C"],
+        name="C",
+        attrs={
+            "units": "1",
+            "long_name": "Cross-Spectral Density Vector Components",
+            "coverage_content_type": "coordinate",
+        },
+    )
 
     def bin_average(self, raw_ds, out_ds=None, names=None):
         """
-        Bin the dataset and calculate the ensemble averages of each 
+        Bin the dataset and calculate the ensemble averages of each
         variable.
 
         Parameters
         ----------
         raw_ds : xarray.Dataset
            The raw data structure to be binned
         out_ds : xarray.Dataset
@@ -555,44 +633,50 @@
 
         if names is None:
             names = raw_ds.data_vars
 
         for ky in names:
             # set up dimensions and coordinates for Dataset
             dims_list = raw_ds[ky].dims
+            if any([ar for ar in dims_list if "altraw" in ar]):
+                continue
             coords_dict = {}
             for nm in dims_list:
-                if 'time' in nm:
+                if "time" in nm:
                     coords_dict[nm] = self.mean(raw_ds[ky][nm].values)
                 else:
                     coords_dict[nm] = raw_ds[ky][nm].values
 
             # create Dataset
-            if 'ensemble' not in ky:
+            if "ensemble" not in ky:
                 try:  # variables with time coordinate
-                    out_ds[ky] = xr.DataArray(self.mean(raw_ds[ky].values),
-                                              coords=coords_dict,
-                                              dims=dims_list,
-                                              attrs=raw_ds[ky].attrs
-                                              ).astype('float32')
+                    out_ds[ky] = xr.DataArray(
+                        self.mean(raw_ds[ky].values),
+                        coords=coords_dict,
+                        dims=dims_list,
+                        attrs=raw_ds[ky].attrs,
+                    ).astype("float32")
                 except:  # variables not needing averaging
                     pass
             # Add standard deviation
             std = self.standard_deviation(raw_ds.velds.U_mag.values)
-            out_ds['U_std'] = xr.DataArray(
-                std.astype('float32'),
+            out_ds["U_std"] = xr.DataArray(
+                std.astype("float32"),
                 dims=raw_ds.vel.dims[1:],
-                attrs={'units': 'm s-1',
-                       'long_name': 'Water Velocity Standard Deviation'})
+                attrs={
+                    "units": "m s-1",
+                    "long_name": "Water Velocity Standard Deviation",
+                },
+            )
 
         return out_ds
 
-    def bin_variance(self, raw_ds, out_ds=None, names=None, suffix='_var'):
+    def bin_variance(self, raw_ds, out_ds=None, names=None, suffix="_var"):
         """
-        Bin the dataset and calculate the ensemble variances of each 
+        Bin the dataset and calculate the ensemble variances of each
         variable. Complementary to `bin_average()`.
 
         Parameters
         ----------
         raw_ds : xarray.Dataset
            The raw data structure to be binned.
         out_ds : xarray.Dataset
@@ -626,29 +710,32 @@
 
         if names is None:
             names = raw_ds.data_vars
 
         for ky in names:
             # set up dimensions and coordinates for dataarray
             dims_list = raw_ds[ky].dims
+            if any([ar for ar in dims_list if "altraw" in ar]):
+                continue
             coords_dict = {}
             for nm in dims_list:
-                if 'time' in nm:
+                if "time" in nm:
                     coords_dict[nm] = self.mean(raw_ds[ky][nm].values)
                 else:
                     coords_dict[nm] = raw_ds[ky][nm].values
 
             # create Dataset
-            if 'ensemble' not in ky:
+            if "ensemble" not in ky:
                 try:  # variables with time coordinate
-                    out_ds[ky+suffix] = xr.DataArray(self.variance(raw_ds[ky].values),
-                                                     coords=coords_dict,
-                                                     dims=dims_list,
-                                                     attrs=raw_ds[ky].attrs
-                                                     ).astype('float32')
+                    out_ds[ky + suffix] = xr.DataArray(
+                        self.variance(raw_ds[ky].values),
+                        coords=coords_dict,
+                        dims=dims_list,
+                        attrs=raw_ds[ky].attrs,
+                    ).astype("float32")
                 except:  # variables not needing averaging
                     pass
 
         return out_ds
 
     def autocovariance(self, veldat, n_bin=None):
         """
@@ -676,150 +763,218 @@
 
         This has the advantage that the 0 index is actually zero-lag.
         """
 
         indat = veldat.values
 
         n_bin = self._parse_nbin(n_bin)
-        out = np.empty(self._outshape(indat.shape, n_bin=n_bin)[:-1] +
-                       [int(n_bin // 4)], dtype=indat.dtype)
+        out = np.empty(
+            self._outshape(indat.shape, n_bin=n_bin)[:-1] + [int(n_bin // 4)],
+            dtype=indat.dtype,
+        )
         dt1 = self.reshape(indat, n_pad=n_bin / 2 - 2)
         # Here we de-mean only on the 'valid' range:
-        dt1 = dt1 - dt1[..., :, int(n_bin // 4):
-                        int(-n_bin // 4)].mean(-1)[..., None]
+        dt1 = dt1 - dt1[..., :, int(n_bin // 4) : int(-n_bin // 4)].mean(-1)[..., None]
         dt2 = self.demean(indat)
         se = slice(int(n_bin // 4) - 1, None, 1)
         sb = slice(int(n_bin // 4) - 1, None, -1)
         for slc in slice1d_along_axis(dt1.shape, -1):
-            tmp = np.correlate(dt1[slc], dt2[slc], 'valid')
+            tmp = np.correlate(dt1[slc], dt2[slc], "valid")
             # The zero-padding in reshape means we compute coherence
             # from one-sided time-series for first and last points.
             if slc[-2] == 0:
                 out[slc] = tmp[se]
             elif slc[-2] == dt2.shape[-2] - 1:
                 out[slc] = tmp[sb]
             else:
                 # For the others we take the average of the two sides.
                 out[slc] = (tmp[se] + tmp[sb]) / 2
 
         dims_list, coords_dict = self._new_coords(veldat)
         # tack on new coordinate
-        dims_list.append('lag')
-        coords_dict['lag'] = np.arange(n_bin//4)
+        dims_list.append("lag")
+        coords_dict["lag"] = np.arange(n_bin // 4)
 
-        da = xr.DataArray(out.astype('float32'),
-                          coords=coords_dict,
-                          dims=dims_list,)
-        da['lag'].attrs['units'] = 'timestep'
+        da = xr.DataArray(
+            out.astype("float32"),
+            coords=coords_dict,
+            dims=dims_list,
+        )
+        da["lag"].attrs["units"] = "timestep"
 
         return da
 
+    def turbulence_intensity(self, U_mag, noise=0, thresh=0, detrend=False):
+        """
+        Calculate noise-corrected turbulence intensity.
+
+        Parameters
+        ----------
+        U_mag : xarray.DataArray
+          Raw horizontal velocity magnitude
+        noise : numeric
+          Instrument noise level in same units as velocity. Typically
+          found from `<adv or adp>.turbulence.doppler_noise_level`.
+          Default: None.
+        thresh : numeric
+          Theshold below which TI will not be calculated
+        detrend : bool (default: False)
+          Detrend the velocity data (True), or simply de-mean it
+          (False), prior to computing TI.
+        """
+
+        if "xarray" in type(U_mag).__module__:
+            U = U_mag.values
+        if "xarray" in type(noise).__module__:
+            noise = noise.values
+
+        if detrend:
+            up = self.detrend(U)
+        else:
+            up = self.demean(U)
+
+        # Take RMS and subtract noise
+        u_rms = np.sqrt(np.nanmean(up**2, axis=-1) - noise**2)
+        u_mag = self.mean(U)
+
+        ti = np.ma.masked_where(u_mag < thresh, u_rms / u_mag)
+
+        dims = U_mag.dims
+        coords = {}
+        for nm in U_mag.dims:
+            if "time" in nm:
+                coords[nm] = self.mean(U_mag[nm].values)
+            else:
+                coords[nm] = U_mag[nm].values
+
+        return xr.DataArray(
+            ti.data.astype("float32"),
+            coords=coords,
+            dims=dims,
+            attrs={
+                "units": "% [0,1]",
+                "long_name": "Turbulence Intensity",
+                "comment": f"TI was corrected from a noise level of {noise} m/s",
+            },
+        )
+
     def turbulent_kinetic_energy(self, veldat, noise=None, detrend=True):
         """
-        Calculate the turbulent kinetic energy (TKE) (variances 
+        Calculate the turbulent kinetic energy (TKE) (variances
         of u,v,w).
 
         Parameters
         ----------
         veldat : xarray.DataArray
-          Velocity data array from ADV or single beam from ADCP. 
+          Velocity data array from ADV or single beam from ADCP.
           The last dimension is assumed to be time.
         noise : float or array-like
-          A vector of the noise levels of the velocity data with 
-          the same first dimension as the velocity vector.
+          Instrument noise level in same units as velocity. Typically
+          found from `<adv or adp>.turbulence.doppler_noise_level`.
+          Default: None.
         detrend : bool (default: False)
           Detrend the velocity data (True), or simply de-mean it
-          (False), prior to computing tke. Note: the psd routines
+          (False), prior to computing TKE. Note: the PSD routines
           use detrend, so if you want to have the same amount of
           variance here as there use ``detrend=True``.
-          
+
         Returns
         -------
         tke_vec : xarray.DataArray
           dataArray containing u'u'_, v'v'_ and w'w'_
         """
 
-        if 'xarray' in type(veldat).__module__:
+        if "xarray" in type(veldat).__module__:
             vel = veldat.values
-        if 'xarray' in type(noise).__module__:
+        if "xarray" in type(noise).__module__:
             noise = noise.values
 
         if len(np.shape(vel)) > 2:
-            raise ValueError("This function is only valid for calculating TKE using "
-                            "velocity from an ADV or a single ADCP beam.")
+            raise ValueError(
+                "This function is only valid for calculating TKE using "
+                "velocity from an ADV or a single ADCP beam."
+            )
 
         # Calc TKE
         if detrend:
-            out = np.nanmean(self.detrend(vel)**2, axis=-1)
+            out = np.nanmean(self.detrend(vel) ** 2, axis=-1)
         else:
-            out = np.nanmean(self.demean(vel)**2, axis=-1)
+            out = np.nanmean(self.demean(vel) ** 2, axis=-1)
 
-        if 'dir' in veldat.dims:
+        if "dir" in veldat.dims:
             # Subtract noise
             if noise is not None:
                 if np.shape(noise)[0] != 3:
                     raise Exception(
-                        'Noise should have same first dimension as velocity')
+                        "Noise should have same first dimension as velocity"
+                    )
                 out[0] -= noise[0] ** 2
                 out[1] -= noise[1] ** 2
                 out[2] -= noise[2] ** 2
             # Set coords
-            dims = ['tke', 'time']
-            coords = {'tke': self.tke,
-                      'time': self.mean(veldat.time.values)}
+            dims = ["tke", "time"]
+            coords = {"tke": self.tke, "time": self.mean(veldat.time.values)}
         else:
             # Subtract noise
             if noise is not None:
                 if np.shape(noise) > np.shape(vel):
                     raise Exception(
-                        'Noise should have same or fewer dimensions as velocity')
-                out -= noise ** 2
+                        "Noise should have same or fewer dimensions as velocity"
+                    )
+                out -= noise**2
             # Set coords
             dims = veldat.dims
             coords = {}
             for nm in veldat.dims:
-                if 'time' in nm:
+                if "time" in nm:
                     coords[nm] = self.mean(veldat[nm].values)
                 else:
                     coords[nm] = veldat[nm].values
 
         return xr.DataArray(
-            out.astype('float32'),
+            out.astype("float32"),
             dims=dims,
             coords=coords,
-            attrs={'units': 'm2 s-2',
-                   'long_name': 'TKE Vector',
-                   'standard_name': 'specific_turbulent_kinetic_energy_of_sea_water'})
-
-    def power_spectral_density(self, veldat,
-                               freq_units='rad/s',
-                               fs=None,
-                               window='hann',
-                               noise=None,
-                               n_bin=None, n_fft=None, n_pad=None,
-                               step=None):
+            attrs={
+                "units": "m2 s-2",
+                "long_name": "TKE Vector",
+                "standard_name": "specific_turbulent_kinetic_energy_of_sea_water",
+            },
+        )
+
+    def power_spectral_density(
+        self,
+        veldat,
+        freq_units="rad/s",
+        fs=None,
+        window="hann",
+        noise=0,
+        n_bin=None,
+        n_fft=None,
+        n_pad=None,
+        step=None,
+    ):
         """
         Calculate the power spectral density of velocity.
 
         Parameters
         ----------
         veldat : xr.DataArray
           The raw velocity data (of dims 'dir' and 'time').
         freq_units : string
-          Frequency units of the returned spectra in either Hz or rad/s 
+          Frequency units of the returned spectra in either Hz or rad/s
           (`f` or :math:`\\omega`)
         fs : float (optional)
           The sample rate. Default is `binner.fs`
         window : string or array
           Specify the window function.
           Options: 1, None, 'hann', 'hamm'
-        noise : float or array-like
-          A vector of the noise levels of the velocity data with 
-          the same first dimension as the velocity vector.
-          Default = 0.
+        noise : numeric or array
+          Instrument noise level in same units as velocity.
+          Default: 0 (ADCP) or [0, 0, 0] (ADV).
         n_bin : int (optional)
           The bin-size. Default: from the binner.
         n_fft : int (optional)
           The fft size. Default: from the binner.
         n_pad : int (optional)
           The number of values to pad with zero. Default = 0.
         step : int (optional)
@@ -831,80 +986,97 @@
         -------
         psd : xarray.DataArray (3, M, N_FFT)
           The spectra in the 'u', 'v', and 'w' directions.
         """
 
         fs_in = self._parse_fs(fs)
         n_fft = self._parse_nfft(n_fft)
-        if 'xarray' in type(veldat).__module__:
+        if "xarray" in type(veldat).__module__:
             vel = veldat.values
-        if 'xarray' in type(noise).__module__:
-            noise = noise.values
-        if ('rad' not in freq_units) and ('Hz' not in freq_units):
+        if ("rad" not in freq_units) and ("Hz" not in freq_units):
             raise ValueError("`freq_units` should be one of 'Hz' or 'rad/s'")
-        
+
         # Create frequency vector, also checks whether using f or omega
-        if 'rad' in freq_units:
-            fs = 2*np.pi*fs_in
-            freq_units = 'rad s-1'
-            units = 'm2 s-1 rad-1'
+        if "rad" in freq_units:
+            fs = 2 * np.pi * fs_in
+            freq_units = "rad s-1"
+            units = "m2 s-1 rad-1"
         else:
             fs = fs_in
-            freq_units = 'Hz'
-            units = 'm2 s-2 Hz-1'
-        freq = xr.DataArray(self._fft_freq(fs=fs_in, units=freq_units, n_fft=n_fft),
-                            dims=['freq'],
-                            name='freq',
-                            attrs={'units': freq_units,
-                                   'long_name': 'FFT Frequency Vector',
-                                   'coverage_content_type': 'coordinate'}
-                            ).astype('float32')
+            freq_units = "Hz"
+            units = "m2 s-2 Hz-1"
+        freq = xr.DataArray(
+            self._fft_freq(fs=fs_in, units=freq_units, n_fft=n_fft),
+            dims=["freq"],
+            name="freq",
+            attrs={
+                "units": freq_units,
+                "long_name": "FFT Frequency Vector",
+                "coverage_content_type": "coordinate",
+            },
+        ).astype("float32")
 
         # Spectra, if input is full velocity or a single array
-        if len(vel.shape) == 2:
-            assert vel.shape[0] == 3, "Function can only handle 1D or 3D arrays." \
-                " If ADCP data, please select a specific depth bin."
-            if (noise is not None) and (np.shape(noise)[0] != 3):
-                raise Exception(
-                    'Noise should have same first dimension as velocity')
+        if len(vel.shape) >= 2:
+            if vel.shape[0] != 3:
+                raise ValueError(
+                    "Function can only handle 1D or 3D arrays."
+                    " If ADCP data, please select a specific depth bin."
+                )
+            if np.array(noise).any():
+                if np.size(noise) != 3:
+                    raise ValueError("Noise is expected to be an array of 3 scalars")
             else:
+                # Reset default to list of 3 zeros
                 noise = np.array([0, 0, 0])
-            out = np.empty(self._outshape_fft(vel[:3].shape, n_fft=n_fft, n_bin=n_bin),
-                           dtype=np.float32)
+
+            out = np.empty(
+                self._outshape_fft(vel[:3].shape, n_fft=n_fft, n_bin=n_bin),
+                dtype=np.float32,
+            )
             for idx in range(3):
-                out[idx] = self._psd_base(vel[idx],
-                                          fs=fs,
-                                          noise=noise[idx],
-                                          window=window,
-                                          n_bin=n_bin,
-                                          n_pad=n_pad,
-                                          n_fft=n_fft,
-                                          step=step)
-            coords = {'S': self.S,
-                      'time': self.mean(veldat['time'].values),
-                      'freq': freq}
-            dims = ['S', 'time', 'freq']
+                out[idx] = self._psd_base(
+                    vel[idx],
+                    fs=fs,
+                    noise=noise[idx],
+                    window=window,
+                    n_bin=n_bin,
+                    n_pad=n_pad,
+                    n_fft=n_fft,
+                    step=step,
+                )
+            coords = {
+                "S": self.S,
+                "time": self.mean(veldat["time"].values),
+                "freq": freq,
+            }
+            dims = ["S", "time", "freq"]
         else:
-            if (noise is not None) and (len(np.shape(noise)) > 1):
-                raise Exception(
-                    'Noise should have same first dimension as velocity')
-            else:
-                noise = np.array(0)
-            out = self._psd_base(vel,
-                                 fs=fs,
-                                 noise=noise,
-                                 window=window,
-                                 n_bin=n_bin,
-                                 n_pad=n_pad,
-                                 n_fft=n_fft,
-                                 step=step)
-            coords = {veldat.dims[-1]: self.mean(veldat[veldat.dims[-1]].values),
-                      'freq': freq}
-            dims = [veldat.dims[-1], 'freq']
+            if np.array(noise).any() and np.size(noise) > 1:
+                raise ValueError("Noise is expected to be a scalar")
+
+            out = self._psd_base(
+                vel,
+                fs=fs,
+                noise=noise,
+                window=window,
+                n_bin=n_bin,
+                n_pad=n_pad,
+                n_fft=n_fft,
+                step=step,
+            )
+            coords = {
+                veldat.dims[-1]: self.mean(veldat[veldat.dims[-1]].values),
+                "freq": freq,
+            }
+            dims = [veldat.dims[-1], "freq"]
 
         return xr.DataArray(
-            out.astype('float32'),
+            out.astype("float32"),
             coords=coords,
             dims=dims,
-            attrs={'units': units,
-                   'n_fft': n_fft,
-                   'long_name': 'Power Spectral Density'})
+            attrs={
+                "units": units,
+                "n_fft": n_fft,
+                "long_name": "Power Spectral Density",
+            },
+        )
```

### Comparing `mhkit-0.7.0/mhkit/mooring/graphics.py` & `mhkit-0.8.0/mhkit/mooring/graphics.py`

 * *Files 6% similar despite different names*

```diff
@@ -25,16 +25,30 @@
 """
 
 import matplotlib.pyplot as plt
 import xarray as xr
 from matplotlib.animation import FuncAnimation
 
 
-def animate(dsani, dimension='2d', xaxis='x', yaxis='z', zaxis='y', xlim=None, ylim=None, zlim=None,
-            interval=10, repeat=False, xlabel=None, ylabel=None, zlabel=None, title=None):
+def animate(
+    dsani,
+    dimension="2d",
+    xaxis="x",
+    yaxis="z",
+    zaxis="y",
+    xlim=None,
+    ylim=None,
+    zlim=None,
+    interval=10,
+    repeat=False,
+    xlabel=None,
+    ylabel=None,
+    zlabel=None,
+    title=None,
+):
     """
     Graphics function that creates a 2D or 3D animation of the node positions of a mooring line over time.
 
     Parameters
     ----------
     dsani : xr.Dataset
         Xarray dataset object containing MoorDyn node variables (ie 'Node0px')
@@ -69,105 +83,114 @@
     -------
     matplotlib.animation.FuncAnimation
         Animation object
 
     Raises
     ------
     TypeError
-        Checks for correct input types for dsani, dimension, xaxis, yaxis, zaxis, xlim, ylim, 
+        Checks for correct input types for dsani, dimension, xaxis, yaxis, zaxis, xlim, ylim,
         zlim, interval, repeat, xlabel, ylabel, zlabel, and title
     """
-    _validate_input(dsani, xlim, ylim, interval, repeat,
-                    xlabel, ylabel, title, dimension)
-    if dimension == '3d':
+    _validate_input(
+        dsani, xlim, ylim, interval, repeat, xlabel, ylabel, title, dimension
+    )
+    if dimension == "3d":
         if not isinstance(zlim, (list, type(None))):
-            raise TypeError('zlim must be of type list')
+            raise TypeError("zlim must be of type list")
         if not isinstance(zlabel, (str, type(None))):
-            raise TypeError('zlabel must be of type str')
+            raise TypeError("zlabel must be of type str")
     if not isinstance(xaxis, str):
-        raise TypeError('xaxis must be of type str')
+        raise TypeError("xaxis must be of type str")
     if not isinstance(yaxis, str):
-        raise TypeError('yaxis must be of type str')
+        raise TypeError("yaxis must be of type str")
     if not isinstance(zaxis, str):
-        raise TypeError('zaxis must be of type str')
+        raise TypeError("zaxis must be of type str")
 
     current_idx = list(dsani.dims.mapping.keys())[0]
-    dsani = dsani.rename({current_idx: 'time'})
+    dsani = dsani.rename({current_idx: "time"})
 
     nodes_x, nodes_y, nodes_z = _get_axis_nodes(dsani, xaxis, yaxis, zaxis)
 
     if not xlim:
         xlim = _find_limits(dsani[nodes_x])
     if not ylim:
         ylim = _find_limits(dsani[nodes_y])
-    if dimension == '3d' and not zlim:
+    if dimension == "3d" and not zlim:
         zlim = _find_limits(dsani[nodes_z])
 
     fig = plt.figure()
-    if dimension == '3d':
-        ax = fig.add_subplot(projection='3d')
+    if dimension == "3d":
+        ax = fig.add_subplot(projection="3d")
     else:
         ax = fig.add_subplot()
     ax.grid()
 
-    if dimension == '2d':
-        ln, = ax.plot([], [], '-o')
+    if dimension == "2d":
+        (ln,) = ax.plot([], [], "-o")
 
         def init():
             ax.set(xlim=xlim, ylim=ylim)
             _set_labels(ax, xlabel, ylabel, title)
             return ln
 
         def update(frame):
             x = dsani[nodes_x].isel(time=frame).to_array().values
             y = dsani[nodes_y].isel(time=frame).to_array().values
             ln.set_data(x, y)
 
-    elif dimension == '3d':
-        ln, = ax.plot([], [], [], '-o')
+    elif dimension == "3d":
+        (ln,) = ax.plot([], [], [], "-o")
 
         def init():
             ax.set(xlim3d=xlim, ylim3d=ylim, zlim3d=zlim)
             _set_labels(ax, xlabel, ylabel, title, zlabel)
             return ln
 
         def update(frame):
             x = dsani[nodes_x].isel(time=frame).to_array().values
             y = dsani[nodes_y].isel(time=frame).to_array().values
             z = dsani[nodes_z].isel(time=frame).to_array().values
             ln.set_data(x, y)
             ln.set_3d_properties(z)
 
-    ani = FuncAnimation(fig, update, frames=len(dsani.time),
-                        init_func=init, interval=interval, repeat=repeat)
+    ani = FuncAnimation(
+        fig,
+        update,
+        frames=len(dsani.time),
+        init_func=init,
+        interval=interval,
+        repeat=repeat,
+    )
 
     return ani
 
 
-def _validate_input(dsani, xlim, ylim, interval, repeat, xlabel, ylabel, title, dimension):
+def _validate_input(
+    dsani, xlim, ylim, interval, repeat, xlabel, ylabel, title, dimension
+):
     """
     Validate common input parameters for animate function.
     """
     if not isinstance(dsani, xr.Dataset):
-        raise TypeError('dsani must be of type xr.Dataset')
+        raise TypeError("dsani must be of type xr.Dataset")
     if not isinstance(xlim, (list, type(None))):
-        raise TypeError('xlim must be of type list')
+        raise TypeError("xlim must be of type list")
     if not isinstance(ylim, (list, type(None))):
-        raise TypeError('ylim must be of type list')
+        raise TypeError("ylim must be of type list")
     if not isinstance(interval, int):
-        raise TypeError('interval must be of type int')
+        raise TypeError("interval must be of type int")
     if not isinstance(repeat, bool):
-        raise TypeError('repeat must be of type bool')
+        raise TypeError("repeat must be of type bool")
     if not isinstance(xlabel, (str, type(None))):
-        raise TypeError('xlabel must be of type str')
+        raise TypeError("xlabel must be of type str")
     if not isinstance(ylabel, (str, type(None))):
-        raise TypeError('ylabel must be of type str')
+        raise TypeError("ylabel must be of type str")
     if not isinstance(title, (str, type(None))):
-        raise TypeError('title must be of type str')
-    if dimension not in ['2d', '3d']:
+        raise TypeError("title must be of type str")
+    if dimension not in ["2d", "3d"]:
         raise ValueError('dimension must be either "2d" or "3d"')
 
 
 def _get_axis_nodes(dsani, xaxis, yaxis, zaxis):
     """
     Helper function to generate the list of nodes along each axis.
 
@@ -187,18 +210,18 @@
     nodesX : list
         List of nodes along the x-axis
     nodesY : list
         List of nodes along the y-axis
     nodesZ : list
         List of nodes along the z-axis
     """
-    nodes = [s for s in list(dsani.data_vars) if 'Node' in s]
-    nodes_x = [s for s in nodes if f'p{xaxis}' in s]
-    nodes_y = [s for s in nodes if f'p{yaxis}' in s]
-    nodes_z = [s for s in nodes if f'p{zaxis}' in s]
+    nodes = [s for s in list(dsani.data_vars) if "Node" in s]
+    nodes_x = [s for s in nodes if f"p{xaxis}" in s]
+    nodes_y = [s for s in nodes if f"p{yaxis}" in s]
+    nodes_z = [s for s in nodes if f"p{zaxis}" in s]
 
     return nodes_x, nodes_y, nodes_z
 
 
 def _find_limits(dataset):
     """Auto calculate the min and max plot limits based on provided dataset
 
@@ -209,17 +232,17 @@
 
     Returns
     -------
     list
         Min and max plot limits for axis
     """
     x_1 = dataset.min().to_array().min().values
-    x_1 = x_1 - abs(x_1*0.1)
+    x_1 = x_1 - abs(x_1 * 0.1)
     x_2 = dataset.max().to_array().max().values
-    x_2 = x_2 + abs(x_2*0.1)
+    x_2 = x_2 + abs(x_2 * 0.1)
     return [x_1, x_2]
 
 
 def _set_labels(ax, xlabel=None, ylabel=None, title=None, zlabel=None):
     """
     Helper function to set the labels and title for a plot.
```

### Comparing `mhkit-0.7.0/mhkit/mooring/io.py` & `mhkit-0.8.0/mhkit/mooring/io.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,23 +12,24 @@
 each line in the output file, parses various sets of properties and parameters, and stores 
 them as attributes in the provided dataset.
 
 Typical usage example:
 
     dataset = read_moordyn(filepath="FAST.MD.out", input_file="FAST.MD.input")
 """
+
 import os
 import pandas as pd
 
 
 def read_moordyn(filepath, input_file=None):
     """
-    Reads in MoorDyn OUT files such as "FAST.MD.out" and 
-    "FAST.MD.Line1.out" and stores inside xarray. Also allows for 
-    parsing and storage of MoorDyn input file as attributes inside 
+    Reads in MoorDyn OUT files such as "FAST.MD.out" and
+    "FAST.MD.Line1.out" and stores inside xarray. Also allows for
+    parsing and storage of MoorDyn input file as attributes inside
     the xarray.
 
     Parameters
     ----------
     filepath : str
         Path to MoorDyn OUT file
     inputfile : str (optional)
@@ -41,23 +42,24 @@
 
     Raises
     ------
     TypeError
         Checks for correct input types for filepath and input_file
     """
     if not isinstance(filepath, str):
-        raise TypeError('filepath must be of type str')
+        raise TypeError("filepath must be of type str")
     if input_file:
         if not isinstance(input_file, str):
-            raise TypeError('input_file must be of type str')
+            raise TypeError("input_file must be of type str")
     if not os.path.isfile(filepath):
         raise FileNotFoundError(f"No file found at provided path: {filepath}")
 
-    data = pd.read_csv(filepath, header=0, skiprows=[
-                       1], sep=' ', skipinitialspace=True, index_col=0)
+    data = pd.read_csv(
+        filepath, header=0, skiprows=[1], sep=" ", skipinitialspace=True, index_col=0
+    )
     data = data.dropna(axis=1)
     dataset = data.to_xarray()
 
     if input_file:
         dataset = _moordyn_input(input_file, dataset)
 
     return dataset
@@ -76,81 +78,87 @@
 
     Returns
     -------
     xr.Dataset
         return Dataset that includes input file parameters as attributes
     """
 
-    with open(input_file, 'r', encoding='utf-8') as moordyn_file:
-        for line in moordyn_file:          # loop through each line in the file
+    with open(input_file, "r", encoding="utf-8") as moordyn_file:
+        for line in moordyn_file:  # loop through each line in the file
             # get line type property sets
-            if line.count('---') > 0 and (line.upper().count('LINE DICTIONARY') > 0 or
-                                          line.upper().count('LINE TYPES') > 0):
+            if line.count("---") > 0 and (
+                line.upper().count("LINE DICTIONARY") > 0
+                or line.upper().count("LINE TYPES") > 0
+            ):
                 linetypes = dict()
                 # skip this header line, plus channel names and units lines
                 line = next(moordyn_file)
                 variables = line.split()
                 line = next(moordyn_file)
                 units = line.split()
                 line = next(moordyn_file)
-                while line.count('---') == 0:
+                while line.count("---") == 0:
                     entries = line.split()
                     linetypes[entries[0]] = dict()
                     for x in range(1, len(entries)):
                         linetypes[entries[0]][variables[x]] = entries[x]
                     line = next(moordyn_file)
-                linetypes['units'] = units[1:]
-                dataset.attrs['LINE_TYPES'] = linetypes
+                linetypes["units"] = units[1:]
+                dataset.attrs["LINE_TYPES"] = linetypes
 
             # get properties of each Point
-            if line.count('---') > 0 and (line.upper().count('POINTS') > 0
-                                          or line.upper().count('POINT LIST') > 0
-                                          or line.upper().count('POINT PROPERTIES') > 0):
+            if line.count("---") > 0 and (
+                line.upper().count("POINTS") > 0
+                or line.upper().count("POINT LIST") > 0
+                or line.upper().count("POINT PROPERTIES") > 0
+            ):
                 # skip this header line, plus channel names and units lines
                 line = next(moordyn_file)
                 variables = line.split()
                 line = next(moordyn_file)
                 units = line.split()
                 line = next(moordyn_file)
                 points = dict()
-                while line.count('---') == 0:
+                while line.count("---") == 0:
                     entries = line.split()
                     points[entries[0]] = dict()
                     for x in range(1, len(entries)):
                         points[entries[0]][variables[x]] = entries[x]
                     line = next(moordyn_file)
-                points['units'] = units[1:]
-                dataset.attrs['POINTS'] = points
+                points["units"] = units[1:]
+                dataset.attrs["POINTS"] = points
 
             # get properties of each line
-            if line.count('---') > 0 and (line.upper().count('LINES') > 0
-                                          or line.upper().count('LINE LIST') > 0
-                                          or line.upper().count('LINE PROPERTIES') > 0):
+            if line.count("---") > 0 and (
+                line.upper().count("LINES") > 0
+                or line.upper().count("LINE LIST") > 0
+                or line.upper().count("LINE PROPERTIES") > 0
+            ):
                 # skip this header line, plus channel names and units lines
                 line = next(moordyn_file)
                 variables = line.split()
                 line = next(moordyn_file)
                 units = line.split()
                 line = next(moordyn_file)
                 lines = {}
-                while line.count('---') == 0:
+                while line.count("---") == 0:
                     entries = line.split()
                     lines[entries[0]] = dict()
                     for x in range(1, len(entries)):
                         lines[entries[0]][variables[x]] = entries[x]
                     line = next(moordyn_file)
-                lines['units'] = units[1:]
-                dataset.attrs['LINES'] = lines
+                lines["units"] = units[1:]
+                dataset.attrs["LINES"] = lines
 
             # get options entries
-            if line.count('---') > 0 and "options" in line.lower():
+            if line.count("---") > 0 and "options" in line.lower():
                 line = next(moordyn_file)  # skip this header line
                 options = {}
-                while line.count('---') == 0:
+                while line.count("---") == 0:
                     entries = line.split()
                     options[entries[1]] = entries[0]
                     line = next(moordyn_file)
-                dataset.attrs['OPTIONS'] = options
+                dataset.attrs["OPTIONS"] = options
 
     moordyn_file.close()
 
     return dataset
```

### Comparing `mhkit-0.7.0/mhkit/mooring/main.py` & `mhkit-0.8.0/mhkit/mooring/main.py`

 * *Files 11% similar despite different names*

```diff
@@ -23,44 +23,45 @@
     ------
     ValueError
         Checks for mininum number of nodes necessary to calculate laylength
     TypeError
         Checks for correct input types for ds, depth, and tolerance
     """
     if not isinstance(dataset, xr.Dataset):
-        raise TypeError('dataset must be of type xr.Dataset')
+        raise TypeError("dataset must be of type xr.Dataset")
     if not isinstance(depth, (float, int)):
-        raise TypeError('depth must be of type float or int')
+        raise TypeError("depth must be of type float or int")
     if not isinstance(tolerance, (float, int)):
-        raise TypeError('tolerance must be of type float or int')
+        raise TypeError("tolerance must be of type float or int")
 
     # get channel names
     chans = list(dataset.keys())
-    nodes_x = [x for x in chans if 'x' in x]
-    nodes_y = [y for y in chans if 'y' in y]
-    nodes_z = [z for z in chans if 'z' in z]
+    nodes_x = [x for x in chans if "x" in x]
+    nodes_y = [y for y in chans if "y" in y]
+    nodes_z = [z for z in chans if "z" in z]
 
     # check if the dataset contains the necessary 'x', 'y', 'z' nodes
     if not nodes_x or not nodes_y or not nodes_z:
-        raise ValueError('The dataset must contain x, y, and z node data')
+        raise ValueError("The dataset must contain x, y, and z node data")
 
     if len(nodes_z) < 3:
         raise ValueError(
-            'This function requires at least 3 nodes to calculate lay length')
+            "This function requires at least 3 nodes to calculate lay length"
+        )
 
     # find name of first z point where tolerance is exceeded
-    laypoint = dataset[nodes_z].where(dataset[nodes_z] > depth+abs(tolerance))
+    laypoint = dataset[nodes_z].where(dataset[nodes_z] > depth + abs(tolerance))
     laypoint = laypoint.to_dataframe().dropna(axis=1).columns[0]
     # get previous z-point
     lay_indx = nodes_z.index(laypoint) - 1
     lay_z = nodes_z[lay_indx]
     # get corresponding x-point and y-point node names
-    lay_x = lay_z[:-1] + 'x'
-    lay_y = lay_z[:-1] + 'y'
+    lay_x = lay_z[:-1] + "x"
+    lay_y = lay_z[:-1] + "y"
     lay_0x = nodes_x[0]
     lay_0y = nodes_y[0]
     # find distance between initial point and lay point
     laylength_x = dataset[lay_x] - dataset[lay_0x]
     laylength_y = dataset[lay_y] - dataset[lay_0y]
-    line_lay_length = (laylength_x**2 + laylength_y**2) ** (1/2)
+    line_lay_length = (laylength_x**2 + laylength_y**2) ** (1 / 2)
 
     return line_lay_length
```

### Comparing `mhkit-0.7.0/mhkit/river/io/d3d.py` & `mhkit-0.8.0/mhkit/river/io/d3d.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,628 +1,809 @@
 from mhkit.utils import unorm
 import scipy.interpolate as interp
 import numpy as np
 import pandas as pd
+import xarray as xr
 import netCDF4
 import warnings
 
 
 def get_all_time(data):
-    '''
-    Returns all of the time stamps from a D3D simulation passed to the function 
+    """
+    Returns all of the time stamps from a D3D simulation passed to the function
     as a NetCDF object (data)
-    
+
     Parameters
     ----------
-    data: NetCDF4 object 
+    data: NetCDF4 object
        A NetCDF4 object that contains spatial data, e.g. velocity or shear
-       stress generated by running a Delft3D model.  
+       stress generated by running a Delft3D model.
 
     Returns
     -------
     seconds_run: array
         Returns an array of integers representing the number of seconds after
         the simulation started and that the data object contains a snapshot of
         simulation conditions at that time.
-    '''
-    
-    assert type(data)== netCDF4._netCDF4.Dataset, 'data must be NetCDF4 object' 
+    """
+
+    if not isinstance(data, netCDF4._netCDF4.Dataset):
+        raise TypeError("data must be a NetCDF4 object")
 
-    seconds_run = np.ma.getdata(data.variables['time'][:], False)
+    seconds_run = np.ma.getdata(data.variables["time"][:], False)
 
     return seconds_run
 
 
 def index_to_seconds(data, time_index):
-    '''
-    The function will return 'seconds_run' if passed a 'time_index' 
+    """
+    The function will return 'seconds_run' if passed a 'time_index'
 
     Parameters
     ----------
-    data: NetCDF4 object 
+    data: NetCDF4 object
        A NetCDF4 object that contains spatial data, e.g. velocity or shear
-       stress, generated by running a Delft3D model.  
-    time_index: int 
+       stress, generated by running a Delft3D model.
+    time_index: int
         A positive integer to pull the time index from the dataset. 0 being closest
         to time 0. Default is last time index -1.
 
     Returns
     -------
     seconds_run: int, float
         The 'seconds_run' is the seconds corresponding to the 'time_index' increments.
-    '''
+    """
     return _convert_time(data, time_index=time_index)
 
 
 def seconds_to_index(data, seconds_run):
-    '''
+    """
     The function will return the nearest 'time_index' in the data if passed an
     integer number of 'seconds_run'
-    
+
     Parameters
     ----------
-    data: NetCDF4 object 
+    data: NetCDF4 object
         A NetCDF4 object that contains spatial data, e.g. velocity or shear
-        stress, generated by running a Delft3D model.  
+        stress, generated by running a Delft3D model.
     seconds_run: int, float
-        A positive integer or float that represents the amount of time in seconds 
+        A positive integer or float that represents the amount of time in seconds
         passed since starting the simulation.
 
     Returns
     -------
     time_index: int
-        The 'time_index' is a positive integer starting from 0 
+        The 'time_index' is a positive integer starting from 0
         and incrementing until in simulation is complete.
-    '''
+    """
     return _convert_time(data, seconds_run=seconds_run)
 
 
 def _convert_time(data, time_index=None, seconds_run=None):
-    '''
-    Converts a time index to seconds or seconds to a time index. The user 
-    must specify 'time_index' or 'seconds_run' (Not both). The function 
-    will returns 'seconds_run' if passed a 'time_index' or will return the 
+    """
+    Converts a time index to seconds or seconds to a time index. The user
+    must specify 'time_index' or 'seconds_run' (Not both). The function
+    will returns 'seconds_run' if passed a 'time_index' or will return the
     closest 'time_index' if passed a number of 'seconds_run'.
 
     Parameters
     ----------
-    data: NetCDF4 object 
+    data: NetCDF4 object
        A NetCDF4 object that contains spatial data, e.g. velocity or shear
-       stress, generated by running a Delft3D model.  
-    time_index: int 
+       stress, generated by running a Delft3D model.
+    time_index: int
         An integer to pull the time index from the dataset. 0 being closest
-        to the start time. 
+        to the start time.
     seconds_run: int, float
-        An integer or float that represents the amount of time in seconds 
+        An integer or float that represents the amount of time in seconds
         passed since starting the simulation.
 
     Returns
     -------
     QoI: int, float
-        The quantity of interest is the unknown value either the 'time_index' 
-        or the 'seconds_run'. The 'time_index' is an integer starting from 0 
+        The quantity of interest is the unknown value either the 'time_index'
+        or the 'seconds_run'. The 'time_index' is an integer starting from 0
         and incrementing until in simulation is complete. The 'seconds_run' is
         the seconds corresponding to the 'time_index' increments.
-    '''
-    
-    assert type(data)== netCDF4._netCDF4.Dataset, 'data must be NetCDF4 object'
-    assert time_index or seconds_run, 'input of time_index or seconds_run needed'
-    assert not(time_index and seconds_run), f'only one time_index or seconds_run'
-    assert isinstance(time_index, (int, float)) or isinstance(seconds_run, (int,
-            float)),'time_index or seconds_run input must be a int or float'
-   
+    """
+
+    if not isinstance(data, netCDF4._netCDF4.Dataset):
+        raise TypeError("data must be NetCDF4 object")
+
+    if not (time_index or seconds_run):
+        raise ValueError("Input of time_index or seconds_run needed")
+
+    if time_index and seconds_run:
+        raise ValueError("Only one of time_index or seconds_run should be provided")
+
+    if not (
+        isinstance(time_index, (int, float)) or isinstance(seconds_run, (int, float))
+    ):
+        raise TypeError("time_index or seconds_run input must be an int or float")
+
     times = get_all_time(data)
-    
+
     if time_index:
-        QoI= times[time_index]
+        QoI = times[time_index]
     if seconds_run:
-        try: 
-            idx=np.where(times == seconds_run)
-            QoI=idx[0][0]
-        except: 
+        try:
+            idx = np.where(times == seconds_run)
+            QoI = idx[0][0]
+        except:
             idx = (np.abs(times - seconds_run)).argmin()
-            QoI= idx
-            warnings.warn( f'Warning: seconds_run not found. Closest time stamp' 
-                          +'found {times[idx]}', stacklevel= 2)
+            QoI = idx
+            warnings.warn(
+                "Warning: seconds_run not found. Closest time stamp"
+                + f"found {times[idx]}",
+                stacklevel=2,
+            )
 
     return QoI
 
 
-def get_layer_data(data, variable, layer_index=-1, time_index=-1):
-    '''
-    Get variable data from the NetCDF4 object at a specified layer and timestep. 
+def get_layer_data(data, variable, layer_index=-1, time_index=-1, to_pandas=True):
+    """
+    Get variable data from the NetCDF4 object at a specified layer and timestep.
     If the data is 2D the layer_index is ignored.
 
     Parameters
     ----------
     data: NetCDF4 object
        A NetCDF4 object that contains spatial data, e.g. velocity or shear
        stress, generated by running a Delft3D model.
     variable: string
         Delft3D outputs many vairables. The full list can be
-        found using "data.variables.keys()" in the console. 
+        found using "data.variables.keys()" in the console.
     layer_index: int
-         An integer to pull out a layer from the dataset. 0 being closest 
+         An integer to pull out a layer from the dataset. 0 being closest
          to the surface. Default is the bottom layer, found with input -1.
-    time_index: int 
+    time_index: int
         An integer to pull the time index from the dataset. 0 being closest
         to the start time. Default is last time index, found with input -1.
+    to_pandas : bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
+
     Returns
     -------
-    layer_data: DataFrame
-        DataFrame with columns of "x", "y", "waterdepth", and "waterlevel" location
-        of the specified layer, variable values "v", and the "time" the 
+    layer_data: pd.DataFrame or xr.Dataset
+        Dataset with columns of "x", "y", "waterdepth", and "waterlevel" location
+        of the specified layer, variable values "v", and the "time" the
         simulation has run. The waterdepth is measured from the water surface and the
-        "waterlevel" is the water level diffrencein meters from the zero water level. 
-    '''
-    
-    assert isinstance(time_index, int), 'time_index  must be an int'
-    assert isinstance(layer_index, int), 'layer_index  must be an int'
-    assert type(data)== netCDF4._netCDF4.Dataset, 'data must be NetCDF4 object'
-    assert variable in data.variables.keys(), 'variable not recognized'
+        "waterlevel" is the water level diffrencein meters from the zero water level.
+    """
+
+    if not isinstance(time_index, int):
+        raise TypeError("time_index must be an int")
+
+    if not isinstance(layer_index, int):
+        raise TypeError("layer_index must be an int")
+
+    if not isinstance(data, netCDF4._netCDF4.Dataset):
+        raise TypeError("data must be NetCDF4 object")
+
+    if variable not in data.variables.keys():
+        raise ValueError("variable not recognized")
+
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
     coords = str(data.variables[variable].coordinates).split()
-    var=data.variables[variable][:]
-    max_time_index= data['time'].shape[0]-1 # to account for zero index
-    assert abs(time_index) <= max_time_index, (f'time_index must be less than'
-                  +'the absolute value of the max time index {max_time_index}')
-
-    x=np.ma.getdata(data.variables[coords[0]][:], False) 
-    y=np.ma.getdata(data.variables[coords[1]][:], False)
-    
-    
-    if type(var[0][0]) == np.ma.core.MaskedArray: 
-        max_layer= len(var[0][0])
-        
-        assert abs(layer_index) <= max_layer,( f'layer_index must be less than'
-                                              +'the max layer {max_layer}')
-        v= np.ma.getdata(var[time_index,:,layer_index], False)
-        dimensions= 3
-    
-    else: 
-        assert type(var[0][0])== np.float64, 'data not  recognized'
-        dimensions= 2
-        v= np.ma.getdata(var[time_index,:], False)
-        
-    #waterdepth 
+    var = data.variables[variable][:]
+    max_time_index = data["time"].shape[0] - 1  # to account for zero index
+
+    if abs(time_index) > max_time_index:
+        raise ValueError(
+            f"time_index must be less than the absolute value of the max time index {max_time_index}"
+        )
+
+    x = np.ma.getdata(data.variables[coords[0]][:], False)
+    y = np.ma.getdata(data.variables[coords[1]][:], False)
+
+    if type(var[0][0]) == np.ma.core.MaskedArray:
+        max_layer = len(var[0][0])
+
+        if abs(layer_index) > max_layer:
+            raise ValueError(f"layer_index must be less than the max layer {max_layer}")
+
+        v = np.ma.getdata(var[time_index, :, layer_index], False)
+        dimensions = 3
+
+    else:
+        if type(var[0][0]) != np.float64:
+            raise TypeError("data not recognized")
+
+        dimensions = 2
+        v = np.ma.getdata(var[time_index, :], False)
+
+    # waterdepth
     if "mesh2d" in variable:
-        cords_to_layers= {'mesh2d_face_x mesh2d_face_y': {'name':'mesh2d_nLayers', 
-                                'coords':data.variables['mesh2d_layer_sigma'][:]},
-                       'mesh2d_edge_x mesh2d_edge_y': {'name':'mesh2d_nInterfaces', 
-                                'coords':data.variables['mesh2d_interface_sigma'][:]}}
-        bottom_depth=np.ma.getdata(data.variables['mesh2d_waterdepth'][time_index, :], False)
-        waterlevel= np.ma.getdata(data.variables['mesh2d_s1'][time_index, :], False)
-        coords = str(data.variables['waterdepth'].coordinates).split()
-        
+        cords_to_layers = {
+            "mesh2d_face_x mesh2d_face_y": {
+                "name": "mesh2d_nLayers",
+                "coords": data.variables["mesh2d_layer_sigma"][:],
+            },
+            "mesh2d_edge_x mesh2d_edge_y": {
+                "name": "mesh2d_nInterfaces",
+                "coords": data.variables["mesh2d_interface_sigma"][:],
+            },
+        }
+        bottom_depth = np.ma.getdata(
+            data.variables["mesh2d_waterdepth"][time_index, :], False
+        )
+        waterlevel = np.ma.getdata(data.variables["mesh2d_s1"][time_index, :], False)
+        coords = str(data.variables["waterdepth"].coordinates).split()
+
+    elif str(data.variables[variable].coordinates) == "FlowElem_xcc FlowElem_ycc":
+        cords_to_layers = {
+            "FlowElem_xcc FlowElem_ycc": {
+                "name": "laydim",
+                "coords": data.variables["LayCoord_cc"][:],
+            },
+            "FlowLink_xu FlowLink_yu": {
+                "name": "wdim",
+                "coords": data.variables["LayCoord_w"][:],
+            },
+        }
+        bottom_depth = np.ma.getdata(data.variables["waterdepth"][time_index, :], False)
+        waterlevel = np.ma.getdata(data.variables["s1"][time_index, :], False)
+        coords = str(data.variables["waterdepth"].coordinates).split()
     else:
-        cords_to_layers= {'FlowElem_xcc FlowElem_ycc':{'name':'laydim', 
-                                'coords':data.variables['LayCoord_cc'][:]},
-                           'FlowLink_xu FlowLink_yu': {'name':'wdim', 
-                                   'coords':data.variables['LayCoord_w'][:]}}
-        bottom_depth=np.ma.getdata(data.variables['waterdepth'][time_index, :], False)
-        waterlevel= np.ma.getdata(data.variables['s1'][time_index, :], False)
-        coords = str(data.variables['waterdepth'].coordinates).split()
-        
-    layer_dim =  str(data.variables[variable].coordinates)
-    
-    cord_sys= cords_to_layers[layer_dim]['coords']
-    layer_percentages= np.ma.getdata(cord_sys, False) #accumulative
-
-    if layer_dim == 'FlowLink_xu FlowLink_yu':
-        #interpolate 
-        x_laydim=np.ma.getdata(data.variables[coords[0]][:], False) 
-        y_laydim=np.ma.getdata(data.variables[coords[1]][:], False)
-        points_laydim = np.array([ [x, y] for x, y in zip(x_laydim, y_laydim)])
-        
+        cords_to_layers = {
+            "FlowElem_xcc FlowElem_ycc LayCoord_cc LayCoord_cc": {
+                "name": "laydim",
+                "coords": data.variables["LayCoord_cc"][:],
+            },
+            "FlowLink_xu FlowLink_yu": {
+                "name": "wdim",
+                "coords": data.variables["LayCoord_w"][:],
+            },
+        }
+        bottom_depth = np.ma.getdata(data.variables["waterdepth"][time_index, :], False)
+        waterlevel = np.ma.getdata(data.variables["s1"][time_index, :], False)
+        coords = str(data.variables["waterdepth"].coordinates).split()
+
+    layer_dim = str(data.variables[variable].coordinates)
+
+    cord_sys = cords_to_layers[layer_dim]["coords"]
+    layer_percentages = np.ma.getdata(cord_sys, False)  # accumulative
+
+    if layer_dim == "FlowLink_xu FlowLink_yu":
+        # interpolate
+        x_laydim = np.ma.getdata(data.variables[coords[0]][:], False)
+        y_laydim = np.ma.getdata(data.variables[coords[1]][:], False)
+        points_laydim = np.array([[x, y] for x, y in zip(x_laydim, y_laydim)])
+
         coords_request = str(data.variables[variable].coordinates).split()
-        x_wdim=np.ma.getdata(data.variables[coords_request[0]][:], False) 
-        y_wdim=np.ma.getdata(data.variables[coords_request[1]][:], False)
-        points_wdim=np.array([ [x, y] for x, y in zip(x_wdim, y_wdim)])
-        
-        bottom_depth_wdim = interp.griddata(points_laydim, bottom_depth,
-                                            points_wdim)
-        water_level_wdim= interp.griddata(points_laydim, waterlevel,
-                                            points_wdim)
-        
-        idx_bd= np.where(np.isnan(bottom_depth_wdim))
-        
-        for i in idx_bd: 
-            bottom_depth_wdim[i]= interp.griddata(points_laydim, bottom_depth,
-                                              points_wdim[i], method='nearest')
-            water_level_wdim[i]= interp.griddata(points_laydim, waterlevel,
-                                              points_wdim[i], method='nearest')
-
-    
-    waterdepth=[]
-    
-    if dimensions== 2:
-        if layer_dim == 'FlowLink_xu FlowLink_yu': 
+        x_wdim = np.ma.getdata(data.variables[coords_request[0]][:], False)
+        y_wdim = np.ma.getdata(data.variables[coords_request[1]][:], False)
+        points_wdim = np.array([[x, y] for x, y in zip(x_wdim, y_wdim)])
+
+        bottom_depth_wdim = interp.griddata(points_laydim, bottom_depth, points_wdim)
+        water_level_wdim = interp.griddata(points_laydim, waterlevel, points_wdim)
+
+        idx_bd = np.where(np.isnan(bottom_depth_wdim))
+
+        for i in idx_bd:
+            bottom_depth_wdim[i] = interp.griddata(
+                points_laydim, bottom_depth, points_wdim[i], method="nearest"
+            )
+            water_level_wdim[i] = interp.griddata(
+                points_laydim, waterlevel, points_wdim[i], method="nearest"
+            )
+
+    waterdepth = []
+
+    if dimensions == 2:
+        if layer_dim == "FlowLink_xu FlowLink_yu":
             z = [bottom_depth_wdim]
-            waterlevel=water_level_wdim
+            waterlevel = water_level_wdim
         else:
             z = [bottom_depth]
     else:
-        if layer_dim == 'FlowLink_xu FlowLink_yu': 
-            z = [bottom_depth_wdim*layer_percentages[layer_index]]
-            waterlevel=water_level_wdim
+        if layer_dim == "FlowLink_xu FlowLink_yu":
+            z = [bottom_depth_wdim * layer_percentages[layer_index]]
+            waterlevel = water_level_wdim
         else:
-            z = [bottom_depth*layer_percentages[layer_index]]
-    waterdepth=np.append(waterdepth, z)
+            z = [bottom_depth * layer_percentages[layer_index]]
+    waterdepth = np.append(waterdepth, z)
 
-    time= np.ma.getdata(data.variables['time'][time_index], False)*np.ones(len(x))
+    time = np.ma.getdata(data.variables["time"][time_index], False) * np.ones(len(x))
 
-    layer= np.array([ [x_i, y_i, d_i, w_i, v_i, t_i] for x_i, y_i, d_i, w_i, v_i, t_i in
-                     zip(x, y, waterdepth, waterlevel, v, time)]) 
-    layer_data = pd.DataFrame(layer, columns=['x', 'y', 'waterdepth','waterlevel', 'v', 'time'])
+    index = np.arange(0, len(time))
+    layer_data = xr.Dataset(
+        data_vars={
+            "x": (["index"], x),
+            "y": (["index"], y),
+            "waterdepth": (["index"], waterdepth),
+            "waterlevel": (["index"], waterlevel),
+            "v": (["index"], v),
+            "time": (["index"], time),
+        },
+        coords={"index": index},
+    )
+
+    if to_pandas:
+        layer_data = layer_data.to_pandas()
 
     return layer_data
 
 
-def create_points(x, y, waterdepth):
-    '''
-    Turns three coordinate inputs into a single output DataFrame of points. 
-    In any order the three inputs can consist of 3 points, 2 points and 1 array,
-    or 1 point and 2 arrays. The final output DataFrame will be the unique
-    combinations of the 3 inputs. 
-    
+def create_points(x, y, waterdepth, to_pandas=True):
+    """
+    Generate a Dataset of points from combinations of input coordinates.
+
+    This function accepts three inputs and combines them to generate a
+    Dataset of points. The inputs can be:
+    - 3 points
+    - 2 points and 1 array
+    - 1 point and 2 arrays
+    - 3 arrays (x and y must have the same size)
+
+    For 3 points or less, every combination will be in the output.
+    For 3 arrays, x and y are treated as coordinate pairs and combined
+    with each value from the waterdepth array.
+
     Parameters
     ----------
-    x: float, array or int 
-        x values to create points.
-    y: float, array or int
-        y values to create points.
-    waterdepth: float, array or int
-        waterdepth values to create points.
+    x : int, float, array-like
+        X values (longitude) for the points.
+    y : int, float, array-like
+        Y values (latitude) for the points.
+    waterdepth : int, float, array-like
+        Waterdepth values for the points.
+    to_pandas : bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
 
     Returns
     -------
-    points: DateFrame 
-        DataFrame with columns x, y and waterdepth points. 
-        
-    Example 
+    points : xr.Dataset or pd.DataFrame
+        A Dataset with columns 'x', 'y', and 'waterdepth' representing the generated points.
+
+    Example
     -------
-    If the inputs are 2 arrays:  and [3,4,5] and 1 point [6], the output 
-    will contain 6 array combinations of the 3 inputs as shown.
-    
-    x=np.array([1,2])
-    y=np.array([3,4,5])
-    waterdepth= 6
-    d3d.create_points(x,y,waterdepth)
-    
+    2 arrays and 1 point:
+    >>> x = np.array([1, 2])
+    >>> y = np.array([3, 4, 5])
+    >>> waterdepth = 6
+    >>> create_points(x, y, waterdepth)
+
        x    y    waterdepth
     0  1.0  3.0  6.0
     1  2.0  3.0  6.0
     2  1.0  4.0  6.0
     3  2.0  4.0  6.0
     4  1.0  5.0  6.0
-    5  2.0  5.0  6.0        
-    '''
-    
-    assert isinstance(x, (int, float, np.ndarray)), ('x must be a int, float'
-                                                     +' or array')
-    assert isinstance(y, (int, float, np.ndarray)), ('y must be a int, float'
-                                                     +' or array')
-    assert isinstance(waterdepth, (int, float, np.ndarray)), ('waterdepth must be a int, float'
-                                                     +' or array')
-    
-    directions = {0:{'name':  'x',
-                     'values': x},
-                  1:{'name':  'y',
-                     'values': y},
-                  2:{'name':  'waterdepth',
-                     'values': waterdepth}}
+    5  2.0  5.0  6.0
 
-    for i in directions:
-        try:
-            N=len(directions[i]['values'])
-        except:
-            directions[i]['values'] = np.array([directions[i]['values']]) 
-            N=len(directions[i]['values'])  
-        if N == 1 :
-            directions[i]['type']= 'point'      
-        elif N > 1 :
-            directions[i]['type']= 'array'
-        else:
-            raise Exception(f'length of direction {directions[i]["name"]} was'
-                            +'neagative or zero')
-    
-    # Check how many times point is in "types" 
-    types= [directions[i]['type'] for i in directions]
-    N_points = types.count('point')
-
-    if N_points >= 2:
-        lens = np.array([len(directions[d]['values'])  for d in directions])
-        max_len_idx = lens.argmax()
-        not_max_idxs= [i for i in directions.keys()]
-        
-        del not_max_idxs[max_len_idx]
-        
-        for not_max in not_max_idxs:     
-            N= len(directions[max_len_idx]['values'])
-            vals =np.ones(N)*directions[not_max]['values']
-            directions[not_max]['values'] = np.array(vals)
-                    
-        x_new = directions[0]['values']
-        y_new = directions[1]['values']
-        depth_new = directions[2]['values']
-            
-        request= np.array([ [x_i, y_i, depth_i] for x_i, y_i, depth_i in zip(x_new, 
-                                                             y_new, depth_new)]) 
-        points= pd.DataFrame(request, columns=[ 'x', 'y', 'waterdepth'])
-        
-    elif N_points == 1: 
-        # treat as plane
-        #find index of point 
-        idx_point = types.index('point')
-        max_idxs= [i for i in directions.keys()]
-        print(max_idxs)
-        del max_idxs[idx_point]
-        #find vectors 
-        XX, YY = np.meshgrid(directions[max_idxs[0]]['values'],
-                             directions[max_idxs[1]]['values'] )
-        N_X=np.shape(XX)[1]
-        N_Y=np.shape(YY)[0]
-        ZZ= np.ones((N_Y,N_X))*directions[idx_point]['values'] 
-     
-        request= np.array([ [x_i, y_i, z_i] for x_i, y_i, z_i in zip(XX.ravel(),
-                            YY.ravel() , ZZ.ravel())]) 
-        columns=[ directions[max_idxs[0]]['name'],  
-                 directions[max_idxs[1]]['name'],  directions[idx_point]['name']]
-        
-        points= pd.DataFrame(request, columns=columns)
-    else: 
-        raise Exception('Can provide at most two arrays')
-
-    return points 
-
-
-def variable_interpolation(data, variables, points='cells', edges= 'none'):
-    '''
-    Interpolate multiple variables from the Delft3D onto the same points. 
+    3 arrays (x and y must have the same length):
+    >>> x = np.array([1, 2, 3])
+    >>> y = np.array([4, 5, 6])
+    >>> waterdepth = np.array([1, 2])
+    >>> create_points(x, y, waterdepth)
+
+       x    y    waterdepth
+    0  1.0  4.0  1.0
+    1  2.0  5.0  1.0
+    2  3.0  6.0  1.0
+    3  1.0  4.0  2.0
+    4  2.0  5.0  2.0
+    5  4.0  6.0  2.0
+    """
+
+    # Check input types
+    inputs = {"x": x, "y": y, "waterdepth": waterdepth}
+    for name, value in inputs.items():
+        # Convert lists to numpy arrays
+        if isinstance(value, list):
+            value = np.array(value)
+            inputs[name] = value  # Update the value in the dictionary
+
+        # Check data type
+        if not isinstance(value, (int, float, np.ndarray, pd.Series, xr.DataArray)):
+            raise TypeError(
+                f"{name} must be an int, float, np.ndarray, pd.Series, or xr.DataArray. Got: {type(value)}"
+            )
+
+        # Check for empty arrays
+        if isinstance(value, (np.ndarray, pd.Series, xr.DataArray)) and len(value) == 0:
+            raise ValueError(f"{name} should not be an empty array")
+
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
+    x_array_like = not isinstance(x, (int, float))
+    y_array_like = not isinstance(y, (int, float))
+    waterdepth_array_like = not isinstance(waterdepth, (int, float))
+
+    if x_array_like and y_array_like and waterdepth_array_like:
+        # if all inputs are arrays, grid the coordinate and waterdepth
+        y_grid, waterdepth_grid = np.meshgrid(y, waterdepth)
+        y_grid = y_grid.ravel()
+        waterdepth_grid = waterdepth_grid.ravel()
+
+        x_grid, _ = np.meshgrid(x, waterdepth)
+        x_grid = x_grid.ravel()
+    else:
+        # if at least one input is a point, grid all inputs
+        x_grid, y_grid, waterdepth_grid = np.meshgrid(x, y, waterdepth)
+        x_grid = x_grid.ravel()
+        y_grid = y_grid.ravel()
+        waterdepth_grid = waterdepth_grid.ravel()
+
+    index = np.arange(0, len(x_grid))
+    points = xr.Dataset(
+        data_vars={
+            "x": (["index"], x_grid),
+            "y": (["index"], y_grid),
+            "waterdepth": (["index"], waterdepth_grid),
+        },
+        coords={"index": index},
+    )
+
+    if to_pandas:
+        points = points.to_pandas()
+
+    return points
+
+
+def variable_interpolation(
+    data,
+    variables,
+    points="cells",
+    edges="none",
+    x_max_lim=float("inf"),
+    x_min_lim=float("-inf"),
+    y_max_lim=float("inf"),
+    y_min_lim=float("-inf"),
+    to_pandas=True,
+):
+    """
+    Interpolate multiple variables from the Delft3D onto the same points.
 
     Parameters
     ----------
-    data: NetCDF4 object 
+    data: NetCDF4 object
        A NetCDF4 object that contains spatial data, e.g. velocity or shear
-       stress generated by running a Delft3D model.  
+       stress generated by running a Delft3D model.
     variables: array of strings
         Name of variables to interpolate, e.g. 'turkin1', 'ucx', 'ucy' and 'ucz'.
         The full list can be found using "data.variables.keys()" in the console.
-    points: string, DataFrame  
+    points: string, pd.DataFrame, or xr.Dataset
         The points to interpolate data onto.
           'cells'- interpolates all data onto the Delft3D cell coordinate system (Default)
-          'faces'- interpolates all dada onto the Delft3D face coordinate system 
-          DataFrame of x, y, and waterdepth coordinates - Interpolates data onto user 
+          'faces'- interpolates all dada onto the Delft3D face coordinate system
+          Dataset of x, y, and waterdepth coordinates - Interpolates data onto user
           povided points. Can be created with `create_points` function.
-    edges: sting: 'nearest'
-        If edges is set to 'nearest' the code will fill in nan values with nearest 
-        interpolation. Otherwise only linear interpolarion will be used. 
-  
+    edges: string: 'nearest'
+        If edges is set to 'nearest' the code will fill in nan values with nearest
+        interpolation. Otherwise only linear interpolarion will be used.
+    to_pandas : bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
+
     Returns
     -------
-    transformed_data: DataFrame  
-        Variables on specified grid points saved under the input variable names 
-        and the x, y, and waterdepth coordinates of those points. 
-    '''
-    
-    assert isinstance(points, (str, pd.DataFrame)),('points must be a string ' 
-                    +'or DataFrame')
-    if isinstance ( points, str):
-       assert any([points == 'cells', points=='faces']), ('points must be'
-                                                          +' cells or faces')
-    assert type(data)== netCDF4._netCDF4.Dataset, 'data must be nerCDF4 object'
+    transformed_data: pd.DataFrame or xr.Dataset
+        Variables on specified grid points saved under the input variable names
+        and the x, y, and waterdepth coordinates of those points.
+    """
+
+    if not isinstance(points, (str, pd.DataFrame, xr.Dataset)):
+        raise TypeError(
+            f"points must be a string, pd.DataFrame, or xr.Dataset. Got {type(points)}."
+        )
+
+    if isinstance(points, xr.Dataset):
+        points = points.to_pandas()
+
+    if isinstance(points, str):
+        if not (points == "cells" or points == "faces"):
+            raise ValueError(
+                f"If a string, points must be cells or faces. Got {points}"
+            )
+
+    if not isinstance(data, netCDF4._netCDF4.Dataset):
+        raise TypeError(f"data must be netCDF4 object. Got {type(data)}")
+
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
 
     data_raw = {}
     for var in variables:
-        var_data_df = get_all_data_points(data, var,time_index=-1)   
-        var_data_df=var_data_df.loc[:,~var_data_df.T.duplicated(keep='first')]        
-        data_raw[var] = var_data_df 
-    if type(points) == pd.DataFrame:  
-        print('points provided')
-    elif points=='faces':
-        points = data_raw['ucx'][['x','y','waterdepth']]
-    elif points=='cells':
-        points = data_raw['turkin1'][['x','y','waterdepth']]
-    
-    transformed_data= points.copy(deep=True)
-    
-    for var in variables :    
-        transformed_data[var] = interp.griddata(data_raw[var][['x','y','waterdepth']],
-                                        data_raw[var][var], points[['x','y','waterdepth']])
-        if edges == 'nearest' :
-            idx= np.where(np.isnan(transformed_data[var]))
-        
+        var_data_df = get_all_data_points(data, var, time_index=-1, to_pandas=True)
+        var_data_df["depth"] = var_data_df.waterdepth - var_data_df.waterlevel  # added
+        var_data_df = var_data_df.loc[:, ~var_data_df.T.duplicated(keep="first")]
+        var_data_df = var_data_df[var_data_df.x > x_min_lim]
+        var_data_df = var_data_df[var_data_df.x < x_max_lim]
+        var_data_df = var_data_df[var_data_df.y > y_min_lim]
+        var_data_df = var_data_df[var_data_df.y < y_max_lim]
+        data_raw[var] = var_data_df
+    if isinstance(points, pd.DataFrame):
+        print("points provided")
+    elif points == "faces":
+        points = data_raw["ucx"][["x", "y", "waterdepth"]]
+    elif points == "cells":
+        points = data_raw["turkin1"][["x", "y", "waterdepth"]]
+
+    transformed_data = points.copy(deep=True)
+
+    for var in variables:
+        transformed_data[var] = interp.griddata(
+            data_raw[var][["x", "y", "waterdepth"]],  # waterdepth to depth
+            data_raw[var][var],
+            points[["x", "y", "waterdepth"]],
+        )
+        if edges == "nearest":
+            idx = np.where(np.isnan(transformed_data[var]))
+
             if len(idx[0]):
-                for i in idx[0]: 
-                    transformed_data[var][i]= (interp
-                                              .griddata(data_raw[var][['x','y','waterdepth']], 
-                                                data_raw[var][var],
-                                                [points['x'][i],points['y'][i],
-                                                points['waterdepth'][i]], method='nearest'))
-     
+                for i in idx[0]:
+                    transformed_data[var][i] = interp.griddata(
+                        data_raw[var][["x", "y", "waterdepth"]],
+                        data_raw[var][var],
+                        [points["x"][i], points["y"][i], points["waterdepth"][i]],
+                        method="nearest",
+                    )
+
+    if not to_pandas:
+        transformed_data = transformed_data.to_dataset()
+
     return transformed_data
 
 
-def get_all_data_points(data, variable, time_index=-1):  
-    '''
-    Get data points for a passed variable for all layers at a specified time from 
-    the Delft3D NetCDF4 object by iterating over the `get_layer_data` function. 
+def get_all_data_points(data, variable, time_index=-1, to_pandas=True):
+    """
+    Get data points for a passed variable for all layers at a specified time from
+    the Delft3D NetCDF4 object by iterating over the `get_layer_data` function.
 
     Parameters
     ----------
-    data: Netcdf4 object 
+    data: Netcdf4 object
        A NetCDF4 object that contains spatial data, e.g. velocity or shear
-       stress, generated by running a Delft3D model.  
+       stress, generated by running a Delft3D model.
     variable: string
         Delft3D variable. The full list can be of variables can be
-        found using "data.variables.keys()" in the console. 
+        found using "data.variables.keys()" in the console.
     time_index: int
-        An integer to pull the time step from the dataset. 
+        An integer to pull the time step from the dataset.
         Default is last time step, found with the input -1.
-        
+    to_pandas : bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
+
     Returns
     -------
-    all_data: DataFrame 
+    all_data: xr.Dataset or pd.Dataframe
         Dataframe with columns x, y, waterdepth, waterlevel, variable, and time.
-        The waterdepth is measured from the water surface and the "waterlevel" is 
+        The waterdepth is measured from the water surface and the "waterlevel" is
         the water level diffrence in meters from the zero water level.
- 
-    '''  
-    
-    assert isinstance(time_index, int), 'time_index  must be a int'
-    assert type(data)== netCDF4._netCDF4.Dataset, 'data must be NetCDF4 object'
-    assert variable in data.variables.keys(), 'variable not recognized'
+
+    """
+
+    if not isinstance(time_index, int):
+        raise TypeError("time_index must be an int")
+
+    if not isinstance(data, netCDF4._netCDF4.Dataset):
+        raise TypeError("data must be NetCDF4 object")
+
+    if variable not in data.variables.keys():
+        raise ValueError("variable not recognized")
+
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
 
     max_time_index = len(data.variables[variable][:])
-    assert abs(time_index) <= max_time_index, (f'time_index must be less than'
-                                      +'the max time index {max_time_index}')
+    if abs(time_index) > max_time_index:
+        raise ValueError(
+            f"time_index must be less than the max time index {max_time_index}"
+        )
 
     if "mesh2d" in variable:
-        cords_to_layers= {'mesh2d_face_x mesh2d_face_y': {'name':'mesh2d_nLayers', 
-                                'coords':data.variables['mesh2d_layer_sigma'][:]},
-                       'mesh2d_edge_x mesh2d_edge_y': {'name':'mesh2d_nInterfaces', 
-                                'coords':data.variables['mesh2d_interface_sigma'][:]}}
+        cords_to_layers = {
+            "mesh2d_face_x mesh2d_face_y": {
+                "name": "mesh2d_nLayers",
+                "coords": data.variables["mesh2d_layer_sigma"][:],
+            },
+            "mesh2d_edge_x mesh2d_edge_y": {
+                "name": "mesh2d_nInterfaces",
+                "coords": data.variables["mesh2d_interface_sigma"][:],
+            },
+        }
+
+    elif str(data.variables[variable].coordinates) == "FlowElem_xcc FlowElem_ycc":
+        cords_to_layers = {
+            "FlowElem_xcc FlowElem_ycc": {
+                "name": "laydim",
+                "coords": data.variables["LayCoord_cc"][:],
+            },
+            "FlowLink_xu FlowLink_yu": {
+                "name": "wdim",
+                "coords": data.variables["LayCoord_w"][:],
+            },
+        }
+    else:
+        cords_to_layers = {
+            "FlowElem_xcc FlowElem_ycc LayCoord_cc LayCoord_cc": {
+                "name": "laydim",
+                "coords": data.variables["LayCoord_cc"][:],
+            },
+            "FlowLink_xu FlowLink_yu": {
+                "name": "wdim",
+                "coords": data.variables["LayCoord_w"][:],
+            },
+        }
+
+    layer_dim = str(data.variables[variable].coordinates)
+
+    try:
+        cord_sys = cords_to_layers[layer_dim]["coords"]
+    except:
+        raise Exception("Coordinates not recognized.")
     else:
-        cords_to_layers= {'FlowElem_xcc FlowElem_ycc':{'name':'laydim', 
-                                'coords':data.variables['LayCoord_cc'][:]},
-                           'FlowLink_xu FlowLink_yu': {'name':'wdim', 
-                                   'coords':data.variables['LayCoord_w'][:]}}
-
-    layer_dim =  str(data.variables[variable].coordinates)
-    
-    try:    
-        cord_sys= cords_to_layers[layer_dim]['coords']
-    except: 
-        raise Exception('Coordinates not recognized.')
-    else: 
-        layer_percentages= np.ma.getdata(cord_sys, False) 
-        
-    x_all=[]
-    y_all=[]
-    depth_all=[]
-    water_level_all=[]
-    v_all=[]
-    time_all=[]
-    
+        layer_percentages = np.ma.getdata(cord_sys, False)
+
+    x_all = []
+    y_all = []
+    depth_all = []
+    water_level_all = []
+    v_all = []
+    time_all = []
+
     layers = range(len(layer_percentages))
     for layer in layers:
-        layer_data= get_layer_data(data, variable, layer, time_index)
+        layer_data = get_layer_data(data, variable, layer, time_index)
 
-        x_all=np.append(x_all, layer_data.x)
-        y_all=np.append(y_all, layer_data.y)
-        depth_all=np.append(depth_all, layer_data.waterdepth)
-        water_level_all=np.append(water_level_all, layer_data.waterlevel)
-        v_all=np.append(v_all, layer_data.v)
-        time_all= np.append(time_all, layer_data.time)
-    
-    known_points = np.array([ [x, y, waterdepth, waterlevel, v, time] 
-                    for x, y, waterdepth, waterlevel, v, time in zip(x_all, y_all, 
-                                depth_all, water_level_all, v_all, time_all)])
-    
-    all_data= pd.DataFrame(known_points, columns=['x','y','waterdepth', 'waterlevel'
-                                                  ,f'{variable}', 'time'])
+        x_all = np.append(x_all, layer_data.x)
+        y_all = np.append(y_all, layer_data.y)
+        depth_all = np.append(depth_all, layer_data.waterdepth)
+        water_level_all = np.append(water_level_all, layer_data.waterlevel)
+        v_all = np.append(v_all, layer_data.v)
+        time_all = np.append(time_all, layer_data.time)
+
+    index = np.arange(0, len(time_all))
+    all_data = xr.Dataset(
+        data_vars={
+            "x": (["index"], x_all),
+            "y": (["index"], y_all),
+            "waterdepth": (["index"], depth_all),
+            "waterlevel": (["index"], water_level_all),
+            f"{variable}": (["index"], v_all),
+            "time": (["index"], time_all),
+        },
+        coords={"index": index},
+    )
 
-    return all_data
+    if to_pandas:
+        all_data = all_data.to_pandas()
 
+    return all_data
 
 
-def turbulent_intensity(data, points='cells', time_index= -1,
-                        intermediate_values = False ):
-    '''
-    Calculate the turbulent intensity percentage for a given data set for the 
+def turbulent_intensity(
+    data, points="cells", time_index=-1, intermediate_values=False, to_pandas=True
+):
+    """
+    Calculate the turbulent intensity percentage for a given data set for the
     specified points. Assumes variable names: ucx, ucy, ucz and turkin1.
 
     Parameters
     ----------
-    data: NetCDF4 object 
+    data: NetCDF4 object
        A NetCDF4 object that contains spatial data, e.g. velocity or shear
        stress, generated by running a Delft3D model.
-    points: string, DataFrame  
-        Points to interpolate data onto. 
+    points: string, pd.DataFrame, xr.Dataset
+        Points to interpolate data onto.
           'cells': interpolates all data onto velocity coordinate system (Default).
           'faces': interpolates all data onto the TKE coordinate system.
-          DataFrame of x, y, and z coordinates: Interpolates data onto user 
-          provided points. 
-    time_index: int 
+          DataFrame of x, y, and z coordinates: Interpolates data onto user
+          provided points.
+    time_index: int
         An integer to pull the time step from the dataset. Default is
-        late time step -1.  
+        late time step -1.
     intermediate_values: boolean (optional)
-        If false the function will return position and turbulent intensity values. 
+        If false the function will return position and turbulent intensity values.
         If true the function will return position(x,y,z) and values need to calculate
         turbulent intensity (ucx, uxy, uxz and turkin1) in a Dataframe.  Default False.
-   
+    to_pandas : bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
+
     Returns
     -------
-      TI_data: Dataframe
-        If intermediate_values is true all values are output. 
-        If intermediate_values is equal to false only turbulent_intesity and 
-        x, y, and z variables are output.  
-            x- position in the x direction 
-            y- position in the y direction 
+      TI_data: xr.Dataset or pd.DataFrame
+        If intermediate_values is true all values are output.
+        If intermediate_values is equal to false only turbulent_intesity and
+        x, y, and z variables are output.
+            x- position in the x direction
+            y- position in the y direction
             waterdepth- position in the vertical direction
             turbulet_intesity- turbulent kinetic energy divided by the root
                                 mean squared velocity
-            turkin1- turbulent kinetic energy 
-            ucx- velocity in the x direction 
-            ucy- velocity in the y direction 
-            ucz- velocity in the vertical direction 
-    '''
-    
-    assert isinstance(points, (str, pd.DataFrame)),('points must a string or'
-                                                    +' DataFrame')
-    if isinstance ( points, str):
-       assert any([points == 'cells', points=='faces']), ('points must be cells'
-                                                          +' or faces')
-    assert isinstance(time_index, int), 'time_index  must be a int'
-    max_time_index= data['time'].shape[0]-1 # to account for zero index
-    assert abs(time_index) <= max_time_index, (f'time_index must be less than'
-                  +'the absolute value of the max time index {max_time_index}')
-    assert type(data)== netCDF4._netCDF4.Dataset, 'data must be nerCDF4 object'
-    assert 'turkin1' in data.variables.keys(), ('Varaiable turkin1 not'
-                                                +' present in Data')
-    assert 'ucx' in data.variables.keys(),'Varaiable ucx 1 not present in Data'
-    assert 'ucy' in data.variables.keys(),'Varaiable ucy 1 not present in Data'
-    assert 'ucz' in data.variables.keys(),'Varaiable ucz 1 not present in Data'
+            turkin1- turbulent kinetic energy
+            ucx- velocity in the x direction
+            ucy- velocity in the y direction
+            ucz- velocity in the vertical direction
+    """
+
+    if not isinstance(points, (str, pd.DataFrame, xr.Dataset)):
+        raise TypeError("points must be a string, pd.DataFrame, xr.Dataset")
+
+    if isinstance(points, str):
+        if not (points == "cells" or points == "faces"):
+            raise ValueError("points must be cells or faces")
+
+    if not isinstance(time_index, int):
+        raise TypeError("time_index must be an int")
+
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
+    if isinstance(points, xr.Dataset):
+        points = points.to_pandas()
+
+    max_time_index = data["time"].shape[0] - 1  # to account for zero index
+    if abs(time_index) > max_time_index:
+        raise ValueError(
+            f"time_index must be less than the absolute value of the max time index {max_time_index}"
+        )
+
+    if not isinstance(data, netCDF4._netCDF4.Dataset):
+        raise TypeError("data must be netCDF4 object")
+
+    for variable in ["turkin1", "ucx", "ucy", "ucz"]:
+        if variable not in data.variables.keys():
+            raise ValueError(f"Variable {variable} not present in Data")
 
-    TI_vars= ['turkin1', 'ucx', 'ucy', 'ucz']
+    TI_vars = ["turkin1", "ucx", "ucy", "ucz"]
     TI_data_raw = {}
     for var in TI_vars:
-        var_data_df = get_all_data_points(data, var ,time_index)           
-        TI_data_raw[var] = var_data_df 
-    if type(points) == pd.DataFrame:  
-        print('points provided')
-    elif points=='faces':
-        points = TI_data_raw['turkin1'].drop(['waterlevel','turkin1'],axis=1)
-    elif points=='cells':
-        points = TI_data_raw['ucx'].drop(['waterlevel','ucx'],axis=1)
-       
+        var_data_df = get_all_data_points(data, var, time_index)
+        TI_data_raw[var] = var_data_df
+    if type(points) == pd.DataFrame:
+        print("points provided")
+    elif points == "faces":
+        points = TI_data_raw["turkin1"].drop(["waterlevel", "turkin1"], axis=1)
+    elif points == "cells":
+        points = TI_data_raw["ucx"].drop(["waterlevel", "ucx"], axis=1)
+
     TI_data = points.copy(deep=True)
 
-    for var in TI_vars:    
-        TI_data[var] = interp.griddata(TI_data_raw[var][['x','y','waterdepth']],
-                                TI_data_raw[var][var], points[['x','y','waterdepth']])
-        idx= np.where(np.isnan(TI_data[var]))
-        
+    for var in TI_vars:
+        TI_data[var] = interp.griddata(
+            TI_data_raw[var][["x", "y", "waterdepth"]],
+            TI_data_raw[var][var],
+            points[["x", "y", "waterdepth"]],
+        )
+        idx = np.where(np.isnan(TI_data[var]))
+
         if len(idx[0]):
-            for i in idx[0]: 
-                TI_data[var][i]= interp.griddata(TI_data_raw[var][['x','y','waterdepth']], 
-                                TI_data_raw[var][var],
-                                [points['x'][i],points['y'][i], points['waterdepth'][i]],
-                                method='nearest')
-
-    u_mag=unorm(np.array(TI_data['ucx']),np.array(TI_data['ucy']), 
-                np.array(TI_data['ucz']))
-    
-    neg_index=np.where( TI_data['turkin1']<0)
-    zero_bool= np.isclose( TI_data['turkin1'][ TI_data['turkin1']<0].array, 
-               np.zeros(len( TI_data['turkin1'][TI_data['turkin1']<0].array)),
-               atol=1.0e-4)
-    zero_ind= neg_index[0][zero_bool]
-    non_zero_ind= neg_index[0][~zero_bool]
-    TI_data.loc[zero_ind,'turkin1']=np.zeros(len(zero_ind))
-    TI_data.loc[non_zero_ind,'turkin1']=[np.nan]*len(non_zero_ind)
-        
-    TI_data['turbulent_intensity']= np.sqrt(2/3*TI_data['turkin1'])/u_mag * 100 #%
-    
+            for i in idx[0]:
+                TI_data[var][i] = interp.griddata(
+                    TI_data_raw[var][["x", "y", "waterdepth"]],
+                    TI_data_raw[var][var],
+                    [points["x"][i], points["y"][i], points["waterdepth"][i]],
+                    method="nearest",
+                )
+
+    u_mag = unorm(
+        np.array(TI_data["ucx"]), np.array(TI_data["ucy"]), np.array(TI_data["ucz"])
+    )
+
+    neg_index = np.where(TI_data["turkin1"] < 0)
+    zero_bool = np.isclose(
+        TI_data["turkin1"][TI_data["turkin1"] < 0].array,
+        np.zeros(len(TI_data["turkin1"][TI_data["turkin1"] < 0].array)),
+        atol=1.0e-4,
+    )
+    zero_ind = neg_index[0][zero_bool]
+    non_zero_ind = neg_index[0][~zero_bool]
+    TI_data.loc[zero_ind, "turkin1"] = np.zeros(len(zero_ind))
+    TI_data.loc[non_zero_ind, "turkin1"] = [np.nan] * len(non_zero_ind)
+
+    TI_data["turbulent_intensity"] = (
+        np.sqrt(2 / 3 * TI_data["turkin1"]) / u_mag * 100
+    )  # %
+
     if intermediate_values == False:
-        TI_data= TI_data.drop(TI_vars, axis = 1)
-        
+        TI_data = TI_data.drop(TI_vars, axis=1)
+
+    if not to_pandas:
+        TI_data = TI_data.to_dataset()
+
     return TI_data
```

### Comparing `mhkit-0.7.0/mhkit/river/performance.py` & `mhkit-0.8.0/mhkit/river/performance.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,111 +1,123 @@
 import numpy as np
 
+
 def circular(diameter):
     """
-    Calculates the equivalent diameter and projected capture area of a 
+    Calculates the equivalent diameter and projected capture area of a
     circular turbine
-    
+
     Parameters
     ------------
     diameter : int/float
         Turbine diameter [m]
-        
+
     Returns
     ---------
     equivalent_diameter : float
        Equivalent diameter [m]
     projected_capture_area : float
         Projected capture area [m^2]
     """
-    assert isinstance(diameter, (int,float)), 'diameter must be of type int or float'
-    
+    if not isinstance(diameter, (int, float)):
+        raise TypeError(f"diameter must be of type int or float. Got: {type(diameter)}")
+
     equivalent_diameter = diameter
-    projected_capture_area = (1/4)*np.pi*(equivalent_diameter**2) 
-    
+    projected_capture_area = (1 / 4) * np.pi * (equivalent_diameter**2)
+
     return equivalent_diameter, projected_capture_area
 
+
 def ducted(duct_diameter):
     """
     Calculates the equivalent diameter and projected capture area of a
     ducted turbine
-    
+
     Parameters
     ------------
     duct_diameter : int/float
         Duct diameter [m]
-        
+
     Returns
     ---------
     equivalent_diameter : float
        Equivalent diameter [m]
     projected_capture_area : float
         Projected capture area [m^2]
     """
-    assert isinstance(duct_diameter, (int,float)), 'duct_diameter must be of type int or float'
-    
+    if not isinstance(duct_diameter, (int, float)):
+        raise TypeError(
+            f"duct_diameter must be of type int or float. Got: {type(duct_diameter)}"
+        )
+
     equivalent_diameter = duct_diameter
-    projected_capture_area = (1/4)*np.pi*(equivalent_diameter**2) 
+    projected_capture_area = (1 / 4) * np.pi * (equivalent_diameter**2)
 
     return equivalent_diameter, projected_capture_area
 
+
 def rectangular(h, w):
     """
-    Calculates the equivalent diameter and projected capture area of a 
+    Calculates the equivalent diameter and projected capture area of a
     retangular turbine
-    
+
     Parameters
     ------------
     h : int/float
         Turbine height [m]
     w : int/float
         Turbine width [m]
-        
+
     Returns
     ---------
     equivalent_diameter : float
        Equivalent diameter [m]
     projected_capture_area : float
         Projected capture area [m^2]
     """
-    assert isinstance(h, (int,float)), 'h must be of type int or float'
-    assert isinstance(w, (int,float)), 'w must be of type int or float'
-    
-    equivalent_diameter = np.sqrt(4.*h*w / np.pi) 
-    projected_capture_area = h*w
+    if not isinstance(h, (int, float)):
+        raise TypeError(f"h must be of type int or float. Got: {type(h)}")
+    if not isinstance(w, (int, float)):
+        raise TypeError(f"w must be of type int or float. Got: {type(w)}")
+
+    equivalent_diameter = np.sqrt(4.0 * h * w / np.pi)
+    projected_capture_area = h * w
 
     return equivalent_diameter, projected_capture_area
 
+
 def multiple_circular(diameters):
     """
-    Calculates the equivalent diameter and projected capture area of a 
+    Calculates the equivalent diameter and projected capture area of a
     multiple circular turbine
-    
+
     Parameters
     ------------
-    diameters: list 
+    diameters: list
         List of device diameters [m]
-        
+
     Returns
     ---------
     equivalent_diameter : float
        Equivalent diameter [m]
     projected_capture_area : float
         Projected capture area [m^2]
     """
-    assert isinstance(diameters, list), 'diameters must be of type list'
-    
+    if not isinstance(diameters, list):
+        raise TypeError(f"diameters must be of type list. Got: {type(diameters)}")
+
     diameters_squared = [x**2 for x in diameters]
     equivalent_diameter = np.sqrt(sum(diameters_squared))
-    projected_capture_area = 0.25*np.pi*sum(diameters_squared)
+    projected_capture_area = 0.25 * np.pi * sum(diameters_squared)
 
     return equivalent_diameter, projected_capture_area
 
-def tip_speed_ratio(rotor_speed,rotor_diameter,inflow_speed):
-    '''
+
+def tip_speed_ratio(rotor_speed, rotor_diameter, inflow_speed):
+    """
     Function used to calculate the tip speed ratio (TSR) of a MEC device with rotor
 
     Parameters
     -----------
     rotor_speed : numpy array
         Rotor speed [revolutions per second]
     rotor_diameter : float/int
@@ -113,32 +125,39 @@
     inflow_speed : numpy array
         Velocity of inflow condition [m/s]
 
     Returns
     --------
     TSR : numpy array
         Calculated tip speed ratio (TSR)
-    '''
-    
-    try: rotor_speed = np.asarray(rotor_speed)
-    except: 'rotor_speed must be of type np.ndarray'        
-    try: inflow_speed = np.asarray(inflow_speed)
-    except: 'inflow_speed must be of type np.ndarray'
-    
-    assert isinstance(rotor_diameter, (float,int)), 'rotor diameter must be of type int or float'
+    """
 
+    try:
+        rotor_speed = np.asarray(rotor_speed)
+    except:
+        "rotor_speed must be of type np.ndarray"
+    try:
+        inflow_speed = np.asarray(inflow_speed)
+    except:
+        "inflow_speed must be of type np.ndarray"
+
+    if not isinstance(rotor_diameter, (float, int)):
+        raise TypeError(
+            f"rotor_diameter must be of type int or float. Got: {type(rotor_diameter)}"
+        )
 
-    rotor_velocity = rotor_speed * np.pi*rotor_diameter
+    rotor_velocity = rotor_speed * np.pi * rotor_diameter
 
     TSR = rotor_velocity / inflow_speed
 
     return TSR
 
-def power_coefficient(power,inflow_speed,capture_area,rho):
-    '''
+
+def power_coefficient(power, inflow_speed, capture_area, rho):
+    """
     Function that calculates the power coefficient of MEC device
 
     Parameters
     -----------
     power : numpy array
         Power output signal of device after losses [W]
     inflow_speed : numpy array
@@ -148,24 +167,31 @@
     rho : float/int
         Density of environment [kg/m^3]
 
     Returns
     --------
     Cp : numpy array
         Power coefficient of device [-]
-    '''
-    
-    try: power = np.asarray(power)
-    except: 'power must be of type np.ndarray'
-    try: inflow_speed = np.asarray(inflow_speed)
-    except: 'inflow_speed must be of type np.ndarray'
-    
-    assert isinstance(capture_area, (float,int)), 'capture_area must be of type int or float'
-    assert isinstance(rho, (float,int)), 'rho must be of type int or float'
+    """
+
+    try:
+        power = np.asarray(power)
+    except:
+        "power must be of type np.ndarray"
+    try:
+        inflow_speed = np.asarray(inflow_speed)
+    except:
+        "inflow_speed must be of type np.ndarray"
+
+    if not isinstance(capture_area, (float, int)):
+        raise TypeError(
+            f"capture_area must be of type int or float. Got: {type(capture_area)}"
+        )
+    if not isinstance(rho, (float, int)):
+        raise TypeError(f"rho must be of type int or float. Got: {type(rho)}")
 
     # Predicted power from inflow
-    power_in = (0.5 * rho * capture_area * inflow_speed**3)
+    power_in = 0.5 * rho * capture_area * inflow_speed**3
 
-    Cp = power / power_in 
+    Cp = power / power_in
 
     return Cp
-
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/base.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/base.py`

 * *Files 7% similar despite different names*

```diff
@@ -3,52 +3,54 @@
 from xarray.testing import assert_allclose as _assert_allclose
 from os.path import abspath, dirname, join, normpath, relpath
 import numpy as np
 
 
 def rfnm(filename):
     testdir = dirname(abspath(__file__))
-    datadir = normpath(join(testdir, relpath(
-        '../../../examples/data/dolfyn/test_data/')))
-    return datadir + '/' + filename
+    datadir = normpath(
+        join(testdir, relpath("../../../examples/data/dolfyn/test_data/"))
+    )
+    return datadir + "/" + filename
 
 
 def exdt(filename):
     testdir = dirname(abspath(__file__))
-    exdir = normpath(join(testdir, relpath('../../../examples/data/dolfyn/')))
-    return exdir + '/' + filename
+    exdir = normpath(join(testdir, relpath("../../../examples/data/dolfyn/")))
+    return exdir + "/" + filename
 
 
 def assert_allclose(dat0, dat1, *args, **kwargs):
     # For problematic time check
     names = []
     for v in dat0.variables:
         if np.issubdtype(dat0[v].dtype, np.datetime64):
             dat0[v] = time.dt642epoch(dat0[v])
             dat1[v] = time.dt642epoch(dat1[v])
             names.append(v)
     # Check coords and data_vars
     _assert_allclose(dat0, dat1, *args, **kwargs)
     # Check attributes
     for nm in dat0.attrs:
-        assert dat0.attrs[nm] == dat1.attrs[nm], "The " + \
-            nm + " attribute does not match."
+        assert dat0.attrs[nm] == dat1.attrs[nm], (
+            "The " + nm + " attribute does not match."
+        )
     # If test debugging
     for v in names:
         dat0[v] = time.epoch2dt64(dat0[v])
         dat1[v] = time.epoch2dt64(dat1[v])
 
 
 def load_netcdf(name, *args, **kwargs):
     return io.load(rfnm(name), *args, **kwargs)
 
 
 def save_netcdf(data, name, *args, **kwargs):
     io.save(data, rfnm(name), *args, **kwargs)
 
 
-def load_matlab(name,  *args, **kwargs):
+def load_matlab(name, *args, **kwargs):
     return io.load_mat(rfnm(name), *args, **kwargs)
 
 
-def save_matlab(data, name,  *args, **kwargs):
+def save_matlab(data, name, *args, **kwargs):
     io.save_mat(data, rfnm(name), *args, **kwargs)
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_analysis.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_analysis.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,13 @@
 from . import test_read_adp as tr, test_read_adv as tv
-from mhkit.tests.dolfyn.base import load_netcdf as load, save_netcdf as save, assert_allclose
+from mhkit.tests.dolfyn.base import (
+    load_netcdf as load,
+    save_netcdf as save,
+    assert_allclose,
+)
 from mhkit.dolfyn import VelBinner, read_example
 import mhkit.dolfyn.adv.api as avm
 import mhkit.dolfyn.adp.api as apm
 from xarray.testing import assert_identical
 import unittest
 import pytest
 import numpy as np
@@ -11,137 +15,187 @@
 make_data = False
 
 
 class analysis_testcase(unittest.TestCase):
     @classmethod
     def setUpClass(self):
         self.adv1 = tv.dat.copy(deep=True)
-        self.adv2 = read_example('vector_burst_mode01.VEC', nens=90)
+        self.adv2 = read_example("vector_burst_mode01.VEC", nens=90)
         self.adv_tool = VelBinner(n_bin=self.adv1.fs, fs=self.adv1.fs)
 
         self.adp = tr.dat_sig.copy(deep=True)
         with pytest.warns(UserWarning):
-            self.adp_tool = VelBinner(n_bin=self.adp.fs*20,
-                                      fs=self.adp.fs,
-                                      n_fft=self.adp.fs*40)
+            self.adp_tool = VelBinner(
+                n_bin=self.adp.fs * 20, fs=self.adp.fs, n_fft=self.adp.fs * 40
+            )
 
     @classmethod
     def tearDownClass(self):
         pass
 
     def test_do_func(self):
         ds_vec = self.adv_tool.bin_average(self.adv1)
         ds_vec = self.adv_tool.bin_variance(self.adv1, out_ds=ds_vec)
 
         # test non-integer bin sizes
-        mean_test = self.adv_tool.mean(self.adv1['vel'].values, n_bin=ds_vec.fs*1.01)
+        mean_test = self.adv_tool.mean(self.adv1["vel"].values, n_bin=ds_vec.fs * 1.01)
 
         ds_sig = self.adp_tool.bin_average(self.adp)
         ds_sig = self.adp_tool.bin_variance(self.adp, out_ds=ds_sig)
 
         if make_data:
-            save(ds_vec, 'vector_data01_avg.nc')
-            save(ds_sig, 'BenchFile01_avg.nc')
+            save(ds_vec, "vector_data01_avg.nc")
+            save(ds_sig, "BenchFile01_avg.nc")
             return
 
-        assert np.sum(mean_test-ds_vec.vel.values) == 0, "Mean test failed"
-        assert_allclose(ds_vec, load('vector_data01_avg.nc'), atol=1e-6)
-        assert_allclose(ds_sig, load('BenchFile01_avg.nc'), atol=1e-6)
+        assert np.sum(mean_test - ds_vec.vel.values) == 0, "Mean test failed"
+        assert_allclose(ds_vec, load("vector_data01_avg.nc"), atol=1e-6)
+        assert_allclose(ds_sig, load("BenchFile01_avg.nc"), atol=1e-6)
 
     def test_calc_func(self):
         c = self.adv_tool
         c2 = self.adp_tool
 
         test_ds = type(self.adv1)()
         test_ds_adp = type(self.adp)()
 
-        test_ds['acov'] = c.autocovariance(self.adv1.vel)
-        test_ds['tke_vec_detrend'] = c.turbulent_kinetic_energy(
-            self.adv1.vel, detrend=True)
-        test_ds['tke_vec_demean'] = c.turbulent_kinetic_energy(
-            self.adv1.vel, detrend=False)
-        test_ds['psd'] = c.power_spectral_density(
-            self.adv1.vel, freq_units='Hz')
+        test_ds["acov"] = c.autocovariance(self.adv1.vel)
+        test_ds["tke_vec_detrend"] = c.turbulent_kinetic_energy(
+            self.adv1.vel, detrend=True
+        )
+        test_ds["tke_vec_demean"] = c.turbulent_kinetic_energy(
+            self.adv1.vel, detrend=False
+        )
+        test_ds["psd"] = c.power_spectral_density(self.adv1.vel, freq_units="Hz")
 
         # Test ADCP single vector spectra, cross-spectra to test radians code
-        test_ds_adp['psd_b5'] = c2.power_spectral_density(
-            self.adp.vel_b5.isel(range_b5=5), freq_units='rad', window='hamm')
-        test_ds_adp['tke_b5'] = c2.turbulent_kinetic_energy(self.adp.vel_b5)
+        test_ds_adp["psd_b5"] = c2.power_spectral_density(
+            self.adp.vel_b5.isel(range_b5=5), freq_units="rad", window="hamm"
+        )
+        test_ds_adp["tke_b5"] = c2.turbulent_kinetic_energy(self.adp.vel_b5)
 
         if make_data:
-            save(test_ds, 'vector_data01_func.nc')
-            save(test_ds_adp, 'BenchFile01_func.nc')
+            save(test_ds, "vector_data01_func.nc")
+            save(test_ds_adp, "BenchFile01_func.nc")
             return
 
-        assert_allclose(test_ds, load('vector_data01_func.nc'), atol=1e-6)
-        assert_allclose(test_ds_adp, load('BenchFile01_func.nc'), atol=1e-6)
+        assert_allclose(test_ds, load("vector_data01_func.nc"), atol=1e-6)
+        assert_allclose(test_ds_adp, load("BenchFile01_func.nc"), atol=1e-6)
 
     def test_fft_freq(self):
-        f = self.adv_tool._fft_freq(units='Hz')
-        omega = self.adv_tool._fft_freq(units='rad/s')
+        f = self.adv_tool._fft_freq(units="Hz")
+        omega = self.adv_tool._fft_freq(units="rad/s")
 
-        np.testing.assert_equal(f, np.arange(1, 17, 1, dtype='float'))
-        np.testing.assert_equal(omega, np.arange(
-            1, 17, 1, dtype='float')*(2*np.pi))
+        np.testing.assert_equal(f, np.arange(1, 17, 1, dtype="float"))
+        np.testing.assert_equal(omega, np.arange(1, 17, 1, dtype="float") * (2 * np.pi))
 
     def test_adv_turbulence(self):
         dat = tv.dat.copy(deep=True)
         bnr = avm.ADVBinner(n_bin=20.0, fs=dat.fs)
         tdat = bnr(dat)
-        acov = bnr.autocovariance(dat.vel)
+        acov = bnr.autocovariance(dat["vel"])
 
-        assert_identical(tdat, avm.turbulence_statistics(
-            dat, n_bin=20.0, fs=dat.fs))
+        assert_identical(tdat, avm.turbulence_statistics(dat, n_bin=20.0, fs=dat.fs))
 
-        tdat['stress_detrend'] = bnr.reynolds_stress(dat.vel)
-        tdat['stress_demean'] = bnr.reynolds_stress(dat.vel, detrend=False)
-        tdat['csd'] = bnr.cross_spectral_density(
-            dat.vel, freq_units='rad', window='hamm', n_fft_coh=10)
-        tdat['LT83'] = bnr.dissipation_rate_LT83(tdat.psd, tdat.velds.U_mag)
-        tdat['SF'] = bnr.dissipation_rate_SF(dat.vel[0], tdat.velds.U_mag)
-        tdat['TE01'] = bnr.dissipation_rate_TE01(dat, tdat)
-        tdat['L'] = bnr.integral_length_scales(acov, tdat.velds.U_mag)
+        tdat["stress_detrend"] = bnr.reynolds_stress(dat["vel"])
+        tdat["stress_demean"] = bnr.reynolds_stress(dat["vel"], detrend=False)
+        tdat["csd"] = bnr.cross_spectral_density(
+            dat["vel"], freq_units="rad", window="hamm", n_fft_coh=10
+        )
+        tdat["LT83"] = bnr.dissipation_rate_LT83(tdat["psd"], tdat.velds.U_mag)
+        tdat["noise"] = bnr.doppler_noise_level(tdat["psd"], pct_fN=0.8)
+        tdat["LT83_noise"] = bnr.dissipation_rate_LT83(
+            tdat["psd"], tdat.velds.U_mag, noise=tdat["noise"]
+        )
+        tdat["SF"] = bnr.dissipation_rate_SF(dat["vel"][0], tdat.velds.U_mag)
+        tdat["TE01"] = bnr.dissipation_rate_TE01(dat, tdat)
+        tdat["L"] = bnr.integral_length_scales(acov, tdat.velds.U_mag)
         slope_check = bnr.check_turbulence_cascade_slope(
-            tdat['psd'][-1].mean('time'), freq_range=[10, 100])
+            tdat["psd"][-1].mean("time"), freq_range=[10, 100]
+        )
+        tdat["psd_noise"] = bnr.power_spectral_density(
+            dat["vel"], freq_units="rad", noise=[0.06, 0.04, 0.01]
+        )
 
         if make_data:
-            save(tdat, 'vector_data01_bin.nc')
+            save(tdat, "vector_data01_bin.nc")
             return
 
         assert np.round(slope_check[0].values, 4), 0.1713
-        assert_allclose(tdat, load('vector_data01_bin.nc'), atol=1e-6)
-
+        assert_allclose(tdat, load("vector_data01_bin.nc"), atol=1e-6)
 
     def test_adcp_turbulence(self):
-        dat = tr.dat_sig_i.copy(deep=True)
-        bnr = apm.ADPBinner(n_bin=20.0, fs=dat.fs, diff_style='centered')
+        dat = tr.dat_sig_tide.copy(deep=True)
+        dat.velds.rotate2("earth")
+        dat.attrs["principal_heading"] = apm.calc_principal_heading(
+            dat.vel.mean("range")
+        )
+        bnr = apm.ADPBinner(n_bin=20.0, fs=dat.fs, diff_style="centered")
         tdat = bnr.bin_average(dat)
-        tdat['dudz'] = bnr.dudz(tdat.vel)
-        tdat['dvdz'] = bnr.dvdz(tdat.vel)
-        tdat['dwdz'] = bnr.dwdz(tdat.vel)
-        tdat['tau2'] = bnr.shear_squared(tdat.vel)
-        tdat['psd'] = bnr.power_spectral_density(dat['vel'].isel(
-            dir=2, range=len(dat.range)//2), freq_units='Hz')
-        tdat['noise'] = bnr.doppler_noise_level(tdat['psd'], pct_fN=0.8)
-        tdat['stress_vec4'] = bnr.reynolds_stress_4beam(
-            dat, noise=tdat['noise'], orientation='up', beam_angle=25)
-        tdat['tke_vec5'], tdat['stress_vec5'] = bnr.stress_tensor_5beam(
-            dat, noise=tdat['noise'], orientation='up', beam_angle=25, tke_only=False)
-        tdat['tke'] = bnr.total_turbulent_kinetic_energy(
-            dat, noise=tdat['noise'], orientation='up', beam_angle=25)
+
+        tdat["dudz"] = bnr.dudz(tdat["vel"])
+        tdat["dvdz"] = bnr.dvdz(tdat["vel"])
+        tdat["dwdz"] = bnr.dwdz(tdat["vel"])
+        tdat["tau2"] = bnr.shear_squared(tdat["vel"])
+        tdat["I"] = tdat.velds.I
+        tdat["ti"] = bnr.turbulence_intensity(dat.velds.U_mag, detrend=False)
+        dat.velds.rotate2("beam")
+
+        tdat["psd"] = bnr.power_spectral_density(
+            dat["vel"].isel(dir=2, range=len(dat.range) // 2), freq_units="Hz"
+        )
+        tdat["noise"] = bnr.doppler_noise_level(tdat["psd"], pct_fN=0.8)
+        tdat["stress_vec4"] = bnr.reynolds_stress_4beam(
+            dat, noise=tdat["noise"], orientation="up", beam_angle=25
+        )
+        tdat["tke_vec5"], tdat["stress_vec5"] = bnr.stress_tensor_5beam(
+            dat, noise=tdat["noise"], orientation="up", beam_angle=25, tke_only=False
+        )
+        tdat["tke"] = bnr.total_turbulent_kinetic_energy(
+            dat, noise=tdat["noise"], orientation="up", beam_angle=25
+        )
+        tdat["ti_noise"] = bnr.turbulence_intensity(
+            dat.velds.U_mag, detrend=False, noise=tdat["noise"]
+        )
         # This is "negative" for this code check
-        tdat['wpwp'] = bnr.turbulent_kinetic_energy(dat['vel_b5'], noise=tdat['noise'])
-        tdat['dissipation_rate_LT83'] = bnr.dissipation_rate_LT83(
-            tdat['psd'], tdat.velds.U_mag.isel(range=len(dat.range)//2), freq_range=[0.2, 0.4])
-        tdat['dissipation_rate_SF'], tdat['noise_SF'], tdat['D_SF'] = bnr.dissipation_rate_SF(
-            dat.vel.isel(dir=2), r_range=[1, 5])
-        tdat['friction_vel'] = bnr.friction_velocity(
-            tdat, upwp_=tdat['stress_vec5'].sel(tau='upwp_'), z_inds=slice(1, 5), H=50)
+        tdat["wpwp"] = bnr.turbulent_kinetic_energy(dat["vel_b5"], noise=tdat["noise"])
+        tdat["dissipation_rate_LT83"] = bnr.dissipation_rate_LT83(
+            tdat["psd"],
+            tdat.velds.U_mag.isel(range=len(dat.range) // 2),
+            freq_range=[0.2, 0.4],
+        )
+        tdat["dissipation_rate_LT83_noise"] = bnr.dissipation_rate_LT83(
+            tdat["psd"],
+            tdat.velds.U_mag.isel(range=len(dat.range) // 2),
+            freq_range=[0.2, 0.4],
+            noise=tdat["noise"],
+        )
+        (
+            tdat["dissipation_rate_SF"],
+            tdat["noise_SF"],
+            tdat["D_SF"],
+        ) = bnr.dissipation_rate_SF(dat.vel.isel(dir=2), r_range=[1, 5])
+        tdat["friction_vel"] = bnr.friction_velocity(
+            tdat, upwp_=tdat["stress_vec5"].sel(tau="upwp_"), z_inds=slice(1, 5), H=50
+        )
         slope_check = bnr.check_turbulence_cascade_slope(
-            tdat['psd'].mean('time'), freq_range=[0.4, 4])
+            tdat["psd"].mean("time"), freq_range=[0.4, 4]
+        )
+        tdat["psd_noise"] = bnr.power_spectral_density(
+            dat["vel"].isel(dir=2, range=len(dat.range) // 2),
+            freq_units="Hz",
+            noise=0.01,
+        )
 
         if make_data:
-            save(tdat, 'Sig1000_IMU_bin.nc')
+            save(tdat, "Sig1000_tidal_bin.nc")
             return
 
+        with pytest.raises(Exception):
+            bnr.calc_psd(dat["vel"], freq_units="Hz", noise=0.01)
+
+        with pytest.raises(Exception):
+            bnr.calc_psd(dat["vel"][0], freq_units="Hz", noise=0.01)
+
         assert np.round(slope_check[0].values, 4), -1.0682
-        assert_allclose(tdat, load('Sig1000_IMU_bin.nc'), atol=1e-6)
+
+        assert_allclose(tdat, load("Sig1000_tidal_bin.nc"), atol=1e-6)
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_api.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_api.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 from .base import load_netcdf as load, rfnm
 import unittest
 
 
 make_data = False
-vec = load('vector_data01.nc')
-sig = load('BenchFile01.nc')
-rdi = load('RDI_test01.nc')
+vec = load("vector_data01.nc")
+sig = load("BenchFile01.nc")
+rdi = load("RDI_test01.nc")
 
 
 class api_testcase(unittest.TestCase):
     def test_repr(self):
         _str = []
-        for dat, fnm in [(vec, rfnm('vector_data01.repr.txt')),
-                         (sig, rfnm('BenchFile01.repr.txt')),
-                         (rdi, rfnm('RDI_test01.repr.txt')), ]:
+        for dat, fnm in [
+            (vec, rfnm("vector_data01.repr.txt")),
+            (sig, rfnm("BenchFile01.repr.txt")),
+            (rdi, rfnm("RDI_test01.repr.txt")),
+        ]:
             _str = dat.velds.__repr__()
             if make_data:
-                with open(fnm, 'w') as fl:
+                with open(fnm, "w") as fl:
                     fl.write(_str)
             else:
-                with open(fnm, 'r') as fl:
+                with open(fnm, "r") as fl:
                     test_str = fl.read()
                 assert test_str == _str
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_clean.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_clean.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,109 +11,107 @@
 
 class clean_testcase(unittest.TestCase):
     def test_GN2002(self):
         td = tv.dat.copy(deep=True)
         td_imu = tv.dat_imu.copy(deep=True)
 
         mask = avm.clean.GN2002(td.vel, npt=20)
-        td['vel'] = avm.clean.clean_fill(
-            td.vel, mask, method='cubic', maxgap=6)
-        td['vel_clean_1D'] = avm.clean.fill_nan_ensemble_mean(
-            td.vel[0], mask[0], fs=1, window=45)
-        td['vel_clean_2D'] = avm.clean.fill_nan_ensemble_mean(
-            td.vel, mask, fs=1, window=45)
+        td["vel"] = avm.clean.clean_fill(td.vel, mask, method="cubic", maxgap=6)
+        td["vel_clean_1D"] = avm.clean.fill_nan_ensemble_mean(
+            td.vel[0], mask[0], fs=1, window=45
+        )
+        td["vel_clean_2D"] = avm.clean.fill_nan_ensemble_mean(
+            td.vel, mask, fs=1, window=45
+        )
 
         mask = avm.clean.GN2002(td_imu.vel, npt=20)
-        td_imu['vel'] = avm.clean.clean_fill(
-            td_imu.vel, mask, method='cubic', maxgap=6)
+        td_imu["vel"] = avm.clean.clean_fill(td_imu.vel, mask, method="cubic", maxgap=6)
 
         if make_data:
-            save(td, 'vector_data01_GN.nc')
-            save(td_imu, 'vector_data_imu01_GN.nc')
+            save(td, "vector_data01_GN.nc")
+            save(td_imu, "vector_data_imu01_GN.nc")
             return
 
-        assert_allclose(td, load('vector_data01_GN.nc'), atol=1e-6)
-        assert_allclose(td_imu, load('vector_data_imu01_GN.nc'), atol=1e-6)
+        assert_allclose(td, load("vector_data01_GN.nc"), atol=1e-6)
+        assert_allclose(td_imu, load("vector_data_imu01_GN.nc"), atol=1e-6)
 
     def test_spike_thresh(self):
         td = tv.dat_imu.copy(deep=True)
 
         mask = avm.clean.spike_thresh(td.vel, thresh=10)
-        td['vel'] = avm.clean.clean_fill(
-            td.vel, mask, method='cubic', maxgap=6)
+        td["vel"] = avm.clean.clean_fill(td.vel, mask, method="cubic", maxgap=6)
 
         if make_data:
-            save(td, 'vector_data01_sclean.nc')
+            save(td, "vector_data01_sclean.nc")
             return
 
-        assert_allclose(td, load('vector_data01_sclean.nc'), atol=1e-6)
+        assert_allclose(td, load("vector_data01_sclean.nc"), atol=1e-6)
 
     def test_range_limit(self):
         td = tv.dat_imu.copy(deep=True)
 
         mask = avm.clean.range_limit(td.vel)
-        td['vel'] = avm.clean.clean_fill(
-            td.vel, mask, method='cubic', maxgap=6)
+        td["vel"] = avm.clean.clean_fill(td.vel, mask, method="cubic", maxgap=6)
 
         if make_data:
-            save(td, 'vector_data01_rclean.nc')
+            save(td, "vector_data01_rclean.nc")
             return
 
-        assert_allclose(td, load('vector_data01_rclean.nc'), atol=1e-6)
+        assert_allclose(td, load("vector_data01_rclean.nc"), atol=1e-6)
 
     def test_clean_upADCP(self):
         td_awac = tp.dat_awac.copy(deep=True)
         td_sig = tp.dat_sig_tide.copy(deep=True)
 
         apm.clean.find_surface_from_P(td_awac, salinity=30)
         td_awac = apm.clean.nan_beyond_surface(td_awac, beam_angle=20)
 
         apm.clean.set_range_offset(td_sig, 0.6)
         apm.clean.find_surface_from_P(td_sig, salinity=31)
         td_sig = apm.clean.nan_beyond_surface(td_sig)
         td_sig = apm.clean.correlation_filter(td_sig, thresh=50)
 
         if make_data:
-            save(td_awac, 'AWAC_test01_clean.nc')
-            save(td_sig, 'Sig1000_tidal_clean.nc')
+            save(td_awac, "AWAC_test01_clean.nc")
+            save(td_sig, "Sig1000_tidal_clean.nc")
             return
 
-        assert_allclose(td_awac, load('AWAC_test01_clean.nc'), atol=1e-6)
-        assert_allclose(td_sig, load('Sig1000_tidal_clean.nc'), atol=1e-6)
+        assert_allclose(td_awac, load("AWAC_test01_clean.nc"), atol=1e-6)
+        assert_allclose(td_sig, load("Sig1000_tidal_clean.nc"), atol=1e-6)
 
     def test_clean_downADCP(self):
         td = tp.dat_sig_ie.copy(deep=True)
 
         # First remove bad data
-        td['vel'] = apm.clean.val_exceeds_thresh(td.vel, thresh=3)
-        td['vel'] = apm.clean.fillgaps_time(td.vel)
-        td['vel_b5'] = apm.clean.fillgaps_time(td.vel_b5)
-        td['vel'] = apm.clean.fillgaps_depth(td.vel)
-        td['vel_b5'] = apm.clean.fillgaps_depth(td.vel_b5)
+        td["vel"] = apm.clean.val_exceeds_thresh(td.vel, thresh=3)
+        td["vel"] = apm.clean.fillgaps_time(td.vel)
+        td["vel_b5"] = apm.clean.fillgaps_time(td.vel_b5)
+        td["vel"] = apm.clean.fillgaps_depth(td.vel)
+        td["vel_b5"] = apm.clean.fillgaps_depth(td.vel_b5)
 
         # Then clean below seabed
         apm.clean.set_range_offset(td, 0.5)
         apm.clean.find_surface(td, thresh=10, nfilt=3)
         td = apm.clean.nan_beyond_surface(td)
 
         if make_data:
-            save(td, 'Sig500_Echo_clean.nc')
+            save(td, "Sig500_Echo_clean.nc")
             return
 
-        assert_allclose(td, load('Sig500_Echo_clean.nc'), atol=1e-6)
+        assert_allclose(td, load("Sig500_Echo_clean.nc"), atol=1e-6)
 
     def test_orient_filter(self):
         td_sig = tp.dat_sig_i.copy(deep=True)
         td_sig = apm.clean.medfilt_orient(td_sig)
-        apm.rotate2(td_sig, 'earth', inplace=True)
+        apm.rotate2(td_sig, "earth", inplace=True)
 
         td_rdi = tp.dat_rdi.copy(deep=True)
         td_rdi = apm.clean.medfilt_orient(td_rdi)
-        apm.rotate2(td_rdi, 'earth', inplace=True)
+        apm.rotate2(td_rdi, "earth", inplace=True)
 
         if make_data:
-            save(td_sig, 'Sig1000_IMU_ofilt.nc')
-            save(td_rdi, 'RDI_test01_ofilt.nc')
+            save(td_sig, "Sig1000_IMU_ofilt.nc")
+            save(td_rdi, "RDI_test01_ofilt.nc")
             return
 
-        assert_allclose(td_sig, load('Sig1000_IMU_ofilt.nc'), atol=1e-6)
-        assert_allclose(td_rdi, load('RDI_test01_ofilt.nc'), atol=1e-6)
+        assert_allclose(td_sig, load("Sig1000_IMU_ofilt.nc"), atol=1e-6)
+        assert_allclose(td_rdi, load("RDI_test01_ofilt.nc"), atol=1e-6)
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_motion.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_motion.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,17 @@
 import numpy as np
 import xarray as xr
 from mhkit.dolfyn.adv.motion import correct_motion
 
 from . import test_read_adv as tv
-from mhkit.tests.dolfyn.base import load_netcdf as load, save_netcdf as save, assert_allclose
+from mhkit.tests.dolfyn.base import (
+    load_netcdf as load,
+    save_netcdf as save,
+    assert_allclose,
+)
 from mhkit.dolfyn.adv import api
 from mhkit.dolfyn.io.api import read_example as read
 import unittest
 
 make_data = False
 
 
@@ -25,71 +29,70 @@
         tdm10.velds.set_declination(10.0, inplace=True)
         tdm10 = api.correct_motion(tdm10)
 
         # test setting declination to 0 doesn't affect correction
         tdm0 = tv.dat_imu.copy(deep=True)
         tdm0.velds.set_declination(0.0, inplace=True)
         tdm0 = api.correct_motion(tdm0)
-        tdm0.attrs.pop('declination')
-        tdm0.attrs.pop('declination_in_orientmat')
+        tdm0.attrs.pop("declination")
+        tdm0.attrs.pop("declination_in_orientmat")
 
         # test motion-corrected data rotation
         tdmE = tv.dat_imu.copy(deep=True)
         tdmE.velds.set_declination(10.0, inplace=True)
-        tdmE.velds.rotate2('earth', inplace=True)
+        tdmE.velds.rotate2("earth", inplace=True)
         tdmE = api.correct_motion(tdmE)
 
         # ensure trailing nans are removed from AHRS data
-        ahrs = read('vector_data_imu01.VEC', userdata=True)
-        for var in ['accel', 'angrt', 'mag']:
-            assert not ahrs[var].isnull().any(
-            ), "nan's in {} variable".format(var)
+        ahrs = read("vector_data_imu01.VEC", userdata=True)
+        for var in ["accel", "angrt", "mag"]:
+            assert not ahrs[var].isnull().any(), "nan's in {} variable".format(var)
 
         if make_data:
-            save(tdm, 'vector_data_imu01_mc.nc')
-            save(tdm10, 'vector_data_imu01_mcDeclin10.nc')
-            save(tdmj, 'vector_data_imu01-json_mc.nc')
+            save(tdm, "vector_data_imu01_mc.nc")
+            save(tdm10, "vector_data_imu01_mcDeclin10.nc")
+            save(tdmj, "vector_data_imu01-json_mc.nc")
             return
 
-        cdm10 = load('vector_data_imu01_mcDeclin10.nc')
+        cdm10 = load("vector_data_imu01_mcDeclin10.nc")
 
-        assert_allclose(tdm, load('vector_data_imu01_mc.nc'), atol=1e-7)
+        assert_allclose(tdm, load("vector_data_imu01_mc.nc"), atol=1e-7)
         assert_allclose(tdm10, tdmj, atol=1e-7)
         assert_allclose(tdm0, tdm, atol=1e-7)
         assert_allclose(tdm10, cdm10, atol=1e-7)
         assert_allclose(tdmE, cdm10, atol=1e-7)
-        assert_allclose(tdmj, load('vector_data_imu01-json_mc.nc'), atol=1e-7)
+        assert_allclose(tdmj, load("vector_data_imu01-json_mc.nc"), atol=1e-7)
 
     def test_sep_probes(self):
         tdm = tv.dat_imu.copy(deep=True)
         tdm = api.correct_motion(tdm, separate_probes=True)
 
         if make_data:
-            save(tdm, 'vector_data_imu01_mcsp.nc')
+            save(tdm, "vector_data_imu01_mcsp.nc")
             return
 
-        assert_allclose(tdm, load('vector_data_imu01_mcsp.nc'), atol=1e-7)
+        assert_allclose(tdm, load("vector_data_imu01_mcsp.nc"), atol=1e-7)
 
     def test_duty_cycle(self):
-        tdc = load('vector_duty_cycle.nc')
+        tdc = load("vector_duty_cycle.nc")
         tdc.velds.set_inst2head_rotmat(np.eye(3))
-        tdc.attrs['inst2head_vec'] = [0.5, 0, 0.1]
+        tdc.attrs["inst2head_vec"] = [0.5, 0, 0.1]
 
         # with duty cycle code
         td = correct_motion(tdc, accel_filtfreq=0.03, to_earth=False)
         td_ENU = correct_motion(tdc, accel_filtfreq=0.03, to_earth=True)
 
         # Wrapped function
         n_burst = 50
-        n_ensembles = len(tdc.time)//n_burst
+        n_ensembles = len(tdc.time) // n_burst
         cd = xr.Dataset()
-        tdc.attrs.pop('duty_cycle_n_burst')
+        tdc.attrs.pop("duty_cycle_n_burst")
         for i in range(n_ensembles):
-            cd0 = tdc.isel(time=slice(n_burst*i, n_burst*i+n_burst))
+            cd0 = tdc.isel(time=slice(n_burst * i, n_burst * i + n_burst))
             cd0 = correct_motion(cd0, accel_filtfreq=0.03, to_earth=False)
-            cd = xr.merge((cd, cd0), combine_attrs='no_conflicts')
-        cd.attrs['duty_cycle_n_burst'] = n_burst
+            cd = xr.merge((cd, cd0), combine_attrs="no_conflicts")
+        cd.attrs["duty_cycle_n_burst"] = n_burst
 
-        cd_ENU = cd.velds.rotate2('earth', inplace=False)
+        cd_ENU = cd.velds.rotate2("earth", inplace=False)
 
         assert_allclose(td, cd, atol=1e-7)
         assert_allclose(td_ENU, cd_ENU, atol=1e-7)
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_orient.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_orient.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,20 +4,33 @@
 import numpy as np
 from numpy.testing import assert_allclose
 import unittest
 
 
 def check_hpr(h, p, r, omatin):
     omat = euler2orient(h, p, r)
-    assert_allclose(omat, omatin, atol=1e-13, err_msg='Orientation matrix different than expected!\nExpected:\n{}\nGot:\n{}'
-                    .format(np.array(omatin), omat))
+    assert_allclose(
+        omat,
+        omatin,
+        atol=1e-13,
+        err_msg="Orientation matrix different than expected!\nExpected:\n{}\nGot:\n{}".format(
+            np.array(omatin), omat
+        ),
+    )
     hpr = orient2euler(omat)
-    assert_allclose(hpr, [h, p, r], atol=1e-13, err_msg="Angles different than specified, orient2euler and euler2orient are "
-                    "antisymmetric!\nExpected:\n{}\nGot:\n{}"
-                    .format(hpr, np.array([h, p, r]), ))
+    assert_allclose(
+        hpr,
+        [h, p, r],
+        atol=1e-13,
+        err_msg="Angles different than specified, orient2euler and euler2orient are "
+        "antisymmetric!\nExpected:\n{}\nGot:\n{}".format(
+            hpr,
+            np.array([h, p, r]),
+        ),
+    )
 
 
 class orient_testcase(unittest.TestCase):
     def test_hpr_defs(self):
         """
         These tests confirm that the euler2orient and orient2euler functions
         are consistent, and that they follow the conventions defined in the
@@ -38,71 +51,137 @@
           - roll is positive according to the right-hand-rule around the
             instument's x-axis
 
         IF YOU MAKE CHANGES TO THESE CONVENTIONS, BE SURE TO UPDATE THE
         DOCUMENTATION.
 
         """
-        check_hpr(0, 0, 0, [[0, 1, 0],
-                            [-1, 0, 0],
-                            [0, 0, 1], ])
-
-        check_hpr(90, 0, 0, [[1, 0, 0],
-                             [0, 1, 0],
-                             [0, 0, 1], ])
-
-        check_hpr(90, 0, 90, [[1, 0, 0],
-                              [0, 0, 1],
-                              [0, -1, 0], ])
-
-        sq2 = 1. / np.sqrt(2)
-        check_hpr(45, 0, 0, [[sq2, sq2, 0],
-                             [-sq2, sq2, 0],
-                             [0, 0, 1], ])
-
-        check_hpr(0, 45, 0, [[0, sq2, sq2],
-                             [-1, 0, 0],
-                             [0, -sq2, sq2], ])
-
-        check_hpr(0, 0, 45, [[0, 1, 0],
-                             [-sq2, 0, sq2],
-                             [sq2, 0, sq2], ])
-
-        check_hpr(90, 45, 90, [[sq2, 0, sq2],
-                               [-sq2, 0, sq2],
-                               [0, -1, 0], ])
+        check_hpr(
+            0,
+            0,
+            0,
+            [
+                [0, 1, 0],
+                [-1, 0, 0],
+                [0, 0, 1],
+            ],
+        )
+
+        check_hpr(
+            90,
+            0,
+            0,
+            [
+                [1, 0, 0],
+                [0, 1, 0],
+                [0, 0, 1],
+            ],
+        )
+
+        check_hpr(
+            90,
+            0,
+            90,
+            [
+                [1, 0, 0],
+                [0, 0, 1],
+                [0, -1, 0],
+            ],
+        )
+
+        sq2 = 1.0 / np.sqrt(2)
+        check_hpr(
+            45,
+            0,
+            0,
+            [
+                [sq2, sq2, 0],
+                [-sq2, sq2, 0],
+                [0, 0, 1],
+            ],
+        )
+
+        check_hpr(
+            0,
+            45,
+            0,
+            [
+                [0, sq2, sq2],
+                [-1, 0, 0],
+                [0, -sq2, sq2],
+            ],
+        )
+
+        check_hpr(
+            0,
+            0,
+            45,
+            [
+                [0, 1, 0],
+                [-sq2, 0, sq2],
+                [sq2, 0, sq2],
+            ],
+        )
+
+        check_hpr(
+            90,
+            45,
+            90,
+            [
+                [sq2, 0, sq2],
+                [-sq2, 0, sq2],
+                [0, -1, 0],
+            ],
+        )
 
         c30 = np.cos(np.deg2rad(30))
         s30 = np.sin(np.deg2rad(30))
-        check_hpr(30, 0, 0, [[s30, c30, 0],
-                             [-c30, s30, 0],
-                             [0, 0, 1], ])
+        check_hpr(
+            30,
+            0,
+            0,
+            [
+                [s30, c30, 0],
+                [-c30, s30, 0],
+                [0, 0, 1],
+            ],
+        )
 
     def test_pr_declination(self):
         # Test to confirm that pitch and roll don't change when you set
         # declination
         declin = 15.37
 
-        dat = load('vector_data_imu01.nc')
-        h0, p0, r0 = orient2euler(dat['orientmat'].values)
+        dat = load("vector_data_imu01.nc")
+        h0, p0, r0 = orient2euler(dat["orientmat"].values)
 
         set_declination(dat, declin, inplace=True)
-        h1, p1, r1 = orient2euler(dat['orientmat'].values)
+        h1, p1, r1 = orient2euler(dat["orientmat"].values)
 
-        assert_allclose(p0, p1, atol=1e-5,
-                        err_msg="Pitch changes when setting declination")
-        assert_allclose(r0, r1, atol=1e-5,
-                        err_msg="Roll changes when setting declination")
-        assert_allclose(h0 + declin, h1, atol=1e-5, err_msg="incorrect heading change when "
-                        "setting declination")
+        assert_allclose(
+            p0, p1, atol=1e-5, err_msg="Pitch changes when setting declination"
+        )
+        assert_allclose(
+            r0, r1, atol=1e-5, err_msg="Roll changes when setting declination"
+        )
+        assert_allclose(
+            h0 + declin,
+            h1,
+            atol=1e-5,
+            err_msg="incorrect heading change when " "setting declination",
+        )
 
     def test_q_hpr(self):
-        dat = load('Sig1000_IMU.nc')
+        dat = load("Sig1000_IMU.nc")
 
         dcm = quaternion2orient(dat.quaternions)
 
-        assert_allclose(dat.orientmat, dcm, atol=5e-4,
-                        err_msg="Disagreement b/t quaternion-calc'd & HPR-calc'd orientmat")
+        assert_allclose(
+            dat.orientmat,
+            dcm,
+            atol=5e-4,
+            err_msg="Disagreement b/t quaternion-calc'd & HPR-calc'd orientmat",
+        )
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_read_adp.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_read_adp.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,157 +8,184 @@
 import pytest
 import os
 
 make_data = False
 load = tb.load_netcdf
 save = tb.save_netcdf
 
-dat_rdi = load('RDI_test01.nc')
-dat_rdi_7f79 = load('RDI_7f79.nc')
-dat_rdi_bt = load('RDI_withBT.nc')
-dat_vm_ws = load('vmdas01_wh.nc')
-dat_vm_os = load('vmdas02_os.nc')
-dat_wr1 = load('winriver01.nc')
-dat_wr2 = load('winriver02.nc')
-dat_rp = load('RiverPro_test01.nc')
-dat_trsc = load('winriver02_transect.nc')
-
-dat_awac = load('AWAC_test01.nc')
-dat_awac_ud = load('AWAC_test01_ud.nc')
-dat_hwac = load('H-AWAC_test01.nc')
-dat_sig = load('BenchFile01.nc')
-dat_sig_i = load('Sig1000_IMU.nc')
-dat_sig_i_ud = load('Sig1000_IMU_ud.nc')
-dat_sig_ieb = load('VelEchoBT01.nc')
-dat_sig_ie = load('Sig500_Echo.nc')
-dat_sig_tide = load('Sig1000_tidal.nc')
-dat_sig_skip = load('Sig_SkippedPings01.nc')
-dat_sig_badt = load('Sig1000_BadTime01.nc')
-dat_sig5_leiw = load('Sig500_last_ensemble_is_whole.nc')
+dat_rdi = load("RDI_test01.nc")
+dat_rdi_7f79 = load("RDI_7f79.nc")
+dat_rdi_7f79_2 = load("RDI_7f79_2.nc")
+dat_rdi_bt = load("RDI_withBT.nc")
+dat_vm_ws = load("vmdas01_wh.nc")
+dat_vm_os = load("vmdas02_os.nc")
+dat_wr1 = load("winriver01.nc")
+dat_wr2 = load("winriver02.nc")
+dat_rp = load("RiverPro_test01.nc")
+dat_trsc = load("winriver02_transect.nc")
+
+dat_awac = load("AWAC_test01.nc")
+dat_awac_ud = load("AWAC_test01_ud.nc")
+dat_hwac = load("H-AWAC_test01.nc")
+dat_sig = load("BenchFile01.nc")
+dat_sig_i = load("Sig1000_IMU.nc")
+dat_sig_i_ud = load("Sig1000_IMU_ud.nc")
+dat_sig_ieb = load("VelEchoBT01.nc")
+dat_sig_ie = load("Sig500_Echo.nc")
+dat_sig_tide = load("Sig1000_tidal.nc")
+dat_sig_skip = load("Sig_SkippedPings01.nc")
+dat_sig_badt = load("Sig1000_BadTime01.nc")
+dat_sig5_leiw = load("Sig500_last_ensemble_is_whole.nc")
+dat_sig_dp2 = load("dual_profile.nc")
 
 
 class io_adp_testcase(unittest.TestCase):
     def test_io_rdi(self):
-        warnings.simplefilter('ignore', UserWarning)
+        warnings.simplefilter("ignore", UserWarning)
         nens = 100
-        td_rdi = read('RDI_test01.000')
-        td_7f79 = read('RDI_7f79.000')
-        td_rdi_bt = read('RDI_withBT.000', nens=nens)
-        td_vm = read('vmdas01_wh.ENX', nens=nens)
-        td_os = read('vmdas02_os.ENR', nens=nens)
-        td_wr1 = read('winriver01.PD0')
-        td_wr2 = read('winriver02.PD0')
-        td_rp = read('RiverPro_test01.PD0', nens=nens)
-        td_transect = read('winriver02_transect.PD0', nens=nens)
+        td_rdi = read("RDI_test01.000")
+        td_7f79 = read("RDI_7f79.000")
+        td_7f79_2 = read("RDI_7f79_2.000")
+        td_rdi_bt = read("RDI_withBT.000", nens=nens)
+        td_vm = read("vmdas01_wh.ENX", nens=nens)
+        td_os = read("vmdas02_os.ENR", nens=nens)
+        td_wr1 = read("winriver01.PD0")
+        td_wr2 = read("winriver02.PD0")
+        td_rp = read("RiverPro_test01.PD0")
+        td_transect = read("winriver02_transect.PD0", nens=nens)
 
         if make_data:
-            save(td_rdi, 'RDI_test01.nc')
-            save(td_7f79, 'RDI_7f79.nc')
-            save(td_rdi_bt, 'RDI_withBT.nc')
-            save(td_vm, 'vmdas01_wh.nc')
-            save(td_os, 'vmdas02_os.nc')
-            save(td_wr1, 'winriver01.nc')
-            save(td_wr2, 'winriver02.nc')
-            save(td_rp, 'RiverPro_test01.nc')
-            save(td_transect, 'winriver02_transect.nc')
+            save(td_rdi, "RDI_test01.nc")
+            save(td_7f79, "RDI_7f79.nc")
+            save(td_7f79_2, "RDI_7f79_2.nc")
+            save(td_rdi_bt, "RDI_withBT.nc")
+            save(td_vm, "vmdas01_wh.nc")
+            save(td_os, "vmdas02_os.nc")
+            save(td_wr1, "winriver01.nc")
+            save(td_wr2, "winriver02.nc")
+            save(td_rp, "RiverPro_test01.nc")
+            save(td_transect, "winriver02_transect.nc")
             return
 
         assert_allclose(td_rdi, dat_rdi, atol=1e-6)
         assert_allclose(td_7f79, dat_rdi_7f79, atol=1e-6)
+        assert_allclose(td_7f79_2, dat_rdi_7f79_2, atol=1e-6)
         assert_allclose(td_rdi_bt, dat_rdi_bt, atol=1e-6)
         assert_allclose(td_vm, dat_vm_ws, atol=1e-6)
         assert_allclose(td_os, dat_vm_os, atol=1e-6)
         assert_allclose(td_wr1, dat_wr1, atol=1e-6)
         assert_allclose(td_wr2, dat_wr2, atol=1e-6)
         assert_allclose(td_rp, dat_rp, atol=1e-6)
         assert_allclose(td_transect, dat_trsc, atol=1e-6)
 
     def test_io_nortek(self):
         nens = 100
         with pytest.warns(UserWarning):
-            td_awac = read('AWAC_test01.wpr', userdata=False, nens=[0, nens])
-        td_awac_ud = read('AWAC_test01.wpr', nens=nens)
-        td_hwac = read('H-AWAC_test01.wpr')
+            td_awac = read("AWAC_test01.wpr", userdata=False, nens=[0, nens])
+        td_awac_ud = read("AWAC_test01.wpr", nens=nens)
+        td_hwac = read("H-AWAC_test01.wpr")
 
         if make_data:
-            save(td_awac, 'AWAC_test01.nc')
-            save(td_awac_ud, 'AWAC_test01_ud.nc')
-            save(td_hwac, 'H-AWAC_test01.nc')
+            save(td_awac, "AWAC_test01.nc")
+            save(td_awac_ud, "AWAC_test01_ud.nc")
+            save(td_hwac, "H-AWAC_test01.nc")
             return
 
         assert_allclose(td_awac, dat_awac, atol=1e-6)
         assert_allclose(td_awac_ud, dat_awac_ud, atol=1e-6)
         assert_allclose(td_hwac, dat_hwac, atol=1e-6)
 
     def test_io_nortek2(self):
         nens = 100
-        td_sig = read('BenchFile01.ad2cp', nens=nens)
-        td_sig_i = read('Sig1000_IMU.ad2cp', userdata=False, nens=nens)
-        td_sig_i_ud = read('Sig1000_IMU.ad2cp', nens=nens)
-        td_sig_ieb = read('VelEchoBT01.ad2cp', nens=nens)
-        td_sig_ie = read('Sig500_Echo.ad2cp', nens=nens)
-        td_sig_tide = read('Sig1000_tidal.ad2cp', nens=nens)
+        td_sig = read("BenchFile01.ad2cp", nens=nens, rebuild_index=True)
+        td_sig_i = read(
+            "Sig1000_IMU.ad2cp", userdata=False, nens=nens, rebuild_index=True
+        )
+        td_sig_i_ud = read("Sig1000_IMU.ad2cp", nens=nens, rebuild_index=True)
+        td_sig_ieb = read("VelEchoBT01.ad2cp", nens=nens, rebuild_index=True)
+        td_sig_ie = read("Sig500_Echo.ad2cp", nens=nens, rebuild_index=True)
+        td_sig_tide = read("Sig1000_tidal.ad2cp", nens=nens, rebuild_index=True)
+        # Only need to test 2nd dataset
+        td_sig_dp1, td_sig_dp2 = read("dual_profile.ad2cp")
 
         with pytest.warns(UserWarning):
             # This issues a warning...
-            td_sig_skip = read('Sig_SkippedPings01.ad2cp')
+            td_sig_skip = read("Sig_SkippedPings01.ad2cp")
 
         with pytest.warns(UserWarning):
-            td_sig_badt = sig.read_signature(
-                tb.rfnm('Sig1000_BadTime01.ad2cp'))
+            td_sig_badt = sig.read_signature(tb.rfnm("Sig1000_BadTime01.ad2cp"))
 
         # Make sure we read all the way to the end of the file.
         # This file ends exactly at the end of an ensemble.
-        td_sig5_leiw = read('Sig500_last_ensemble_is_whole.ad2cp')
+        td_sig5_leiw = read("Sig500_last_ensemble_is_whole.ad2cp")
 
-        os.remove(tb.exdt('BenchFile01.ad2cp.index'))
-        os.remove(tb.exdt('Sig1000_IMU.ad2cp.index'))
-        os.remove(tb.exdt('VelEchoBT01.ad2cp.index'))
-        os.remove(tb.exdt('Sig500_Echo.ad2cp.index'))
-        os.remove(tb.exdt('Sig1000_tidal.ad2cp.index'))
-        os.remove(tb.exdt('Sig_SkippedPings01.ad2cp.index'))
-        os.remove(tb.exdt('Sig500_last_ensemble_is_whole.ad2cp.index'))
-        os.remove(tb.rfnm('Sig1000_BadTime01.ad2cp.index'))
+        os.remove(tb.exdt("BenchFile01.ad2cp.index"))
+        os.remove(tb.exdt("Sig1000_IMU.ad2cp.index"))
+        os.remove(tb.exdt("VelEchoBT01.ad2cp.index"))
+        os.remove(tb.exdt("Sig500_Echo.ad2cp.index"))
+        os.remove(tb.exdt("Sig1000_tidal.ad2cp.index"))
+        os.remove(tb.exdt("Sig_SkippedPings01.ad2cp.index"))
+        os.remove(tb.exdt("Sig500_last_ensemble_is_whole.ad2cp.index"))
+        os.remove(tb.rfnm("Sig1000_BadTime01.ad2cp.index"))
+        os.remove(tb.exdt("dual_profile.ad2cp.index"))
 
         if make_data:
-            save(td_sig, 'BenchFile01.nc')
-            save(td_sig_i, 'Sig1000_IMU.nc')
-            save(td_sig_i_ud, 'Sig1000_IMU_ud.nc')
-            save(td_sig_ieb, 'VelEchoBT01.nc')
-            save(td_sig_ie, 'Sig500_Echo.nc')
-            save(td_sig_tide, 'Sig1000_tidal.nc')
-            save(td_sig_skip, 'Sig_SkippedPings01.nc')
-            save(td_sig_badt, 'Sig1000_BadTime01.nc')
-            save(td_sig5_leiw, 'Sig500_last_ensemble_is_whole.nc')
+            save(td_sig, "BenchFile01.nc")
+            save(td_sig_i, "Sig1000_IMU.nc")
+            save(td_sig_i_ud, "Sig1000_IMU_ud.nc")
+            save(td_sig_ieb, "VelEchoBT01.nc")
+            save(td_sig_ie, "Sig500_Echo.nc")
+            save(td_sig_tide, "Sig1000_tidal.nc")
+            save(td_sig_skip, "Sig_SkippedPings01.nc")
+            save(td_sig_badt, "Sig1000_BadTime01.nc")
+            save(td_sig5_leiw, "Sig500_last_ensemble_is_whole.nc")
+            save(td_sig_dp2, "dual_profile.nc")
             return
 
         assert_allclose(td_sig, dat_sig, atol=1e-6)
         assert_allclose(td_sig_i, dat_sig_i, atol=1e-6)
         assert_allclose(td_sig_i_ud, dat_sig_i_ud, atol=1e-6)
         assert_allclose(td_sig_ieb, dat_sig_ieb, atol=1e-6)
         assert_allclose(td_sig_ie, dat_sig_ie, atol=1e-6)
         assert_allclose(td_sig_tide, dat_sig_tide, atol=1e-6)
         assert_allclose(td_sig5_leiw, dat_sig5_leiw, atol=1e-6)
         assert_allclose(td_sig_skip, dat_sig_skip, atol=1e-6)
         assert_allclose(td_sig_badt, dat_sig_badt, atol=1e-6)
+        assert_allclose(td_sig_dp2, dat_sig_dp2, atol=1e-6)
 
     def test_nortek2_crop(self):
         # Test file cropping function
-        crop_ensembles(infile=tb.exdt('Sig500_Echo.ad2cp'),
-                       outfile=tb.exdt('Sig500_Echo_crop.ad2cp'),
-                       range=[50, 100])
-        td_sig_ie_crop = read('Sig500_Echo_crop.ad2cp')
+        crop_ensembles(
+            infile=tb.exdt("Sig500_Echo.ad2cp"),
+            outfile=tb.exdt("Sig500_Echo_crop.ad2cp"),
+            range=[50, 100],
+        )
+        td_sig_ie_crop = read("Sig500_Echo_crop.ad2cp")
+
+        crop_ensembles(
+            infile=tb.exdt("BenchFile01.ad2cp"),
+            outfile=tb.exdt("BenchFile01_crop.ad2cp"),
+            range=[50, 100],
+        )
+        td_sig_crop = read("BenchFile01_crop.ad2cp")
 
         if make_data:
-            save(td_sig_ie_crop, 'Sig500_Echo_crop.nc')
+            save(td_sig_ie_crop, "Sig500_Echo_crop.nc")
+            save(td_sig_crop, "BenchFile01_crop.nc")
             return
 
-        os.remove(tb.exdt('Sig500_Echo.ad2cp.index'))
-        os.remove(tb.exdt('Sig500_Echo_crop.ad2cp'))
-        os.remove(tb.exdt('Sig500_Echo_crop.ad2cp.index'))
+        os.remove(tb.exdt("Sig500_Echo.ad2cp.index"))
+        os.remove(tb.exdt("Sig500_Echo_crop.ad2cp"))
+        os.remove(tb.exdt("Sig500_Echo_crop.ad2cp.index"))
+        os.remove(tb.exdt("BenchFile01.ad2cp.index"))
+        os.remove(tb.exdt("BenchFile01_crop.ad2cp"))
+        os.remove(tb.exdt("BenchFile01_crop.ad2cp.index"))
+
+        cd_sig_ie_crop = load("Sig500_Echo_crop.nc")
+        cd_sig_crop = load("BenchFile01_crop.nc")
 
-        cd_sig_ie_crop = load('Sig500_Echo_crop.nc')
         assert_allclose(td_sig_ie_crop, cd_sig_ie_crop, atol=1e-6)
+        assert_allclose(td_sig_crop, cd_sig_crop, atol=1e-6)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_read_adv.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_read_adv.py`

 * *Files 16% similar despite different names*

```diff
@@ -5,43 +5,45 @@
 import unittest
 
 make_data = False
 load = tb.load_netcdf
 save = tb.save_netcdf
 assert_allclose = tb.assert_allclose
 
-dat = load('vector_data01')
-dat_imu = load('vector_data_imu01')
-dat_imu_json = load('vector_data_imu01-json')
-dat_burst = load('vector_burst_mode01')
+dat = load("vector_data01")
+dat_imu = load("vector_data_imu01")
+dat_imu_json = load("vector_data_imu01-json")
+dat_burst = load("vector_burst_mode01")
 
 
 class io_adv_testcase(unittest.TestCase):
     def test_io_adv(self):
         nens = 100
-        td = read('vector_data01.VEC', nens=nens)
-        tdm = read('vector_data_imu01.VEC', userdata=False, nens=nens)
-        tdb = read('vector_burst_mode01.VEC', nens=nens)
-        tdm2 = read('vector_data_imu01.VEC',
-                    userdata=tb.exdt('vector_data_imu01.userdata.json'),
-                    nens=nens)
+        td = read("vector_data01.VEC", nens=nens)
+        tdm = read("vector_data_imu01.VEC", userdata=False, nens=nens)
+        tdb = read("vector_burst_mode01.VEC", nens=nens)
+        tdm2 = read(
+            "vector_data_imu01.VEC",
+            userdata=tb.exdt("vector_data_imu01.userdata.json"),
+            nens=nens,
+        )
 
         # These values are not correct for this data but I'm adding them for
         # test purposes only.
         set_inst2head_rotmat(tdm, np.eye(3), inplace=True)
-        tdm.attrs['inst2head_vec'] = [-1.0, 0.5, 0.2]
+        tdm.attrs["inst2head_vec"] = [-1.0, 0.5, 0.2]
 
         if make_data:
-            save(td, 'vector_data01.nc')
-            save(tdm, 'vector_data_imu01.nc')
-            save(tdb, 'vector_burst_mode01.nc')
-            save(tdm2, 'vector_data_imu01-json.nc')
+            save(td, "vector_data01.nc")
+            save(tdm, "vector_data_imu01.nc")
+            save(tdb, "vector_burst_mode01.nc")
+            save(tdm2, "vector_data_imu01-json.nc")
             return
 
         assert_allclose(td, dat, atol=1e-6)
         assert_allclose(tdm, dat_imu, atol=1e-6)
         assert_allclose(tdb, dat_burst, atol=1e-6)
         assert_allclose(tdm2, dat_imu_json, atol=1e-6)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_read_io.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_read_io.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,102 +1,119 @@
 from . import test_read_adp as tp
 from . import test_read_adv as tv
-from mhkit.tests.dolfyn.base import assert_allclose, save_netcdf, save_matlab, load_matlab, exdt, rfnm
+from mhkit.tests.dolfyn.base import (
+    assert_allclose,
+    save_netcdf,
+    save_matlab,
+    load_matlab,
+    exdt,
+    rfnm,
+)
 import mhkit.dolfyn.io.rdi as wh
 import mhkit.dolfyn.io.nortek as awac
 import mhkit.dolfyn.io.nortek2 as sig
 from mhkit.dolfyn.io.api import read_example as read
 import unittest
 import pytest
 import os
 
 make_data = False
 
 
 class io_testcase(unittest.TestCase):
     def test_save(self):
         ds = tv.dat.copy(deep=True)
-
-        save_netcdf(ds, 'test_save')
-        save_matlab(ds, 'test_save')
-
-        assert os.path.exists(rfnm('test_save.nc'))
-        assert os.path.exists(rfnm('test_save.mat'))
+        ds2 = tp.dat_sig.copy(deep=True)
+        save_netcdf(ds, "test_save")
+        save_netcdf(ds2, "test_save_comp.nc", compression=True)
+        save_matlab(ds, "test_save")
+
+        assert os.path.exists(rfnm("test_save.nc"))
+        assert os.path.exists(rfnm("test_save_comp.nc"))
+        assert os.path.exists(rfnm("test_save.mat"))
+
+        os.remove(rfnm("test_save.nc"))
+        os.remove(rfnm("test_save_comp.nc"))
+        os.remove(rfnm("test_save.mat"))
 
     def test_matlab_io(self):
         nens = 100
-        td_vec = read('vector_data_imu01.VEC', nens=nens)
-        td_rdi_bt = read('RDI_withBT.000', nens=nens)
+        td_vec = read("vector_data_imu01.VEC", nens=nens)
+        td_rdi_bt = read("RDI_withBT.000", nens=nens)
 
         # This read should trigger a warning about the declination being
         # defined in two places (in the binary .ENX files), and in the
         # .userdata.json file. NOTE: DOLfYN defaults to using what is in
         # the .userdata.json file.
-        with pytest.warns(UserWarning, match='magnetic_var_deg'):
-            td_vm = read('vmdas01_wh.ENX', nens=nens)
+        with pytest.warns(UserWarning, match="magnetic_var_deg"):
+            td_vm = read("vmdas01_wh.ENX", nens=nens)
 
         if make_data:
-            save_matlab(td_vec, 'dat_vec')
-            save_matlab(td_rdi_bt, 'dat_rdi_bt')
-            save_matlab(td_vm, 'dat_vm')
+            save_matlab(td_vec, "dat_vec")
+            save_matlab(td_rdi_bt, "dat_rdi_bt")
+            save_matlab(td_vm, "dat_vm")
             return
 
-        mat_vec = load_matlab('dat_vec.mat')
-        mat_rdi_bt = load_matlab('dat_rdi_bt.mat')
-        mat_vm = load_matlab('dat_vm.mat')
+        mat_vec = load_matlab("dat_vec.mat")
+        mat_rdi_bt = load_matlab("dat_rdi_bt.mat")
+        mat_vm = load_matlab("dat_vm.mat")
 
         assert_allclose(td_vec, mat_vec, atol=1e-6)
         assert_allclose(td_rdi_bt, mat_rdi_bt, atol=1e-6)
         assert_allclose(td_vm, mat_vm, atol=1e-6)
 
     def test_debugging(self):
         def read_txt(fname, loc):
-            with open(loc(fname), 'r') as f:
+            with open(loc(fname), "r") as f:
                 string = f.read()
             return string
 
         def clip_file(fname):
             log = read_txt(fname, exdt)
-            newlines = [i for i, ltr in enumerate(log) if ltr == '\n']
+            newlines = [i for i, ltr in enumerate(log) if ltr == "\n"]
             try:
-                log = log[:newlines[100]+1]
+                log = log[: newlines[100] + 1]
             except:
                 pass
-            with open(rfnm(fname), 'w') as f:
+            with open(rfnm(fname), "w") as f:
                 f.write(log)
 
         def read_file_and_test(fname):
             td = read_txt(fname, exdt)
             cd = read_txt(fname, rfnm)
             assert cd in td
             os.remove(exdt(fname))
 
         nens = 100
-        wh.read_rdi(exdt('RDI_withBT.000'), nens, debug_level=3)
-        awac.read_nortek(exdt('AWAC_test01.wpr'), nens, debug=True, do_checksum=True)
-        awac.read_nortek(exdt('vector_data_imu01.VEC'), nens, debug=True, do_checksum=True)
-        sig.read_signature(exdt('Sig500_Echo.ad2cp'), nens, rebuild_index=True, debug=True)
-        os.remove(exdt('Sig500_Echo.ad2cp.index'))
+        wh.read_rdi(exdt("RDI_withBT.000"), nens, debug_level=3)
+        awac.read_nortek(exdt("AWAC_test01.wpr"), nens, debug=True, do_checksum=True)
+        awac.read_nortek(
+            exdt("vector_data_imu01.VEC"), nens, debug=True, do_checksum=True
+        )
+        sig.read_signature(
+            exdt("Sig500_Echo.ad2cp"), nens, rebuild_index=True, debug=True
+        )
+        os.remove(exdt("Sig500_Echo.ad2cp.index"))
 
         if make_data:
-            clip_file('RDI_withBT.dolfyn.log')
-            clip_file('AWAC_test01.dolfyn.log')
-            clip_file('vector_data_imu01.dolfyn.log')
-            clip_file('Sig500_Echo.dolfyn.log')
+            clip_file("RDI_withBT.dolfyn.log")
+            clip_file("AWAC_test01.dolfyn.log")
+            clip_file("vector_data_imu01.dolfyn.log")
+            clip_file("Sig500_Echo.dolfyn.log")
             return
 
-        read_file_and_test('RDI_withBT.dolfyn.log')
-        read_file_and_test('AWAC_test01.dolfyn.log')
-        read_file_and_test('vector_data_imu01.dolfyn.log')
-        read_file_and_test('Sig500_Echo.dolfyn.log')
+        read_file_and_test("RDI_withBT.dolfyn.log")
+        read_file_and_test("AWAC_test01.dolfyn.log")
+        read_file_and_test("vector_data_imu01.dolfyn.log")
+        read_file_and_test("Sig500_Echo.dolfyn.log")
 
     def test_read_warnings(self):
         with self.assertRaises(Exception):
-            wh.read_rdi(exdt('H-AWAC_test01.wpr'))
+            wh.read_rdi(exdt("H-AWAC_test01.wpr"))
         with self.assertRaises(Exception):
-            awac.read_nortek(exdt('BenchFile01.ad2cp'))
+            awac.read_nortek(exdt("BenchFile01.ad2cp"))
         with self.assertRaises(Exception):
-            sig.read_signature(exdt('AWAC_test01.wpr'))
+            sig.read_signature(exdt("AWAC_test01.wpr"))
         with self.assertRaises(IOError):
-            read(rfnm('AWAC_test01.nc'))
+            read(rfnm("AWAC_test01.nc"))
         with self.assertRaises(Exception):
-            save_netcdf(tp.dat_rdi, 'test_save.fail')
+            save_netcdf(tp.dat_rdi, "test_save.fail")
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_rotate_adv.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_rotate_adv.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,176 +1,178 @@
 from . import test_read_adv as tr
 from .base import load_netcdf as load, save_netcdf as save, assert_allclose
-from mhkit.dolfyn.rotate.api import rotate2, calc_principal_heading, \
-    set_declination, set_inst2head_rotmat
+from mhkit.dolfyn.rotate.api import (
+    rotate2,
+    calc_principal_heading,
+    set_declination,
+    set_inst2head_rotmat,
+)
 from mhkit.dolfyn.rotate.base import euler2orient, orient2euler
 import numpy as np
 import numpy.testing as npt
 import unittest
+
 make_data = False
 
 
 class rotate_adv_testcase(unittest.TestCase):
     def test_heading(self):
         td = tr.dat_imu.copy(deep=True)
 
         head, pitch, roll = orient2euler(td)
-        td['pitch'].values = pitch
-        td['roll'].values = roll
-        td['heading'].values = head
+        td["pitch"].values = pitch
+        td["roll"].values = roll
+        td["heading"].values = head
 
         if make_data:
-            save(td, 'vector_data_imu01_head_pitch_roll.nc')
+            save(td, "vector_data_imu01_head_pitch_roll.nc")
             return
-        cd = load('vector_data_imu01_head_pitch_roll.nc')
+        cd = load("vector_data_imu01_head_pitch_roll.nc")
 
         assert_allclose(td, cd, atol=1e-6)
 
     def test_inst2head_rotmat(self):
         # Validated test
         td = tr.dat.copy(deep=True)
 
         # Swap x,y, reverse z
-        set_inst2head_rotmat(td, [[0, 1, 0],
-                                  [1, 0, 0],
-                                  [0, 0, -1]], inplace=True)
+        set_inst2head_rotmat(td, [[0, 1, 0], [1, 0, 0], [0, 0, -1]], inplace=True)
 
         # Coords don't get altered here
         npt.assert_allclose(td.vel[0].values, tr.dat.vel[1].values, atol=1e-6)
         npt.assert_allclose(td.vel[1].values, tr.dat.vel[0].values, atol=1e-6)
         npt.assert_allclose(td.vel[2].values, -tr.dat.vel[2].values, atol=1e-6)
 
         # Validation for non-symmetric rotations
         td = tr.dat.copy(deep=True)
-        R = euler2orient(20, 30, 60, units='degrees')  # arbitrary angles
+        R = euler2orient(20, 30, 60, units="degrees")  # arbitrary angles
         td = set_inst2head_rotmat(td, R, inplace=False)
         vel1 = td.vel
         # validate that a head->inst rotation occurs (transpose of inst2head_rotmat)
         vel2 = np.dot(R, tr.dat.vel)
 
         npt.assert_allclose(vel1.values, vel2, atol=1e-6)
 
     def test_rotate_inst2earth(self):
         td = tr.dat.copy(deep=True)
-        rotate2(td, 'earth', inplace=True)
+        rotate2(td, "earth", inplace=True)
         tdm = tr.dat_imu.copy(deep=True)
-        rotate2(tdm, 'earth', inplace=True)
+        rotate2(tdm, "earth", inplace=True)
         tdo = tr.dat.copy(deep=True)
-        omat = tdo['orientmat']
-        tdo = rotate2(tdo.drop_vars('orientmat'), 'earth', inplace=False)
-        tdo['orientmat'] = omat
+        omat = tdo["orientmat"]
+        tdo = rotate2(tdo.drop_vars("orientmat"), "earth", inplace=False)
+        tdo["orientmat"] = omat
 
         if make_data:
-            save(td, 'vector_data01_rotate_inst2earth.nc')
-            save(tdm, 'vector_data_imu01_rotate_inst2earth.nc')
+            save(td, "vector_data01_rotate_inst2earth.nc")
+            save(tdm, "vector_data_imu01_rotate_inst2earth.nc")
             return
 
-        cd = load('vector_data01_rotate_inst2earth.nc')
-        cdm = load('vector_data_imu01_rotate_inst2earth.nc')
+        cd = load("vector_data01_rotate_inst2earth.nc")
+        cdm = load("vector_data_imu01_rotate_inst2earth.nc")
 
         assert_allclose(td, cd, atol=1e-6)
         assert_allclose(tdm, cdm, atol=1e-6)
         assert_allclose(tdo, cd, atol=1e-6)
 
     def test_rotate_earth2inst(self):
-        td = load('vector_data01_rotate_inst2earth.nc')
-        rotate2(td, 'inst', inplace=True)
-        tdm = load('vector_data_imu01_rotate_inst2earth.nc')
-        rotate2(tdm, 'inst', inplace=True)
+        td = load("vector_data01_rotate_inst2earth.nc")
+        rotate2(td, "inst", inplace=True)
+        tdm = load("vector_data_imu01_rotate_inst2earth.nc")
+        rotate2(tdm, "inst", inplace=True)
 
         cd = tr.dat.copy(deep=True)
         cdm = tr.dat_imu.copy(deep=True)
         # The heading/pitch/roll data gets modified during rotation, so it
         # doesn't go back to what it was.
-        cdm = cdm.drop_vars(['heading', 'pitch', 'roll'])
-        tdm = tdm.drop_vars(['heading', 'pitch', 'roll'])
+        cdm = cdm.drop_vars(["heading", "pitch", "roll"])
+        tdm = tdm.drop_vars(["heading", "pitch", "roll"])
 
         assert_allclose(td, cd, atol=1e-6)
         assert_allclose(tdm, cdm, atol=1e-6)
 
     def test_rotate_inst2beam(self):
         td = tr.dat.copy(deep=True)
-        rotate2(td, 'beam', inplace=True)
+        rotate2(td, "beam", inplace=True)
         tdm = tr.dat_imu.copy(deep=True)
-        rotate2(tdm, 'beam', inplace=True)
+        rotate2(tdm, "beam", inplace=True)
 
         if make_data:
-            save(td, 'vector_data01_rotate_inst2beam.nc')
-            save(tdm, 'vector_data_imu01_rotate_inst2beam.nc')
+            save(td, "vector_data01_rotate_inst2beam.nc")
+            save(tdm, "vector_data_imu01_rotate_inst2beam.nc")
             return
 
-        cd = load('vector_data01_rotate_inst2beam.nc')
-        cdm = load('vector_data_imu01_rotate_inst2beam.nc')
+        cd = load("vector_data01_rotate_inst2beam.nc")
+        cdm = load("vector_data_imu01_rotate_inst2beam.nc")
 
         assert_allclose(td, cd, atol=1e-6)
         assert_allclose(tdm, cdm, atol=1e-6)
 
     def test_rotate_beam2inst(self):
-        td = load('vector_data01_rotate_inst2beam.nc')
-        rotate2(td, 'inst', inplace=True)
-        tdm = load('vector_data_imu01_rotate_inst2beam.nc')
-        rotate2(tdm, 'inst', inplace=True)
+        td = load("vector_data01_rotate_inst2beam.nc")
+        rotate2(td, "inst", inplace=True)
+        tdm = load("vector_data_imu01_rotate_inst2beam.nc")
+        rotate2(tdm, "inst", inplace=True)
 
         cd = tr.dat.copy(deep=True)
         cdm = tr.dat_imu.copy(deep=True)
 
         assert_allclose(td, cd, atol=1e-5)
         assert_allclose(tdm, cdm, atol=1e-5)
 
     def test_rotate_earth2principal(self):
-        td = load('vector_data01_rotate_inst2earth.nc')
-        td.attrs['principal_heading'] = calc_principal_heading(td['vel'])
-        rotate2(td, 'principal', inplace=True)
-        tdm = load('vector_data_imu01_rotate_inst2earth.nc')
-        tdm.attrs['principal_heading'] = calc_principal_heading(tdm['vel'])
-        rotate2(tdm, 'principal', inplace=True)
+        td = load("vector_data01_rotate_inst2earth.nc")
+        td.attrs["principal_heading"] = calc_principal_heading(td["vel"])
+        rotate2(td, "principal", inplace=True)
+        tdm = load("vector_data_imu01_rotate_inst2earth.nc")
+        tdm.attrs["principal_heading"] = calc_principal_heading(tdm["vel"])
+        rotate2(tdm, "principal", inplace=True)
 
         if make_data:
-            save(td, 'vector_data01_rotate_earth2principal.nc')
-            save(tdm, 'vector_data_imu01_rotate_earth2principal.nc')
+            save(td, "vector_data01_rotate_earth2principal.nc")
+            save(tdm, "vector_data_imu01_rotate_earth2principal.nc")
             return
 
-        cd = load('vector_data01_rotate_earth2principal.nc')
-        cdm = load('vector_data_imu01_rotate_earth2principal.nc')
+        cd = load("vector_data01_rotate_earth2principal.nc")
+        cdm = load("vector_data_imu01_rotate_earth2principal.nc")
 
         assert_allclose(td, cd, atol=1e-6)
         assert_allclose(tdm, cdm, atol=1e-6)
 
     def test_rotate_earth2principal_set_declination(self):
         declin = 3.875
-        td = load('vector_data01_rotate_inst2earth.nc')
+        td = load("vector_data01_rotate_inst2earth.nc")
         td0 = td.copy(deep=True)
 
-        td.attrs['principal_heading'] = calc_principal_heading(td['vel'])
-        rotate2(td, 'principal', inplace=True)
+        td.attrs["principal_heading"] = calc_principal_heading(td["vel"])
+        rotate2(td, "principal", inplace=True)
         set_declination(td, declin, inplace=True)
-        rotate2(td, 'earth', inplace=True)
+        rotate2(td, "earth", inplace=True)
 
         set_declination(td0, -1, inplace=True)
         set_declination(td0, declin, inplace=True)
-        td0.attrs['principal_heading'] = calc_principal_heading(td0['vel'])
-        rotate2(td0, 'earth', inplace=True)
+        td0.attrs["principal_heading"] = calc_principal_heading(td0["vel"])
+        rotate2(td0, "earth", inplace=True)
 
         assert_allclose(td0, td, atol=1e-6)
 
     def test_rotate_warnings(self):
         warn1 = tr.dat.copy(deep=True)
         warn2 = tr.dat.copy(deep=True)
-        warn2.attrs['coord_sys'] = 'flow'
+        warn2.attrs["coord_sys"] = "flow"
         warn3 = tr.dat.copy(deep=True)
-        warn3.attrs['inst_model'] = 'ADV'
+        warn3.attrs["inst_model"] = "ADV"
         warn4 = tr.dat.copy(deep=True)
-        warn4.attrs['inst_model'] = 'adv'
+        warn4.attrs["inst_model"] = "adv"
 
         with self.assertRaises(Exception):
-            rotate2(warn1, 'ship')
+            rotate2(warn1, "ship")
         with self.assertRaises(Exception):
-            rotate2(warn2, 'earth')
+            rotate2(warn2, "earth")
         with self.assertRaises(Exception):
             set_inst2head_rotmat(warn3, np.eye(3))
-            set_inst2head_rotmat(warn3, np.eye(3))
         with self.assertRaises(Exception):
             set_inst2head_rotmat(warn4, np.eye(3))
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_shortcuts.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_shortcuts.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,49 +10,48 @@
 make_data = False
 
 
 class analysis_testcase(unittest.TestCase):
     @classmethod
     def setUpClass(self):
         dat = tv.dat.copy(deep=True)
-        self.dat = rotate2(dat, 'earth', inplace=False)
-        self.tdat = avm.turbulence_statistics(
-            self.dat, n_bin=20.0, fs=self.dat.fs)
+        self.dat = rotate2(dat, "earth", inplace=False)
+        self.tdat = avm.turbulence_statistics(self.dat, n_bin=20.0, fs=self.dat.fs)
 
         short = xr.Dataset()
-        short['u'] = self.tdat.velds.u
-        short['v'] = self.tdat.velds.v
-        short['w'] = self.tdat.velds.w
-        short['U'] = self.tdat.velds.U
-        short['U_mag'] = self.tdat.velds.U_mag
-        short['U_dir'] = self.tdat.velds.U_dir
+        short["u"] = self.tdat.velds.u
+        short["v"] = self.tdat.velds.v
+        short["w"] = self.tdat.velds.w
+        short["U"] = self.tdat.velds.U
+        short["U_mag"] = self.tdat.velds.U_mag
+        short["U_dir"] = self.tdat.velds.U_dir
         short["upup_"] = self.tdat.velds.upup_
         short["vpvp_"] = self.tdat.velds.vpvp_
         short["wpwp_"] = self.tdat.velds.wpwp_
         short["upvp_"] = self.tdat.velds.upvp_
         short["upwp_"] = self.tdat.velds.upwp_
         short["vpwp_"] = self.tdat.velds.vpwp_
-        short['tke'] = self.tdat.velds.tke
-        short['I'] = self.tdat.velds.I
-        short['E_coh'] = self.tdat.velds.E_coh
-        short['I_tke'] = self.tdat.velds.I_tke
+        short["tke"] = self.tdat.velds.tke
+        short["I"] = self.tdat.velds.I
+        short["E_coh"] = self.tdat.velds.E_coh
+        short["I_tke"] = self.tdat.velds.I_tke
         self.short = short
 
     @classmethod
     def tearDownClass(self):
         pass
 
     def test_shortcuts(self):
         ds = self.short.copy(deep=True)
         if make_data:
-            save(ds, 'vector_data01_u.nc')
+            save(ds, "vector_data01_u.nc")
             return
 
-        assert_allclose(ds, load('vector_data01_u.nc'), atol=1e-6)
+        assert_allclose(ds, load("vector_data01_u.nc"), atol=1e-6)
 
     def test_save_complex_data(self):
         # netcdf4 cannot natively handle complex values
         # This test is a sanity check that ensures this code's
         # workaround functions
         ds_save = self.short.copy(deep=True)
-        save(ds_save, 'test_save.nc')
-        assert os.path.exists(rfnm('test_save.nc'))
+        save(ds_save, "test_save.nc")
+        assert os.path.exists(rfnm("test_save.nc"))
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_time.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_time.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,19 +16,20 @@
         dt1 = time.dt642date(td.time[0])
         dt_off = time.epoch2date(time.dt642epoch(td.time), offset_hr=-7)
         t_str = time.epoch2date(time.dt642epoch(td.time), to_str=True)
 
         assert_equal(dt[0], datetime(2012, 6, 12, 12, 0, 2, 687283))
         assert_equal(dt1, [datetime(2012, 6, 12, 12, 0, 2, 687283)])
         assert_equal(dt_off[0], datetime(2012, 6, 12, 5, 0, 2, 687283))
-        assert_equal(t_str[0], '2012-06-12 12:00:02.687283')
+        assert_equal(t_str[0], "2012-06-12 12:00:02.687283")
 
         # Validated based on data in ad2cp.index file
-        assert_equal(time.dt642date(dat_sig.time[0])[0],
-                     datetime(2017, 7, 24, 17, 0, 0, 63500))
+        assert_equal(
+            time.dt642date(dat_sig.time[0])[0], datetime(2017, 7, 24, 17, 0, 0, 63500)
+        )
         # This should always be true
         assert_equal(time.epoch2date([0])[0], datetime(1970, 1, 1, 0, 0))
 
     def test_datetime(self):
         td = trv.dat_imu.copy(deep=True)
 
         dt = time.dt642date(td.time)
@@ -44,9 +45,9 @@
         dt2 = time.matlab2date(dn)
         epoch = np.array(time.date2epoch(dt2))
 
         assert_allclose(time.dt642epoch(td.time.values), epoch, atol=1e-6)
         assert_equal(dn[0], 735032.5000311028)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/dolfyn/test_vs_nortek.py` & `mhkit-0.8.0/mhkit/tests/dolfyn/test_vs_nortek.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,112 +10,111 @@
 Testing against velocity and bottom-track velocity data in Nortek mat files
 exported from Nortek SignatureDeployment software.
 
 """
 
 
 def load_nortek_matfile(filename):
-    data = sio.loadmat(filename,
-                       struct_as_record=False,
-                       squeeze_me=True)
-    d = data['Data']
+    data = sio.loadmat(filename, struct_as_record=False, squeeze_me=True)
+    d = data["Data"]
     # print(d._fieldnames)
-    burst = 'Burst'
-    bt = 'BottomTrack'
+    burst = "Burst"
+    bt = "BottomTrack"
 
-    beam = ['_VelBeam1', '_VelBeam2', '_VelBeam3', '_VelBeam4']
-    b5 = 'IBurst_VelBeam5'
-    inst = ['_VelX', '_VelY', '_VelZ1', '_VelZ2']
-    earth = ['_VelEast', '_VelNorth', '_VelUp1', '_VelUp2']
-    axis = {'beam': beam, 'inst': inst, 'earth': earth}
-    AHRS = 'Burst_AHRSRotationMatrix'  # , 'IBurst_AHRSRotationMatrix']
+    beam = ["_VelBeam1", "_VelBeam2", "_VelBeam3", "_VelBeam4"]
+    b5 = "IBurst_VelBeam5"
+    inst = ["_VelX", "_VelY", "_VelZ1", "_VelZ2"]
+    earth = ["_VelEast", "_VelNorth", "_VelUp1", "_VelUp2"]
+    axis = {"beam": beam, "inst": inst, "earth": earth}
+    AHRS = "Burst_AHRSRotationMatrix"  # , 'IBurst_AHRSRotationMatrix']
 
-    vel = {'beam': {}, 'inst': {}, 'earth': {}}
+    vel = {"beam": {}, "inst": {}, "earth": {}}
     for ky in vel.keys():
         for i in range(len(axis[ky])):
-            vel[ky][i] = np.transpose(getattr(d, burst+axis[ky][i]))
-        vel[ky] = np.stack((vel[ky][0], vel[ky][1],
-                            vel[ky][2], vel[ky][3]), axis=0)
+            vel[ky][i] = np.transpose(getattr(d, burst + axis[ky][i]))
+        vel[ky] = np.stack((vel[ky][0], vel[ky][1], vel[ky][2], vel[ky][3]), axis=0)
 
     if AHRS in d._fieldnames:
-        vel['omat'] = np.transpose(getattr(d, AHRS))
+        vel["omat"] = np.transpose(getattr(d, AHRS))
 
     if b5 in d._fieldnames:
-        vel['b5'] = np.transpose(getattr(d, b5))
-        #vel['omat5'] = getattr(d, AHRS[1])
+        vel["b5"] = np.transpose(getattr(d, b5))
+        # vel['omat5'] = getattr(d, AHRS[1])
 
-    if bt+beam[0] in d._fieldnames:
-        vel_bt = {'beam': {}, 'inst': {}, 'earth': {}}
+    if bt + beam[0] in d._fieldnames:
+        vel_bt = {"beam": {}, "inst": {}, "earth": {}}
         for ky in vel_bt.keys():
             for i in range(len(axis[ky])):
-                vel_bt[ky][i] = np.transpose(getattr(d, bt+axis[ky][i]))
-            vel_bt[ky] = np.stack((vel_bt[ky][0], vel_bt[ky][1],
-                                   vel_bt[ky][2], vel_bt[ky][3]), axis=0)
+                vel_bt[ky][i] = np.transpose(getattr(d, bt + axis[ky][i]))
+            vel_bt[ky] = np.stack(
+                (vel_bt[ky][0], vel_bt[ky][1], vel_bt[ky][2], vel_bt[ky][3]), axis=0
+            )
 
         return vel, vel_bt
     else:
         return vel
 
 
 def rotate(axis):
     # BenchFile01.ad2cp
     td_sig = rotate2(tr.dat_sig, axis, inplace=False)
     # Sig1000_IMU.ad2cp no userdata
     td_sig_i = rotate2(tr.dat_sig_i, axis, inplace=False)
     # VelEchoBT01.ad2cp
-    td_sig_ieb = rotate2(tr.dat_sig_ieb, axis,
-                         inplace=False)
+    td_sig_ieb = rotate2(tr.dat_sig_ieb, axis, inplace=False)
     # Sig500_Echo.ad2cp
-    td_sig_ie = rotate2(tr.dat_sig_ie, axis,
-                        inplace=False)
+    td_sig_ie = rotate2(tr.dat_sig_ie, axis, inplace=False)
 
-    td_sig_vel = load_nortek_matfile(base.rfnm('BenchFile01.mat'))
-    td_sig_i_vel = load_nortek_matfile(base.rfnm('Sig1000_IMU.mat'))
-    td_sig_ieb_vel, vel_bt = load_nortek_matfile(base.rfnm('VelEchoBT01.mat'))
-    td_sig_ie_vel = load_nortek_matfile(base.rfnm('Sig500_Echo.mat'))
+    td_sig_vel = load_nortek_matfile(base.rfnm("BenchFile01.mat"))
+    td_sig_i_vel = load_nortek_matfile(base.rfnm("Sig1000_IMU.mat"))
+    td_sig_ieb_vel, vel_bt = load_nortek_matfile(base.rfnm("VelEchoBT01.mat"))
+    td_sig_ie_vel = load_nortek_matfile(base.rfnm("Sig500_Echo.mat"))
 
     nens = 100
     # ARHS inst2earth orientation matrix check
     # Checks the 1,1 element because the nortek orientmat's shape is [9,:] as
     # opposed to [3,3,:]
-    if axis == 'inst':
-        assert_allclose(td_sig_i.orientmat[0][0].values,
-                        td_sig_i_vel['omat'][0, :nens], atol=1e-7)
-        assert_allclose(td_sig_ieb.orientmat[0][0].values,
-                        td_sig_ieb_vel['omat'][0, :][..., :nens], atol=1e-7)
+    if axis == "inst":
+        assert_allclose(
+            td_sig_i.orientmat[0][0].values, td_sig_i_vel["omat"][0, :nens], atol=1e-7
+        )
+        assert_allclose(
+            td_sig_ieb.orientmat[0][0].values,
+            td_sig_ieb_vel["omat"][0, :][..., :nens],
+            atol=1e-7,
+        )
 
     # 4-beam velocity
     assert_allclose(td_sig.vel.values, td_sig_vel[axis][..., :nens], atol=1e-5)
-    assert_allclose(td_sig_i.vel.values,
-                    td_sig_i_vel[axis][..., :nens], atol=5e-3)
-    assert_allclose(td_sig_ieb.vel.values,
-                    td_sig_ieb_vel[axis][..., :nens], atol=5e-3)
-    assert_allclose(td_sig_ie.vel.values,
-                    td_sig_ie_vel[axis][..., :nens], atol=1e-5)
+    assert_allclose(td_sig_i.vel.values, td_sig_i_vel[axis][..., :nens], atol=5e-3)
+    assert_allclose(td_sig_ieb.vel.values, td_sig_ieb_vel[axis][..., :nens], atol=5e-3)
+    assert_allclose(td_sig_ie.vel.values, td_sig_ie_vel[axis][..., :nens], atol=1e-5)
 
     # 5th-beam velocity
-    if axis == 'beam':
-        assert_allclose(td_sig_i.vel_b5.values,
-                        td_sig_i_vel['b5'][..., :nens], atol=1e-5)
-        assert_allclose(td_sig_ieb.vel_b5.values,
-                        td_sig_ieb_vel['b5'][..., :nens], atol=1e-5)
-        assert_allclose(td_sig_ie.vel_b5.values,
-                        td_sig_ie_vel['b5'][..., :nens], atol=1e-5)
+    if axis == "beam":
+        assert_allclose(
+            td_sig_i.vel_b5.values, td_sig_i_vel["b5"][..., :nens], atol=1e-5
+        )
+        assert_allclose(
+            td_sig_ieb.vel_b5.values, td_sig_ieb_vel["b5"][..., :nens], atol=1e-5
+        )
+        assert_allclose(
+            td_sig_ie.vel_b5.values, td_sig_ie_vel["b5"][..., :nens], atol=1e-5
+        )
 
     # bottom-track
-    assert_allclose(td_sig_ieb.vel_bt.values,
-                    vel_bt[axis][..., :nens], atol=5e-3)
+    assert_allclose(td_sig_ieb.vel_bt.values, vel_bt[axis][..., :nens], atol=5e-3)
 
 
 class nortek_testcase(unittest.TestCase):
     def test_rotate2_beam(self):
-        rotate('beam')
+        rotate("beam")
 
     def test_rotate2_inst(self):
-        rotate('inst')
+        rotate("inst")
 
     def test_rotate2_earth(self):
-        rotate('earth')
+        rotate("earth")
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/power/test_power.py` & `mhkit-0.8.0/mhkit/tests/power/test_power.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,112 +1,186 @@
-
-from os.path import abspath, dirname, join, isfile, normpath, relpath
+from os.path import abspath, dirname, join, normpath, relpath
 import mhkit.power as power
 import pandas as pd
+import xarray as xr
 import numpy as np
 import unittest
-import os
 
 
 testdir = dirname(abspath(__file__))
-datadir = normpath(join(testdir,relpath('../../../examples/data/power')))
+datadir = normpath(join(testdir, relpath("../../../examples/data/power")))
 
-class TestDevice(unittest.TestCase):
 
+class TestDevice(unittest.TestCase):
     @classmethod
     def setUpClass(self):
         self.t = 600
         fs = 1000
-        sample_frequency = 1000 # = fs 
-        self.samples = np.linspace(0, self.t, int(fs*self.t), endpoint=False)
+        self.samples = np.linspace(0, self.t, int(fs * self.t), endpoint=False)
         self.frequency = 60
-        self.freq_array = np.ones(len(self.samples))*60
-        harmonics_int = np.arange(0,60*60,5)
+        self.freq_array = np.ones(len(self.samples)) * 60
+        harmonics_int = np.arange(0, 60 * 60, 5)
         self.harmonics_int = harmonics_int
-        self.interharmonic = np.zeros(len(harmonics_int)) #since this is an idealized sin wave, the interharmonics should be zero
+        # since this is an idealized sin wave, the interharmonics should be zero
+        self.interharmonic = np.zeros(len(harmonics_int))
         self.harmonics_vals = np.zeros(len(harmonics_int))
-        self.harmonics_vals[12]= 1.0  #setting 60th harmonic to amplitude of the signal
+        # setting 60th harmonic to amplitude of the signal
+        self.harmonics_vals[12] = 1.0
+
+        # harmonic groups should be equal to every 12th harmonic in this idealized example
+        self.harmonic_groups = self.harmonics_vals[0::12]
+        self.thcd = (
+            0.0  # Since this is an idealized sin wave, there should be no distortion
+        )
 
-        self.harmonic_groups = self.harmonics_vals[0::12] #harmonic groups should be equal to every 12th harmonic in this idealized example
-        self.thcd = 0.0 #Since this is an idealized sin wave, there should be no distortion 
-        
         self.signal = np.sin(2 * np.pi * self.frequency * self.samples)
-        
-        self.current_data = [[1,2,3],[4,5,6],[7,8,9],[10,11,12]]
-        self.voltage_data = [[1,5,9],[2,6,10],[3,7,11],[4,8,12]]
+
+        self.current_data = np.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
+        self.voltage_data = np.asarray([[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]])
 
     @classmethod
     def tearDownClass(self):
         pass
 
-    def test_harmonics_sine_wave(self):
-
+    def test_harmonics_sine_wave_pandas(self):
         current = pd.Series(self.signal, index=self.samples)
         harmonics = power.quality.harmonics(current, 1000, self.frequency)
 
-        for i,j in zip(harmonics.values, self.harmonics_vals):
-            self.assertAlmostEqual(i[0], j,1)
+        for i, j in zip(harmonics["data"].values, self.harmonics_vals):
+            self.assertAlmostEqual(i, j, 1)
 
-    def test_harmonic_subgroup_sine_wave(self):
-        current = pd.Series(self.signal, index=self.samples)
-        harmonics = pd.DataFrame(self.harmonics_vals, 
-                          index= self.harmonics_int)
-        hsg = power.quality.harmonic_subgroups(harmonics,self.frequency)
-        for i,j in zip(hsg.values,self.harmonic_groups):
-            self.assertAlmostEqual(i[0], j,1)
+    def test_harmonics_sine_wave_xarray(self):
+        current = xr.DataArray(
+            data=self.signal, dims="index", coords={"index": self.samples}
+        )
+        harmonics = power.quality.harmonics(current, 1000, self.frequency)
 
-    def test_TCHD_sine_wave(self):
-        current = pd.Series(self.signal, index=self.samples)
-        harmonics = pd.DataFrame(self.harmonics_vals, 
-                         index= self.harmonics_int)
-        hsg = power.quality.harmonic_subgroups(harmonics,self.frequency)
+        for i, j in zip(harmonics["data"].values, self.harmonics_vals):
+            self.assertAlmostEqual(i, j, 1)
 
-        TCHD = power.quality.total_harmonic_current_distortion(hsg,18.8) # had to just put a random rated current in here
-        self.assertAlmostEqual(TCHD.values[0],self.thcd)
+    def test_harmonic_subgroup_sine_wave_pandas(self):
+        harmonics = pd.DataFrame(self.harmonics_vals, index=self.harmonics_int)
+        hsg = power.quality.harmonic_subgroups(harmonics, self.frequency)
+
+        for i, j in zip(hsg.values, self.harmonic_groups):
+            self.assertAlmostEqual(i[0], j, 1)
+
+    def test_harmonic_subgroup_sine_wave_xarray(self):
+        harmonics = xr.Dataset(
+            data_vars={"harmonics": (["index"], self.harmonics_vals)},
+            coords={"index": self.harmonics_int},
+        )
+        hsg = power.quality.harmonic_subgroups(harmonics, self.frequency)
+
+        for i, j in zip(hsg.values, self.harmonic_groups):
+            self.assertAlmostEqual(i[0], j, 1)
+
+    def test_TCHD_sine_wave_pandas(self):
+        harmonics = pd.DataFrame(self.harmonics_vals, index=self.harmonics_int)
+        hsg = power.quality.harmonic_subgroups(harmonics, self.frequency)
+        TCHD = power.quality.total_harmonic_current_distortion(hsg)
+
+        self.assertAlmostEqual(TCHD.values[0], self.thcd)
+
+    def test_TCHD_sine_wave_xarray(self):
+        harmonics = xr.Dataset(
+            data_vars={"harmonics": (["index"], self.harmonics_vals)},
+            coords={"index": self.harmonics_int},
+        )
+        hsg = power.quality.harmonic_subgroups(harmonics, self.frequency)
+        TCHD = power.quality.total_harmonic_current_distortion(hsg)
+
+        self.assertAlmostEqual(TCHD.values[0], self.thcd)
+
+    def test_interharmonics_sine_wave_pandas(self):
+        harmonics = pd.DataFrame(self.harmonics_vals, index=self.harmonics_int)
+        inter_harmonics = power.quality.interharmonics(harmonics, self.frequency)
+
+        for i, j in zip(inter_harmonics.values, self.interharmonic):
+            self.assertAlmostEqual(i[0], j, 1)
+
+    def test_interharmonics_sine_wave_xarray(self):
+        harmonics = xr.Dataset(
+            data_vars={"harmonics": (["index"], self.harmonics_vals)},
+            coords={"index": self.harmonics_int},
+        )
+        inter_harmonics = power.quality.interharmonics(harmonics, self.frequency)
 
-    def test_interharmonics_sine_wave(self):
-        current = pd.Series(self.signal, index=self.samples)
-        harmonics = pd.DataFrame(self.harmonics_vals, 
-                         index= self.harmonics_int)
+        for i, j in zip(inter_harmonics.values, self.interharmonic):
+            self.assertAlmostEqual(i[0], j, 1)
 
-        inter_harmonics = power.quality.interharmonics(harmonics,self.frequency)
+    def test_instfreq_pandas(self):
+        um = pd.Series(self.signal, index=self.samples)
 
-        for i,j in zip(inter_harmonics.values, self.interharmonic):
-            self.assertAlmostEqual(i[0], j,1)
+        freq = power.characteristics.instantaneous_frequency(um)
+        for i in freq.values:
+            self.assertAlmostEqual(i[0], self.frequency, 1)
 
+    def test_instfreq_xarray(self):
+        um = pd.Series(self.signal, index=self.samples)
+        um = um.to_xarray()
 
-    def test_instfreq(self):
-        um = pd.Series(self.signal,index = self.samples)
-        
         freq = power.characteristics.instantaneous_frequency(um)
         for i in freq.values:
-            self.assertAlmostEqual(i[0], self.frequency,1)
+            self.assertAlmostEqual(i[0], self.frequency, 1)
+
+    def test_dc_power_pandas(self):
+        current = pd.DataFrame(self.current_data, columns=["A1", "A2", "A3"])
+        voltage = pd.DataFrame(self.voltage_data, columns=["V1", "V2", "V3"])
 
-    def test_dc_power_DataFrame(self):
-        current = pd.DataFrame(self.current_data, columns=['A1', 'A2', 'A3'])
-        voltage = pd.DataFrame(self.voltage_data, columns=['V1', 'V2', 'V3'])
         P = power.characteristics.dc_power(voltage, current)
-        self.assertEqual(P.sum()['Gross'], (voltage.values * current.values).sum())
-        
-    def test_dc_power_Series(self):
-        current = pd.DataFrame(self.current_data, columns=['A1', 'A2', 'A3'])
-        voltage = pd.DataFrame(self.voltage_data, columns=['V1', 'V2', 'V3'])
-        P = power.characteristics.dc_power(voltage['V1'], current['A1'])
-        self.assertEqual(P.sum()['Gross'], sum( voltage['V1'] * current['A1']))
-
-    def test_ac_power_three_phase(self):
-        current = pd.DataFrame(self.current_data, columns=['A1', 'A2', 'A3'])
-        voltage = pd.DataFrame(self.voltage_data, columns=['V1', 'V2', 'V3'])
-        
-        P1 = power.characteristics.ac_power_three_phase( voltage, current, 1, False)
+        P_test = (self.current_data * self.voltage_data).sum()
+        self.assertEqual(P.sum()["Gross"], P_test)
+
+        P = power.characteristics.dc_power(voltage["V1"], current["A1"])
+        P_test = (self.current_data[:, 0] * self.voltage_data[:, 0]).sum()
+        self.assertEqual(P.sum()["Gross"], P_test)
+
+    def test_dc_power_xarray(self):
+        current = pd.DataFrame(self.current_data, columns=["A1", "A2", "A3"])
+        voltage = pd.DataFrame(self.voltage_data, columns=["V1", "V2", "V3"])
+        current = current.to_xarray()
+        voltage = voltage.to_xarray()
+
+        P = power.characteristics.dc_power(voltage, current)
+        P_test = (self.current_data * self.voltage_data).sum()
+        self.assertEqual(P.sum()["Gross"], P_test)
+
+        P = power.characteristics.dc_power(voltage["V1"], current["A1"])
+        P_test = (self.current_data[:, 0] * self.voltage_data[:, 0]).sum()
+        self.assertEqual(P.sum()["Gross"], P_test)
+
+    def test_ac_power_three_phase_pandas(self):
+        current = pd.DataFrame(self.current_data, columns=["A1", "A2", "A3"])
+        voltage = pd.DataFrame(self.voltage_data, columns=["V1", "V2", "V3"])
+
+        P1 = power.characteristics.ac_power_three_phase(voltage, current, 1, False)
+        P1b = power.characteristics.ac_power_three_phase(voltage, current, 0.5, False)
+        P2 = power.characteristics.ac_power_three_phase(voltage, current, 1, True)
+        P2b = power.characteristics.ac_power_three_phase(voltage, current, 0.5, True)
+
+        P_test = (self.current_data * self.voltage_data).sum()
+        self.assertEqual(P1.sum().iloc[0], P_test)
+        self.assertEqual(P1b.sum().iloc[0], P_test / 2)
+        self.assertAlmostEqual(P2.sum().iloc[0], P_test * np.sqrt(3), 2)
+        self.assertAlmostEqual(P2b.sum().iloc[0], P_test * np.sqrt(3) / 2, 2)
+
+    def test_ac_power_three_phase_xarray(self):
+        current = pd.DataFrame(self.current_data, columns=["A1", "A2", "A3"])
+        voltage = pd.DataFrame(self.voltage_data, columns=["V1", "V2", "V3"])
+        current = current.to_xarray()
+        voltage = voltage.to_xarray()
+
+        P1 = power.characteristics.ac_power_three_phase(voltage, current, 1, False)
         P1b = power.characteristics.ac_power_three_phase(voltage, current, 0.5, False)
-        P2 = power.characteristics.ac_power_three_phase( voltage, current,1, True)
+        P2 = power.characteristics.ac_power_three_phase(voltage, current, 1, True)
         P2b = power.characteristics.ac_power_three_phase(voltage, current, 0.5, True)
-        
-        self.assertEqual(P1.sum()[0], 584)
-        self.assertEqual(P1b.sum()[0], 584/2)
-        self.assertAlmostEqual(P2.sum()[0], 1011.518, 2)
-        self.assertAlmostEqual(P2b.sum()[0], 1011.518/2, 2)
-
-if __name__ == '__main__':
-    unittest.main() 
-        
+
+        P_test = (self.current_data * self.voltage_data).sum()
+        self.assertEqual(P1.sum().iloc[0], P_test)
+        self.assertEqual(P1b.sum().iloc[0], P_test / 2)
+        self.assertAlmostEqual(P2.sum().iloc[0], P_test * np.sqrt(3), 2)
+        self.assertAlmostEqual(P2b.sum().iloc[0], P_test * np.sqrt(3) / 2, 2)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/river/test_io.py` & `mhkit-0.8.0/mhkit/tests/river/test_io_d3d.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,227 +1,302 @@
-from os.path import abspath, dirname, join, isfile, normpath, relpath
+from os.path import abspath, dirname, join, normpath
 from numpy.testing import assert_array_almost_equal
-from pandas.testing import assert_frame_equal
 import scipy.interpolate as interp
-import matplotlib.pylab as plt
 import mhkit.river as river
+import mhkit.tidal as tidal
 import pandas as pd
+import xarray as xr
 import numpy as np
 import unittest
 import netCDF4
 import os
 
 
 testdir = dirname(abspath(__file__))
-plotdir = join(testdir, 'plots')
+plotdir = join(testdir, "plots")
 isdir = os.path.isdir(plotdir)
-if not isdir: os.mkdir(plotdir)
-datadir = normpath(join(testdir,'..','..','..','examples','data','river'))
+if not isdir:
+    os.mkdir(plotdir)
+datadir = normpath(join(testdir, "..", "..", "..", "examples", "data", "river"))
 
 
 class TestIO(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
-        d3ddatadir = normpath(join(datadir,'d3d'))
-        
-        filename= 'turbineTest_map.nc'
-        self.d3d_flume_data = netCDF4.Dataset(join(d3ddatadir,filename))
-        
+        d3ddatadir = normpath(join(datadir, "d3d"))
+
+        filename = "turbineTest_map.nc"
+        self.d3d_flume_data = netCDF4.Dataset(join(d3ddatadir, filename))
+
     @classmethod
     def tearDownClass(self):
         pass
-    
-    def test_load_usgs_data_instantaneous(self):
-        file_name = join(datadir, 'USGS_08313000_Jan2019_instantaneous.json')
-        data = river.io.usgs.read_usgs_file(file_name)
-        
-        self.assertEqual(data.columns, ['Discharge, cubic feet per second'])
-        self.assertEqual(data.shape, (2972, 1)) # 4 data points are missing
-        
-    def test_load_usgs_data_daily(self):
-        file_name = join(datadir, 'USGS_08313000_Jan2019_daily.json')
-        data = river.io.usgs.read_usgs_file(file_name)
-
-        expected_index = pd.date_range('2019-01-01', '2019-01-31', freq='D')
-        self.assertEqual(data.columns, ['Discharge, cubic feet per second'])
-        self.assertEqual((data.index == expected_index.tz_localize('UTC')).all(), True)
-        self.assertEqual(data.shape, (31, 1))
-
-
-    def test_request_usgs_data_daily(self):
-        data=river.io.usgs.request_usgs_data(station="15515500",
-                            parameter='00060',
-                            start_date='2009-08-01',
-                            end_date='2009-08-10',
-                            data_type='Daily')
-        self.assertEqual(data.columns, ['Discharge, cubic feet per second'])
-        self.assertEqual(data.shape, (10, 1))
-    
-   
-    def test_request_usgs_data_instant(self):
-        data=river.io.usgs.request_usgs_data(station="15515500",
-                            parameter='00060',
-                            start_date='2009-08-01',
-                            end_date='2009-08-10',
-                            data_type='Instantaneous')
-        self.assertEqual(data.columns, ['Discharge, cubic feet per second'])
-        # Every 15 minutes or 4 times per hour
-        self.assertEqual(data.shape, (10*24*4, 1))
 
-    def test_get_all_time(self): 
-        data= self.d3d_flume_data
+    def test_get_all_time(self):
+        data = self.d3d_flume_data
         seconds_run = river.io.d3d.get_all_time(data)
-        seconds_run_expected= np.ndarray(shape=(5,), buffer= np.array([0, 60, 120, 180, 240]), dtype=int)
+        seconds_run_expected = np.ndarray(
+            shape=(5,), buffer=np.array([0, 60, 120, 180, 240]), dtype=int
+        )
         np.testing.assert_array_equal(seconds_run, seconds_run_expected)
-        
-    def test_convert_time(self): 
-        data= self.d3d_flume_data
+
+    def test_convert_time(self):
+        data = self.d3d_flume_data
         time_index = 2
-        seconds_run = river.io.d3d.index_to_seconds(data, time_index = time_index)
-        seconds_run_expected = 120 
+        seconds_run = river.io.d3d.index_to_seconds(data, time_index=time_index)
+        seconds_run_expected = 120
         self.assertEqual(seconds_run, seconds_run_expected)
         seconds_run = 60
-        time_index= river.io.d3d.seconds_to_index(data, seconds_run = seconds_run)
+        time_index = river.io.d3d.seconds_to_index(data, seconds_run=seconds_run)
         time_index_expected = 1
         self.assertEqual(time_index, time_index_expected)
         seconds_run = 62
-        time_index= river.io.d3d.seconds_to_index(data, seconds_run = seconds_run)
+        time_index = river.io.d3d.seconds_to_index(data, seconds_run=seconds_run)
         time_index_expected = 1
-        output_expected= f'ERROR: invalid seconds_run. Closest seconds_run found {time_index_expected}'
+        output_expected = f"ERROR: invalid seconds_run. Closest seconds_run found {time_index_expected}"
         self.assertWarns(UserWarning)
 
-    def test_layer_data(self): 
-        data=self.d3d_flume_data
-        variable = ['ucx', 's1']
+    def test_convert_time_from_tidal(self):
+        """
+        Test the conversion of time from using tidal import of d3d
+        """
+        data = self.d3d_flume_data
+        time_index = 2
+        seconds_run = tidal.io.d3d.index_to_seconds(data, time_index=time_index)
+        seconds_run_expected = 120
+        self.assertEqual(seconds_run, seconds_run_expected)
+
+    def test_layer_data(self):
+        data = self.d3d_flume_data
+        variable = ["ucx", "s1"]
         for var in variable:
-            layer=2 
-            time_index= 3
-            layer_data= river.io.d3d.get_layer_data(data, var, layer, time_index)
+            layer = 2
+            time_index = 3
+            layer_data = river.io.d3d.get_layer_data(data, var, layer, time_index)
             layer_compare = 2
-            time_index_compare= 4
-            layer_data_expected= river.io.d3d.get_layer_data(data,
-                                                            var, layer_compare,
-                                                            time_index_compare)
-           
-            assert_array_almost_equal(layer_data.x,layer_data_expected.x, decimal = 2)
-            assert_array_almost_equal(layer_data.y,layer_data_expected.y, decimal = 2)
-            assert_array_almost_equal(layer_data.v,layer_data_expected.v, decimal= 2)
-        
-
-        
-    def test_create_points(self):
-        x=np.linspace(1, 3, num= 3)
-        y=np.linspace(1, 3, num= 3)
-        z=1 
-        points= river.io.d3d.create_points(x,y,z)
-        x=[1,2,3,1,2,3,1,2,3]
-        y=[1,1,1,2,2,2,3,3,3]
-        z=[1,1,1,1,1,1,1,1,1]
-        points_array= np.array([ [x_i, y_i, z_i] for x_i, y_i, z_i in zip(x, y, z)]) 
-        points_expected= pd.DataFrame(points_array, columns=('x','y','z'))
-        assert_array_almost_equal(points, points_expected,decimal = 2)
-        
-        x=np.linspace(1, 3, num= 3)
-        y=2
-        z=1 
-        points= river.io.d3d.create_points(x,y,z)
-        x=[1,2,3]
-        y=[2,2,2]
-        z=[1,1,1]
-        points_array= np.array([ [x_i, y_i, z_i] for x_i, y_i, z_i in zip(x, y, z)]) 
-        points_expected= pd.DataFrame(points_array, columns=('x','y','z'))
-        assert_array_almost_equal(points, points_expected,decimal = 2)  
-        
-        x=3
-        y=2
-        z=1 
-        points= river.io.d3d.create_points(x,y,z)
-        output_expected='Can provide at most two arrays'
-        self.assertWarns(UserWarning)
-        
+            time_index_compare = 4
+            layer_data_expected = river.io.d3d.get_layer_data(
+                data, var, layer_compare, time_index_compare
+            )
+
+            assert_array_almost_equal(layer_data.x, layer_data_expected.x, decimal=2)
+            assert_array_almost_equal(layer_data.y, layer_data_expected.y, decimal=2)
+            assert_array_almost_equal(layer_data.v, layer_data_expected.v, decimal=2)
+
+    def test_create_points_three_points(self):
+        """
+        Test the scenario where all three inputs (x, y, z) are points.
+        """
+        x, y, z = 1, 2, 3
+
+        expected = pd.DataFrame([[x, y, z]], columns=["x", "y", "waterdepth"])
+
+        points = river.io.d3d.create_points(x, y, z)
+        assert_array_almost_equal(points.values, expected.values, decimal=2)
+
+    def test_create_points_invalid_input(self):
+        """
+        Test scenarios where invalid inputs are provided to the function.
+        """
+        with self.assertRaises(TypeError):
+            river.io.d3d.create_points("invalid", 2, 3)
+
+    def test_create_points_two_arrays_one_point(self):
+        """
+        Test with two arrays and one point.
+        """
+        result = river.io.d3d.create_points(np.array([1, 2]), np.array([3]), 4)
+        expected = pd.DataFrame({"x": [1, 2], "y": [3, 3], "waterdepth": [4, 4]})
+        pd.testing.assert_frame_equal(
+            result,
+            expected,
+            check_dtype=False,
+            check_names=False,
+            check_index_type=False,
+        )
+
+    def test_create_points_user_made_two_arrays_one_point(self):
+        """
+        Test the scenario where all three inputs (x, y, z) are created from
+        points.
+        """
+        x, y, z = np.linspace(1, 3, num=3), np.linspace(1, 3, num=3), 1
+
+        # Adjust the order of the expected values
+        expected_data = [
+            [i, j, 1] for j in y for i in x
+        ]  # Notice the swapped loop order
+        expected = pd.DataFrame(expected_data, columns=["x", "y", "waterdepth"])
+
+        points = river.io.d3d.create_points(x, y, z)
+        assert_array_almost_equal(points.values, expected.values, decimal=2)
+
+    def test_create_points_mismatched_array_lengths(self):
+        """
+        Test the scenario where x and y are arrays of different lengths.
+        """
+        with self.assertRaises(ValueError):
+            river.io.d3d.create_points(
+                np.array([1, 2, 3]), np.array([1, 2]), np.array([3, 4])
+            )
+
+    def test_create_pointsempty_arrays(self):
+        """
+        Test the scenario where provided arrays are empty.
+        """
+        with self.assertRaises(ValueError):
+            river.io.d3d.create_points([], [], [])
+
+    def test_create_points_mixed_data_types(self):
+        """
+        Test a combination of np.ndarray, pd.Series, and xr.DataArray.
+        """
+        x = np.array([1, 2])
+        y = pd.Series([3, 4])
+        z = xr.DataArray([5, 6])
+        result = river.io.d3d.create_points(x, y, z)
+        expected = pd.DataFrame(
+            {"x": [1, 2, 1, 2], "y": [3, 4, 3, 4], "waterdepth": [5, 5, 6, 6]}
+        )
+
+        pd.testing.assert_frame_equal(
+            result,
+            expected,
+            check_dtype=False,
+            check_names=False,
+            check_index_type=False,
+        )
+
+    def test_create_points_array_like_inputs(self):
+        """
+        Test array-like inputs such as lists.
+        """
+        result = river.io.d3d.create_points([1, 2], [3, 4], [5, 6])
+        expected = pd.DataFrame(
+            {"x": [1, 2, 1, 2], "y": [3, 4, 3, 4], "waterdepth": [5, 5, 6, 6]}
+        )
+
+        pd.testing.assert_frame_equal(
+            result,
+            expected,
+            check_dtype=False,
+            check_names=False,
+            check_index_type=False,
+        )
+
     def test_variable_interpolation(self):
-        data=self.d3d_flume_data
-        variables= ['ucx','turkin1']
-        transformes_data= river.io.d3d.variable_interpolation(data, variables, points= 'faces', edges='nearest')
-        self.assertEqual(np.size(transformes_data['ucx']), np.size(transformes_data['turkin1']))
-        transformes_data= river.io.d3d.variable_interpolation(data, variables, points= 'cells', edges='nearest')
-        self.assertEqual(np.size(transformes_data['ucx']), np.size(transformes_data['turkin1']))        
-        x=np.linspace(1, 3, num= 3)
-        y=np.linspace(1, 3, num= 3)
-        waterdepth=1 
-        points= river.io.d3d.create_points(x,y,waterdepth)
-        transformes_data= river.io.d3d.variable_interpolation(data, variables, points= points)
-        self.assertEqual(np.size(transformes_data['ucx']), np.size(transformes_data['turkin1']))
-        
-    def test_get_all_data_points(self): 
-        data=self.d3d_flume_data
-        variable= 'ucx'
-        time_step= 3
+        data = self.d3d_flume_data
+        variables = ["ucx", "turkin1"]
+        transformes_data = river.io.d3d.variable_interpolation(
+            data, variables, points="faces", edges="nearest"
+        )
+        self.assertEqual(
+            np.size(transformes_data["ucx"]), np.size(transformes_data["turkin1"])
+        )
+        transformes_data = river.io.d3d.variable_interpolation(
+            data, variables, points="cells", edges="nearest"
+        )
+        self.assertEqual(
+            np.size(transformes_data["ucx"]), np.size(transformes_data["turkin1"])
+        )
+        x = np.linspace(1, 3, num=3)
+        y = np.linspace(1, 3, num=3)
+        waterdepth = 1
+        points = river.io.d3d.create_points(x, y, waterdepth)
+        transformes_data = river.io.d3d.variable_interpolation(
+            data, variables, points=points
+        )
+        self.assertEqual(
+            np.size(transformes_data["ucx"]), np.size(transformes_data["turkin1"])
+        )
+
+    def test_get_all_data_points(self):
+        data = self.d3d_flume_data
+        variable = "ucx"
+        time_step = 3
         output = river.io.d3d.get_all_data_points(data, variable, time_step)
-        size_output = np.size(output) 
-        time_step_compair=4
-        output_expected= river.io.d3d.get_all_data_points(data, variable, time_step_compair)
-        size_output_expected= np.size(output_expected)
+        size_output = np.size(output)
+        time_step_compair = 4
+        output_expected = river.io.d3d.get_all_data_points(
+            data, variable, time_step_compair
+        )
+        size_output_expected = np.size(output_expected)
         self.assertEqual(size_output, size_output_expected)
- 
-    
-    def test_unorm(self): 
-        x=np.linspace(1, 3, num= 3)
-        y=np.linspace(1, 3, num= 3)
-        z=np.linspace(1, 3, num= 3)
-        unorm = river.io.d3d.unorm(x,y,z)
-        unorm_expected= [np.sqrt(1**2+1**2+1**2),np.sqrt(2**2+2**2+2**2), np.sqrt(3**2+3**2+3**2)]
-        assert_array_almost_equal(unorm, unorm_expected, decimal = 2) 
-    
-    def test_turbulent_intensity(self): 
-        data=self.d3d_flume_data
-        time_index= -1
-        x_test=np.linspace(1, 17, num= 10)
-        y_test=np.linspace(3, 3, num= 10)
-        waterdepth_test=np.linspace(1, 1, num= 10)
-       
-        test_points = np.array([ [x, y, waterdepth] for x, y, waterdepth in zip(x_test, y_test, waterdepth_test)])
-        points= pd.DataFrame(test_points, columns=['x','y','waterdepth'])
-        
-        TI= river.io.d3d.turbulent_intensity(data, points, time_index)
 
-        TI_vars= ['turkin1', 'ucx', 'ucy', 'ucz']
+    def test_unorm(self):
+        x = np.linspace(1, 3, num=3)
+        y = np.linspace(1, 3, num=3)
+        z = np.linspace(1, 3, num=3)
+        unorm = river.io.d3d.unorm(x, y, z)
+        unorm_expected = [
+            np.sqrt(1**2 + 1**2 + 1**2),
+            np.sqrt(2**2 + 2**2 + 2**2),
+            np.sqrt(3**2 + 3**2 + 3**2),
+        ]
+        assert_array_almost_equal(unorm, unorm_expected, decimal=2)
+
+    def test_turbulent_intensity(self):
+        data = self.d3d_flume_data
+        time_index = -1
+        x_test = np.linspace(1, 17, num=10)
+        y_test = np.linspace(3, 3, num=10)
+        waterdepth_test = np.linspace(1, 1, num=10)
+
+        test_points = np.array(
+            [
+                [x, y, waterdepth]
+                for x, y, waterdepth in zip(x_test, y_test, waterdepth_test)
+            ]
+        )
+        points = pd.DataFrame(test_points, columns=["x", "y", "waterdepth"])
+
+        TI = river.io.d3d.turbulent_intensity(data, points, time_index)
+
+        TI_vars = ["turkin1", "ucx", "ucy", "ucz"]
         TI_data_raw = {}
         for var in TI_vars:
-            #get all data
-            var_data_df = river.io.d3d.get_all_data_points(data, var,time_index)           
-            TI_data_raw[var] = var_data_df 
-            TI_data= points.copy(deep=True)
-        
-        for var in TI_vars:    
-            TI_data[var] = interp.griddata(TI_data_raw[var][['x','y','waterdepth']],
-                                TI_data_raw[var][var], points[['x','y','waterdepth']])
-            idx= np.where(np.isnan(TI_data[var]))
-        
+            # get all data
+            var_data_df = river.io.d3d.get_all_data_points(data, var, time_index)
+            TI_data_raw[var] = var_data_df
+            TI_data = points.copy(deep=True)
+
+        for var in TI_vars:
+            TI_data[var] = interp.griddata(
+                TI_data_raw[var][["x", "y", "waterdepth"]],
+                TI_data_raw[var][var],
+                points[["x", "y", "waterdepth"]],
+            )
+            idx = np.where(np.isnan(TI_data[var]))
+
             if len(idx[0]):
-                for i in idx[0]: 
-                    TI_data[var][i]= interp.griddata(TI_data_raw[var][['x','y','waterdepth']], 
-                                TI_data_raw[var][var],
-                                [points['x'][i],points['y'][i], points['waterdepth'][i]],
-                                method='nearest')
-        
-        u_mag=river.io.d3d.unorm(TI_data['ucx'],TI_data['ucy'], TI_data['ucz'])
-        turbulent_intensity_expected= (np.sqrt(2/3*TI_data['turkin1'])/u_mag)*100
-       
-        
-        assert_array_almost_equal(TI.turbulent_intensity, turbulent_intensity_expected, decimal = 2)     
-        
-        TI = river.io.d3d.turbulent_intensity(data, points='faces')
-        TI_size = np.size(TI['turbulent_intensity'])
-        turkin1= river.io.d3d.get_all_data_points(data, 'turkin1',time_index)
-        turkin1_size= np.size(turkin1['turkin1'])
+                for i in idx[0]:
+                    TI_data[var][i] = interp.griddata(
+                        TI_data_raw[var][["x", "y", "waterdepth"]],
+                        TI_data_raw[var][var],
+                        [points["x"][i], points["y"][i], points["waterdepth"][i]],
+                        method="nearest",
+                    )
+
+        u_mag = river.io.d3d.unorm(TI_data["ucx"], TI_data["ucy"], TI_data["ucz"])
+        turbulent_intensity_expected = (
+            np.sqrt(2 / 3 * TI_data["turkin1"]) / u_mag
+        ) * 100
+
+        assert_array_almost_equal(
+            TI.turbulent_intensity, turbulent_intensity_expected, decimal=2
+        )
+
+        TI = river.io.d3d.turbulent_intensity(data, points="faces")
+        TI_size = np.size(TI["turbulent_intensity"])
+        turkin1 = river.io.d3d.get_all_data_points(data, "turkin1", time_index)
+        turkin1_size = np.size(turkin1["turkin1"])
         self.assertEqual(TI_size, turkin1_size)
-        
-        TI = river.io.d3d.turbulent_intensity(data, points='cells')
-        TI_size = np.size(TI['turbulent_intensity'])
-        ucx= river.io.d3d.get_all_data_points(data, 'ucx',time_index)
-        ucx_size= np.size(ucx['ucx'])
+
+        TI = river.io.d3d.turbulent_intensity(data, points="cells")
+        TI_size = np.size(TI["turbulent_intensity"])
+        ucx = river.io.d3d.get_all_data_points(data, "ucx", time_index)
+        ucx_size = np.size(ucx["ucx"])
         self.assertEqual(TI_size, ucx_size)
-if __name__ == '__main__':
-    unittest.main() 
 
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/river/test_performance.py` & `mhkit-0.8.0/mhkit/tests/river/test_performance.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,73 +8,77 @@
 import numpy as np
 import unittest
 import netCDF4
 import os
 
 
 testdir = dirname(abspath(__file__))
-plotdir = join(testdir, 'plots')
+plotdir = join(testdir, "plots")
 isdir = os.path.isdir(plotdir)
-if not isdir: os.mkdir(plotdir)
-datadir = normpath(join(testdir,'..','..','..','examples','data','river'))
+if not isdir:
+    os.mkdir(plotdir)
+datadir = normpath(join(testdir, "..", "..", "..", "examples", "data", "river"))
 
 
 class TestPerformance(unittest.TestCase):
     @classmethod
     def setUpClass(self):
         self.diameter = 1
         self.height = 2
         self.width = 3
-        self.diameters = [1,2,3,4]
+        self.diameters = [1, 2, 3, 4]
 
     @classmethod
     def tearDownClass(self):
         pass
-    
+
     def test_circular(self):
-        eq, ca = river.performance.circular(self.diameter) 
+        eq, ca = river.performance.circular(self.diameter)
         self.assertEqual(eq, self.diameter)
-        self.assertEqual(ca, 0.25*np.pi*self.diameter**2.)
+        self.assertEqual(ca, 0.25 * np.pi * self.diameter**2.0)
 
     def test_ducted(self):
-        eq, ca =river.performance.ducted(self.diameter) 
+        eq, ca = river.performance.ducted(self.diameter)
         self.assertEqual(eq, self.diameter)
-        self.assertEqual(ca, 0.25*np.pi*self.diameter**2.)
-    
+        self.assertEqual(ca, 0.25 * np.pi * self.diameter**2.0)
+
     def test_rectangular(self):
         eq, ca = river.performance.rectangular(self.height, self.width)
         self.assertAlmostEqual(eq, 2.76, places=2)
-        self.assertAlmostEqual(ca, self.height*self.width, places=2)
+        self.assertAlmostEqual(ca, self.height * self.width, places=2)
 
     def test_multiple_circular(self):
         eq, ca = river.performance.multiple_circular(self.diameters)
         self.assertAlmostEqual(eq, 5.48, places=2)
         self.assertAlmostEqual(ca, 23.56, places=2)
 
     def test_tip_speed_ratio(self):
-        rotor_speed = [15,16,17,18] # create array of rotor speeds
-        rotor_diameter = 77 # diameter of rotor for GE 1.5
-        inflow_speed = [13,13,13,13] # array of wind speeds
-        TSR_answer = [4.7,5.0,5.3,5.6]
-        
-        TSR = river.performance.tip_speed_ratio(np.asarray(rotor_speed)/60,rotor_diameter,inflow_speed)
+        rotor_speed = [15, 16, 17, 18]  # create array of rotor speeds
+        rotor_diameter = 77  # diameter of rotor for GE 1.5
+        inflow_speed = [13, 13, 13, 13]  # array of wind speeds
+        TSR_answer = [4.7, 5.0, 5.3, 5.6]
+
+        TSR = river.performance.tip_speed_ratio(
+            np.asarray(rotor_speed) / 60, rotor_diameter, inflow_speed
+        )
 
-        for i,j in zip(TSR,TSR_answer):
-            self.assertAlmostEqual(i,j,delta=0.05)
+        for i, j in zip(TSR, TSR_answer):
+            self.assertAlmostEqual(i, j, delta=0.05)
 
     def test_power_coefficient(self):
         # data obtained from power performance report of wind turbine
-        inflow_speed = [4,6,8,10,12,14,16,18,20]
-        power_out = np.asarray([59,304,742,1200,1400,1482,1497,1497,1511])
+        inflow_speed = [4, 6, 8, 10, 12, 14, 16, 18, 20]
+        power_out = np.asarray([59, 304, 742, 1200, 1400, 1482, 1497, 1497, 1511])
         capture_area = 4656.63
         rho = 1.225
-        Cp_answer = [0.320,0.493,0.508,0.421,0.284,0.189,0.128,0.090,0.066]
-        
-        Cp = river.performance.power_coefficient(power_out*1000,inflow_speed,capture_area,rho)
-
-        for i,j in zip(Cp,Cp_answer):
-            self.assertAlmostEqual(i,j,places=2)        
-
-      
-if __name__ == '__main__':
-    unittest.main() 
+        Cp_answer = [0.320, 0.493, 0.508, 0.421, 0.284, 0.189, 0.128, 0.090, 0.066]
+
+        Cp = river.performance.power_coefficient(
+            power_out * 1000, inflow_speed, capture_area, rho
+        )
+
+        for i, j in zip(Cp, Cp_answer):
+            self.assertAlmostEqual(i, j, places=2)
+
 
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/tidal/test_io.py` & `mhkit-0.8.0/mhkit/tests/tidal/test_io.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,122 +8,155 @@
 These tests include:
 - Reading data from a JSON file
 - Requesting NOAA data with basic parameters
 - Requesting NOAA data with the write_json parameter
 - Requesting NOAA data with invalid date format
 - Requesting NOAA data with the end date before the start date
 """
+
 from os.path import abspath, dirname, join, normpath, relpath
 import unittest
 import os
 import json
 
 import numpy as np
 import mhkit.tidal as tidal
 
 
 testdir = dirname(abspath(__file__))
-plotdir = join(testdir, 'plots')
+plotdir = join(testdir, "plots")
 isdir = os.path.isdir(plotdir)
 if not isdir:
     os.mkdir(plotdir)
-datadir = normpath(join(testdir, relpath('../../../examples/data/tidal')))
+datadir = normpath(join(testdir, relpath("../../../examples/data/tidal")))
 
 
 class TestIO(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
         pass
 
     @classmethod
     def tearDownClass(self):
         pass
 
     def test_load_noaa_data(self):
         """
-        Test that the read_noaa_json function reads data from a 
+        Test that the read_noaa_json function reads data from a
         JSON file and returns a DataFrame and metadata with the
         correct shape and columns.
         """
-        file_name = join(datadir, 's08010.json')
+        file_name = join(datadir, "s08010.json")
         data, metadata = tidal.io.noaa.read_noaa_json(file_name)
-        self.assertTrue(np.all(data.columns == ['s', 'd', 'b']))
+        self.assertTrue(np.all(data.columns == ["s", "d", "b"]))
         self.assertEqual(data.shape, (18890, 3))
+        self.assertEqual(metadata["id"], "s08010")
+
+    def test_load_noaa_data_xarray(self):
+        """
+        Test that the read_noaa_json function reads data from a
+        JSON file and returns a DataFrame and metadata with the
+        correct shape and columns.
+        """
+        file_name = join(datadir, "s08010.json")
+        data = tidal.io.noaa.read_noaa_json(file_name, to_pandas=False)
+        self.assertTrue(np.all(list(data.variables) == ["index", "s", "d", "b"]))
+        self.assertEqual(len(data["index"]), 18890)
+        self.assertEqual(data.attrs["id"], "s08010")
 
     def test_request_noaa_data_basic(self):
         """
         Test the request_noaa_data function with basic input parameters
-        and verify that the returned DataFrame and metadata have the 
+        and verify that the returned DataFrame and metadata have the
         correct shape and columns.
         """
         data, metadata = tidal.io.noaa.request_noaa_data(
-            station='s08010',
-            parameter='currents',
-            start_date='20180101',
-            end_date='20180102',
+            station="s08010",
+            parameter="currents",
+            start_date="20180101",
+            end_date="20180102",
             proxy=None,
-            write_json=None
+            write_json=None,
         )
-        self.assertTrue(np.all(data.columns == ['s', 'd', 'b']))
+        self.assertTrue(np.all(data.columns == ["s", "d", "b"]))
         self.assertEqual(data.shape, (183, 3))
+        self.assertEqual(metadata["id"], "s08010")
+
+    def test_request_noaa_data_basic_xarray(self):
+        """
+        Test the request_noaa_data function with basic input parameters
+        and verify that the returned DataFrame and metadata have the
+        correct shape and columns.
+        """
+        data = tidal.io.noaa.request_noaa_data(
+            station="s08010",
+            parameter="currents",
+            start_date="20180101",
+            end_date="20180102",
+            proxy=None,
+            write_json=None,
+            to_pandas=False,
+        )
+        self.assertTrue(np.all(list(data.variables) == ["index", "s", "d", "b"]))
+        self.assertEqual(len(data["index"]), 183)
+        self.assertEqual(data.attrs["id"], "s08010")
 
     def test_request_noaa_data_write_json(self):
         """
         Test the request_noaa_data function with the write_json parameter
-        and verify that the returned JSON file has the correct structure 
+        and verify that the returned JSON file has the correct structure
         and can be loaded back into a dictionary.
         """
-        test_json_file = 'test_noaa_data.json'
-        data, metadata = tidal.io.noaa.request_noaa_data(
-            station='s08010',
-            parameter='currents',
-            start_date='20180101',
-            end_date='20180102',
+        test_json_file = "test_noaa_data.json"
+        _, _ = tidal.io.noaa.request_noaa_data(
+            station="s08010",
+            parameter="currents",
+            start_date="20180101",
+            end_date="20180102",
             proxy=None,
-            write_json=test_json_file
+            write_json=test_json_file,
         )
         self.assertTrue(os.path.isfile(test_json_file))
 
         with open(test_json_file) as f:
             loaded_data = json.load(f)
 
         os.remove(test_json_file)  # Clean up the test JSON file
 
-        self.assertIn('metadata', loaded_data)
-        self.assertIn('s', loaded_data)
-        self.assertIn('d', loaded_data)
-        self.assertIn('b', loaded_data)
+        self.assertIn("metadata", loaded_data)
+        self.assertIn("s", loaded_data["columns"])
+        self.assertIn("d", loaded_data["columns"])
+        self.assertIn("b", loaded_data["columns"])
 
     def test_request_noaa_data_invalid_dates(self):
         """
         Test the request_noaa_data function with an invalid date format
         and verify that it raises a ValueError.
         """
         with self.assertRaises(ValueError):
             tidal.io.noaa.request_noaa_data(
-                station='s08010',
-                parameter='currents',
-                start_date='2018-01-01',  # Invalid date format
-                end_date='20180102',
+                station="s08010",
+                parameter="currents",
+                start_date="2018-01-01",  # Invalid date format
+                end_date="20180102",
                 proxy=None,
-                write_json=None
+                write_json=None,
             )
 
     def test_request_noaa_data_end_before_start(self):
         """
-        Test the request_noaa_data function with the end date before 
+        Test the request_noaa_data function with the end date before
         the start date and verify that it raises a ValueError.
         """
         with self.assertRaises(ValueError):
             tidal.io.noaa.request_noaa_data(
-                station='s08010',
-                parameter='currents',
-                start_date='20180102',
-                end_date='20180101',  # End date before start date
+                station="s08010",
+                parameter="currents",
+                start_date="20180102",
+                end_date="20180101",  # End date before start date
                 proxy=None,
-                write_json=None
+                write_json=None,
             )
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/tidal/test_resource.py` & `mhkit-0.8.0/mhkit/tests/tidal/test_resource.py`

 * *Files 9% similar despite different names*

```diff
@@ -3,107 +3,112 @@
 import os
 import numpy as np
 import pandas as pd
 import matplotlib.pylab as plt
 import mhkit.tidal as tidal
 
 testdir = dirname(abspath(__file__))
-plotdir = join(testdir, 'plots')
+plotdir = join(testdir, "plots")
 isdir = os.path.isdir(plotdir)
-if not isdir: os.mkdir(plotdir)
-datadir = normpath(join(testdir,relpath('../../../examples/data/tidal')))
+if not isdir:
+    os.mkdir(plotdir)
+datadir = normpath(join(testdir, relpath("../../../examples/data/tidal")))
 
 
 class TestResource(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
-        file_name = join(datadir, 's08010.json')
+        file_name = join(datadir, "s08010.json")
         self.data, self.metadata = tidal.io.noaa.read_noaa_json(file_name)
-        self.data.s = self.data.s / 100. # convert to m/s
+        self.data.s = self.data.s / 100.0  # convert to m/s
         self.flood = 171.5
         self.ebb = 354.5
 
-
     @classmethod
     def tearDownClass(self):
         pass
-    
-    def test_exceedance_probability(self):
-        df = pd.DataFrame.from_records( {'vals': np.array([ 1, 2, 3, 4, 5, 6, 7, 8, 9])} )
-        df['F'] = tidal.resource.exceedance_probability(df.vals)
-        self.assertEqual(df['F'].min(), 10)
-        self.assertEqual(df['F'].max(), 90)
 
+    def test_exceedance_probability(self):
+        df = pd.DataFrame.from_records({"vals": np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])})
+        df["F"] = tidal.resource.exceedance_probability(df.vals)
+        self.assertEqual(df["F"].min(), 10)
+        self.assertEqual(df["F"].max(), 90)
+
+    def test_principal_flow_directions(self):
+        width_direction = 10
+        direction1, direction2 = tidal.resource.principal_flow_directions(
+            self.data.d, width_direction
+        )
+        self.assertEqual(direction1, 172.0)
+        self.assertEqual(round(direction2, 1), round(352.3, 1))
 
-    def test_principal_flow_directions(self):    
-        width_direction=10
-        direction1, direction2 = tidal.resource.principal_flow_directions(self.data.d, width_direction)
-        self.assertEqual(direction1,172.0) 
-        self.assertEqual(round(direction2,1),round(352.3,1))                                                                                   
-    
     def test_plot_current_timeseries(self):
-        filename = abspath(join(plotdir, 'tidal_plot_current_timeseries.png'))
+        filename = abspath(join(plotdir, "tidal_plot_current_timeseries.png"))
         if isfile(filename):
             os.remove(filename)
-        
+
         plt.figure()
         tidal.graphics.plot_current_timeseries(self.data.d, self.data.s, 172)
-        plt.savefig(filename, format='png')
+        plt.savefig(filename, format="png")
         plt.close()
-        
+
         self.assertTrue(isfile(filename))
-        
+
     def test_plot_joint_probability_distribution(self):
-        filename = abspath(join(plotdir, 'tidal_plot_joint_probability_distribution.png'))
+        filename = abspath(
+            join(plotdir, "tidal_plot_joint_probability_distribution.png")
+        )
         if isfile(filename):
             os.remove(filename)
-        
+
         plt.figure()
-        tidal.graphics.plot_joint_probability_distribution(self.data.d, self.data.s, 1, 0.1)
-        plt.savefig(f'{filename}')
+        tidal.graphics.plot_joint_probability_distribution(
+            self.data.d, self.data.s, 1, 0.1
+        )
+        plt.savefig(f"{filename}")
         plt.close()
-        
+
         self.assertTrue(isfile(filename))
-    
+
     def test_plot_rose(self):
-        filename = abspath(join(plotdir, 'tidal_plot_rose.png'))
+        filename = abspath(join(plotdir, "tidal_plot_rose.png"))
         if isfile(filename):
             os.remove(filename)
-        
+
         plt.figure()
         tidal.graphics.plot_rose(self.data.d, self.data.s, 1, 0.1)
-        plt.savefig(f'{filename}')
+        plt.savefig(f"{filename}")
         plt.close()
-        
+
         self.assertTrue(isfile(filename))
 
     def test_tidal_phase_probability(self):
-        filename = abspath(join(plotdir, 'tidal_plot_tidal_phase_probability.png'))
+        filename = abspath(join(plotdir, "tidal_plot_tidal_phase_probability.png"))
         if isfile(filename):
             os.remove(filename)
-        
+
         plt.figure()
-        tidal.graphics.tidal_phase_probability(self.data.d, self.data.s, 
-            self.flood, self.ebb)
-        plt.savefig(f'{filename}')
+        tidal.graphics.tidal_phase_probability(
+            self.data.d, self.data.s, self.flood, self.ebb
+        )
+        plt.savefig(f"{filename}")
         plt.close()
-        
+
         self.assertTrue(isfile(filename))
-        
+
     def test_tidal_phase_exceedance(self):
-        filename = abspath(join(plotdir, 'tidal_plot_tidal_phase_exceedance.png'))
+        filename = abspath(join(plotdir, "tidal_plot_tidal_phase_exceedance.png"))
         if isfile(filename):
             os.remove(filename)
-        
+
         plt.figure()
-        tidal.graphics.tidal_phase_exceedance(self.data.d, self.data.s, 
-            self.flood, self.ebb)
-        plt.savefig(f'{filename}')
+        tidal.graphics.tidal_phase_exceedance(
+            self.data.d, self.data.s, self.flood, self.ebb
+        )
+        plt.savefig(f"{filename}")
         plt.close()
-        
-        self.assertTrue(isfile(filename))        
 
+        self.assertTrue(isfile(filename))
 
-if __name__ == '__main__':
-    unittest.main() 
 
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/wave/io/test_cdip.py` & `mhkit-0.8.0/mhkit/tests/wave/io/test_cdip.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,188 +1,197 @@
-from os.path import abspath, dirname, join, isfile, normpath, relpath
-from pandas.testing import assert_frame_equal
-from numpy.testing import assert_allclose
-from scipy.interpolate import interp1d
-from random import seed, randint
+from os.path import abspath, dirname, join, isfile, normpath
 import matplotlib.pylab as plt
 from datetime import datetime
-import xarray.testing as xrt
 import mhkit.wave as wave
-from io import StringIO
-import pandas as pd
-import numpy as np
-import contextlib
 import unittest
 import netCDF4
-import inspect
-import pickle
-import time
-import json
-import sys
+import pytz
 import os
 
 
 testdir = dirname(abspath(__file__))
-datadir = normpath(join(testdir,'..','..','..','..','examples','data','wave'))
+datadir = normpath(join(testdir, "..", "..", "..", "..", "examples", "data", "wave"))
 
 
 class TestIOcdip(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
-        b067_1996='http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/' + \
-                   'archive/067p1/067p1_d04.nc'
+        b067_1996 = (
+            "http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/"
+            + "archive/067p1/067p1_d04.nc"
+        )
         self.test_nc = netCDF4.Dataset(b067_1996)
 
-        self.vars2D = [ 'waveEnergyDensity', 'waveMeanDirection',
-                        'waveA1Value', 'waveB1Value', 'waveA2Value',
-                        'waveB2Value', 'waveCheckFactor', 'waveSpread',
-                        'waveM2Value', 'waveN2Value']
+        self.vars2D = [
+            "waveEnergyDensity",
+            "waveMeanDirection",
+            "waveA1Value",
+            "waveB1Value",
+            "waveA2Value",
+            "waveB2Value",
+            "waveCheckFactor",
+            "waveSpread",
+            "waveM2Value",
+            "waveN2Value",
+        ]
 
     @classmethod
     def tearDownClass(self):
         pass
 
     def test_validate_date(self):
-        date='2013-11-12'
+        date = "2013-11-12"
         start_date = wave.io.cdip._validate_date(date)
         assert isinstance(start_date, datetime)
 
-        date='11-12-2012'
+        date = "11-12-2012"
         self.assertRaises(ValueError, wave.io.cdip._validate_date, date)
 
     def test_request_netCDF_historic(self):
-        station_number='067'
-        nc = wave.io.cdip.request_netCDF(station_number, 'historic')
+        station_number = "067"
+        nc = wave.io.cdip.request_netCDF(station_number, "historic")
         isinstance(nc, netCDF4.Dataset)
 
     def test_request_netCDF_realtime(self):
-        station_number='067'
-        nc = wave.io.cdip.request_netCDF(station_number, 'realtime')
+        station_number = "067"
+        nc = wave.io.cdip.request_netCDF(station_number, "realtime")
         isinstance(nc, netCDF4.Dataset)
 
-
     def test_start_and_end_of_year(self):
         year = 2020
         start_day, end_day = wave.io.cdip._start_and_end_of_year(year)
 
         assert isinstance(start_day, datetime)
         assert isinstance(end_day, datetime)
 
-        expected_start = datetime(year,1,1)
-        expected_end = datetime(year,12,31)
+        expected_start = datetime(year, 1, 1)
+        expected_end = datetime(year, 12, 31)
 
         self.assertEqual(start_day, expected_start)
         self.assertEqual(end_day, expected_end)
 
     def test_dates_to_timestamp(self):
+        start_date = datetime(1996, 10, 2, tzinfo=pytz.UTC)
+        end_date = datetime(1996, 10, 20, tzinfo=pytz.UTC)
 
-        start_date='1996-10-02'
-        end_date='1996-10-20'
-
-        start_stamp, end_stamp = wave.io.cdip._dates_to_timestamp(self.test_nc,
-            start_date=start_date, end_date=end_date)
+        start_stamp, end_stamp = wave.io.cdip._dates_to_timestamp(
+            self.test_nc, start_date=start_date, end_date=end_date
+        )
 
-        start_dt =  datetime.utcfromtimestamp(start_stamp)
-        end_dt =  datetime.utcfromtimestamp(end_stamp)
+        start_dt = datetime.utcfromtimestamp(start_stamp).replace(tzinfo=pytz.UTC)
+        end_dt = datetime.utcfromtimestamp(end_stamp).replace(tzinfo=pytz.UTC)
 
-        self.assertTrue(start_dt.strftime('%Y-%m-%d') == start_date)
-        self.assertTrue(end_dt.strftime('%Y-%m-%d') == end_date)
+        self.assertEqual(start_dt, start_date)
+        self.assertEqual(end_dt, end_date)
 
     def test_get_netcdf_variables_all2Dvars(self):
-        data = wave.io.cdip.get_netcdf_variables(self.test_nc,
-            all_2D_variables=True)
-        returned_keys = [key for key in data['data']['wave2D'].keys()]
-        self.assertTrue( returned_keys == self.vars2D)
+        data = wave.io.cdip.get_netcdf_variables(
+            self.test_nc, all_2D_variables=True, to_pandas=False
+        )
+        returned_keys = [key for key in data["data"]["wave2D"].keys()]
+        self.assertTrue(set(returned_keys) == set(self.vars2D))
 
     def test_get_netcdf_variables_params(self):
-        parameters =['waveHs', 'waveTp','notParam', 'waveMeanDirection']
-        data = wave.io.cdip.get_netcdf_variables(self.test_nc,
-            parameters=parameters)
-
-        returned_keys_1D = [key for key in data['data']['wave'].keys()]
-        returned_keys_2D = [key for key in data['data']['wave2D'].keys()]
-        returned_keys_metadata = [key for key in data['metadata']['wave']]
-
-        self.assertTrue( returned_keys_1D == ['waveHs', 'waveTp'])
-        self.assertTrue( returned_keys_2D == ['waveMeanDirection'])
-        self.assertTrue( returned_keys_metadata == ['waveFrequency'])
+        parameters = ["waveHs", "waveTp", "notParam", "waveMeanDirection"]
+        data = wave.io.cdip.get_netcdf_variables(self.test_nc, parameters=parameters)
 
+        returned_keys_1D = set([key for key in data["data"]["wave"].keys()])
+        returned_keys_2D = [key for key in data["data"]["wave2D"].keys()]
+        returned_keys_metadata = [key for key in data["metadata"]["wave"]]
+
+        self.assertTrue(returned_keys_1D == set(["waveHs", "waveTp"]))
+        self.assertTrue(returned_keys_2D == ["waveMeanDirection"])
+        self.assertTrue(returned_keys_metadata == ["waveFrequency"])
 
     def test_get_netcdf_variables_time_slice(self):
-        start_date='1996-10-01'
-        end_date='1996-10-31'
+        start_date = "1996-10-01"
+        end_date = "1996-10-31"
 
-        data = wave.io.cdip.get_netcdf_variables(self.test_nc,
-                start_date=start_date, end_date=end_date,
-                parameters='waveHs')
+        data = wave.io.cdip.get_netcdf_variables(
+            self.test_nc, start_date=start_date, end_date=end_date, parameters="waveHs"
+        )
 
-        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
-        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
-
-        self.assertTrue(data['data']['wave'].index[-1] < end_dt)
-        self.assertTrue(data['data']['wave'].index[0] > start_dt)
+        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
+        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
 
+        self.assertTrue(data["data"]["wave"].index[-1] < end_dt)
+        self.assertTrue(data["data"]["wave"].index[0] > start_dt)
 
     def test_request_parse_workflow_multiyear(self):
-        station_number = '067'
-        year1=2011
-        year2=2013
+        station_number = "067"
+        year1 = 2011
+        year2 = 2013
         years = [year1, year2]
-        parameters =['waveHs', 'waveMeanDirection', 'waveA1Value']
-        data = wave.io.cdip.request_parse_workflow(station_number=station_number,
-            years=years, parameters =parameters )
-
-        expected_index0 = datetime(year1,1,1)
-        expected_index_final = datetime(year2,12,31)
-
-        wave1D = data['data']['wave']
-        self.assertEqual(wave1D.index[0].floor('d').to_pydatetime(), expected_index0)
-
-        self.assertEqual(wave1D.index[-1].floor('d').to_pydatetime(), expected_index_final)
-
-        for key,wave2D  in data['data']['wave2D'].items():
-            self.assertEqual(wave2D.index[0].floor('d').to_pydatetime(), expected_index0)
-            self.assertEqual(wave2D.index[-1].floor('d').to_pydatetime(), expected_index_final)
-
+        parameters = ["waveHs", "waveMeanDirection", "waveA1Value"]
+        data = wave.io.cdip.request_parse_workflow(
+            station_number=station_number, years=years, parameters=parameters
+        )
+
+        expected_index0 = datetime(year1, 1, 1)
+        expected_index_final = datetime(year2, 12, 31)
+
+        wave1D = data["data"]["wave"]
+        self.assertEqual(wave1D.index[0].floor("d").to_pydatetime(), expected_index0)
+
+        self.assertEqual(
+            wave1D.index[-1].floor("d").to_pydatetime(), expected_index_final
+        )
+
+        for key, wave2D in data["data"]["wave2D"].items():
+            self.assertEqual(
+                wave2D.index[0].floor("d").to_pydatetime(), expected_index0
+            )
+            self.assertEqual(
+                wave2D.index[-1].floor("d").to_pydatetime(), expected_index_final
+            )
 
     def test_plot_boxplot(self):
-        filename = abspath(join(testdir, 'wave_plot_boxplot.png'))
+        filename = abspath(join(testdir, "wave_plot_boxplot.png"))
         if isfile(filename):
             os.remove(filename)
 
-        station_number = '067'
+        station_number = "067"
         year = 2011
-        data = wave.io.cdip.request_parse_workflow(station_number=station_number,years=year,
-                       parameters =['waveHs'],
-                       all_2D_variables=False)
+        data = wave.io.cdip.request_parse_workflow(
+            station_number=station_number,
+            years=year,
+            parameters=["waveHs"],
+            all_2D_variables=False,
+        )
 
         plt.figure()
-        wave.graphics.plot_boxplot(data['data']['wave']['waveHs'])
-        plt.savefig(filename, format='png')
+        wave.graphics.plot_boxplot(data["data"]["wave"]["waveHs"])
+        plt.savefig(filename, format="png")
         plt.close()
 
         self.assertTrue(isfile(filename))
-
+        os.remove(filename)
 
     def test_plot_compendium(self):
-        filename = abspath(join(testdir, 'wave_plot_boxplot.png'))
+        filename = abspath(join(testdir, "wave_plot_boxplot.png"))
         if isfile(filename):
             os.remove(filename)
 
-        station_number = '067'
+        station_number = "067"
         year = 2011
-        data = wave.io.cdip.request_parse_workflow(station_number=station_number,years=year,
-                       parameters =['waveHs', 'waveTp', 'waveDp'],
-                       all_2D_variables=False)
+        data = wave.io.cdip.request_parse_workflow(
+            station_number=station_number,
+            years=year,
+            parameters=["waveHs", "waveTp", "waveDp"],
+            all_2D_variables=False,
+        )
 
         plt.figure()
-        wave.graphics.plot_compendium(data['data']['wave']['waveHs'],
-            data['data']['wave']['waveTp'], data['data']['wave']['waveDp'] )
-        plt.savefig(filename, format='png')
+        wave.graphics.plot_compendium(
+            data["data"]["wave"]["waveHs"],
+            data["data"]["wave"]["waveTp"],
+            data["data"]["wave"]["waveDp"],
+        )
+        plt.savefig(filename, format="png")
         plt.close()
 
         self.assertTrue(isfile(filename))
+        os.remove(filename)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/wave/io/test_ndbc.py` & `mhkit-0.8.0/mhkit/tests/wave/io/test_ndbc.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,183 +1,231 @@
 from os.path import abspath, dirname, join, isfile, normpath
 from pandas.testing import assert_frame_equal
 import matplotlib.pylab as plt
 from datetime import datetime
 import mhkit.wave as wave
 from io import StringIO
 import pandas as pd
+import xarray as xr
 import numpy as np
 import contextlib
 import unittest
 import os
 
 
 testdir = dirname(abspath(__file__))
-datadir = normpath(join(testdir, '..', '..', '..',
-                   '..', 'examples', 'data', 'wave'))
+datadir = normpath(join(testdir, "..", "..", "..", "..", "examples", "data", "wave"))
 
 
 class TestIOndbc(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
-        self.expected_columns_metRT = ['WDIR', 'WSPD', 'GST', 'WVHT', 'DPD',
-                                       'APD', 'MWD', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'VIS', 'PTDY', 'TIDE']
-        self.expected_units_metRT = {'WDIR': 'degT', 'WSPD': 'm/s', 'GST': 'm/s',
-                                     'WVHT': 'm', 'DPD': 'sec', 'APD': 'sec', 'MWD': 'degT', 'PRES': 'hPa',
-                                     'ATMP': 'degC', 'WTMP': 'degC', 'DEWP': 'degC', 'VIS': 'nmi',
-                                     'PTDY': 'hPa', 'TIDE': 'ft'}
-
-        self.expected_columns_metH = ['WDIR', 'WSPD', 'GST', 'WVHT', 'DPD',
-                                      'APD', 'MWD', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'VIS', 'TIDE']
-        self.expected_units_metH = {'WDIR': 'degT', 'WSPD': 'm/s', 'GST': 'm/s',
-                                    'WVHT': 'm', 'DPD': 'sec', 'APD': 'sec', 'MWD': 'deg', 'PRES': 'hPa',
-                                    'ATMP': 'degC', 'WTMP': 'degC', 'DEWP': 'degC', 'VIS': 'nmi',
-                                    'TIDE': 'ft'}
-        self.filenames = ['46042w1996.txt.gz',
-                          '46029w1997.txt.gz',
-                          '46029w1998.txt.gz']
-        self.swden = pd.read_csv(join(datadir, self.filenames[0]), sep=r'\s+',
-                                 compression='gzip')
+        self.expected_columns_metRT = [
+            "WDIR",
+            "WSPD",
+            "GST",
+            "WVHT",
+            "DPD",
+            "APD",
+            "MWD",
+            "PRES",
+            "ATMP",
+            "WTMP",
+            "DEWP",
+            "VIS",
+            "PTDY",
+            "TIDE",
+        ]
+        self.expected_units_metRT = {
+            "WDIR": "degT",
+            "WSPD": "m/s",
+            "GST": "m/s",
+            "WVHT": "m",
+            "DPD": "sec",
+            "APD": "sec",
+            "MWD": "degT",
+            "PRES": "hPa",
+            "ATMP": "degC",
+            "WTMP": "degC",
+            "DEWP": "degC",
+            "VIS": "nmi",
+            "PTDY": "hPa",
+            "TIDE": "ft",
+        }
+
+        self.expected_columns_metH = [
+            "WDIR",
+            "WSPD",
+            "GST",
+            "WVHT",
+            "DPD",
+            "APD",
+            "MWD",
+            "PRES",
+            "ATMP",
+            "WTMP",
+            "DEWP",
+            "VIS",
+            "TIDE",
+        ]
+        self.expected_units_metH = {
+            "WDIR": "degT",
+            "WSPD": "m/s",
+            "GST": "m/s",
+            "WVHT": "m",
+            "DPD": "sec",
+            "APD": "sec",
+            "MWD": "deg",
+            "PRES": "hPa",
+            "ATMP": "degC",
+            "WTMP": "degC",
+            "DEWP": "degC",
+            "VIS": "nmi",
+            "TIDE": "ft",
+        }
+        self.filenames = ["46042w1996.txt.gz", "46029w1997.txt.gz", "46029w1998.txt.gz"]
+        self.swden = pd.read_csv(
+            join(datadir, self.filenames[0]), sep=r"\s+", compression="gzip"
+        )
 
-        buoy = '42012'
+        buoy = "42012"
         year = 2021
-        date = np.datetime64('2021-02-21T12:40:00')
-        directional_data_all = wave.io.ndbc.request_directional_data(
-            buoy, year)
+        date = np.datetime64("2021-02-21T12:40:00")
+        directional_data_all = wave.io.ndbc.request_directional_data(buoy, year)
         self.directional_data = directional_data_all.sel(date=date)
 
     @classmethod
     def tearDownClass(self):
         pass
 
     # Realtime data
     def test_ndbc_read_realtime_met(self):
-        data, units = wave.io.ndbc.read_file(join(datadir, '46097.txt'))
+        data, units = wave.io.ndbc.read_file(join(datadir, "46097.txt"))
         expected_index0 = datetime(2019, 4, 2, 13, 50)
-        self.assertSetEqual(set(data.columns), set(
-            self.expected_columns_metRT))
+        self.assertSetEqual(set(data.columns), set(self.expected_columns_metRT))
         self.assertEqual(data.index[0], expected_index0)
         self.assertEqual(data.shape, (6490, 14))
         self.assertEqual(units, self.expected_units_metRT)
 
     # Historical data
     def test_ndbnc_read_historical_met(self):
         # QC'd monthly data, Aug 2019
-        data, units = wave.io.ndbc.read_file(
-            join(datadir, '46097h201908qc.txt'))
+        data, units = wave.io.ndbc.read_file(join(datadir, "46097h201908qc.txt"))
         expected_index0 = datetime(2019, 8, 1, 0, 0)
         self.assertSetEqual(set(data.columns), set(self.expected_columns_metH))
         self.assertEqual(data.index[0], expected_index0)
         self.assertEqual(data.shape, (4464, 13))
         self.assertEqual(units, self.expected_units_metH)
 
     # Spectral data
     def test_ndbc_read_spectral(self):
-        data, units = wave.io.ndbc.read_file(join(datadir, 'data.txt'))
-        self.assertEqual(data.shape, (743, 47))
+        data, units = wave.io.ndbc.read_file(join(datadir, "data.txt"), to_pandas=False)
+        self.assertEqual(len(data.data_vars), 47)
+        self.assertEqual(len(data["dim_0"]), 743)
         self.assertEqual(units, None)
 
     # Continuous wind data
     def test_ndbc_read_cwind_no_units(self):
-        data, units = wave.io.ndbc.read_file(join(datadir, '42a01c2003.txt'))
+        data, units = wave.io.ndbc.read_file(join(datadir, "42a01c2003.txt"))
         self.assertEqual(data.shape, (4320, 5))
         self.assertEqual(units, None)
 
     def test_ndbc_read_cwind_units(self):
-        data, units = wave.io.ndbc.read_file(join(datadir, '46002c2016.txt'))
+        data, units = wave.io.ndbc.read_file(join(datadir, "46002c2016.txt"))
         self.assertEqual(data.shape, (28468, 5))
-        self.assertEqual(units, wave.io.ndbc.parameter_units('cwind'))
+        self.assertEqual(units, wave.io.ndbc.parameter_units("cwind"))
 
     def test_ndbc_available_data(self):
-        data = wave.io.ndbc.available_data('swden', buoy_number='46029')
+        data = wave.io.ndbc.available_data("swden", buoy_number="46029")
         cols = data.columns.tolist()
-        exp_cols = ['id', 'year', 'filename']
+        exp_cols = ["id", "year", "filename"]
         self.assertEqual(cols, exp_cols)
 
         years = [int(year) for year in data.year.tolist()]
-        exp_years = [*range(1996, 1996+len(years))]
+        exp_years = [*range(1996, 1996 + len(years))]
         self.assertEqual(years, exp_years)
         self.assertEqual(data.shape, (len(data), 3))
 
     def test__ndbc_parse_filenames(self):
         filenames = pd.Series(self.filenames)
-        buoys = wave.io.ndbc._parse_filenames('swden', filenames)
+        buoys = wave.io.ndbc._parse_filenames("swden", filenames)
         years = buoys.year.tolist()
         numbers = buoys.id.tolist()
         fnames = buoys.filename.tolist()
 
         self.assertEqual(buoys.shape, (len(filenames), 3))
-        self.assertListEqual(years, ['1996', '1997', '1998'])
-        self.assertListEqual(numbers, ['46042', '46029', '46029'])
+        self.assertListEqual(years, ["1996", "1997", "1998"])
+        self.assertListEqual(numbers, ["46042", "46029", "46029"])
         self.assertListEqual(fnames, self.filenames)
 
     def test_ndbc_request_data(self):
         filenames = pd.Series(self.filenames[0])
-        ndbc_data = wave.io.ndbc.request_data('swden', filenames)
-        self.assertTrue(self.swden.equals(ndbc_data['1996']))
+        ndbc_data = wave.io.ndbc.request_data("swden", filenames, to_pandas=False)
+        self.assertTrue(xr.Dataset(self.swden).equals(ndbc_data["1996"]))
 
     def test_ndbc_request_data_from_dataframe(self):
         filenames = pd.DataFrame(pd.Series(data=self.filenames[0]))
-        ndbc_data = wave.io.ndbc.request_data('swden', filenames)
-        assert_frame_equal(self.swden, ndbc_data['1996'])
+        ndbc_data = wave.io.ndbc.request_data("swden", filenames)
+        assert_frame_equal(self.swden, ndbc_data["1996"])
 
     def test_ndbc_request_data_filenames_length(self):
-        with self.assertRaises(AssertionError):
-            wave.io.ndbc.request_data('swden', pd.Series(dtype=float))
+        with self.assertRaises(ValueError):
+            wave.io.ndbc.request_data("swden", pd.Series(dtype=float))
 
     def test_ndbc_to_datetime_index(self):
-        dt = wave.io.ndbc.to_datetime_index('swden', self.swden)
+        dt = wave.io.ndbc.to_datetime_index("swden", self.swden)
         self.assertEqual(type(dt.index), pd.DatetimeIndex)
-        self.assertFalse({'YY', 'MM', 'DD', 'hh'}.issubset(dt.columns))
+        self.assertFalse({"YY", "MM", "DD", "hh"}.issubset(dt.columns))
 
     def test_ndbc_request_data_empty_file(self):
         temp_stdout = StringIO()
         # known empty file. If NDBC replaces, this test may fail.
         filename = "42008h1984.txt.gz"
-        buoy_id = '42008'
-        year = '1984'
+        buoy_id = "42008"
+        year = "1984"
         with contextlib.redirect_stdout(temp_stdout):
-            wave.io.ndbc.request_data('stdmet', pd.Series(filename))
+            wave.io.ndbc.request_data("stdmet", pd.Series(filename))
         output = temp_stdout.getvalue().strip()
-        msg = (f'The NDBC buoy {buoy_id} for year {year} with '
-               f'filename {filename} is empty or missing '
-               'data. Please omit this file from your data '
-               'request in the future.')
+        msg = (
+            f"The NDBC buoy {buoy_id} for year {year} with "
+            f"filename {filename} is empty or missing "
+            "data. Please omit this file from your data "
+            "request in the future."
+        )
         self.assertEqual(output, msg)
 
     def test_ndbc_request_multiple_files_with_empty_file(self):
         temp_stdout = StringIO()
         # known empty file. If NDBC replaces, this test may fail.
-        empty_file = '42008h1984.txt.gz'
-        working_file = '46042h1996.txt.gz'
+        empty_file = "42008h1984.txt.gz"
+        working_file = "46042h1996.txt.gz"
         filenames = pd.Series([empty_file, working_file])
+
         with contextlib.redirect_stdout(temp_stdout):
-            ndbc_data = wave.io.ndbc.request_data('stdmet', filenames)
+            ndbc_data = wave.io.ndbc.request_data("stdmet", filenames)
         self.assertEqual(1, len(ndbc_data))
 
     def test_ndbc_dates_to_datetime(self):
         dt = wave.io.ndbc.dates_to_datetime(self.swden)
         self.assertEqual(datetime(1996, 1, 1, 1, 0), dt[1])
 
     def test_ndbc_date_string_to_datetime(self):
         swden = self.swden.copy(deep=True)
-        swden['mm'] = np.zeros(len(swden)).astype(int).astype(str)
-        year_string = 'YY'
-        year_fmt = '%y'
-        parse_columns = [year_string, 'MM', 'DD', 'hh', 'mm']
-        df = wave.io.ndbc._date_string_to_datetime(swden, parse_columns,
-                                                   year_fmt)
-        dt = df['date']
+        swden["mm"] = np.zeros(len(swden)).astype(int).astype(str)
+        year_string = "YY"
+        year_fmt = "%y"
+        parse_columns = [year_string, "MM", "DD", "hh", "mm"]
+        df = wave.io.ndbc._date_string_to_datetime(swden, parse_columns, year_fmt)
+        dt = df["date"]
         self.assertEqual(datetime(1996, 1, 1, 1, 0), dt[1])
 
     def test_ndbc_parameter_units(self):
-        parameter = 'swden'
+        parameter = "swden"
         units = wave.io.ndbc.parameter_units(parameter)
-        self.assertEqual(units[parameter], '(m*m)/Hz')
+        self.assertEqual(units[parameter], "(m*m)/Hz")
 
     def test_ndbc_request_directional_data(self):
         data = self.directional_data
         # correct 5 parameters
         self.assertEqual(len(data), 5)
         self.assertIn("swden", data)
         self.assertIn("swdir", data)
@@ -185,75 +233,78 @@
         self.assertIn("swr1", data)
         self.assertIn("swr2", data)
         # correct number of data points
         self.assertEqual(len(data.frequency), 47)
 
     def test_ndbc_create_spread_function(self):
         directions = np.arange(0, 360, 2.0)
-        spread = wave.io.ndbc.create_spread_function(
-            self.directional_data, directions)
+        spread = wave.io.ndbc.create_spread_function(self.directional_data, directions)
         self.assertEqual(spread.shape, (47, 180))
-        self.assertEqual(spread.units, '1/Hz/deg')
+        self.assertEqual(spread.units, "1/Hz/deg")
 
     def test_ndbc_create_directional_spectrum(self):
         directions = np.arange(0, 360, 2.0)
         spectrum = wave.io.ndbc.create_directional_spectrum(
-            self.directional_data, directions)
+            self.directional_data, directions
+        )
         self.assertEqual(spectrum.shape, (47, 180))
-        self.assertEqual(spectrum.units, 'm^2/Hz/deg')
+        self.assertEqual(spectrum.units, "m^2/Hz/deg")
 
     def test_plot_directional_spectrum(self):
         directions = np.arange(0, 360, 2.0)
         spectrum = wave.io.ndbc.create_spread_function(
-            self.directional_data, directions)
+            self.directional_data, directions
+        )
         wave.graphics.plot_directional_spectrum(
             spectrum,
-            min=0.0,
+            color_level_min=0.0,
             fill=True,
             nlevels=6,
             name="Elevation Variance",
-            units="m^2")
+            units="m^2",
+        )
 
-        filename = abspath(join(testdir, 'wave_plot_directional_spectrum.png'))
+        filename = abspath(join(testdir, "wave_plot_directional_spectrum.png"))
         if isfile(filename):
             os.remove(filename)
         plt.savefig(filename)
 
         self.assertTrue(isfile(filename))
         os.remove(filename)
 
     def test_get_buoy_metadata(self):
         metadata = wave.io.ndbc.get_buoy_metadata("46042")
         expected_keys = {
-            'buoy',
-            'provider',
-            'type',
-            'SCOOP payload',
-            'lat',
-            'lon',
-            'Site elevation',
-            'Air temp height',
-            'Anemometer height',
-            'Barometer elevation',
-            'Sea temp depth',
-            'Water depth',
-            'Watch circle radius'
+            "buoy",
+            "provider",
+            "type",
+            "SCOOP payload",
+            "lat",
+            "lon",
+            "Site elevation",
+            "Air temp height",
+            "Anemometer height",
+            "Barometer elevation",
+            "Sea temp depth",
+            "Water depth",
+            "Watch circle radius",
         }
         self.assertSetEqual(set(metadata.keys()), expected_keys)
         self.assertEqual(
-            metadata['provider'], 'Owned and maintained by National Data Buoy Center')
-        self.assertEqual(metadata['type'], '3-meter foam buoy w/ seal cage')
-        self.assertAlmostEqual(float(metadata['lat']), 36.785)
-        self.assertAlmostEqual(float(metadata['lon']), 122.396)
-        self.assertEqual(metadata['Site elevation'], 'sea level')
+            metadata["provider"], "Owned and maintained by National Data Buoy Center"
+        )
+        self.assertEqual(metadata["type"], "3-meter foam buoy w/ seal cage")
+        self.assertAlmostEqual(float(metadata["lat"]), 36.785)
+        self.assertAlmostEqual(float(metadata["lon"]), 122.396)
+        self.assertEqual(metadata["Site elevation"], "sea level")
 
     def test_get_buoy_metadata_invalid_station(self):
         with self.assertRaises(ValueError):
             wave.io.ndbc.get_buoy_metadata("invalid_station")
 
     def test_get_buoy_metadata_nonexistent_station(self):
         with self.assertRaises(ValueError):
             wave.io.ndbc.get_buoy_metadata("99999")
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/wave/io/test_swan.py` & `mhkit-0.8.0/mhkit/tests/wave/io/test_swan.py`

 * *Files 23% similar despite different names*

```diff
@@ -5,79 +5,93 @@
 from random import seed, randint
 import matplotlib.pylab as plt
 from datetime import datetime
 import xarray.testing as xrt
 import mhkit.wave as wave
 from io import StringIO
 import pandas as pd
+import xarray as xr
 import numpy as np
 import contextlib
 import unittest
 import netCDF4
 import inspect
 import pickle
 import time
 import json
 import sys
 import os
 
 
 testdir = dirname(abspath(__file__))
-datadir = normpath(join(testdir,'..','..','..','..','examples','data','wave'))
+datadir = normpath(join(testdir, "..", "..", "..", "..", "examples", "data", "wave"))
 
 
 class TestSWAN(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
-        swan_datadir = join(datadir,'swan')
-        self.table_file = join(swan_datadir,'SWANOUT.DAT')
-        self.swan_block_mat_file = join(swan_datadir,'SWANOUT.MAT')
-        self.swan_block_txt_file = join(swan_datadir,'SWANOUTBlock.DAT')
-        self.expected_table = pd.read_csv(self.table_file, sep='\s+', comment='%',
-                  names=['Xp', 'Yp', 'Hsig', 'Dir', 'RTpeak', 'TDir'])
+        swan_datadir = join(datadir, "swan")
+        self.table_file = join(swan_datadir, "SWANOUT.DAT")
+        self.swan_block_mat_file = join(swan_datadir, "SWANOUT.MAT")
+        self.swan_block_txt_file = join(swan_datadir, "SWANOUTBlock.DAT")
+        self.expected_table = pd.read_csv(
+            self.table_file,
+            sep="\s+",
+            comment="%",
+            names=["Xp", "Yp", "Hsig", "Dir", "RTpeak", "TDir"],
+        )
 
     @classmethod
     def tearDownClass(self):
         pass
 
     def test_read_table(self):
         swan_table, swan_meta = wave.io.swan.read_table(self.table_file)
         assert_frame_equal(self.expected_table, swan_table)
 
     def test_read_block_mat(self):
-        swanBlockMat, metaDataMat = wave.io.swan.read_block(self.swan_block_mat_file )
+        swanBlockMat, metaDataMat = wave.io.swan.read_block(self.swan_block_mat_file)
         self.assertEqual(len(swanBlockMat), 4)
-        self.assertAlmostEqual(self.expected_table['Hsig'].sum(),
-                               swanBlockMat['Hsig'].sum().sum(), places=1)
+        self.assertAlmostEqual(
+            self.expected_table["Hsig"].sum(),
+            swanBlockMat["Hsig"].sum().sum(),
+            places=1,
+        )
 
     def test_read_block_txt(self):
         swanBlockTxt, metaData = wave.io.swan.read_block(self.swan_block_txt_file)
         self.assertEqual(len(swanBlockTxt), 4)
-        sumSum = swanBlockTxt['Significant wave height'].sum().sum()
-        self.assertAlmostEqual(self.expected_table['Hsig'].sum(),
-                               sumSum, places=-2)
+        sumSum = swanBlockTxt["Significant wave height"].sum().sum()
+        self.assertAlmostEqual(self.expected_table["Hsig"].sum(), sumSum, places=-2)
+
+    def test_read_block_txt_xarray(self):
+        swanBlockTxt, metaData = wave.io.swan.read_block(
+            self.swan_block_txt_file, to_pandas=False
+        )
+        self.assertEqual(len(swanBlockTxt), 4)
+        sumSum = swanBlockTxt["Significant wave height"].sum().sum()
+        self.assertAlmostEqual(self.expected_table["Hsig"].sum(), sumSum, places=-2)
 
     def test_block_to_table(self):
-        x=np.arange(5)
-        y=np.arange(5,10)
-        df = pd.DataFrame(np.random.rand(5,5), columns=x, index=y)
+        x = np.arange(5)
+        y = np.arange(5, 10)
+        df = pd.DataFrame(np.random.rand(5, 5), columns=x, index=y)
         dff = wave.io.swan.block_to_table(df)
-        self.assertEqual(dff.shape, (len(x)*len(y), 3))
+        self.assertEqual(dff.shape, (len(x) * len(y), 3))
         self.assertTrue(all(dff.x.unique() == np.unique(x)))
 
     def test_dictionary_of_block_to_table(self):
-        x=np.arange(5)
-        y=np.arange(5,10)
-        df = pd.DataFrame(np.random.rand(5,5), columns=x, index=y)
-        keys = ['data1', 'data2']
+        x = np.arange(5)
+        y = np.arange(5, 10)
+        df = pd.DataFrame(np.random.rand(5, 5), columns=x, index=y)
+        keys = ["data1", "data2"]
         data = [df, df]
-        dict_of_dfs = dict(zip(keys,data))
+        dict_of_dfs = dict(zip(keys, data))
         dff = wave.io.swan.dictionary_of_block_to_table(dict_of_dfs)
-        self.assertEqual(dff.shape, (len(x)*len(y), 2+len(keys)))
+        self.assertEqual(dff.shape, (len(x) * len(y), 2 + len(keys)))
         self.assertTrue(all(dff.x.unique() == np.unique(x)))
         for key in keys:
             self.assertTrue(key in dff.keys())
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/wave/io/test_wecsim.py` & `mhkit-0.8.0/mhkit/tests/wave/io/test_wecsim.py`

 * *Files 22% similar despite different names*

```diff
@@ -18,71 +18,79 @@
 import time
 import json
 import sys
 import os
 
 
 testdir = dirname(abspath(__file__))
-datadir = normpath(join(testdir,'..','..','..','..','examples','data','wave'))
+datadir = normpath(join(testdir, "..", "..", "..", "..", "examples", "data", "wave"))
 
 
 class TestWECSim(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
         pass
 
     @classmethod
     def tearDownClass(self):
         pass
 
     ### WEC-Sim data, no mooring
     def test_read_wecSim_no_mooring(self):
-        ws_output = wave.io.wecsim.read_output(join(datadir, 'RM3_matlabWorkspace_structure.mat'))
-        self.assertEqual(ws_output['wave'].elevation.name,'elevation')
-        self.assertEqual(ws_output['bodies']['body1'].name,'float')
-        self.assertEqual(ws_output['ptos'].name,'PTO1')
-        self.assertEqual(ws_output['constraints'].name,'Constraint1')
-        self.assertEqual(len(ws_output['mooring']),0)
-        self.assertEqual(len(ws_output['moorDyn']),0)
-        self.assertEqual(len(ws_output['ptosim']),0)
-        self.assertEqual(len(ws_output['cables']),0)
-    
+        ws_output = wave.io.wecsim.read_output(
+            join(datadir, "RM3_matlabWorkspace_structure.mat")
+        )
+        self.assertEqual(ws_output["wave"].elevation.name, "elevation")
+        self.assertEqual(ws_output["bodies"]["body1"].name, "float")
+        self.assertEqual(ws_output["ptos"].name, "PTO1")
+        self.assertEqual(ws_output["constraints"].name, "Constraint1")
+        self.assertEqual(len(ws_output["mooring"]), 0)
+        self.assertEqual(len(ws_output["moorDyn"]), 0)
+        self.assertEqual(len(ws_output["ptosim"]), 0)
+        self.assertEqual(len(ws_output["cables"]), 0)
+
     ### WEC-Sim data, with cable
     def test_read_wecSim_cable(self):
-        ws_output = wave.io.wecsim.read_output(join(datadir, 'Cable_matlabWorkspace_structure.mat'))
-        self.assertEqual(ws_output['wave'].elevation.name,'elevation')
-        self.assertEqual(ws_output['bodies']['body1'].name,'BuoyDraft5cm')
-        self.assertEqual(ws_output['cables'].name,'Cable')
-        self.assertEqual(ws_output['constraints']['constraint1'].name,'Mooring')
-        self.assertEqual(len(ws_output['mooring']),0)
-        self.assertEqual(len(ws_output['moorDyn']),0)
-        self.assertEqual(len(ws_output['ptosim']),0)
-        self.assertEqual(len(ws_output['ptos']),0)
+        ws_output = wave.io.wecsim.read_output(
+            join(datadir, "Cable_matlabWorkspace_structure.mat"),
+            to_pandas=False,
+        )
+        self.assertEqual(ws_output["wave"]["elevation"].name, "elevation")
+        self.assertEqual(
+            ws_output["bodies"]["body1"]["position_dof1"].name, "position_dof1"
+        )
+        self.assertEqual(len(ws_output["mooring"]), 0)
+        self.assertEqual(len(ws_output["moorDyn"]), 0)
+        self.assertEqual(len(ws_output["ptosim"]), 0)
+        self.assertEqual(len(ws_output["ptos"]), 0)
 
     ### WEC-Sim data, with mooring
     def test_read_wecSim_with_mooring(self):
-        ws_output = wave.io.wecsim.read_output(join(datadir, 'RM3MooringMatrix_matlabWorkspace_structure.mat'))
-        self.assertEqual(ws_output['wave'].elevation.name,'elevation')
-        self.assertEqual(ws_output['bodies']['body1'].name,'float')
-        self.assertEqual(ws_output['ptos'].name,'PTO1')
-        self.assertEqual(ws_output['constraints'].name,'Constraint1')
-        self.assertEqual(len(ws_output['mooring']),40001)
-        self.assertEqual(len(ws_output['moorDyn']),0)
-        self.assertEqual(len(ws_output['ptosim']),0)
-        self.assertEqual(len(ws_output['cables']),0)
+        ws_output = wave.io.wecsim.read_output(
+            join(datadir, "RM3MooringMatrix_matlabWorkspace_structure.mat")
+        )
+        self.assertEqual(ws_output["wave"].elevation.name, "elevation")
+        self.assertEqual(ws_output["bodies"]["body1"].name, "float")
+        self.assertEqual(ws_output["ptos"].name, "PTO1")
+        self.assertEqual(ws_output["constraints"].name, "Constraint1")
+        self.assertEqual(len(ws_output["mooring"]), 40001)
+        self.assertEqual(len(ws_output["moorDyn"]), 0)
+        self.assertEqual(len(ws_output["ptosim"]), 0)
+        self.assertEqual(len(ws_output["cables"]), 0)
 
     ### WEC-Sim data, with moorDyn
     def test_read_wecSim_with_moorDyn(self):
-        ws_output = wave.io.wecsim.read_output(join(datadir, 'RM3MoorDyn_matlabWorkspace_structure.mat'))
-        self.assertEqual(ws_output['wave'].elevation.name,'elevation')
-        self.assertEqual(ws_output['bodies']['body1'].name,'float')
-        self.assertEqual(ws_output['ptos'].name,'PTO1')
-        self.assertEqual(ws_output['constraints'].name,'Constraint1')
-        self.assertEqual(len(ws_output['mooring']),40001)
-        self.assertEqual(len(ws_output['moorDyn']),7)
-        self.assertEqual(len(ws_output['ptosim']),0)
-        self.assertEqual(len(ws_output['cables']),0)
+        ws_output = wave.io.wecsim.read_output(
+            join(datadir, "RM3MoorDyn_matlabWorkspace_structure.mat")
+        )
+        self.assertEqual(ws_output["wave"].elevation.name, "elevation")
+        self.assertEqual(ws_output["bodies"]["body1"].name, "float")
+        self.assertEqual(ws_output["ptos"].name, "PTO1")
+        self.assertEqual(ws_output["constraints"].name, "Constraint1")
+        self.assertEqual(len(ws_output["mooring"]), 40001)
+        self.assertEqual(len(ws_output["moorDyn"]), 7)
+        self.assertEqual(len(ws_output["ptosim"]), 0)
+        self.assertEqual(len(ws_output["cables"]), 0)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/wave/test_performance.py` & `mhkit-0.8.0/mhkit/tests/wave/test_performance.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,130 +1,147 @@
 from os.path import abspath, dirname, join, isfile, normpath, relpath
-from pandas.testing import assert_frame_equal
-from numpy.testing import assert_allclose
-from scipy.interpolate import interp1d
-from random import seed, randint
 import matplotlib.pylab as plt
-from datetime import datetime
 import xarray.testing as xrt
 import mhkit.wave as wave
-from io import StringIO
 import pandas as pd
 import numpy as np
-import contextlib
 import unittest
-import netCDF4
-import inspect
-import pickle
-import time
-import json
-import sys
 import os
 
 
 testdir = dirname(abspath(__file__))
-plotdir = join(testdir, 'plots')
+plotdir = join(testdir, "plots")
 isdir = os.path.isdir(plotdir)
-if not isdir: os.mkdir(plotdir)
-datadir = normpath(join(testdir,relpath('../../../examples/data/wave')))
+if not isdir:
+    os.mkdir(plotdir)
+datadir = normpath(join(testdir, relpath("../../../examples/data/wave")))
 
 
 class TestPerformance(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
         np.random.seed(123)
         Hm0 = np.random.rayleigh(4, 100000)
-        Te = np.random.normal(4.5, .8, 100000)
+        Te = np.random.normal(4.5, 0.8, 100000)
         P = np.random.normal(200, 40, 100000)
         J = np.random.normal(300, 10, 100000)
-        ndbc_data_file = join(datadir,'data.txt')
+        ndbc_data_file = join(datadir, "data.txt")
         [raw_ndbc_data, meta] = wave.io.ndbc.read_file(ndbc_data_file)
         self.S = raw_ndbc_data.T
 
-        self.data = pd.DataFrame({'Hm0': Hm0, 'Te': Te, 'P': P,'J': J})
-        self.Hm0_bins = np.arange(0,19,0.5)
-        self.Te_bins = np.arange(0,9,1)
-        self.expected_stats = ["mean","std","median","count","sum","min","max","freq"]
+        self.data = pd.DataFrame({"Hm0": Hm0, "Te": Te, "P": P, "J": J})
+        self.Hm0_bins = np.arange(0, 19, 0.5)
+        self.Te_bins = np.arange(0, 9, 1)
+        self.expected_stats = [
+            "mean",
+            "std",
+            "median",
+            "count",
+            "sum",
+            "min",
+            "max",
+            "freq",
+        ]
 
     @classmethod
     def tearDownClass(self):
         pass
 
     def test_capture_length(self):
-        L = wave.performance.capture_length(self.data['P'], self.data['J'])
+        L = wave.performance.capture_length(self.data["P"], self.data["J"])
         L_stats = wave.performance.statistics(L)
 
-        self.assertAlmostEqual(L_stats['mean'], 0.6676, 3)
+        self.assertAlmostEqual(L_stats["mean"], 0.6676, 3)
 
     def test_capture_length_matrix(self):
-        L = wave.performance.capture_length(self.data['P'], self.data['J'])
-        LM = wave.performance.capture_length_matrix(self.data['Hm0'], self.data['Te'],
-                        L, 'std', self.Hm0_bins, self.Te_bins)
+        L = wave.performance.capture_length(self.data["P"], self.data["J"])
+        LM = wave.performance.capture_length_matrix(
+            self.data["Hm0"], self.data["Te"], L, "std", self.Hm0_bins, self.Te_bins
+        )
 
-        self.assertEqual(LM.shape, (38,9))
+        self.assertEqual(LM.shape, (38, 9))
         self.assertEqual(LM.isna().sum().sum(), 131)
 
     def test_wave_energy_flux_matrix(self):
-        JM = wave.performance.wave_energy_flux_matrix(self.data['Hm0'], self.data['Te'],
-                        self.data['J'], 'mean', self.Hm0_bins, self.Te_bins)
+        JM = wave.performance.wave_energy_flux_matrix(
+            self.data["Hm0"],
+            self.data["Te"],
+            self.data["J"],
+            "mean",
+            self.Hm0_bins,
+            self.Te_bins,
+        )
 
-        self.assertEqual(JM.shape, (38,9))
+        self.assertEqual(JM.shape, (38, 9))
         self.assertEqual(JM.isna().sum().sum(), 131)
 
     def test_power_matrix(self):
-        L = wave.performance.capture_length(self.data['P'], self.data['J'])
-        LM = wave.performance.capture_length_matrix(self.data['Hm0'], self.data['Te'],
-                        L, 'mean', self.Hm0_bins, self.Te_bins)
-        JM = wave.performance.wave_energy_flux_matrix(self.data['Hm0'], self.data['Te'],
-                        self.data['J'], 'mean', self.Hm0_bins, self.Te_bins)
+        L = wave.performance.capture_length(self.data["P"], self.data["J"])
+        LM = wave.performance.capture_length_matrix(
+            self.data["Hm0"], self.data["Te"], L, "mean", self.Hm0_bins, self.Te_bins
+        )
+        JM = wave.performance.wave_energy_flux_matrix(
+            self.data["Hm0"],
+            self.data["Te"],
+            self.data["J"],
+            "mean",
+            self.Hm0_bins,
+            self.Te_bins,
+        )
         PM = wave.performance.power_matrix(LM, JM)
 
-        self.assertEqual(PM.shape, (38,9))
+        self.assertEqual(PM.shape, (38, 9))
         self.assertEqual(PM.isna().sum().sum(), 131)
 
     def test_mean_annual_energy_production(self):
-        L = wave.performance.capture_length(self.data['P'], self.data['J'])
-        maep = wave.performance.mean_annual_energy_production_timeseries(L, self.data['J'])
+        L = wave.performance.capture_length(self.data["P"], self.data["J"])
+        maep = wave.performance.mean_annual_energy_production_timeseries(
+            L, self.data["J"]
+        )
 
         self.assertAlmostEqual(maep, 1754020.077, 2)
 
-
     def test_plot_matrix(self):
-        filename = abspath(join(plotdir, 'wave_plot_matrix.png'))
+        filename = abspath(join(plotdir, "wave_plot_matrix.png"))
         if isfile(filename):
             os.remove(filename)
 
-        M = wave.performance.wave_energy_flux_matrix(self.data['Hm0'], self.data['Te'],
-                        self.data['J'], 'mean', self.Hm0_bins, self.Te_bins)
+        M = wave.performance.wave_energy_flux_matrix(
+            self.data["Hm0"],
+            self.data["Te"],
+            self.data["J"],
+            "mean",
+            self.Hm0_bins,
+            self.Te_bins,
+        )
 
         plt.figure()
         wave.graphics.plot_matrix(M)
-        plt.savefig(filename, format='png')
+        plt.savefig(filename, format="png")
         plt.close()
 
         self.assertTrue(isfile(filename))
 
     def test_powerperformance_workflow(self):
-        filename = abspath(join(plotdir, 'Capture Length Matrix mean.png'))
+        filename = abspath(join(plotdir, "Capture Length Matrix mean.png"))
         if isfile(filename):
             os.remove(filename)
-        P = pd.Series(np.random.normal(200, 40, 743),index = self.S.columns)
-        statistic = ['mean']
+        P = pd.Series(np.random.normal(200, 40, 743), index=self.S.columns)
+        statistic = ["mean"]
         savepath = plotdir
         show_values = True
         h = 60
         expected = 401239.4822345051
-        x = self.S.T
-        CM,MAEP = wave.performance.power_performance_workflow(self.S, h,
-                        P, statistic, savepath=savepath, show_values=show_values)
+        CM, MAEP = wave.performance.power_performance_workflow(
+            self.S, h, P, statistic, savepath=savepath, show_values=show_values
+        )
 
         self.assertTrue(isfile(filename))
-        self.assertEqual(list(CM.data_vars),self.expected_stats)
+        self.assertEqual(list(CM.data_vars), self.expected_stats)
 
-        error = (expected-MAEP)/expected # SSE
+        error = (expected - MAEP) / expected  # SSE
 
         self.assertLess(error, 1e-6)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/wave/test_resource_metrics.py` & `mhkit-0.8.0/mhkit/tests/wave/test_resource_metrics.py`

 * *Files 21% similar despite different names*

```diff
@@ -5,206 +5,228 @@
 from random import seed, randint
 import matplotlib.pylab as plt
 from datetime import datetime
 import xarray.testing as xrt
 import mhkit.wave as wave
 from io import StringIO
 import pandas as pd
+import xarray as xr
 import numpy as np
 import contextlib
 import unittest
 import netCDF4
 import inspect
 import pickle
 import time
 import json
 import sys
 import os
 
 
 testdir = dirname(abspath(__file__))
-plotdir = join(testdir, 'plots')
+plotdir = join(testdir, "plots")
 isdir = os.path.isdir(plotdir)
-if not isdir: os.mkdir(plotdir)
-datadir = normpath(join(testdir,relpath('../../../examples/data/wave')))
+if not isdir:
+    os.mkdir(plotdir)
+datadir = normpath(join(testdir, relpath("../../../examples/data/wave")))
 
 
 class TestResourceMetrics(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
-        omega = np.arange(0.1,3.5,0.01)
-        self.f = omega/(2*np.pi)
+        omega = np.arange(0.1, 3.5, 0.01)
+        self.f = omega / (2 * np.pi)
         self.Hs = 2.5
         self.Tp = 8
 
-        file_name = join(datadir, 'ValData1.json')
+        file_name = join(datadir, "ValData1.json")
         with open(file_name, "r") as read_file:
             self.valdata1 = pd.DataFrame(json.load(read_file))
 
         self.valdata2 = {}
 
-        file_name = join(datadir, 'ValData2_MC.json')
+        file_name = join(datadir, "ValData2_MC.json")
         with open(file_name, "r") as read_file:
             data = json.load(read_file)
-        self.valdata2['MC'] = data
+        self.valdata2["MC"] = data
         for i in data.keys():
             # Calculate elevation spectra
-            elevation = pd.DataFrame(data[i]['elevation'])
+            elevation = pd.DataFrame(data[i]["elevation"])
             elevation.index = elevation.index.astype(float)
             elevation.sort_index(inplace=True)
-            sample_rate = data[i]['sample_rate']
-            NFFT = data[i]['NFFT']
-            self.valdata2['MC'][i]['S'] = wave.resource.elevation_spectrum(elevation,
-                         sample_rate, NFFT)
+            sample_rate = data[i]["sample_rate"]
+            NFFT = data[i]["NFFT"]
+            self.valdata2["MC"][i]["S"] = wave.resource.elevation_spectrum(
+                elevation, sample_rate, NFFT
+            )
 
-        file_name = join(datadir, 'ValData2_AH.json')
+        file_name = join(datadir, "ValData2_AH.json")
         with open(file_name, "r") as read_file:
             data = json.load(read_file)
-        self.valdata2['AH'] = data
+        self.valdata2["AH"] = data
         for i in data.keys():
             # Calculate elevation spectra
-            elevation = pd.DataFrame(data[i]['elevation'])
+            elevation = pd.DataFrame(data[i]["elevation"])
             elevation.index = elevation.index.astype(float)
             elevation.sort_index(inplace=True)
-            sample_rate = data[i]['sample_rate']
-            NFFT = data[i]['NFFT']
-            self.valdata2['AH'][i]['S'] = wave.resource.elevation_spectrum(elevation,
-                         sample_rate, NFFT)
+            sample_rate = data[i]["sample_rate"]
+            NFFT = data[i]["NFFT"]
+            self.valdata2["AH"][i]["S"] = wave.resource.elevation_spectrum(
+                elevation, sample_rate, NFFT
+            )
 
-        file_name = join(datadir, 'ValData2_CDiP.json')
+        file_name = join(datadir, "ValData2_CDiP.json")
         with open(file_name, "r") as read_file:
             data = json.load(read_file)
-        self.valdata2['CDiP'] = data
+        self.valdata2["CDiP"] = data
         for i in data.keys():
-            temp = pd.Series(data[i]['S']).to_frame('S')
+            temp = pd.Series(data[i]["S"]).to_frame("S")
             temp.index = temp.index.astype(float)
-            self.valdata2['CDiP'][i]['S'] = temp
-
+            self.valdata2["CDiP"][i]["S"] = temp
 
     @classmethod
     def tearDownClass(self):
         pass
 
     def test_kfromw(self):
         for i in self.valdata1.columns:
-            f = np.array(self.valdata1[i]['w'])/(2*np.pi)
-            h = self.valdata1[i]['h']
-            rho = self.valdata1[i]['rho']
+            f = np.array(self.valdata1[i]["w"]) / (2 * np.pi)
+            h = self.valdata1[i]["h"]
+            rho = self.valdata1[i]["rho"]
 
-            expected = self.valdata1[i]['k']
+            expected = self.valdata1[i]["k"]
             k = wave.resource.wave_number(f, h, rho)
-            calculated = k.loc[:,'k'].values
-            error = ((expected-calculated)**2).sum() # SSE
+            calculated = k.loc[:, "k"].values
+            error = ((expected - calculated) ** 2).sum()  # SSE
 
             self.assertLess(error, 1e-6)
 
     def test_kfromw_one_freq(self):
         g = 9.81
         f = 0.1
         h = 1e9
-        w = np.pi*2*f # deep water dispersion
+        w = np.pi * 2 * f  # deep water dispersion
         expected = w**2 / g
         calculated = wave.resource.wave_number(f=f, h=h, g=g).values[0][0]
-        error = np.abs(expected-calculated)
+        error = np.abs(expected - calculated)
         self.assertLess(error, 1e-6)
 
     def test_wave_length(self):
-        k_list=[1,2,10,3]
-        l_expected = (2.*np.pi/np.array(k_list)).tolist()
+        k_array = np.asarray([1.0, 2.0, 10.0, 3.0])
 
-        k_df = pd.DataFrame(k_list,index = [1,2,3,4])
-        k_series= k_df[0]
-        k_array=np.array(k_list)
+        k_int = int(k_array[0])
+        k_float = k_array[0]
+        k_df = pd.DataFrame(k_array, index=[1, 2, 3, 4])
+        k_series = k_df[0]
 
-        for l in [k_list, k_df, k_series, k_array]:
+        for l in [k_array, k_int, k_float, k_df, k_series]:
             l_calculated = wave.resource.wave_length(l)
-            self.assertListEqual(l_expected,l_calculated.tolist())
-
-        idx=0
-        k_int = k_list[idx]
-        l_calculated = wave.resource.wave_length(k_int)
-        self.assertEqual(l_expected[idx],l_calculated)
+            self.assertTrue(np.all(2.0 * np.pi / l == l_calculated))
 
     def test_depth_regime(self):
-        expected = [True,True,False,True]
-        l_list=[1,2,10,3]
-        l_df = pd.DataFrame(l_list,index = [1,2,3,4])
-        l_series= l_df[0]
-        l_array=np.array(l_list)
         h = 10
-        for l in [l_list, l_df, l_series, l_array]:
-            calculated = wave.resource.depth_regime(l,h)
-            self.assertListEqual(expected,calculated.tolist())
-
-        idx=0
-        l_int = l_list[idx]
-        calculated = wave.resource.depth_regime(l_int,h)
-        self.assertEqual(expected[idx],calculated)
 
+        # non-array like formats
+        l_int = 1
+        l_float = 1.0
+        expected = True
+        for l in [l_int, l_float]:
+            calculated = wave.resource.depth_regime(l, h)
+            self.assertTrue(np.all(expected == calculated))
+
+        # array-like formats
+        l_array = np.array([1, 2, 10, 3])
+        l_df = pd.DataFrame(l_array, index=[1, 2, 3, 4])
+        l_series = l_df[0]
+        l_da = xr.DataArray(l_series)
+        l_da.name = "data"
+        l_ds = l_da.to_dataset()
+        expected = [True, True, False, True]
+        for l in [l_array, l_series, l_da, l_ds]:
+            calculated = wave.resource.depth_regime(l, h)
+            self.assertTrue(np.all(expected == calculated))
+
+        # special formatting for pd.DataFrame
+        for l in [l_df]:
+            calculated = wave.resource.depth_regime(l, h)
+            self.assertTrue(np.all(expected == calculated[0]))
 
     def test_wave_celerity(self):
         # Depth regime ratio
-        dr_ratio=2
+        dr_ratio = 2
 
         # small change in f will give similar value cg
-        f=np.linspace(20.0001,20.0005,5)
+        f = np.linspace(20.0001, 20.0005, 5)
 
         # Choose index to spike at. cg spike is inversly proportional to k
-        k_idx=2
-        k_tmp=[1, 1, 0.5, 1, 1]
+        k_idx = 2
+        k_tmp = [1, 1, 0.5, 1, 1]
         k = pd.DataFrame(k_tmp, index=f)
 
         # all shallow
-        cg_shallow1 = wave.resource.wave_celerity(k, h=0.0001,depth_check=True)
-        cg_shallow2 = wave.resource.wave_celerity(k, h=0.0001,depth_check=False)
-        self.assertTrue(all(cg_shallow1.squeeze().values ==
-                            cg_shallow2.squeeze().values))
-
+        cg_shallow1 = wave.resource.wave_celerity(k, h=0.0001, depth_check=True)
+        cg_shallow2 = wave.resource.wave_celerity(k, h=0.0001, depth_check=False)
+        self.assertTrue(
+            all(cg_shallow1.squeeze().values == cg_shallow2.squeeze().values)
+        )
 
         # all deep
-        cg = wave.resource.wave_celerity(k, h=1000,depth_check=True)
-        self.assertTrue(all(np.pi*f/k.squeeze().values == cg.squeeze().values))
+        cg = wave.resource.wave_celerity(k, h=1000, depth_check=True)
+        self.assertTrue(all(np.pi * f / k.squeeze().values == cg.squeeze().values))
 
     def test_energy_flux_deep(self):
-        # Dependent on mhkit.resource.BS spectrum
-        S = wave.resource.jonswap_spectrum(self.f,self.Tp,self.Hs)
+        S = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs)
         Te = wave.resource.energy_period(S)
         Hm0 = wave.resource.significant_wave_height(S)
-        rho=1025
-        g=9.80665
-        coeff = rho*(g**2)/(64*np.pi)
-        J = coeff*(Hm0.squeeze()**2)*Te.squeeze()
 
-        h=-1 # not used when deep=True
+        rho = 1025
+        g = 9.80665
+        coeff = rho * (g**2) / (64 * np.pi)
+        J = coeff * (Hm0.squeeze() ** 2) * Te.squeeze()
+
+        h = -1  # not used when deep=True
         J_calc = wave.resource.energy_flux(S, h, deep=True)
 
         self.assertTrue(J_calc.squeeze() == J)
 
+    def test_energy_flux_shallow(self):
+        S = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs)
+        Te = wave.resource.energy_period(S)
+        Hm0 = wave.resource.significant_wave_height(S)
+
+        rho = 1025
+        g = 9.80665
+        coeff = rho * (g**2) / (64 * np.pi)
+        J = coeff * (Hm0.squeeze() ** 2) * Te.squeeze()
+
+        h = 1000  # effectively deep but without assumptions
+        J_calc = wave.resource.energy_flux(S, h, deep=False)
+        err = np.abs(J_calc.squeeze() - J)
+        self.assertLess(err, 1e-6)
 
     def test_moments(self):
-        for file_i in self.valdata2.keys(): # for each file MC, AH, CDiP
+        for file_i in self.valdata2.keys():  # for each file MC, AH, CDiP
             datasets = self.valdata2[file_i]
-            for s in datasets.keys(): # for each set
+            for s in datasets.keys():  # for each set
                 data = datasets[s]
-                for m in data['m'].keys():
-                    expected = data['m'][m]
-                    S = data['S']
-                    if s == 'CDiP1' or s == 'CDiP6':
-                        f_bins=pd.Series(data['freqBinWidth'])
+                for m in data["m"].keys():
+                    expected = data["m"][m]
+                    S = data["S"]
+                    if s == "CDiP1" or s == "CDiP6":
+                        f_bins = pd.Series(data["freqBinWidth"])
                     else:
                         f_bins = None
 
-                    calculated = wave.resource.frequency_moment(S, int(m)
-                                       ,frequency_bins=f_bins).iloc[0,0]
-                    error = np.abs(expected-calculated)/expected
+                    calculated = wave.resource.frequency_moment(
+                        S, int(m), frequency_bins=f_bins
+                    ).iloc[0, 0]
+                    error = np.abs(expected - calculated) / expected
 
                     self.assertLess(error, 0.01)
 
-
     def test_energy_period_to_peak_period(self):
         # This test checks that if we perform the
         # Te to Tp conversion, we create a spectrum
         # (using Tp) that has the provided Te.
         Hs = 2.5
         Te = np.linspace(5, 20, 10)
         gamma = np.linspace(1, 7, 7)
@@ -214,168 +236,176 @@
                 Tp = wave.resource.energy_period_to_peak_period(T, g)
 
                 f = np.linspace(1 / (10 * Tp), 3 / Tp, 100)
                 S = wave.resource.jonswap_spectrum(f, Tp, Hs, g)
 
                 Te_calc = wave.resource.energy_period(S).values[0][0]
 
-                error = np.abs(T - Te_calc)/Te_calc
+                error = np.abs(T - Te_calc) / Te_calc
                 self.assertLess(error, 0.01)
 
-
     def test_metrics(self):
-       for file_i in self.valdata2.keys(): # for each file MC, AH, CDiP
+        for file_i in self.valdata2.keys():  # for each file MC, AH, CDiP
             datasets = self.valdata2[file_i]
 
-            for s in datasets.keys(): # for each set
-
-
+            for s in datasets.keys():  # for each set
                 data = datasets[s]
-                S = data['S']
-                if file_i == 'CDiP':
-                    f_bins=pd.Series(data['freqBinWidth'])
+                S = data["S"]
+                if file_i == "CDiP":
+                    f_bins = pd.Series(data["freqBinWidth"])
                 else:
                     f_bins = None
 
                 # Hm0
-                expected = data['metrics']['Hm0']
-                calculated = wave.resource.significant_wave_height(S,
-                                        frequency_bins=f_bins).iloc[0,0]
-                error = np.abs(expected-calculated)/expected
-                #print('Hm0', expected, calculated, error)
+                expected = data["metrics"]["Hm0"]
+                calculated = wave.resource.significant_wave_height(
+                    S, frequency_bins=f_bins
+                ).iloc[0, 0]
+                error = np.abs(expected - calculated) / expected
+                # print('Hm0', expected, calculated, error)
                 self.assertLess(error, 0.01)
 
                 # Te
-                expected = data['metrics']['Te']
-                calculated = wave.resource.energy_period(S,
-                                        frequency_bins=f_bins).iloc[0,0]
-                error = np.abs(expected-calculated)/expected
-                #print('Te', expected, calculated, error)
+                expected = data["metrics"]["Te"]
+                calculated = wave.resource.energy_period(S, frequency_bins=f_bins).iloc[
+                    0, 0
+                ]
+                error = np.abs(expected - calculated) / expected
+                # print('Te', expected, calculated, error)
                 self.assertLess(error, 0.01)
 
                 # T0
-                expected = data['metrics']['T0']
-                calculated = wave.resource.average_zero_crossing_period(S,
-                                         frequency_bins=f_bins).iloc[0,0]
-                error = np.abs(expected-calculated)/expected
-                #print('T0', expected, calculated, error)
+                expected = data["metrics"]["T0"]
+                calculated = wave.resource.average_zero_crossing_period(
+                    S, frequency_bins=f_bins
+                ).iloc[0, 0]
+                error = np.abs(expected - calculated) / expected
+                # print('T0', expected, calculated, error)
                 self.assertLess(error, 0.01)
 
                 # Tc
-                expected = data['metrics']['Tc']
-                calculated = wave.resource.average_crest_period(S,
-                # Tc = Tavg**2
-                                     frequency_bins=f_bins).iloc[0,0]**2
-                error = np.abs(expected-calculated)/expected
-                #print('Tc', expected, calculated, error)
+                expected = data["metrics"]["Tc"]
+                calculated = (
+                    wave.resource.average_crest_period(
+                        S,
+                        # Tc = Tavg**2
+                        frequency_bins=f_bins,
+                    ).iloc[0, 0]
+                    ** 2
+                )
+                error = np.abs(expected - calculated) / expected
+                # print('Tc', expected, calculated, error)
                 self.assertLess(error, 0.01)
 
                 # Tm
-                expected = np.sqrt(data['metrics']['Tm'])
-                calculated = wave.resource.average_wave_period(S,
-                                        frequency_bins=f_bins).iloc[0,0]
-                error = np.abs(expected-calculated)/expected
-                #print('Tm', expected, calculated, error)
+                expected = np.sqrt(data["metrics"]["Tm"])
+                calculated = wave.resource.average_wave_period(
+                    S, frequency_bins=f_bins
+                ).iloc[0, 0]
+                error = np.abs(expected - calculated) / expected
+                # print('Tm', expected, calculated, error)
                 self.assertLess(error, 0.01)
 
                 # Tp
-                expected = data['metrics']['Tp']
-                calculated = wave.resource.peak_period(S).iloc[0,0]
-                error = np.abs(expected-calculated)/expected
-                #print('Tp', expected, calculated, error)
+                expected = data["metrics"]["Tp"]
+                calculated = wave.resource.peak_period(S).iloc[0, 0]
+                error = np.abs(expected - calculated) / expected
+                # print('Tp', expected, calculated, error)
                 self.assertLess(error, 0.001)
 
                 # e
-                expected = data['metrics']['e']
-                calculated = wave.resource.spectral_bandwidth(S,
-                                        frequency_bins=f_bins).iloc[0,0]
-                error = np.abs(expected-calculated)/expected
-                #print('e', expected, calculated, error)
+                expected = data["metrics"]["e"]
+                calculated = wave.resource.spectral_bandwidth(
+                    S, frequency_bins=f_bins
+                ).iloc[0, 0]
+                error = np.abs(expected - calculated) / expected
+                # print('e', expected, calculated, error)
                 self.assertLess(error, 0.001)
 
                 # J
-                if file_i != 'CDiP':
-                    for i,j in zip(data['h'],data['J']):
-                        expected = data['J'][j]
-                        calculated = wave.resource.energy_flux(S,i)
-                        error = np.abs(expected-calculated.values)/expected
+                if file_i != "CDiP":
+                    for i, j in zip(data["h"], data["J"]):
+                        expected = data["J"][j]
+                        calculated = wave.resource.energy_flux(S, i)
+                        error = np.abs(expected - calculated.values) / expected
                         self.assertLess(error, 0.1)
 
                 # v
-                if file_i == 'CDiP':
+                if file_i == "CDiP":
                     # this should be updated to run on other datasets
-                    expected = data['metrics']['v']
-                    calculated = wave.resource.spectral_width(S,
-                                        frequency_bins=f_bins).iloc[0,0]
-                    error = np.abs(expected-calculated)/expected
+                    expected = data["metrics"]["v"]
+                    calculated = wave.resource.spectral_width(
+                        S, frequency_bins=f_bins
+                    ).iloc[0, 0]
+                    error = np.abs(expected - calculated) / expected
                     self.assertLess(error, 0.01)
 
-                if file_i == 'MC':
-                    expected = data['metrics']['v']
+                if file_i == "MC":
+                    expected = data["metrics"]["v"]
                     # testing that default uniform frequency bin widths works
-                    calculated = wave.resource.spectral_width(S).iloc[0,0]
-                    error = np.abs(expected-calculated)/expected
+                    calculated = wave.resource.spectral_width(S).iloc[0, 0]
+                    error = np.abs(expected - calculated) / expected
                     self.assertLess(error, 0.01)
 
-
     def test_plot_elevation_timeseries(self):
-        filename = abspath(join(plotdir, 'wave_plot_elevation_timeseries.png'))
+        filename = abspath(join(plotdir, "wave_plot_elevation_timeseries.png"))
         if isfile(filename):
             os.remove(filename)
 
-        data = self.valdata2['MC']
-        temp = pd.DataFrame(data[list(data.keys())[0]]['elevation'])
+        data = self.valdata2["MC"]
+        temp = pd.DataFrame(data[list(data.keys())[0]]["elevation"])
         temp.index = temp.index.astype(float)
         temp.sort_index(inplace=True)
-        eta = temp.iloc[0:100,:]
+        eta = temp.iloc[0:100, :]
 
         plt.figure()
         wave.graphics.plot_elevation_timeseries(eta)
-        plt.savefig(filename, format='png')
+        plt.savefig(filename, format="png")
         plt.close()
 
         self.assertTrue(isfile(filename))
 
-class TestPlotResouceCharacterizations(unittest.TestCase):
 
+class TestPlotResouceCharacterizations(unittest.TestCase):
     @classmethod
     def setUpClass(self):
-        f_name= 'Hm0_Te_46022.json'
-        self.Hm0Te = pd.read_json(join(datadir,f_name))
+        f_name = "Hm0_Te_46022.json"
+        self.Hm0Te = pd.read_json(join(datadir, f_name))
+
     @classmethod
     def tearDownClass(self):
         pass
-    def test_plot_avg_annual_energy_matrix(self):
 
-        filename = abspath(join(plotdir, 'avg_annual_scatter_table.png'))
+    def test_plot_avg_annual_energy_matrix(self):
+        filename = abspath(join(plotdir, "avg_annual_scatter_table.png"))
         if isfile(filename):
             os.remove(filename)
 
         Hm0Te = self.Hm0Te
         Hm0Te.drop(Hm0Te[Hm0Te.Hm0 > 20].index, inplace=True)
-        J = np.random.random(len(Hm0Te))*100
+        J = np.random.random(len(Hm0Te)) * 100
 
         plt.figure()
-        fig = wave.graphics.plot_avg_annual_energy_matrix(Hm0Te.Hm0,
-            Hm0Te.Te, J, Hm0_bin_size=0.5, Te_bin_size=1)
-        plt.savefig(filename, format='png')
+        fig = wave.graphics.plot_avg_annual_energy_matrix(
+            Hm0Te.Hm0, Hm0Te.Te, J, Hm0_bin_size=0.5, Te_bin_size=1
+        )
+        plt.savefig(filename, format="png")
         plt.close()
 
         self.assertTrue(isfile(filename))
 
     def test_plot_monthly_cumulative_distribution(self):
-
-        filename = abspath(join(plotdir, 'monthly_cumulative_distribution.png'))
+        filename = abspath(join(plotdir, "monthly_cumulative_distribution.png"))
         if isfile(filename):
             os.remove(filename)
 
-        a = pd.date_range(start='1/1/2010',  periods=10000, freq='h')
-        S = pd.Series(np.random.random(len(a)) , index=a)
-        ax=wave.graphics.monthly_cumulative_distribution(S)
-        plt.savefig(filename, format='png')
+        a = pd.date_range(start="1/1/2010", periods=10000, freq="h")
+        S = pd.Series(np.random.random(len(a)), index=a)
+        ax = wave.graphics.monthly_cumulative_distribution(S)
+        plt.savefig(filename, format="png")
         plt.close()
 
         self.assertTrue(isfile(filename))
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `mhkit-0.7.0/mhkit/tests/wave/test_resource_spectrum.py` & `mhkit-0.8.0/mhkit/tests/wave/test_resource_spectrum.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,81 +1,73 @@
 from os.path import abspath, dirname, join, isfile, normpath, relpath
 from pandas.testing import assert_frame_equal
 from numpy.testing import assert_allclose
 from scipy.interpolate import interp1d
-from random import seed, randint
 import matplotlib.pylab as plt
-from datetime import datetime
-import xarray.testing as xrt
+import xarray as xr
 import mhkit.wave as wave
-from io import StringIO
 import pandas as pd
 import numpy as np
-import contextlib
 import unittest
-import netCDF4
-import inspect
-import pickle
-import time
-import json
-import sys
 import os
 
 
 testdir = dirname(abspath(__file__))
-plotdir = join(testdir, 'plots')
+plotdir = join(testdir, "plots")
 isdir = os.path.isdir(plotdir)
-if not isdir: os.mkdir(plotdir)
-datadir = normpath(join(testdir,relpath('../../../examples/data/wave')))
+if not isdir:
+    os.mkdir(plotdir)
+datadir = normpath(join(testdir, relpath("../../../examples/data/wave")))
 
 
 class TestResourceSpectrum(unittest.TestCase):
-
     @classmethod
     def setUpClass(self):
         Trep = 600
         df = 1 / Trep
         self.f = np.arange(0, 1, df)
         self.Hs = 2.5
         self.Tp = 8
         self.t = np.arange(0, Trep, 0.05)
 
     @classmethod
     def tearDownClass(self):
         pass
 
     def test_pierson_moskowitz_spectrum(self):
-        S = wave.resource.pierson_moskowitz_spectrum(self.f,self.Tp,self.Hs)
-        Hm0 = wave.resource.significant_wave_height(S).iloc[0,0]
-        Tp0 = wave.resource.peak_period(S).iloc[0,0]
+        S = wave.resource.pierson_moskowitz_spectrum(self.f, self.Tp, self.Hs)
+        Hm0 = wave.resource.significant_wave_height(S).iloc[0, 0]
+        Tp0 = wave.resource.peak_period(S).iloc[0, 0]
 
-        errorHm0 = np.abs(self.Tp - Tp0)/self.Tp
-        errorTp0 = np.abs(self.Hs - Hm0)/self.Hs
+        errorHm0 = np.abs(self.Tp - Tp0) / self.Tp
+        errorTp0 = np.abs(self.Hs - Hm0) / self.Hs
 
         self.assertLess(errorHm0, 0.01)
         self.assertLess(errorTp0, 0.01)
 
     def test_pierson_moskowitz_spectrum_zero_freq(self):
         df = 0.1
         f_zero = np.arange(0, 1, df)
         f_nonzero = np.arange(df, 1, df)
 
         S_zero = wave.resource.pierson_moskowitz_spectrum(f_zero, self.Tp, self.Hs)
-        S_nonzero = wave.resource.pierson_moskowitz_spectrum(f_nonzero, self.Tp, self.Hs)
+        S_nonzero = wave.resource.pierson_moskowitz_spectrum(
+            f_nonzero, self.Tp, self.Hs
+        )
 
         self.assertEqual(S_zero.values.squeeze()[0], 0.0)
         self.assertGreater(S_nonzero.values.squeeze()[0], 0.0)
 
     def test_jonswap_spectrum(self):
         S = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs)
-        Hm0 = wave.resource.significant_wave_height(S).iloc[0,0]
-        Tp0 = wave.resource.peak_period(S).iloc[0,0]
+        Hm0 = wave.resource.significant_wave_height(S).iloc[0, 0]
+        Tp0 = wave.resource.peak_period(S).iloc[0, 0]
 
-        errorHm0 = np.abs(self.Tp - Tp0)/self.Tp
-        errorTp0 = np.abs(self.Hs - Hm0)/self.Hs
+        errorHm0 = np.abs(self.Tp - Tp0) / self.Tp
+        errorTp0 = np.abs(self.Hs - Hm0) / self.Hs
 
         self.assertLess(errorHm0, 0.01)
         self.assertLess(errorTp0, 0.01)
 
     def test_jonswap_spectrum_zero_freq(self):
         df = 0.1
         f_zero = np.arange(0, 1, df)
@@ -83,137 +75,146 @@
 
         S_zero = wave.resource.jonswap_spectrum(f_zero, self.Tp, self.Hs)
         S_nonzero = wave.resource.jonswap_spectrum(f_nonzero, self.Tp, self.Hs)
 
         self.assertEqual(S_zero.values.squeeze()[0], 0.0)
         self.assertGreater(S_nonzero.values.squeeze()[0], 0.0)
 
-    def test_surface_elevation_phases_np_and_pd(self):
-        S0 = wave.resource.jonswap_spectrum(self.f,self.Tp,self.Hs)
-        S1 = wave.resource.jonswap_spectrum(self.f,self.Tp,self.Hs*1.1)
+    def test_surface_elevation_phases_xr_and_pd(self):
+        S0 = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs)
+        S1 = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs * 1.1)
         S = pd.concat([S0, S1], axis=1)
 
         phases_np = np.random.rand(S.shape[0], S.shape[1]) * 2 * np.pi
         phases_pd = pd.DataFrame(phases_np, index=S.index, columns=S.columns)
+        phases_xr = xr.Dataset(phases_pd)
 
-        eta_np = wave.resource.surface_elevation(S, self.t, phases=phases_np, seed=1)
+        eta_xr = wave.resource.surface_elevation(S, self.t, phases=phases_xr, seed=1)
         eta_pd = wave.resource.surface_elevation(S, self.t, phases=phases_pd, seed=1)
 
-        assert_frame_equal(eta_np, eta_pd)
+        assert_frame_equal(eta_xr, eta_pd)
 
     def test_surface_elevation_frequency_bins_np_and_pd(self):
-        S0 = wave.resource.jonswap_spectrum(self.f,self.Tp,self.Hs)
-        S1 = wave.resource.jonswap_spectrum(self.f,self.Tp,self.Hs*1.1)
+        S0 = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs)
+        S1 = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs * 1.1)
         S = pd.concat([S0, S1], axis=1)
 
         eta0 = wave.resource.surface_elevation(S, self.t, seed=1)
 
-        f_bins_np = np.array([np.diff(S.index)[0]]*len(S))
-        f_bins_pd = pd.DataFrame(f_bins_np, index=S.index, columns=['df'])
+        f_bins_np = np.array([np.diff(S.index)[0]] * len(S))
+        f_bins_pd = pd.DataFrame(f_bins_np, index=S.index, columns=["df"])
 
-        eta_np = wave.resource.surface_elevation(S, self.t, frequency_bins=f_bins_np, seed=1)
-        eta_pd = wave.resource.surface_elevation(S, self.t, frequency_bins=f_bins_pd, seed=1)
+        eta_np = wave.resource.surface_elevation(
+            S, self.t, frequency_bins=f_bins_np, seed=1
+        )
+        eta_pd = wave.resource.surface_elevation(
+            S, self.t, frequency_bins=f_bins_pd, seed=1
+        )
 
         assert_frame_equal(eta0, eta_np)
         assert_frame_equal(eta_np, eta_pd)
 
     def test_surface_elevation_moments(self):
         S = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs)
         eta = wave.resource.surface_elevation(S, self.t, seed=1)
         dt = self.t[1] - self.t[0]
-        Sn = wave.resource.elevation_spectrum(eta, 1/dt, len(eta.values),
-                                              detrend=False, window='boxcar',
-                                              noverlap=0)
-
-        m0 = wave.resource.frequency_moment(S,0).m0.values[0]
-        m0n = wave.resource.frequency_moment(Sn,0).m0.values[0]
-        errorm0 = np.abs((m0 - m0n)/m0)
+        Sn = wave.resource.elevation_spectrum(
+            eta, 1 / dt, len(eta.values), detrend=False, window="boxcar", noverlap=0
+        )
+
+        m0 = wave.resource.frequency_moment(S, 0).m0.values[0]
+        m0n = wave.resource.frequency_moment(Sn, 0).m0.values[0]
+        errorm0 = np.abs((m0 - m0n) / m0)
 
         self.assertLess(errorm0, 0.01)
 
-        m1 = wave.resource.frequency_moment(S,1).m1.values[0]
-        m1n = wave.resource.frequency_moment(Sn,1).m1.values[0]
-        errorm1 = np.abs((m1 - m1n)/m1)
+        m1 = wave.resource.frequency_moment(S, 1).m1.values[0]
+        m1n = wave.resource.frequency_moment(Sn, 1).m1.values[0]
+        errorm1 = np.abs((m1 - m1n) / m1)
 
         self.assertLess(errorm1, 0.01)
 
     def test_surface_elevation_rmse(self):
         S = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs)
         eta = wave.resource.surface_elevation(S, self.t, seed=1)
         dt = self.t[1] - self.t[0]
-        Sn = wave.resource.elevation_spectrum(eta, 1/dt, len(eta),
-                                              detrend=False, window='boxcar',
-                                              noverlap=0)
+        Sn = wave.resource.elevation_spectrum(
+            eta, 1 / dt, len(eta), detrend=False, window="boxcar", noverlap=0
+        )
 
         fSn = interp1d(Sn.index.values, Sn.values, axis=0)
-        rmse = (S.values - fSn(S.index.values))**2
-        rmse_sum = (np.sum(rmse)/len(rmse))**0.5
+        Sn_interp = fSn(S.index.values).squeeze()
+        rmse = (S.values.squeeze() - Sn_interp) ** 2
+        rmse_sum = (np.sum(rmse) / len(rmse)) ** 0.5
 
         self.assertLess(rmse_sum, 0.02)
 
     def test_ifft_sum_of_sines(self):
         S = wave.resource.jonswap_spectrum(self.f, self.Tp, self.Hs)
 
-        eta_ifft = wave.resource.surface_elevation(S, self.t, seed=1, method='ifft')
-        eta_sos = wave.resource.surface_elevation(S, self.t, seed=1, method='sum_of_sines')
+        eta_ifft = wave.resource.surface_elevation(S, self.t, seed=1, method="ifft")
+        eta_sos = wave.resource.surface_elevation(
+            S, self.t, seed=1, method="sum_of_sines"
+        )
 
-        assert_allclose(eta_ifft, eta_sos)     
+        assert_allclose(eta_ifft, eta_sos)
 
     def test_plot_spectrum(self):
-        filename = abspath(join(plotdir, 'wave_plot_spectrum.png'))
+        filename = abspath(join(plotdir, "wave_plot_spectrum.png"))
         if isfile(filename):
             os.remove(filename)
 
-        S = wave.resource.pierson_moskowitz_spectrum(self.f,self.Tp,self.Hs)
+        S = wave.resource.pierson_moskowitz_spectrum(self.f, self.Tp, self.Hs)
 
         plt.figure()
         wave.graphics.plot_spectrum(S)
-        plt.savefig(filename, format='png')
+        plt.savefig(filename, format="png")
         plt.close()
 
         self.assertTrue(isfile(filename))
 
     def test_plot_chakrabarti(self):
-        filename = abspath(join(plotdir, 'wave_plot_chakrabarti.png'))
+        filename = abspath(join(plotdir, "wave_plot_chakrabarti.png"))
         if isfile(filename):
             os.remove(filename)
 
         D = 5
         H = 10
         lambda_w = 200
 
         wave.graphics.plot_chakrabarti(H, lambda_w, D)
         plt.savefig(filename)
 
     def test_plot_chakrabarti_np(self):
-        filename = abspath(join(plotdir, 'wave_plot_chakrabarti_np.png'))
+        filename = abspath(join(plotdir, "wave_plot_chakrabarti_np.png"))
         if isfile(filename):
             os.remove(filename)
 
         D = np.linspace(5, 15, 5)
         H = 10 * np.ones_like(D)
         lambda_w = 200 * np.ones_like(D)
 
         wave.graphics.plot_chakrabarti(H, lambda_w, D)
         plt.savefig(filename)
 
         self.assertTrue(isfile(filename))
 
     def test_plot_chakrabarti_pd(self):
-        filename = abspath(join(plotdir, 'wave_plot_chakrabarti_pd.png'))
+        filename = abspath(join(plotdir, "wave_plot_chakrabarti_pd.png"))
         if isfile(filename):
             os.remove(filename)
 
         D = np.linspace(5, 15, 5)
         H = 10 * np.ones_like(D)
         lambda_w = 200 * np.ones_like(D)
-        df = pd.DataFrame([H.flatten(),lambda_w.flatten(),D.flatten()],
-                         index=['H','lambda_w','D']).transpose()
+        df = pd.DataFrame(
+            [H.flatten(), lambda_w.flatten(), D.flatten()], index=["H", "lambda_w", "D"]
+        ).transpose()
 
         wave.graphics.plot_chakrabarti(df.H, df.lambda_w, df.D)
         plt.savefig(filename)
 
         self.assertTrue(isfile(filename))
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     unittest.main()
-
```

### Comparing `mhkit-0.7.0/mhkit/tidal/graphics.py` & `mhkit-0.8.0/mhkit/tidal/graphics.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,77 +1,85 @@
 import numpy as np
-import pandas as pd
 import bisect
 from scipy.interpolate import interpn as _interpn
-from scipy.interpolate import  interp1d
-import matplotlib.pyplot as plt 
-from  mhkit.river.resource import exceedance_probability
+from scipy.interpolate import interp1d
+import matplotlib.pyplot as plt
+from mhkit.river.resource import exceedance_probability
 from mhkit.tidal.resource import _histogram, _flood_or_ebb
 from mhkit.river.graphics import plot_velocity_duration_curve, _xy_plot
+from mhkit.utils import convert_to_dataarray
 
 
 def _initialize_polar(ax=None, metadata=None, flood=None, ebb=None):
     """
     Initializes a polar plots with cardinal directions and ebb/flow
-    
+
     Parameters
     ----------
     ax :axes
     metadata: dictionary
         Contains site meta data
     Returns
     -------
     ax: axes
-    """   
-    
-    if ax==None:
+    """
+
+    if ax == None:
         # Initialize polar plot
         fig = plt.figure(figsize=(12, 8))
         ax = plt.axes(polar=True)
     # Angles are measured clockwise from true north
-    ax.set_theta_zero_location('N')
+    ax.set_theta_zero_location("N")
     ax.set_theta_direction(-1)
-    xticks = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']
+    xticks = ["N", "NE", "E", "SE", "S", "SW", "W", "NW"]
     # Polar plots do not have minor ticks, insert flood/ebb into major ticks
     xtickDegrees = [0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0]
     # Set title and metadata box
     if metadata != None:
         # Set the Title
-        plt.title(metadata['name'])
+        plt.title(metadata["name"])
         # List of strings for metadata box
-        bouy_str = [f'Lat = {float(metadata["lat"]):0.2f}$\degree$', 
-                    f'Lon = {float(metadata["lon"]):0.2f}$\degree$']
+        bouy_str = [
+            f'Lat = {float(metadata["lat"]):0.2f}$\degree$',
+            f'Lon = {float(metadata["lon"]):0.2f}$\degree$',
+        ]
         # Create string for text box
-        bouy_data = '\n'.join(bouy_str)
+        bouy_data = "\n".join(bouy_str)
         # Set the text box
-        ax.text(-0.3, 0.80, bouy_data, transform=ax.transAxes, fontsize=14,
-                verticalalignment='top',bbox=dict(facecolor='none', 
-                edgecolor='k', pad=5) )
+        ax.text(
+            -0.3,
+            0.80,
+            bouy_data,
+            transform=ax.transAxes,
+            fontsize=14,
+            verticalalignment="top",
+            bbox=dict(facecolor="none", edgecolor="k", pad=5),
+        )
     # If defined plot flood and ebb directions as major ticks
     if flood != None:
         # Get flood direction in degrees
         floodDirection = flood
-        # Polar plots do not have minor ticks, 
+        # Polar plots do not have minor ticks,
         #    insert flood/ebb into major ticks
         bisect.insort(xtickDegrees, floodDirection)
         # Get location in list
-        idxFlood = xtickDegrees.index(floodDirection) 
+        idxFlood = xtickDegrees.index(floodDirection)
         # Insert label at appropriate location
-        xticks[idxFlood:idxFlood]=['\nFlood']
+        xticks[idxFlood:idxFlood] = ["\nFlood"]
     if ebb != None:
         # Get flood direction in degrees
-        ebbDirection =ebb
-        # Polar plots do not have minor ticks, 
+        ebbDirection = ebb
+        # Polar plots do not have minor ticks,
         #    insert flood/ebb into major ticks
         bisect.insort(xtickDegrees, ebbDirection)
         # Get location in list
-        idxEbb = xtickDegrees.index(ebbDirection) 
+        idxEbb = xtickDegrees.index(ebbDirection)
         # Insert label at appropriate location
-        xticks[idxEbb:idxEbb]=['\nEbb']
-    ax.set_xticks(np.array(xtickDegrees)*np.pi/180.)  
+        xticks[idxEbb:idxEbb] = ["\nEbb"]
+    ax.set_xticks(np.array(xtickDegrees) * np.pi / 180.0)
     ax.set_xticklabels(xticks)
     return ax
 
 
 def _check_inputs(directions, velocities, flood, ebb):
     """
     Runs checks on inputs for the graphics functions.
@@ -79,224 +87,226 @@
     Parameters
     ----------
     directions: array-like
         Directions in degrees with 0 degrees specified as true north
     velocities: array-like
         Velocities in m/s
     flood: float
-        Direction in degrees added to theta ticks 
+        Direction in degrees added to theta ticks
     ebb: float
         Direction in degrees added to theta ticks
     """
 
-    if not isinstance(velocities,(np.ndarray, pd.Series)):
-        raise TypeError('velocities must be of type np.ndarry or pd.Series')
-    if isinstance(velocities, np.ndarray):
-        velocities = pd.Series(velocities)
-
-    if not isinstance(directions,(np.ndarray, pd.Series)):
-        raise TypeError('directions must be of type np.ndarry or pd.Series')
-    if isinstance(directions, np.ndarray):
-        directions = pd.Series(directions)
+    velocities = convert_to_dataarray(velocities)
+    directions = convert_to_dataarray(directions)
 
     if len(velocities) != len(directions):
-        raise ValueError('velocities and directions must have the same length')
+        raise ValueError("velocities and directions must have the same length")
     if all(np.nan_to_num(velocities.values) < 0):
-        raise ValueError('All velocities must be positive')
-    if all(np.nan_to_num(directions.values) < 0) and all(np.nan_to_num(directions.values) > 360):
-        raise ValueError('directions must be between 0 and 360 degrees')
-    
+        raise ValueError("All velocities must be positive")
+    if all(np.nan_to_num(directions.values) < 0) and all(
+        np.nan_to_num(directions.values) > 360
+    ):
+        raise ValueError("directions must be between 0 and 360 degrees")
     if not isinstance(flood, (int, float, type(None))):
-        raise TypeError('flood must be of type int or float')
+        raise TypeError("flood must be of type int or float")
     if not isinstance(ebb, (int, float, type(None))):
-        raise TypeError('ebb must be of type int or float')
+        raise TypeError("ebb must be of type int or float")
     if flood is not None:
         if (flood < 0) and (flood > 360):
-            raise ValueError('flood must be between 0 and 360 degrees')
+            raise ValueError("flood must be between 0 and 360 degrees")
     if ebb is not None:
         if (ebb < 0) and (ebb > 360):
-            raise ValueError('ebb must be between 0 and 360 degrees')
+            raise ValueError("ebb must be between 0 and 360 degrees")
 
 
 def plot_rose(
-    directions, 
-    velocities, 
-    width_dir, 
-    width_vel, 
-    ax=None, 
-    metadata=None, 
-    flood=None, 
-    ebb=None
-    ):
+    directions,
+    velocities,
+    width_dir,
+    width_vel,
+    ax=None,
+    metadata=None,
+    flood=None,
+    ebb=None,
+):
     """
-    Creates a polar histogram. Direction angles from binned histogram must 
+    Creates a polar histogram. Direction angles from binned histogram must
     be specified such that 0  degrees is north.
 
     Parameters
     ----------
     directions: array-like
         Directions in degrees with 0 degrees specified as true north
     velocities: array-like
         Velocities in m/s
-    width_dir: float 
+    width_dir: float
         Width of directional bins for histogram in degrees
-    width_vel: float 
+    width_vel: float
         Width of velocity bins for histogram in m/s
     ax: float
         Polar plot axes to add polar histogram
     metadata: dictonary
         If provided needs keys ['name', 'lat', 'lon'] for plot title
         and information box on plot
     flood: float
-        Direction in degrees added to theta ticks 
+        Direction in degrees added to theta ticks
     ebb: float
         Direction in degrees added to theta ticks
     Returns
     -------
     ax: figure
         Water current rose plot
     """
 
     _check_inputs(directions, velocities, flood, ebb)
 
     if not isinstance(width_dir, (int, float)):
-        raise TypeError('width_dir must be of type int or float')
+        raise TypeError("width_dir must be of type int or float")
     if not isinstance(width_vel, (int, float)):
-        raise TypeError('width_vel must be of type int or float')
+        raise TypeError("width_vel must be of type int or float")
     if width_dir < 0:
-        raise ValueError('width_dir must be greater than 0')
+        raise ValueError("width_dir must be greater than 0")
     if width_vel < 0:
-        raise ValueError('width_vel must be greater than 0') 
-    
+        raise ValueError("width_vel must be greater than 0")
+
     # Calculate the 2D histogram
     H, dir_edges, vel_edges = _histogram(directions, velocities, width_dir, width_vel)
     # Determine number of bins
     dir_bins = H.shape[0]
     vel_bins = H.shape[1]
-    # Create the angles 
-    thetas = np.arange(0,2*np.pi, 2*np.pi/dir_bins)
+    # Create the angles
+    thetas = np.arange(0, 2 * np.pi, 2 * np.pi / dir_bins)
     # Initialize the polar polt
     ax = _initialize_polar(ax=ax, metadata=metadata, flood=flood, ebb=ebb)
     # Set bar color based on wind speed
     colors = plt.cm.viridis(np.linspace(0, 1.0, vel_bins))
     # Set the current speed bin label names
     # Calculate the 2D histogram
-    labels = [ f'{i:.1f}-{j:.1f}' for i,j in zip(vel_edges[:-1],vel_edges[1:])]
+    labels = [f"{i:.1f}-{j:.1f}" for i, j in zip(vel_edges[:-1], vel_edges[1:])]
     # Initialize the vertical-offset (polar radius) for the stacked bar chart.
     r_offset = np.zeros(dir_bins)
     for vel_bin in range(vel_bins):
         # Plot fist set of bars in all directions
-        ax.bar(thetas, H[:,vel_bin], width=(2*np.pi/dir_bins), 
-               bottom=r_offset, color=colors[vel_bin], label=labels[vel_bin])
+        ax.bar(
+            thetas,
+            H[:, vel_bin],
+            width=(2 * np.pi / dir_bins),
+            bottom=r_offset,
+            color=colors[vel_bin],
+            label=labels[vel_bin],
+        )
         # Increase the radius offset in all directions
-        r_offset = r_offset + H[:,vel_bin]
-    # Add the a legend for current speed bins 
-    plt.legend(loc='best',title='Velocity bins [m/s]', bbox_to_anchor=(1.29, 1.00), ncol=1)
+        r_offset = r_offset + H[:, vel_bin]
+    # Add the a legend for current speed bins
+    plt.legend(
+        loc="best", title="Velocity bins [m/s]", bbox_to_anchor=(1.29, 1.00), ncol=1
+    )
     # Get the r-ticks (polar y-ticks)
     yticks = plt.yticks()
-    # Format y-ticks with  units for clarity 
-    rticks =  [f'{y:.1f}%' for y in yticks[0]]
+    # Format y-ticks with  units for clarity
+    rticks = [f"{y:.1f}%" for y in yticks[0]]
     # Set the y-ticks
-    plt.yticks(yticks[0],rticks)
+    plt.yticks(yticks[0], rticks)
     return ax
 
 
 def plot_joint_probability_distribution(
-    directions, 
-    velocities, 
-    width_dir, 
-    width_vel, 
-    ax=None, 
+    directions,
+    velocities,
+    width_dir,
+    width_vel,
+    ax=None,
     metadata=None,
-    flood=None, 
-    ebb=None
-    ):
+    flood=None,
+    ebb=None,
+):
     """
-    Creates a polar histogram. Direction angles from binned histogram must 
+    Creates a polar histogram. Direction angles from binned histogram must
     be specified such that 0 is north.
 
     Parameters
     ----------
     directions: array-like
         Directions in degrees with 0 degrees specified as true north
     velocities: array-like
         Velocities in m/s
-    width_dir: float 
+    width_dir: float
         Width of directional bins for histogram in degrees
-    width_vel: float 
+    width_vel: float
         Width of velocity bins for histogram in m/s
     ax: float
         Polar plot axes to add polar histogram
     metadata: dictonary
         If provided needs keys ['name', 'Lat', 'Lon'] for plot title
         and information box on plot
     flood: float
-        Direction in degrees added to theta ticks 
+        Direction in degrees added to theta ticks
     ebb: float
         Direction in degrees added to theta ticks
     Returns
     -------
     ax: figure
-        Joint probability distribution  
+        Joint probability distribution
     """
 
     _check_inputs(directions, velocities, flood, ebb)
 
     if not isinstance(width_dir, (int, float)):
-        raise TypeError('width_dir must be of type int or float')
+        raise TypeError("width_dir must be of type int or float")
     if not isinstance(width_vel, (int, float)):
-        raise TypeError('width_vel must be of type int or float')
+        raise TypeError("width_vel must be of type int or float")
     if width_dir < 0:
-        raise ValueError('width_dir must be greater than 0')
+        raise ValueError("width_dir must be greater than 0")
     if width_vel < 0:
-        raise ValueError('width_vel must be greater than 0') 
-    
+        raise ValueError("width_vel must be greater than 0")
+
     # Calculate the 2D histogram
     H, dir_edges, vel_edges = _histogram(directions, velocities, width_dir, width_vel)
     # Initialize the polar polt
     ax = _initialize_polar(ax=ax, metadata=metadata, flood=flood, ebb=ebb)
     # Set the current speed bin label names
-    labels = [ f'{i:.1f}-{j:.1f}' for i,j in zip(vel_edges[:-1],vel_edges[1:])]
+    labels = [f"{i:.1f}-{j:.1f}" for i, j in zip(vel_edges[:-1], vel_edges[1:])]
     # Set vel & dir bins to middle of bin except at ends
-    dir_bins = 0.5*(dir_edges[1:] + dir_edges[:-1]) # set all bins to middle
-    vel_bins = 0.5*(vel_edges[1:] + vel_edges[:-1])
+    dir_bins = 0.5 * (dir_edges[1:] + dir_edges[:-1])  # set all bins to middle
+    vel_bins = 0.5 * (vel_edges[1:] + vel_edges[:-1])
     # Reset end of bin range to edge of bin
     dir_bins[0] = dir_edges[0]
     vel_bins[0] = vel_edges[0]
     dir_bins[-1] = dir_edges[-1]
     vel_bins[-1] = vel_edges[-1]
     # Interpolate the bins back to specific data points
-    z = _interpn((dir_bins, vel_bins),
-                  H , np.vstack([directions,velocities]).T, method = "splinef2d",
-                  bounds_error = False )
-    # Plot the most probable data last 
+    z = _interpn(
+        (dir_bins, vel_bins),
+        H,
+        np.vstack([directions, velocities]).T,
+        method="splinef2d",
+        bounds_error=False,
+    )
+    # Plot the most probable data last
     idx = z.argsort()
     # Convert to radians and order points by probability
-    theta,r,z = directions.values[idx]*np.pi/180, velocities.values[idx], z[idx]
-    # Create scatter plot colored by probability density    
+    theta, r, z = directions.values[idx] * np.pi / 180, velocities.values[idx], z[idx]
+    # Create scatter plot colored by probability density
     sx = ax.scatter(theta, r, c=z, s=5, edgecolor=None)
     # Create colorbar
-    plt.colorbar(sx, ax=ax, label='Joint Probability [%]')
+    plt.colorbar(sx, ax=ax, label="Joint Probability [%]")
 
     # Get the r-ticks (polar y-ticks)
     yticks = ax.get_yticks()
     # Set y-ticks labels
-    ax.set_yticks(yticks) # to avoid matplotlib warning
-    ax.set_yticklabels([f'{y:.1f} $m/s$' for y in yticks])
+    ax.set_yticks(yticks)  # to avoid matplotlib warning
+    ax.set_yticklabels([f"{y:.1f} $m/s$" for y in yticks])
 
     return ax
 
 
 def plot_current_timeseries(
-    directions,
-    velocities, 
-    principal_direction,
-    label=None, 
-    ax=None
-    ):
+    directions, velocities, principal_direction, label=None, ax=None
+):
     """
     Returns a plot of velocity from an array of direction and speed
     data in the direction of the supplied principal_direction.
 
     Parameters
     ----------
     directions: array-like
@@ -304,49 +314,49 @@
     velocities: array-like
         Time-series of speeds [m/s]
     principal_direction: float
         Direction to compute the velocity in [degrees]
     label: string
         Label to use in the legend
     ax : matplotlib axes object
-        Axes for plotting.  If None, then a new figure with a single 
+        Axes for plotting.  If None, then a new figure with a single
         axes is used.
 
     Returns
     -------
     ax: figure
         Time-series plot of current-speed velocity
     """
 
     _check_inputs(directions, velocities, flood=None, ebb=None)
-    
+
     if not isinstance(principal_direction, (int, float)):
-        raise TypeError('principal_direction must be of type int or float') 
+        raise TypeError("principal_direction must be of type int or float")
     if (principal_direction < 0) and (principal_direction > 360):
-        raise ValueError('principal_direction must be between 0 and 360 degrees')
-    
+        raise ValueError("principal_direction must be between 0 and 360 degrees")
+
     # Rotate coordinate system by supplied principal_direction
     principal_directions = directions - principal_direction
     # Calculate the velocity
-    velocity = velocities * np.cos(np.pi/180*principal_directions)
+    velocity = velocities * np.cos(np.pi / 180 * principal_directions)
     # Call on standard xy plotting
-    ax = _xy_plot(velocities.index, velocity, fmt='-', label=label, 
-                  xlabel='Time', ylabel='Velocity [$m/s$]', ax=ax)
+    ax = _xy_plot(
+        velocities.index,
+        velocity,
+        fmt="-",
+        label=label,
+        xlabel="Time",
+        ylabel="Velocity [$m/s$]",
+        ax=ax,
+    )
     return ax
 
 
-def tidal_phase_probability(
-    directions,
-    velocities,
-    flood,
-    ebb, 
-    bin_size=0.1,
-    ax=None
-    ):
-    """    
+def tidal_phase_probability(directions, velocities, flood, ebb, bin_size=0.1, ax=None):
+    """
     Discretizes the tidal series speed by bin size and returns a plot
     of the probability for each bin in the flood or ebb tidal phase.
 
     Parameters
     ----------
     directions: array-like
         Time-series of directions [degrees]
@@ -355,130 +365,147 @@
     flood: float or int
         Principal component of flow in the flood direction [degrees]
     ebb: float or int
         Principal component of flow in the ebb direction [degrees]
     bin_size: float
         Speed bin size. Optional. Deaful = 0.1 m/s
     ax : matplotlib axes object
-        Axes for plotting.  If None, then a new figure with a single 
+        Axes for plotting.  If None, then a new figure with a single
         axes is used.
 
     Returns
     -------
     ax: figure
     """
 
     _check_inputs(directions, velocities, flood, ebb)
     if bin_size < 0:
-        raise ValueError('bin_size must be greater than 0')     
-        
-    if ax==None:
+        raise ValueError("bin_size must be greater than 0")
+
+    if ax == None:
         fig, ax = plt.subplots(figsize=(12, 8))
 
     isEbb = _flood_or_ebb(directions, flood, ebb)
 
-    decimals = round(bin_size/0.1)
-    N_bins = int(round(velocities.max(),decimals)/bin_size)
+    decimals = round(bin_size / 0.1)
+    N_bins = int(round(velocities.max(), decimals) / bin_size)
 
     H, bins = np.histogram(velocities, bins=N_bins)
     H_ebb, bins1 = np.histogram(velocities[isEbb], bins=bins)
     H_flood, bins2 = np.histogram(velocities[~isEbb], bins=bins)
 
-    p_ebb = H_ebb/H
-    p_flood = H_flood/H
+    p_ebb = H_ebb / H
+    p_flood = H_flood / H
 
     center = (bins[:-1] + bins[1:]) / 2
     width = 0.9 * (bins[1] - bins[0])
 
-    mask1 = np.ma.where(p_ebb>=p_flood)
-    mask2 = np.ma.where(p_flood>=p_ebb)
+    mask1 = np.ma.where(p_ebb >= p_flood)
+    mask2 = np.ma.where(p_flood >= p_ebb)
 
-    ax.bar(center[mask1], height=p_ebb[mask1], edgecolor='black', width=width, 
-            label='Ebb',color='blue')
-    ax.bar(center, height=p_flood, edgecolor='black', width=width, 
-            alpha=1, label='Flood',color='orange')
-    ax.bar(center[mask2],height=p_ebb[mask2], alpha=1, edgecolor='black',
-            width=width,color='blue')
-
-    plt.xlabel('Velocity [m/s]')
-    plt.ylabel('Probability')
-    plt.ylim(0,1.0)
+    ax.bar(
+        center[mask1],
+        height=p_ebb[mask1],
+        edgecolor="black",
+        width=width,
+        label="Ebb",
+        color="blue",
+    )
+    ax.bar(
+        center,
+        height=p_flood,
+        edgecolor="black",
+        width=width,
+        alpha=1,
+        label="Flood",
+        color="orange",
+    )
+    ax.bar(
+        center[mask2],
+        height=p_ebb[mask2],
+        alpha=1,
+        edgecolor="black",
+        width=width,
+        color="blue",
+    )
+
+    plt.xlabel("Velocity [m/s]")
+    plt.ylabel("Probability")
+    plt.ylim(0, 1.0)
     plt.legend()
-    plt.grid(linestyle=':')
+    plt.grid(linestyle=":")
 
     return ax
 
 
-def tidal_phase_exceedance(
-    directions,
-    velocities,
-    flood,
-    ebb,
-    bin_size=0.1,
-    ax=None
-    ):
+def tidal_phase_exceedance(directions, velocities, flood, ebb, bin_size=0.1, ax=None):
     """
-    Returns a stacked area plot of the exceedance probability for the 
+    Returns a stacked area plot of the exceedance probability for the
     flood and ebb tidal phases.
 
     Parameters
     ----------
     directions: array-like
         Time-series of directions [degrees]
     velocities: array-like
         Time-series of speeds [m/s]
     flood: float or int
         Principal component of flow in the flood direction [degrees]
     ebb: float or int
-        Principal component of flow in the ebb direction [degrees] 
+        Principal component of flow in the ebb direction [degrees]
     bin_size: float
-        Speed bin size. Optional. Deaful = 0.1 m/s        
+        Speed bin size. Optional. Deaful = 0.1 m/s
     ax : matplotlib axes object
-        Axes for plotting.  If None, then a new figure with a single 
+        Axes for plotting.  If None, then a new figure with a single
         axes is used.
 
     Returns
     -------
-    ax: figure    
+    ax: figure
     """
 
     _check_inputs(directions, velocities, flood, ebb)
     if bin_size < 0:
-        raise ValueError('bin_size must be greater than 0')     
-    
-    if ax==None:
+        raise ValueError("bin_size must be greater than 0")
+
+    if ax == None:
         fig, ax = plt.subplots(figsize=(12, 8))
 
     isEbb = _flood_or_ebb(directions, flood, ebb)
-    
+
     s_ebb = velocities[isEbb]
     s_flood = velocities[~isEbb]
 
-    F = exceedance_probability(velocities)['F']
-    F_ebb = exceedance_probability(s_ebb)['F']
-    F_flood = exceedance_probability(s_flood)['F']
-
-
-    decimals = round(bin_size/0.1)
-    s_new = np.arange(np.around(velocities.min(),decimals), 
-                      np.around(velocities.max(),decimals)+bin_size, bin_size)
+    F = exceedance_probability(velocities)["F"]
+    F_ebb = exceedance_probability(s_ebb)["F"]
+    F_flood = exceedance_probability(s_flood)["F"]
+
+    decimals = round(bin_size / 0.1)
+    s_new = np.arange(
+        np.around(velocities.min(), decimals),
+        np.around(velocities.max(), decimals) + bin_size,
+        bin_size,
+    )
 
     f_total = interp1d(velocities, F, bounds_error=False)
-    f_ebb = interp1d(s_ebb, F_ebb,  bounds_error=False)
-    f_flood = interp1d(s_flood, F_flood,  bounds_error=False)
+    f_ebb = interp1d(s_ebb, F_ebb, bounds_error=False)
+    f_flood = interp1d(s_flood, F_flood, bounds_error=False)
 
     F_total = f_total(s_new)
     F_ebb = f_ebb(s_new)
     F_flood = f_flood(s_new)
 
     F_max_total = np.nanmax(F_ebb) + np.nanmax(F_flood)
 
-    ax.stackplot(s_new, F_ebb/F_max_total*100, 
-                  F_flood/F_max_total*100, labels=['Ebb','Flood'])
+    ax.stackplot(
+        s_new,
+        F_ebb / F_max_total * 100,
+        F_flood / F_max_total * 100,
+        labels=["Ebb", "Flood"],
+    )
 
-    plt.xlabel('velocity [m/s]')
-    plt.ylabel('Probability of Exceedance')
-    plt.legend()  
-    plt.grid(linestyle=':', linewidth=1)
+    plt.xlabel("velocity [m/s]")
+    plt.ylabel("Probability of Exceedance")
+    plt.legend()
+    plt.grid(linestyle=":", linewidth=1)
 
     return ax
-
```

### Comparing `mhkit-0.7.0/mhkit/tidal/io/noaa.py` & `mhkit-0.8.0/mhkit/tidal/io/noaa.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,206 +1,292 @@
 """
 noaa.py
 
-This module provides functions to fetch, process, and read NOAA (National Oceanic and Atmospheric Administration) 
-current data directly from the NOAA Tides and Currents API (https://tidesandcurrents.noaa.gov/api/). It supports 
-loading data into a pandas DataFrame, handling data in XML and JSON formats, and writing data to a JSON file.
+This module provides functions to fetch, process, and read NOAA (National
+Oceanic and Atmospheric Administration) current data directly from the
+NOAA Tides and Currents API (https://tidesandcurrents.noaa.gov/api/). It
+supports loading data into a pandas DataFrame, handling data in XML and 
+JSON formats, and writing data to a JSON file.
 
 Functions:
 ----------
-request_noaa_data(station, parameter, start_date, end_date, proxy=None, write_json=None):
-    Loads NOAA current data from the API into a pandas DataFrame, with optional support for proxy settings and 
-    writing data to a JSON file.
-
-_json_to_dataframe(response):
-    Converts NOAA response data in JSON format into a pandas DataFrame and returns metadata. (Currently, this 
-    function does not return the full dataset requested.)
+request_noaa_data(station, parameter, start_date, end_date, proxy=None, 
+  write_json=None):
+    Loads NOAA current data from the API into a pandas DataFrame, 
+    with optional support for proxy settings and writing data to a JSON
+    file.
 
 _xml_to_dataframe(response):
-    Converts NOAA response data in XML format into a pandas DataFrame and returns metadata.
+    Converts NOAA response data in XML format into a pandas DataFrame
+    and returns metadata.
 
 read_noaa_json(filename):
-    Reads a JSON file containing NOAA data saved from the request_noaa_data function and returns a DataFrame with 
-    timeseries site data and metadata.
+    Reads a JSON file containing NOAA data saved from the request_noaa_data
+    function and returns a DataFrame with timeseries site data and metadata.
 """
+
+import os
 import xml.etree.ElementTree as ET
 import datetime
 import json
 import math
+import shutil
 import pandas as pd
 import requests
+from mhkit.utils.cache import handle_caching
 
 
-def request_noaa_data(station, parameter, start_date, end_date,
-                      proxy=None, write_json=None):
+def request_noaa_data(
+    station,
+    parameter,
+    start_date,
+    end_date,
+    proxy=None,
+    write_json=None,
+    clear_cache=False,
+    to_pandas=True,
+):
     """
-    Loads NOAA current data directly from https://tidesandcurrents.noaa.gov/api/ using a 
-    get request into a pandas DataFrame. NOAA sets max of 31 days between start and end date.
-    See https://co-ops.nos.noaa.gov/api/ for options. All times are reported as GMT and metric
-    units are returned for data.
+    Loads NOAA current data directly from https://tidesandcurrents.noaa.gov/api/
+    into a pandas DataFrame. NOAA sets max of 31 days between start and end date.
+    See https://co-ops.nos.noaa.gov/api/ for options. All times are reported as
+    GMT and metric units are returned for data. Uses cached data if available.
 
     The request URL prints to the screen.
 
     Parameters
     ----------
     station : str
         NOAA current station number (e.g. 'cp0101')
     parameter : str
         NOAA paramter (e.g. '' for Discharge, cubic feet per second)
     start_date : str
         Start date in the format yyyyMMdd
     end_date : str
-        End date in the format yyyyMMdd 
+        End date in the format yyyyMMdd
     proxy : dict or None
-         To request data from behind a firewall, define a dictionary of proxy settings, 
-         for example {"http": 'localhost:8080'}
+         To request data from behind a firewall, define a dictionary of proxy
+         settings, for example {"http": 'localhost:8080'}
     write_json : str or None
         Name of json file to write data
+    clear_cache : bool
+        If True, the cache for this specific request will be cleared.
+    to_pandas : bool, optional
+        Flag to output pandas instead of xarray. Default = True.
 
     Returns
     -------
-    data : pandas DataFrame 
-        Data indexed by datetime with columns named according to the parameter's 
+    data : pandas DataFrame or xarray Dataset
+        Data indexed by datetime with columns named according to the parameter's
         variable description
+    metadata : dict or None
+        Request metadata. If returning xarray, metadata is instead attached to
+        the data's attributes.
     """
-    # Convert start and end dates to datetime objects
-    begin = datetime.datetime.strptime(start_date, '%Y%m%d').date()
-    end = datetime.datetime.strptime(end_date, '%Y%m%d').date()
-
-    # Determine the number of 30 day intervals
-    delta = 30
-    interval = math.ceil(((end - begin).days)/delta)
-
-    # Create date ranges with 30 day intervals
-    date_list = [
-        begin + datetime.timedelta(days=i * delta) for i in range(interval + 1)]
-    date_list[-1] = end
-
-    # Iterate over date_list (30 day intervals) and fetch data
-    data_frames = []
-    for i in range(len(date_list) - 1):
-        start_date = date_list[i].strftime('%Y%m%d')
-        end_date = date_list[i + 1].strftime('%Y%m%d')
-
-        api_query = f"begin_date={start_date}&end_date={end_date}&station={station}&product={parameter}&units=metric&time_zone=gmt&application=web_services&format=xml"
-        data_url = f"https://tidesandcurrents.noaa.gov/api/datagetter?{api_query}"
-
-        print('Data request URL: ', data_url)
-
-        # Get response
-        response = requests.get(url=data_url, proxies=proxy)
-
-        # Convert to DataFrame and save in data_frames list
-        df, metadata = _xml_to_dataframe(response)
-        data_frames.append(df)
-
-    # Concatenate all DataFrames
-    data = pd.concat(data_frames, ignore_index=False)
-
-    # Remove duplicated date values
-    data = data.loc[~data.index.duplicated()]
-
-    # Write json if specified
-    if write_json is not None:
-        with open(write_json, 'w') as outfile:
-            # Convert DataFrame to json
-            jsonData = data.to_json()
-            # Convert to python object data
-            pyData = json.loads(jsonData)
-            # Add metadata to pyData
-            pyData['metadata'] = metadata
-            # Wrtie the pyData to a json file
-            json.dump(pyData, outfile)
-    return data, metadata
-
-
-def _json_to_dataframe(response):
-    '''
-    Returns a dataframe  and metadata from a NOAA
-    response.
-    TODO: This function currently does not return the 
-      full dataset requested.
-    '''
-    text = json.loads(response.text)
-    metadata = text['metadata']
-    # import ipdb; ipdb.set_trace()
-    # Initialize DataFrame
-    data = pd.DataFrame.from_records(
-        text['data'][1], index=[text['data'][1]['t']])
-    # Append all times to DataFrame
-    for i in range(1, len(text['data'])):
-        data.append(pd.DataFrame.from_records(text['data'][i],
-                                              index=[text['data'][i]['t']]))
-    # Convert index to DataFram
-    data.index = pd.to_datetime(data.index)
-    # Remove 't' becuase it is the index
-    del data['t']
-    # List of columns which are string
-    cols = data.columns[data.dtypes.eq('object')]
-    # Convert columns to float
-    data[cols] = data[cols].apply(pd.to_numeric, errors='coerce')
-    return data, metadata
+    # Type check inputs
+    if not isinstance(station, str):
+        raise TypeError(
+            f"Expected 'station' to be of type str, but got {type(station)}"
+        )
+    if not isinstance(parameter, str):
+        raise TypeError(
+            f"Expected 'parameter' to be of type str, but got {type(parameter)}"
+        )
+    if not isinstance(start_date, str):
+        raise TypeError(
+            f"Expected 'start_date' to be of type str, but got {type(start_date)}"
+        )
+    if not isinstance(end_date, str):
+        raise TypeError(
+            f"Expected 'end_date' to be of type str, but got {type(end_date)}"
+        )
+    if proxy and not isinstance(proxy, dict):
+        raise TypeError(
+            f"Expected 'proxy' to be of type dict or None, but got {type(proxy)}"
+        )
+    if write_json and not isinstance(write_json, str):
+        raise TypeError(
+            f"Expected 'write_json' to be of type str or None, but got {type(write_json)}"
+        )
+    if not isinstance(clear_cache, bool):
+        raise TypeError(
+            f"Expected 'clear_cache' to be of type bool, but got {type(clear_cache)}"
+        )
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
+    # Define the path to the cache directory
+    cache_dir = os.path.join(os.path.expanduser("~"), ".cache", "mhkit", "noaa")
+
+    # Create a unique filename based on the function parameters
+    hash_params = f"{station}_{parameter}_{start_date}_{end_date}"
+
+    # Use handle_caching to manage cache
+    cached_data, cached_metadata, cache_filepath = handle_caching(
+        hash_params, cache_dir, write_json=write_json, clear_cache_file=clear_cache
+    )
+
+    if cached_data is not None:
+        if write_json:
+            shutil.copy(cache_filepath, write_json)
+        if to_pandas:
+            return cached_data, cached_metadata
+        else:
+            cached_data = cached_data.to_xarray()
+            cached_data.attrs = cached_metadata
+            return cached_data
+    # If no cached data is available, make the API request
+    # no coverage bc in coverage runs we have already cached the data/ run this code
+    else:  # pragma: no cover
+        # Convert start and end dates to datetime objects
+        begin = datetime.datetime.strptime(start_date, "%Y%m%d").date()
+        end = datetime.datetime.strptime(end_date, "%Y%m%d").date()
+
+        # Determine the number of 30 day intervals
+        delta = 30
+        interval = math.ceil(((end - begin).days) / delta)
+
+        # Create date ranges with 30 day intervals
+        date_list = [
+            begin + datetime.timedelta(days=i * delta) for i in range(interval + 1)
+        ]
+        date_list[-1] = end
+
+        # Iterate over date_list (30 day intervals) and fetch data
+        data_frames = []
+        for i in range(len(date_list) - 1):
+            start_date = date_list[i].strftime("%Y%m%d")
+            end_date = date_list[i + 1].strftime("%Y%m%d")
+
+            api_query = f"begin_date={start_date}&end_date={end_date}&station={station}&product={parameter}&units=metric&time_zone=gmt&application=web_services&format=xml"
+            data_url = f"https://tidesandcurrents.noaa.gov/api/datagetter?{api_query}"
+
+            print("Data request URL: ", data_url)
+
+            # Get response
+            try:
+                response = requests.get(url=data_url, proxies=proxy)
+                response.raise_for_status()
+            except requests.exceptions.HTTPError as err:
+                print(f"HTTP error occurred: {err}")
+                continue
+            except requests.exceptions.RequestException as err:
+                print(f"Error occurred: {err}")
+                continue
+            # Convert to DataFrame and save in data_frames list
+            df, metadata = _xml_to_dataframe(response)
+            data_frames.append(df)
+
+        # Concatenate all DataFrames
+        data = pd.concat(data_frames, ignore_index=False)
+
+        # Remove duplicated date values
+        data = data.loc[~data.index.duplicated()]
+
+        # After making the API request and processing the response, write the
+        #  response to a cache file
+        handle_caching(
+            hash_params,
+            cache_dir,
+            data=data,
+            metadata=metadata,
+            clear_cache_file=clear_cache,
+        )
+
+        if write_json:
+            shutil.copy(cache_filepath, write_json)
+
+        if to_pandas:
+            return data, metadata
+        else:
+            data = data.to_xarray()
+            data.attrs = metadata
+            return data
 
 
 def _xml_to_dataframe(response):
-    '''
+    """
     Returns a dataframe from an xml response
-    '''
+    """
     root = ET.fromstring(response.text)
     metadata = None
     data = None
 
     for child in root:
         # Save meta data dictionary
-        if child.tag == 'metadata':
+        if child.tag == "metadata":
             metadata = child.attrib
-        elif child.tag == 'observations':
+        elif child.tag == "observations":
             data = child
-        elif child.tag == 'error':
-            print('***ERROR: Response returned error')
+        elif child.tag == "error":
+            print("***ERROR: Response returned error")
             return None
 
     if data is None:
-        print('***ERROR: No observations found')
+        print("***ERROR: No observations found")
         return None
 
     # Create a list of DataFrames then Concatenate
-    df = pd.concat([pd.DataFrame(obs.attrib, index=[0])
-                   for obs in data], ignore_index=True)
+    df = pd.concat(
+        [pd.DataFrame(obs.attrib, index=[0]) for obs in data], ignore_index=True
+    )
 
     # Convert time to datetime
-    df['t'] = pd.to_datetime(df.t)
-    df = df.set_index('t')
+    df["t"] = pd.to_datetime(df.t)
+    df = df.set_index("t")
     df.drop_duplicates(inplace=True)
 
     # Convert data to float
-    df[['d', 's']] = df[['d', 's']].apply(pd.to_numeric)
+    df[["d", "s"]] = df[["d", "s"]].apply(pd.to_numeric)
 
     return df, metadata
 
 
-def read_noaa_json(filename):
-    '''
-    Returns site DataFrame and metadata from a json saved from the 
+def read_noaa_json(filename, to_pandas=True):
+    """
+    Returns site DataFrame and metadata from a json saved from the
     request_noaa_data
     Parameters
     ----------
     filename: string
         filename with path of json file to load
+    to_pandas : bool, optional
+        Flag to output pandas instead of xarray. Default = True.
+
     Returns
     -------
     data: DataFrame
-        Timeseries Site data of direction and speed 
-    metadata: dictionary
-        Site metadata
-    '''
+        Timeseries Site data of direction and speed
+    metadata : dictionary or None
+        Site metadata. If returning xarray, metadata is instead attached to
+        the data's attributes.
+    """
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
     with open(filename) as outfile:
-        jsonData = json.load(outfile)
-    # Get the metadata
-    metadata = jsonData['metadata']
-    # Remove metadata entry
-    del jsonData['metadata']
-    # Remainder is DataFrame
-    data = pd.DataFrame.from_dict(jsonData)
-    # Convert from epoch to date time
-    data.index = pd.to_datetime(data.index, unit='ms')
-    return data, metadata
+        json_data = json.load(outfile)
+    try:  # original MHKiT format (deprecate in future)
+        # Get the metadata
+        metadata = json_data["metadata"]
+        # Remove metadata entry
+        del json_data["metadata"]
+        # Remainder is DataFrame
+        data = pd.DataFrame.from_dict(json_data)
+        # Convert from epoch to date time
+        data.index = pd.to_datetime(data.index, unit="ms")
+
+    except ValueError:  # using cache.py format
+        if "metadata" in json_data:
+            metadata = json_data.pop("metadata", None)
+        data = pd.DataFrame(
+            json_data["data"],
+            index=pd.to_datetime(json_data["index"]),
+            columns=json_data["columns"],
+        )
+
+    if to_pandas:
+        return data, metadata
+    else:
+        data = data.to_xarray()
+        data.attrs = metadata
+        return data
```

### Comparing `mhkit-0.7.0/mhkit/tidal/performance.py` & `mhkit-0.8.0/mhkit/tidal/performance.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,16 +1,20 @@
 import numpy as np
-import pandas as pd
 import xarray as xr
-import warnings
+from mhkit.utils import convert_to_dataarray
 
 from mhkit import dolfyn
-from mhkit.river.performance import (circular, ducted, rectangular,
-                                     multiple_circular, tip_speed_ratio,
-                                     power_coefficient)
+from mhkit.river.performance import (
+    circular,
+    ducted,
+    rectangular,
+    multiple_circular,
+    tip_speed_ratio,
+    power_coefficient,
+)
 
 
 def _slice_circular_capture_area(diameter, hub_height, doppler_cell_size):
     """
     Slices a circle (capture area) based on ADCP depth bins mapped
     across the face of the capture area.
 
@@ -25,70 +29,70 @@
 
     doppler_cell_size: numeric
         ADCP depth bin size.
 
     Returns
     ---------
     capture_area_slice: xarray.DataArray
-        Capture area sliced into horizontal slices of height 
+        Capture area sliced into horizontal slices of height
         `doppler_cell_size`, centered on `hub height`.
     """
 
     def area_of_circle_segment(radius, angle):
         # Calculating area of sector
-        area_of_sector = np.pi * radius**2 * (angle/360)
+        area_of_sector = np.pi * radius**2 * (angle / 360)
         # Calculating area of triangle
-        area_of_triangle = 0.5 * radius**2 * np.sin((np.pi*angle)/180)
+        area_of_triangle = 0.5 * radius**2 * np.sin((np.pi * angle) / 180)
         return area_of_sector - area_of_triangle
 
     def point_on_circle(y, r):
         return np.sqrt(r**2 - y**2)
 
     # Capture area - from mhkit.river.performance
     d = diameter
     cs = doppler_cell_size
 
-    A_cap = np.pi*(d/2)**2  # m^2
+    A_cap = np.pi * (d / 2) ** 2  # m^2
     # Need to chop up capture area into slices based on bin size
     # For a cirle:
-    r_min = hub_height - d/2
-    r_max = hub_height + d/2
-    A_edge = np.arange(r_min, r_max+cs, cs)
-    A_rng = A_edge[:-1] + cs/2  # Center of each slice
+    r_min = hub_height - d / 2
+    r_max = hub_height + d / 2
+    A_edge = np.arange(r_min, r_max + cs, cs)
+    A_rng = A_edge[:-1] + cs / 2  # Center of each slice
 
     # y runs from the bottom edge of the lower centerline slice to
     # the top edge of the lowest slice
     # Will need to figure out y if the hub height isn't centered
     y = abs(A_edge - np.mean(A_edge))
-    y[np.where(abs(y) > (d/2))] = d/2
+    y[np.where(abs(y) > (d / 2))] = d / 2
 
     # Even vs odd number of slices
     if y.size % 2:
         odd = 1
     else:
         odd = 0
-        y = y[:len(y)//2]
+        y = y[: len(y) // 2]
         y = np.append(y, 0)
 
-    x = point_on_circle(y, d/2)
-    radii = np.rad2deg(np.arctan(x/y)*2)
+    x = point_on_circle(y, d / 2)
+    radii = np.rad2deg(np.arctan(x / y) * 2)
     # Segments go from outside of circle towards middle
-    As = area_of_circle_segment(d/2, radii)
+    As = area_of_circle_segment(d / 2, radii)
     # Subtract segments to get area of slices
     As_slc = As[1:] - As[:-1]
 
     if not odd:
         # Make middle slice half whole
-        As_slc[-1] = As_slc[-1]*2
+        As_slc[-1] = As_slc[-1] * 2
         # Copy-flip the other slices to get the whole circle
         As_slc = np.append(As_slc, np.flip(As_slc[:-1]))
     else:
         As_slc = abs(As_slc)
 
-    return xr.DataArray(As_slc, coords={'range': A_rng})
+    return xr.DataArray(As_slc, coords={"range": A_rng})
 
 
 def _slice_rectangular_capture_area(height, width, hub_height, doppler_cell_size):
     """
     Slices a rectangular (capture area) based on ADCP depth bins mapped
     across the face of the capture area.
 
@@ -106,80 +110,56 @@
 
     doppler_cell_size: numeric
         ADCP depth bin size.
 
     Returns
     ---------
     capture_area_slice: xarray.DataArray
-        Capture area sliced into horizontal slices of height 
+        Capture area sliced into horizontal slices of height
         `doppler_cell_size`, centered on `hub height`.
     """
 
     # Need to chop up capture area into slices based on bin size
     # For a rectangle it's pretty simple
     cs = doppler_cell_size
-    r_min = hub_height - height/2
-    r_max = hub_height + height/2
-    A_edge = np.arange(r_min, r_max+cs, cs)
-    A_rng = A_edge[:-1] + cs/2  # Center of each slice
-
-    As_slc = np.ones(len(A_rng))*width*cs
-
-    return xr.DataArray(As_slc, coords={'range': A_rng})
-
-
-def _check_dtype(var, var_name):
-    """
-    Checks the datatype of a variable, converting pandas Series to xarray DataArray, 
-    or raising an error if the datatype is neither.
-
-    Parameters
-    -------------
-    var: xr.DataArray or pd.Series
-        The variable to be checked.
-
-    var_name: str
-        The name of the variable, used for error message.
-
-    Returns
-    ---------
-    var: xr.DataArray
-        The input variable, converted to xr.DataArray if it was a pd.Series.
-    """
-
-    if isinstance(var, pd.Series):
-        var = var.to_xarray()
-    elif not isinstance(var, xr.DataArray):
-        raise TypeError(var_name.capitalize() +
-                        ' must be of type xr.DataArray or pd.Series')
-    return var
-
-
-def power_curve(power,
-                velocity,
-                hub_height,
-                doppler_cell_size,
-                sampling_frequency,
-                window_avg_time=600,
-                turbine_profile='circular',
-                diameter=None,
-                height=None,
-                width=None):
+    r_min = hub_height - height / 2
+    r_max = hub_height + height / 2
+    A_edge = np.arange(r_min, r_max + cs, cs)
+    A_rng = A_edge[:-1] + cs / 2  # Center of each slice
+
+    As_slc = np.ones(len(A_rng)) * width * cs
+
+    return xr.DataArray(As_slc, coords={"range": A_rng})
+
+
+def power_curve(
+    power,
+    velocity,
+    hub_height,
+    doppler_cell_size,
+    sampling_frequency,
+    window_avg_time=600,
+    turbine_profile="circular",
+    diameter=None,
+    height=None,
+    width=None,
+    to_pandas=True,
+):
     """
-    Calculates power curve and power statistics for a marine energy 
+    Calculates power curve and power statistics for a marine energy
     device based on IEC/TS 62600-200 section 9.3.
 
     Parameters
     -------------
-    power: pandas.Series or xarray.DataArray (time)
+    power: numpy ndarray, pandas DataFrame, pandas Series, xarray DataArray, or xarray Dataset
         Device power output timeseries.
-    velocity: pandas.Series or xarray.DataArray ([range,] time)
+    velocity: numpy ndarray, pandas DataFrame, pandas Series, xarray DataArray, or xarray Dataset
         1D or 2D streamwise sea water velocity or sea water speed.
     hub_height: numeric
-        Turbine hub height altitude above the seabed. Assumes ADCP 
+        Turbine hub height altitude above the seabed. Assumes ADCP
         depth bins are referenced to the seafloor.
     doppler_cell_size: numeric
         ADCP depth bin size.
     sampling_frequency: numeric
         ADCP sampling frequency in Hz.
     window_avg_time: int, optional
         Time averaging window in seconds. Defaults to 600.
@@ -187,119 +167,149 @@
         Shape of swept area of the turbine. Defaults to 'circular'.
     diameter: numeric, optional
         Required for turbine_profile='circular'. Defaults to None.
     height: numeric, optional
         Required for turbine_profile='rectangular'. Defaults to None.
     width: numeric, optional
         Required for turbine_profile='rectangular'. Defaults to None.
+    to_pandas: bool, optional
+        Flag to output pandas instead of xarray. Default = True.
 
     Returns
     ---------
-    pandas.DataFrame
+    device_power_curve: pandas DataFrame or xarray Dataset
         Power-weighted velocity, mean power, power std dev, max and
         min power vs hub-height velocity.
     """
 
     # Velocity should be a 2D xarray or pandas array and have dims (range, time)
     # Power should have a timestamp coordinate/index
-    power = _check_dtype(power, 'power')
-    velocity = _check_dtype(velocity, 'velocity')
+    power = convert_to_dataarray(power)
+    velocity = convert_to_dataarray(velocity)
     if len(velocity.shape) != 2:
-        raise ValueError("Velocity should be 2 dimensional and have \
-                         dimensions of 'time' (temporal) and 'range' (spatial).")
+        raise ValueError(
+            "Velocity should be 2 dimensional and have \
+                         dimensions of 'time' (temporal) and 'range' (spatial)."
+        )
+
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
 
     # Numeric positive checks
-    numeric_params = [hub_height, doppler_cell_size,
-                      sampling_frequency, window_avg_time]
-    numeric_param_names = ['hub_height', 'doppler_cell_size',
-                           'sampling_frequency', 'window_avg_time']
+    numeric_params = [
+        hub_height,
+        doppler_cell_size,
+        sampling_frequency,
+        window_avg_time,
+    ]
+    numeric_param_names = [
+        "hub_height",
+        "doppler_cell_size",
+        "sampling_frequency",
+        "window_avg_time",
+    ]
     for param, name in zip(numeric_params, numeric_param_names):
         if not isinstance(param, (int, float)):
-            raise TypeError(f'{name} must be numeric.')
+            raise TypeError(f"{name} must be numeric.")
         if param <= 0:
-            raise ValueError(f'{name} must be positive.')
+            raise ValueError(f"{name} must be positive.")
 
     # Turbine profile related checks
-    if turbine_profile not in ['circular', 'rectangular']:
+    if turbine_profile not in ["circular", "rectangular"]:
         raise ValueError(
-            "`turbine_profile` must be one of 'circular' or 'rectangular'.")
-    if turbine_profile == 'circular':
+            "`turbine_profile` must be one of 'circular' or 'rectangular'."
+        )
+    if turbine_profile == "circular":
         if diameter is None:
             raise TypeError(
-                "`diameter` cannot be None for input `turbine_profile` = 'circular'.")
+                "`diameter` cannot be None for input `turbine_profile` = 'circular'."
+            )
         elif not isinstance(diameter, (int, float)) or diameter <= 0:
             raise ValueError("`diameter` must be a positive number.")
         else:  # If the checks pass, calculate A_slc
             A_slc = _slice_circular_capture_area(
-                diameter, hub_height, doppler_cell_size)
+                diameter, hub_height, doppler_cell_size
+            )
     else:  # Rectangular profile
         if height is None or width is None:
             raise TypeError(
-                "`height` and `width` cannot be None for input `turbine_profile` = 'rectangular'.")
-        elif not all(isinstance(val, (int, float)) and val > 0 for val in [height, width]):
+                "`height` and `width` cannot be None for input `turbine_profile` = 'rectangular'."
+            )
+        elif not all(
+            isinstance(val, (int, float)) and val > 0 for val in [height, width]
+        ):
             raise ValueError("`height` and `width` must be positive numbers.")
         else:  # If the checks pass, calculate A_slc
             A_slc = _slice_rectangular_capture_area(
-                height, width, hub_height, doppler_cell_size)
+                height, width, hub_height, doppler_cell_size
+            )
 
     # Streamwise data
     U = abs(velocity)
-    time = U['time'].values
+    time = U["time"].values
     # Interpolate power to velocity timestamps
-    P = power.interp(time=U['time'], method='linear')
+    P = power.interp(time=U["time"], method="linear")
 
     # Power weighted velocity in capture area
     # Interpolate U range to capture area slices, then cube and multiply by area
-    U_hat = U.interp(range=A_slc['range'], method='linear')**3 * A_slc
+    U_hat = U.interp(range=A_slc["range"], method="linear") ** 3 * A_slc
     # Average the velocity across the capture area and divide out area
-    U_hat = (U_hat.sum('range') / A_slc.sum()) ** (-1/3)
+    U_hat = (U_hat.sum("range") / A_slc.sum()) ** (-1 / 3)
 
     # Time-average velocity at hub-height
-    bnr = dolfyn.VelBinner(n_bin=window_avg_time *
-                           sampling_frequency, fs=sampling_frequency)
+    bnr = dolfyn.VelBinner(
+        n_bin=window_avg_time * sampling_frequency, fs=sampling_frequency
+    )
     # Hub-height velocity mean
-    mean_hub_vel = xr.DataArray(bnr.mean(U.sel(range=hub_height, method='nearest').values),
-                                coords={'time': bnr.mean(time)})
+    mean_hub_vel = xr.DataArray(
+        bnr.mean(U.sel(range=hub_height, method="nearest").values),
+        coords={"time": bnr.mean(time)},
+    )
 
     # Power-weighted hub-height velocity mean
-    U_hat_bar = xr.DataArray((bnr.mean(U_hat.values ** 3)) ** (-1/3),
-                             coords={'time': bnr.mean(time)})
+    U_hat_bar = xr.DataArray(
+        (bnr.mean(U_hat.values**3)) ** (-1 / 3), coords={"time": bnr.mean(time)}
+    )
 
     # Average power
-    P_bar = xr.DataArray(bnr.mean(P.values),
-                         coords={'time': bnr.mean(time)})
+    P_bar = xr.DataArray(bnr.mean(P.values), coords={"time": bnr.mean(time)})
 
     # Then reorganize into 0.1 m velocity bins and average
     U_bins = np.arange(0, np.nanmax(mean_hub_vel) + 0.1, 0.1)
-    U_hub_vel = mean_hub_vel.assign_coords(
-        {"time": mean_hub_vel}).rename({"time": "speed"})
+    U_hub_vel = mean_hub_vel.assign_coords({"time": mean_hub_vel}).rename(
+        {"time": "speed"}
+    )
     U_hub_mean = U_hub_vel.groupby_bins("speed", U_bins).mean()
-    U_hat_vel = U_hat_bar.assign_coords(
-        {"time": mean_hub_vel}).rename({"time": "speed"})
+    U_hat_vel = U_hat_bar.assign_coords({"time": mean_hub_vel}).rename(
+        {"time": "speed"}
+    )
     U_hat_mean = U_hat_vel.groupby_bins("speed", U_bins).mean()
 
-    P_bar_vel = P_bar.assign_coords(
-        {"time": mean_hub_vel}).rename({"time": "speed"})
+    P_bar_vel = P_bar.assign_coords({"time": mean_hub_vel}).rename({"time": "speed"})
     P_bar_mean = P_bar_vel.groupby_bins("speed", U_bins).mean()
     P_bar_std = P_bar_vel.groupby_bins("speed", U_bins).std()
     P_bar_max = P_bar_vel.groupby_bins("speed", U_bins).max()
     P_bar_min = P_bar_vel.groupby_bins("speed", U_bins).min()
 
-    out = pd.DataFrame((U_hub_mean.to_series(),
-                        U_hat_mean.to_series(),
-                        P_bar_mean.to_series(),
-                        P_bar_std.to_series(),
-                        P_bar_max.to_series(),
-                        P_bar_min.to_series(),
-                        )).T
-    out.columns = ['U_avg', 'U_avg_power_weighted',
-                   'P_avg', 'P_std', 'P_max', 'P_min']
-    out.index.name = 'U_bins'
+    device_power_curve = xr.Dataset(
+        {
+            "U_avg": U_hub_mean,
+            "U_avg_power_weighted": U_hat_mean,
+            "P_avg": P_bar_mean,
+            "P_std": P_bar_std,
+            "P_max": P_bar_max,
+            "P_min": P_bar_min,
+        }
+    )
+    device_power_curve = device_power_curve.rename({"speed_bins": "U_bins"})
+
+    if to_pandas:
+        device_power_curve = device_power_curve.to_pandas()
 
-    return out
+    return device_power_curve
 
 
 def _average_velocity_bins(U, U_hub, bin_size):
     """
     Groups time-ensembles into velocity bins based on hub-height
     velocity and averages them.
 
@@ -310,26 +320,26 @@
     U_hub: xarray.DataArray
         Sea water velocity at hub height.
     bin_size: numeric
         Velocity averaging window size in m/s.
 
     Returns
     ---------
-    xarray.DataArray
+    U_binned: xarray.DataArray
         Data grouped into velocity bins.
     """
 
     # Reorganize into velocity bins and average
     U_bins = np.arange(0, np.nanmax(U_hub) + bin_size, bin_size)
 
     # Group time-ensembles into velocity bins based on hub-height velocity and average
-    out = U.assign_coords({"time": U_hub}).rename({"time": "speed"})
-    out = out.groupby_bins("speed", U_bins).mean()
+    U_binned = U.assign_coords({"time": U_hub}).rename({"time": "speed"})
+    U_binned = U_binned.groupby_bins("speed", U_bins).mean()
 
-    return out
+    return U_binned
 
 
 def _apply_function(function, bnr, U):
     """
     Applies a specified function ('mean', 'rms', or 'std') to the input
     data array U, grouped into bins as specified by the binning rules in bnr.
 
@@ -347,242 +357,227 @@
     Returns
     ---------
     xarray.DataArray
         The input data array U after the specified function has been
         applied, grouped into bins according to bnr.
     """
 
-    if function == 'mean':
+    if function == "mean":
         # Average data into 5-10 minute ensembles
         return xr.DataArray(
             bnr.mean(abs(U).values),
-            coords={'range': U.range,
-                    'time': bnr.mean(U['time'].values)})
-    elif function == 'rms':
+            coords={"range": U.range, "time": bnr.mean(U["time"].values)},
+        )
+    elif function == "rms":
         # Reshape tidal velocity - returns (range, ensemble-time, ensemble elements)
         U_reshaped = bnr.reshape(abs(U).values)
         # Take root-mean-square
         U_rms = np.sqrt(np.nanmean(U_reshaped**2, axis=-1))
         return xr.DataArray(
-            U_rms,
-            coords={'range': U.range,
-                    'time': bnr.mean(U['time'].values)})
-    elif function == 'std':
+            U_rms, coords={"range": U.range, "time": bnr.mean(U["time"].values)}
+        )
+    elif function == "std":
         # Standard deviation
         return xr.DataArray(
             bnr.standard_deviation(U.values),
-            coords={'range': U.range,
-                    'time': bnr.mean(U['time'].values)})
+            coords={"range": U.range, "time": bnr.mean(U["time"].values)},
+        )
     else:
         raise ValueError(
-            f"Unknown function {function}. Should be one of 'mean', 'rms', or 'std'")
+            f"Unknown function {function}. Should be one of 'mean', 'rms', or 'std'"
+        )
 
 
-def velocity_profiles(velocity,
-                      hub_height,
-                      water_depth,
-                      sampling_frequency,
-                      window_avg_time=600,
-                      function='mean',
-                      ):
+def velocity_profiles(
+    velocity,
+    hub_height,
+    water_depth,
+    sampling_frequency,
+    window_avg_time=600,
+    function="mean",
+    to_pandas=True,
+):
     """
     Calculates profiles of the mean, root-mean-square (RMS), or standard
     deviation(std) of velocity. The chosen metric, specified by `function`,
     is calculated for each `window_avg_time` and bin-averaged based on
     ensemble velocity, as per IEC/TS 62600-200 sections 9.4 and 9.5.
 
     Parameters
     -------------
-    velocity : pandas.Series or xarray.DataArray ([range,] time)
+    velocity : numpy ndarray, pandas DataFrame, pandas Series, xarray DataArray, or xarray Dataset
         1D or 2D streamwise sea water velocity or sea water speed.
     hub_height : numeric
-        Turbine hub height altitude above the seabed. Assumes ADCP depth bins 
+        Turbine hub height altitude above the seabed. Assumes ADCP depth bins
         are referenced to the seafloor.
     water_depth : numeric
         Water depth to seafloor, in same units as velocity `range` coordinate.
     sampling_frequency : numeric
         ADCP sampling frequency in Hz.
     window_avg_time : int, optional
         Time averaging window in seconds. Defaults to 600.
     func : string
         Function to apply. One of 'mean','rms', or 'std'
+    to_pandas: bool, optional
+        Flag to output pandas instead of xarray. Default = True.
 
     Returns
     ---------
-    pandas.DataFrame
+    iec_profiles: pandas.DataFrame
         Average velocity profiles based on ensemble mean velocity.
     """
 
-    velocity = _check_dtype(velocity, 'velocity')
+    velocity = convert_to_dataarray(velocity, "velocity")
     if len(velocity.shape) != 2:
-        raise ValueError("Velocity should be 2 dimensional and have \
-                         dimensions of 'time' (temporal) and 'range' (spatial).")
+        raise ValueError(
+            "Velocity should be 2 dimensional and have \
+                         dimensions of 'time' (temporal) and 'range' (spatial)."
+        )
 
-    if function not in ['mean', 'rms', 'std']:
+    if function not in ["mean", "rms", "std"]:
         raise ValueError("`function` must be one of 'mean', 'rms', or 'std'.")
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
 
     # Streamwise data
     U = velocity
 
     # Create binner
-    bnr = dolfyn.VelBinner(n_bin=window_avg_time *
-                           sampling_frequency, fs=sampling_frequency)
+    bnr = dolfyn.VelBinner(
+        n_bin=window_avg_time * sampling_frequency, fs=sampling_frequency
+    )
     # Take velocity at hub height
-    mean_hub_vel = bnr.mean(U.sel(range=hub_height, method='nearest').values)
+    mean_hub_vel = bnr.mean(U.sel(range=hub_height, method="nearest").values)
 
     # Apply mean, root-mean-square, or standard deviation
     U_out = _apply_function(function, bnr, U)
 
     # Then reorganize into 0.5 m/s velocity bins and average
     profiles = _average_velocity_bins(U_out, mean_hub_vel, bin_size=0.5)
 
     # Extend top and bottom of profiles to the seafloor and sea surface
     # Clip off extra depth bins with nans
     rdx = profiles.isel(speed_bins=0).notnull().sum().values
-    profiles = profiles.isel(range=slice(None, rdx+1))
+    profiles = profiles.isel(range=slice(None, rdx + 1))
     # Set seafloor velocity to 0 m/s
     out_data = np.insert(profiles.data, 0, 0, axis=0)
     # Set max range to the user-provided water depth
-    new_range = np.insert(profiles['range'].data[:-1], 0, 0)
+    new_range = np.insert(profiles["range"].data[:-1], 0, 0)
     new_range = np.append(new_range, water_depth)
     # Create a profiles with new range
-    iec_profiles = xr.DataArray(out_data, coords={'range': new_range,
-                                                  'speed_bins': profiles['speed_bins']})
+    iec_profiles = xr.DataArray(
+        out_data, coords={"range": new_range, "speed_bins": profiles["speed_bins"]}
+    )
     # Forward fill to surface
-    iec_profiles = iec_profiles.ffill('range', limit=None)
+    iec_profiles = iec_profiles.ffill("range", limit=None)
+
+    if to_pandas:
+        iec_profiles = iec_profiles.to_pandas()
 
-    return iec_profiles.to_pandas()
+    return iec_profiles
 
 
-def device_efficiency(power,
-                      velocity,
-                      water_density,
-                      capture_area,
-                      hub_height,
-                      sampling_frequency,
-                      window_avg_time=600):
+def device_efficiency(
+    power,
+    velocity,
+    water_density,
+    capture_area,
+    hub_height,
+    sampling_frequency,
+    window_avg_time=600,
+    to_pandas=True,
+):
     """
     Calculates marine energy device efficiency based on IEC/TS 62600-200 Section 9.7.
 
     Parameters
     -------------
-    power : pandas.Series or xarray.DataArray (time)
+    power : numpy ndarray, pandas DataFrame, pandas Series, xarray DataArray, or xarray Dataset
         Device power output timeseries in Watts.
-    velocity : pandas.Series or xarray.DataArray ([range,] time)
+    velocity : numpy ndarray, pandas DataFrame, pandas Series, xarray DataArray, or xarray Dataset
         1D or 2D streamwise sea water velocity or sea water speed in m/s.
     water_density : float, pandas.Series or xarray.DataArray
         Sea water density in kg/m^3.
     capture_area : numeric
         Swept area of marine energy device.
     hub_height : numeric
-        Turbine hub height altitude above the seabed. Assumes ADCP depth bins 
+        Turbine hub height altitude above the seabed. Assumes ADCP depth bins
         are referenced to the seafloor.
     sampling_frequency : numeric
         ADCP sampling frequency in Hz.
     window_avg_time : int, optional
         Time averaging window in seconds. Defaults to 600.
+    to_pandas: bool, optional
+        Flag to output pandas instead of xarray. Default = True.
 
     Returns
     ---------
-    pandas.Series
+    device_eta : pandas.Series or xarray.DataArray
         Device efficiency (power coefficient) in percent.
     """
 
     # Velocity should be a 2D xarray or pandas array and have dims (range, time)
     # Power should have a timestamp coordinate/index
-    power = _check_dtype(power, 'power')
-    velocity = _check_dtype(velocity, 'velocity')
+    power = convert_to_dataarray(power, "power")
+    velocity = convert_to_dataarray(velocity, "velocity")
     if len(velocity.shape) != 2:
-        raise ValueError("Velocity should be 2 dimensional and have \
-                            dimensions of 'time' (temporal) and 'range' (spatial).")
+        raise ValueError(
+            "Velocity should be 2 dimensional and have \
+                            dimensions of 'time' (temporal) and 'range' (spatial)."
+        )
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
 
     # Streamwise data
     U = abs(velocity)
-    time = U['time'].values
+    time = U["time"].values
 
     # Power: Interpolate to velocity timeseries
-    power = _interpolate_power_to_velocity_timeseries(power, U)
+    power.interp(time=U["time"], method="linear")
 
     # Create binner
-    bnr = dolfyn.VelBinner(n_bin=window_avg_time *
-                           sampling_frequency, fs=sampling_frequency)
+    bnr = dolfyn.VelBinner(
+        n_bin=window_avg_time * sampling_frequency, fs=sampling_frequency
+    )
     # Hub-height velocity
-    mean_hub_vel = xr.DataArray(bnr.mean(U.sel(range=hub_height, method='nearest').values),
-                                coords={'time': bnr.mean(time)})
+    mean_hub_vel = xr.DataArray(
+        bnr.mean(U.sel(range=hub_height, method="nearest").values),
+        coords={"time": bnr.mean(time)},
+    )
     vel_hub = _average_velocity_bins(mean_hub_vel, mean_hub_vel, bin_size=0.1)
 
     # Water density
     rho_vel = _calculate_density(water_density, bnr, mean_hub_vel, time)
 
     # Bin average power
-    P_avg = xr.DataArray(bnr.mean(power.values),
-                         coords={'time': bnr.mean(time)})
+    P_avg = xr.DataArray(bnr.mean(power.values), coords={"time": bnr.mean(time)})
     P_vel = _average_velocity_bins(P_avg, mean_hub_vel, bin_size=0.1)
 
     # Theoretical power resource
-    P_resource = 1/2 * rho_vel * capture_area * vel_hub**3
+    P_resource = 1 / 2 * rho_vel * capture_area * vel_hub**3
 
     # Efficiency
     eta = P_vel / P_resource
 
-    out = pd.DataFrame((vel_hub.to_series(),
-                        eta.to_series(),
-                        )).T
-    out.columns = ['U_avg', 'Efficiency']
-    out.index.name = 'U_bins'
-
-    return out
-
-
-def _interpolate_power_to_velocity_timeseries(power, U):
-    """
-    Interpolates the power timeseries to match the velocity timeseries time points.
-
-    This function checks if the input power is an xarray DataArray or a pandas Series
-    with a DatetimeIndex and performs interpolation accordingly. If the input power 
-    does not match either of these types, a warning is issued and the original power 
-    timeseries is returned.
+    device_eta = xr.Dataset({"U_avg": vel_hub, "Efficiency": eta})
+    device_eta = device_eta.rename({"speed_bins": "U_bins"})
 
-    Parameters
-    -------------
-    power : xarray.DataArray or pandas.Series
-        The device power output timeseries.
-    U : xarray.DataArray
-        2D streamwise sea water velocity or sea water speed.
-
-    Returns
-    ---------
-    xarray.DataArray or pandas.Series
-        Interpolated power timeseries.
+    if to_pandas:
+        device_eta = device_eta.to_pandas()
 
-    Raises
-    ---------
-    Warning
-        If the input power is not a xarray DataArray or pandas Series with 
-        a DatetimeIndex, a warning is issued stating that the function assumes the 
-        power timestamps match the velocity timestamps.
-    """
-
-    if 'xarray' in type(power).__module__:
-        return power.interp(time=U['time'], method='linear')
-    elif 'pandas' in type(power).__module__ and isinstance(power.index, pd.DatetimeIndex):
-        return power.to_xarray().interp(time=U['time'], method='linear')
-    else:
-        warnings.warn(
-            "Assuming `power` timestamps match `velocity` timestamps")
-        return power
+    return device_eta
 
 
 def _calculate_density(water_density, bnr, mean_hub_vel, time):
     """
     Calculates the averaged density for the given time period.
 
-    This function first checks if the water_density is a scalar or an array. 
-    If it is an array, the function calculates the mean density over the time 
-    period using the binner object 'bnr', and then averages it over velocity bins. 
+    This function first checks if the water_density is a scalar or an array.
+    If it is an array, the function calculates the mean density over the time
+    period using the binner object 'bnr', and then averages it over velocity bins.
     If it is a scalar, it directly returns the input density.
 
     Parameters
     -------------
     water_density : numpy.ndarray or float
         Sea water density values in kg/m^3. It can be a scalar or a 1D array.
     bnr : dolfyn.VelBinner object
@@ -591,17 +586,18 @@
         Mean velocity at the hub height.
     time : numpy.ndarray
         Time data array.
 
     Returns
     ---------
     xarray.DataArray or float
-        The averaged water density over velocity bins if water_density is an array, 
+        The averaged water density over velocity bins if water_density is an array,
         or the input scalar water_density.
     """
 
     if np.size(water_density) > 1:
-        rho_avg = xr.DataArray(bnr.mean(water_density.values),
-                               coords={'time': bnr.mean(time)})
+        rho_avg = xr.DataArray(
+            bnr.mean(water_density.values), coords={"time": bnr.mean(time)}
+        )
         return _average_velocity_bins(rho_avg, mean_hub_vel, bin_size=0.1)
     else:
         return water_density
```

### Comparing `mhkit-0.7.0/mhkit/tidal/resource.py` & `mhkit-0.8.0/mhkit/tidal/resource.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,186 +1,196 @@
 import numpy as np
 import math
-import pandas as pd
-from  mhkit.river.resource import exceedance_probability, Froude_number
+from mhkit.river.resource import exceedance_probability, Froude_number
+from mhkit.utils import convert_to_dataarray
+
 
 def _histogram(directions, velocities, width_dir, width_vel):
-    '''
+    """
     Wrapper around numpy histogram 2D. Used to find joint probability
     between directions and velocities. Returns joint probability H as [%].
 
     Parameters
     ----------
     directions: array-like
         Directions in degrees with 0 degrees specified as true north
     velocities: array-like
         Velocities in m/s
-    width_dir: float 
+    width_dir: float
         Width of directional bins for histogram in degrees
-    width_vel: float 
+    width_vel: float
         Width of velocity bins for histogram in m/s
     Returns
     -------
     H: matrix
         Joint probability as [%]
     dir_edges: list
         List of directional bin edges
     vel_edges: list
         List of velocity bin edges
-    '''
+    """
 
-    # Number of directional bins 
-    N_dir = math.ceil(360/width_dir)
-    # Max bin (round up to nearest integer) 
+    # Number of directional bins
+    N_dir = math.ceil(360 / width_dir)
+    # Max bin (round up to nearest integer)
     vel_max = math.ceil(velocities.max())
     # Number of velocity bins
-    N_vel = math.ceil(vel_max/width_vel)
+    N_vel = math.ceil(vel_max / width_vel)
     # 2D Histogram of current speed and direction
-    H, dir_edges, vel_edges = np.histogram2d(directions, velocities, bins=(N_dir,N_vel),
-                                          range=[[0,360],[0,vel_max]], density=True)
+    H, dir_edges, vel_edges = np.histogram2d(
+        directions,
+        velocities,
+        bins=(N_dir, N_vel),
+        range=[[0, 360], [0, vel_max]],
+        density=True,
+    )
     # density = true therefore bin value * bin area summed =1
     bin_area = width_dir * width_vel
     # Convert H values to percent [%]
     H = H * bin_area * 100
     return H, dir_edges, vel_edges
 
 
 def _normalize_angle(degree):
-    '''
+    """
     Normalizes degrees to be between 0 and 360
-    
+
     Parameters
     ----------
     degree: int or float
 
     Returns
     -------
     new_degree: float
         Normalized between 0 and 360 degrees
-    '''
+    """
     # Set new degree as remainder
-    new_degree = degree%360
+    new_degree = degree % 360
     # Ensure positive
-    new_degree = (new_degree + 360) % 360 
+    new_degree = (new_degree + 360) % 360
     return new_degree
 
 
 def principal_flow_directions(directions, width_dir):
-    '''
+    """
     Calculates principal flow directions for ebb and flood cycles
-    
-    The weighted average (over the working velocity range of the TEC) 
-    should be considered to be the principal direction of the current, 
-    and should be used for both the ebb and flood cycles to determine 
-    the TEC optimum orientation. 
+
+    The weighted average (over the working velocity range of the TEC)
+    should be considered to be the principal direction of the current,
+    and should be used for both the ebb and flood cycles to determine
+    the TEC optimum orientation.
 
     Parameters
     ----------
-    directions: pandas.Series or numpy.ndarray
+    directions: numpy ndarray, pandas DataFrame, pandas Series, xarray DataArray, or xarray Dataset
         Flow direction in degrees CW from North, from 0 to 360
-    width_dir: float 
+    width_dir: float
         Width of directional bins for histogram in degrees
 
     Returns
     -------
     principal directions: tuple(float,float)
         Principal directions 1 and 2 in degrees
 
     Notes
     -----
-    One must determine which principal direction is flood and which is 
+    One must determine which principal direction is flood and which is
     ebb based on knowledge of the measurement site.
-    '''
+    """
 
-    if isinstance(directions, np.ndarray):
-        directions=pd.Series(directions)
-    assert(all(directions>=0) and all(directions<=360),
-           'flood must be between 0 and 360 degrees')
+    directions = convert_to_dataarray(directions)
+    if any(directions < 0) or any(directions > 360):
+        violating_values = [d for d in directions if d < 0 or d > 360]
+        raise ValueError(
+            f"directions must be between 0 and 360 degrees. Values out of range: {violating_values}"
+        )
 
-    # Number of directional bins 
-    N_dir=int(360/width_dir)
+    # Number of directional bins
+    N_dir = int(360 / width_dir)
     # Compute directional histogram
-    H1, dir_edges = np.histogram(directions, bins=N_dir,range=[0,360], density=True) 
-    # Convert to perecnt
-    H1 = H1 * 100 # [%]
+    H1, dir_edges = np.histogram(directions, bins=N_dir, range=[0, 360], density=True)
+    # Convert to percent
+    H1 = H1 * 100  # [%]
     # Determine if there are an even or odd number of bins
-    odd = bool( N_dir % 2  )
+    odd = bool(N_dir % 2)
     # Shift by 180 degrees and sum
     if odd:
         # Then split middle bin counts to left and right
-        H0to180    = H1[0:N_dir//2] 
-        H180to360  = H1[N_dir//2+1:]
-        H0to180[-1]   += H1[N_dir//2]/2
-        H180to360[0]  += H1[N_dir//2]/2
-        #Add the two
+        H0to180 = H1[0 : N_dir // 2]
+        H180to360 = H1[N_dir // 2 + 1 :]
+        H0to180[-1] += H1[N_dir // 2] / 2
+        H180to360[0] += H1[N_dir // 2] / 2
+        # Add the two
         H180 = H0to180 + H180to360
     else:
-        H180 =  H1[0:N_dir//2] + H1[N_dir//2:N_dir+1]
+        H180 = H1[0 : N_dir // 2] + H1[N_dir // 2 : N_dir + 1]
 
     # Find the maximum value
     maxDegreeStacked = H180.argmax()
     # Shift by 90 to find angles normal to principal direction
-    floodEbbNormalDegree1 = _normalize_angle(maxDegreeStacked + 90.)
-    # Find the complimentary angle 
-    floodEbbNormalDegree2 = _normalize_angle(floodEbbNormalDegree1+180.)
+    floodEbbNormalDegree1 = _normalize_angle(maxDegreeStacked + 90.0)
+    # Find the complimentary angle
+    floodEbbNormalDegree2 = _normalize_angle(floodEbbNormalDegree1 + 180.0)
     # Reset values so that the Degree1 is the smaller angle, and Degree2 the large
     floodEbbNormalDegree1 = min(floodEbbNormalDegree1, floodEbbNormalDegree2)
-    floodEbbNormalDegree2 = floodEbbNormalDegree1 + 180.
+    floodEbbNormalDegree2 = floodEbbNormalDegree1 + 180.0
     # Slice directions on the 2 semi circles
-    d1 = directions[directions.between(floodEbbNormalDegree1,
-                                       floodEbbNormalDegree2)] 
-    d2 = directions[~directions.between(floodEbbNormalDegree1,
-                                       floodEbbNormalDegree2)] 
+    mask = (directions >= floodEbbNormalDegree1) & (directions <= floodEbbNormalDegree2)
+    d1 = directions[mask]
+    d2 = directions[~mask]
     # Shift second set of of directions to not break between 360 and 0
-    d2 -= 180.
+    d2 -= 180
     # Renormalize the points (gets rid of negatives)
     d2 = _normalize_angle(d2)
     # Number of bins for semi-circle
-    n_dir = int(180/width_dir)
+    n_dir = int(180 / width_dir)
     # Compute 1D histograms on both semi circles
-    Hd1, dir1_edges = np.histogram(d1, bins=n_dir,density=True)
-    Hd2, dir2_edges = np.histogram(d2, bins=n_dir,density=True)
-    # Convert to perecnt
-    Hd1 = Hd1 * 100 # [%]
-    Hd2 = Hd2 * 100 # [%]
+    Hd1, dir1_edges = np.histogram(d1, bins=n_dir, density=True)
+    Hd2, dir2_edges = np.histogram(d2, bins=n_dir, density=True)
+    # Convert to percent
+    Hd1 = Hd1 * 100  # [%]
+    Hd2 = Hd2 * 100  # [%]
     # Principal Directions average of the 2 bins
-    PrincipalDirection1 = 0.5 * (dir1_edges[Hd1.argmax()]+ dir1_edges[Hd1.argmax()+1])
-    PrincipalDirection2 = 0.5 * (dir2_edges[Hd2.argmax()]+ dir2_edges[Hd2.argmax()+1])+180.0
+    PrincipalDirection1 = 0.5 * (
+        dir1_edges[Hd1.argmax()] + dir1_edges[Hd1.argmax() + 1]
+    )
+    PrincipalDirection2 = (
+        0.5 * (dir2_edges[Hd2.argmax()] + dir2_edges[Hd2.argmax() + 1]) + 180.0
+    )
+
+    return PrincipalDirection1, PrincipalDirection2
 
-    return PrincipalDirection1, PrincipalDirection2 
-    
 
 def _flood_or_ebb(d, flood, ebb):
-    '''
-    Returns a mask which is True for directions on the ebb side of the 
-    midpoints between the flood and ebb directions on the unit circle 
+    """
+    Returns a mask which is True for directions on the ebb side of the
+    midpoints between the flood and ebb directions on the unit circle
     and False for directions on the Flood side.
-    
+
     Parameters
     ----------
     d: array-like
         Directions to considered of length N
     flood: float or int
         Principal component of flow in the flood direction in degrees
     ebb: float or int
         Principal component of flow in the ebb direction in degrees
-        
+
     Returns
     -------
     is_ebb: boolean array
-        array of length N which is True for directions on the ebb side 
+        array of length N which is True for directions on the ebb side
         of the midpoints between flood and ebb on the unit circle and
         false otherwise.
-    '''
+    """
     max_angle = max(ebb, flood)
     min_angle = min(ebb, flood)
-    
-    lower_split = (min_angle + (360 - max_angle + min_angle)/2 ) % 360
+
+    lower_split = (min_angle + (360 - max_angle + min_angle) / 2) % 360
     upper_split = lower_split + 180
-    
+
     if lower_split <= ebb < upper_split:
         is_ebb = ((d < upper_split) & (d >= lower_split)).values
     else:
         is_ebb = ~((d < upper_split) & (d >= lower_split)).values
-        
-    return is_ebb
 
+    return is_ebb
```

### Comparing `mhkit-0.7.0/mhkit/utils.py` & `mhkit-0.8.0/mhkit/utils/stat_utils.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,299 +1,270 @@
-from pecos.utils import index_to_datetime
-import matplotlib.pyplot as plt 
-import datetime as dt
 from mhkit import qc
-import pandas as pd 
-import numpy as np 
+import pandas as pd
+import numpy as np
 
 
-_matlab = False # Private variable indicating if mhkit is run through matlab
-
-def get_statistics(data,freq,period=600,vector_channels=[]):
+def get_statistics(data, freq, period=600, vector_channels=[]):
     """
-    Calculate mean, max, min and stdev statistics of continuous data for a 
+    Calculate mean, max, min and stdev statistics of continuous data for a
     given statistical window. Default length of statistical window (period) is
     based on IEC TS 62600-3:2020 ED1. Also allows calculation of statistics for multiple statistical
     windows of continuous data and accounts for vector/directional channels.
 
     Parameters
     ------------
     data : pandas DataFrame
-        Data indexed by datetime with columns of data to be analyzed 
+        Data indexed by datetime with columns of data to be analyzed
     freq : float/int
         Sample rate of data [Hz]
     period : float/int
-        Statistical window of interest [sec], default = 600 
+        Statistical window of interest [sec], default = 600
     vector_channels : string or list (optional)
         List of vector/directional channel names formatted in deg (0-360)
 
     Returns
     ---------
     means,maxs,mins,stdevs : pandas DataFrame
         Calculated statistical values from the data, indexed by the first timestamp
     """
     # Check data type
-    assert isinstance(data, pd.DataFrame), 'data must be of type pd.DataFrame'
-    assert isinstance(freq, (float,int)), 'freq must be of type int or float'
-    assert isinstance(period, (float,int)), 'freq must be of type int or float'
+    if not isinstance(data, pd.DataFrame):
+        raise TypeError(f"data must be of type pd.DataFrame. Got: {type(data)}")
+    if not isinstance(freq, (float, int)):
+        raise TypeError(f"freq must be of type int or float. Got: {type(freq)}")
+    if not isinstance(period, (float, int)):
+        raise TypeError(f"period must be of type int or float. Got: {type(period)}")
     # catch if vector_channels is not an string array
-    if isinstance(vector_channels,str): vector_channels = [vector_channels]
-    assert isinstance(vector_channels, list), 'vector_channels must be a list of strings'
+    if isinstance(vector_channels, str):
+        vector_channels = [vector_channels]
+    if not isinstance(vector_channels, list):
+        raise TypeError(
+            f"vector_channels must be a list of strings. Got: {type(vector_channels)}"
+        )
 
     # Check timestamp using qc module
-    data.index = data.index.round('1ms')
-    dataQC = qc.check_timestamp(data,1/freq)
-    dataQC = dataQC['cleaned_data']
-    
+    data.index = data.index.round("1ms")
+    dataQC = qc.check_timestamp(data, 1 / freq)
+    dataQC = dataQC["cleaned_data"]
+
     # Check to see if data length contains enough data points for statistical window
-    if len(dataQC)%(period*freq) > 0:
-        remain = len(dataQC) % (period*freq)
-        dataQC = dataQC.iloc[0:-int(remain)]
-        print('WARNING: there were not enough data points in the last statistical period. Last '+str(remain)+' points were removed.')
-    
+    if len(dataQC) % (period * freq) > 0:
+        remain = len(dataQC) % (period * freq)
+        dataQC = dataQC.iloc[0 : -int(remain)]
+        print(
+            "WARNING: there were not enough data points in the last statistical period. Last "
+            + str(remain)
+            + " points were removed."
+        )
+
     # Pre-allocate lists
     time = []
     means = []
     maxs = []
     mins = []
     stdev = []
 
     # Get data chunks to performs stats on
-    step = period*freq
-    for i in range(int(len(dataQC)/(period*freq))):
-        datachunk = dataQC.iloc[i*step:(i+1)*step]
+    step = period * freq
+    for i in range(int(len(dataQC) / (period * freq))):
+        datachunk = dataQC.iloc[i * step : (i + 1) * step]
         # Check whether there are any NaNs in datachunk
-        if datachunk.isnull().any().any(): 
-            print('NaNs found in statistical window...check timestamps!')
-            input('Press <ENTER> to continue')
+        if datachunk.isnull().any().any():
+            print("NaNs found in statistical window...check timestamps!")
+            input("Press <ENTER> to continue")
             continue
         else:
             # Get stats
-            time.append(datachunk.index.values[0]) # time vector
-            maxs.append(datachunk.max()) # maxes
-            mins.append(datachunk.min()) # mins
-            means.append(datachunk.mean()) # means
-            stdev.append(datachunk.std()) # standard deviation
+            time.append(datachunk.index.values[0])  # time vector
+            maxs.append(datachunk.max())  # maxes
+            mins.append(datachunk.min())  # mins
+            means.append(datachunk.mean())  # means
+            stdev.append(datachunk.std())  # standard deviation
             # calculate vector averages and std
             for v in vector_channels:
-                vector_avg, vector_std = vector_statistics(datachunk[v])            
-                means[i][v] = vector_avg # overwrite scalar average for channel
-                stdev[i][v] = vector_std # overwrite scalar std for channel
-        
+                vector_avg, vector_std = vector_statistics(datachunk[v])
+                # overwrite scalar average for channel
+                means[i][v] = vector_avg
+                stdev[i][v] = vector_std  # overwrite scalar std for channel
+
     # Convert to DataFrames and set index
-    means = pd.DataFrame(means,index=time)
-    maxs = pd.DataFrame(maxs,index=time)
-    mins = pd.DataFrame(mins,index=time)
-    stdevs = pd.DataFrame(stdev,index=time)
+    means = pd.DataFrame(means, index=time)
+    maxs = pd.DataFrame(maxs, index=time)
+    mins = pd.DataFrame(mins, index=time)
+    stdevs = pd.DataFrame(stdev, index=time)
+
+    return means, maxs, mins, stdevs
 
-    return means,maxs,mins,stdevs
 
 def vector_statistics(data):
     """
     Function used to calculate statistics for vector/directional channels based on
     routine from Campbell data logger and Yamartino algorithm
 
     Parameters
     ----------
     data : pandas Series, numpy array, list
         Vector channel to calculate statistics on [deg, 0-360]
-    
+
     Returns
     -------
     vector_avg : numpy array
         Vector mean statistic
     vector_std : numpy array
         Vector standard deviation statistic
     """
-    try: data = np.array(data)
-    except: pass
-    assert isinstance(data, np.ndarray), 'data must be of type np.ndarray'
-    
+    try:
+        data = np.array(data)
+    except:
+        pass
+    if not isinstance(data, np.ndarray):
+        raise TypeError(f"data must be of type np.ndarray. Got: {type(data)}")
+
     # calculate mean
-    Ux = sum(np.sin(data*np.pi/180))/len(data)
-    Uy = sum(np.cos(data*np.pi/180))/len(data)
-    vector_avg = (90 - np.arctan2(Uy,Ux)*180/np.pi)
-    if vector_avg<0: vector_avg = vector_avg+360
-    elif vector_avg>360: vector_avg = vector_avg-360
-    # calculate standard deviation              
-    magsum = round((Ux**2 + Uy**2)*1e8)/1e8 # round to 8th decimal place to reduce roundoff error
-    epsilon = (1-magsum)**0.5
-    if not np.isreal(epsilon): # check if epsilon is imaginary (error)
+    Ux = sum(np.sin(data * np.pi / 180)) / len(data)
+    Uy = sum(np.cos(data * np.pi / 180)) / len(data)
+    vector_avg = 90 - np.arctan2(Uy, Ux) * 180 / np.pi
+    if vector_avg < 0:
+        vector_avg = vector_avg + 360
+    elif vector_avg > 360:
+        vector_avg = vector_avg - 360
+    # calculate standard deviation
+    # round to 8th decimal place to reduce roundoff error
+    magsum = round((Ux**2 + Uy**2) * 1e8) / 1e8
+    epsilon = (1 - magsum) ** 0.5
+    if not np.isreal(epsilon):  # check if epsilon is imaginary (error)
         vector_std = 0
-        print('WARNING: epsilon contains imaginary value')
+        print("WARNING: epsilon contains imaginary value")
     else:
-        vector_std = np.arcsin(epsilon)*(1+0.1547*epsilon**3)*180/np.pi
+        vector_std = np.arcsin(epsilon) * (1 + 0.1547 * epsilon**3) * 180 / np.pi
 
     return vector_avg, vector_std
 
+
 def unwrap_vector(data):
     """
     Function used to unwrap vectors into 0-360 deg range
 
     Parameters
     ------------
     data : pandas Series, numpy array, list
         Data points to be unwrapped [deg]
-    
+
     Returns
     ---------
     data : numpy array
         Data points unwrapped between 0-360 deg
     """
     # Check data types
     try:
         data = np.array(data)
     except:
         pass
-    assert isinstance(data, np.ndarray), 'data must be of type np.ndarray'
+    if not isinstance(data, np.ndarray):
+        raise TypeError(f"data must be of type np.ndarray. Got: {type(data)}")
 
     # Loop through and unwrap points
     for i in range(len(data)):
         if data[i] < 0:
-            data[i] = data[i]+360
+            data[i] = data[i] + 360
         elif data[i] > 360:
-            data[i] = data[i]-360
+            data[i] = data[i] - 360
     if max(data) > 360 or min(data) < 0:
         data = unwrap_vector(data)
     return data
 
-def matlab_to_datetime(matlab_datenum):
-    """
-    Convert MATLAB datenum format to Python datetime
-
-    Parameters
-    ------------
-    matlab_datenum : numpy array
-        MATLAB datenum to be converted
 
-    Returns
-    ---------
-    time : DateTimeIndex
-        Python datetime values
+def magnitude_phase(x, y, z=None):
     """
-    # Check data types
-    try:
-        matlab_datenum = np.array(matlab_datenum,ndmin=1)
-    except:
-        pass
-    assert isinstance(matlab_datenum, np.ndarray), 'data must be of type np.ndarray'
-
-    # Pre-allocate
-    time = []
-    # loop through dates and convert
-    for t in matlab_datenum:
-        day = dt.datetime.fromordinal(int(t))
-        dayfrac = dt.timedelta(days=t%1) - dt.timedelta(days = 366)
-        time.append(day + dayfrac)
-    
-    time = np.array(time)
-    time = pd.to_datetime(time)
-    return time
+    Retuns magnitude and phase in two or three dimensions.
 
-def excel_to_datetime(excel_num):
-    """
-    Convert Excel datenum format to Python datetime
-
-    Parameters
-    ------------
-    excel_num : numpy array
-        Excel datenums to be converted
-
-    Returns
-    ---------
-    time : DateTimeIndex
-        Python datetime values
-    """
-    # Check data types
-    try:
-        excel_num = np.array(excel_num)
-    except:
-        pass
-    assert isinstance(excel_num, np.ndarray), 'data must be of type np.ndarray'
-
-    # Convert to datetime
-    time = pd.to_datetime('1899-12-30')+pd.to_timedelta(excel_num,'D')
-
-    return time                
-    
-    
-def magnitude_phase(x,y,z=None):
-    '''
-    Retuns magnitude and phase in two or three dimensions. 
-    
     Parameters
     ----------
     x: array_like
         x-component
     y: array_like
         y-component
     z: array_like
         z-component defined positive up. (Optional) Default None.
-    
+
     Returns
     -------
     mag: float or array
         magnitude of the vector
     theta: float or array
         radians from the x-axis
     phi: float or array
-        radians from z-axis defined as positive up. Optional: only 
+        radians from z-axis defined as positive up. Optional: only
         returned when z is passed.
-    '''
-    x=np.array(x)
-    y=np.array(y)
+    """
+    x = np.array(x)
+    y = np.array(y)
 
-    threeD=False
+    threeD = False
     if not isinstance(z, type(None)):
-        z=np.array(z)
-        threeD=True
-        
-    assert isinstance(x, (float,int,np.ndarray))
-    assert isinstance(y, (float,int,np.ndarray))
-    assert isinstance(z, (type(None),float,int,np.ndarray))
-        
+        z = np.array(z)
+        threeD = True
+
+    if not isinstance(x, (float, int, np.ndarray)):
+        raise TypeError(f"x must be of type float, int, or np.ndarray. Got: {type(x)}")
+    if not isinstance(y, (float, int, np.ndarray)):
+        raise TypeError(f"y must be of type float, int, or np.ndarray. Got: {type(y)}")
+    if not isinstance(z, (type(None), float, int, np.ndarray)):
+        raise TypeError(
+            f"If specified, z must be of type float, int, or np.ndarray. Got: {type(z)}"
+        )
+
     if threeD:
         mag = np.sqrt(x**2 + y**2 + z**2)
-        theta = np.arctan2(y,x)
-        phi = np.arctan2(np.sqrt(x**2+y**2),z)
+        theta = np.arctan2(y, x)
+        phi = np.arctan2(np.sqrt(x**2 + y**2), z)
         return mag, theta, phi
     else:
         mag = np.sqrt(x**2 + y**2)
         theta = np.arctan2(y, x)
         return mag, theta
 
-def unorm(x, y ,z):
-    '''
-    Calculates the root mean squared value given three arrays. 
+
+def unorm(x, y, z):
+    """
+    Calculates the root mean squared value given three arrays.
 
     Parameters
     ----------
-    x: array 
-        One input for the root mean squared calculation.(eq. x velocity) 
+    x: array
+        One input for the root mean squared calculation.(eq. x velocity)
     y: array
-        One input for the root mean squared calculation.(eq. y velocity) 
+        One input for the root mean squared calculation.(eq. y velocity)
     z: array
-        One input for the root mean squared calculation.(eq. z velocity) 
+        One input for the root mean squared calculation.(eq. z velocity)
 
     Returns
     -------
-    unorm : array 
+    unorm : array
        The root mean squared of x, y, and z.
-       
-    Example 
+
+    Example
     -------
-    If the inputs are [1,2,3], [4,5,6], and [7,8,9] the code take the 
-    cordinationg value from each array and calculates the root mean squared. 
+    If the inputs are [1,2,3], [4,5,6], and [7,8,9] the code take the
+    cordinationg value from each array and calculates the root mean squared.
     The resulting output is [ 8.1240384,  9.64365076, 11.22497216].
-    '''
-    
-    assert isinstance(x,(np.ndarray, np.float64, pd.Series)), 'x must be an array'
-    assert isinstance(y,(np.ndarray, np.float64, pd.Series)), 'y must be an array'
-    assert isinstance(z,(np.ndarray, np.float64, pd.Series)), 'z must be an array'
-    assert all([len(x) == len(y), len (y) ==len (z)]), ('lengths of arrays must'
-                                                        +' match')
+    """
+
+    if not isinstance(x, (np.ndarray, np.float64, pd.Series)):
+        raise TypeError(
+            f"x must be of type np.ndarray, np.float64, or pd.Series. Got: {type(x)}"
+        )
+    if not isinstance(y, (np.ndarray, np.float64, pd.Series)):
+        raise TypeError(
+            f"y must be of type np.ndarray, np.float64, or pd.Series. Got: {type(y)}"
+        )
+    if not isinstance(z, (np.ndarray, np.float64, pd.Series)):
+        raise TypeError(
+            f"z must be of type np.ndarray, np.float64, or pd.Series. Got: {type(z)}"
+        )
+    if not all([len(x) == len(y), len(y) == len(z)]):
+        raise ValueError("lengths of arrays must match")
 
-    xyz = np.array([x,y,z]) 
-    unorm = np.linalg.norm(xyz, axis= 0)
+    xyz = np.array([x, y, z])
+    unorm = np.linalg.norm(xyz, axis=0)
 
     return unorm
-
```

### Comparing `mhkit-0.7.0/mhkit/wave/contours.py` & `mhkit-0.8.0/mhkit/wave/contours.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,31 +2,36 @@
 from sklearn.decomposition import PCA as skPCA
 from sklearn.metrics import mean_squared_error
 import matplotlib.pyplot as plt
 import scipy.optimize as optim
 import scipy.stats as stats
 import scipy.interpolate as interp
 import numpy as np
+import warnings
+from mhkit.utils import to_numeric_array
 
+import matplotlib
 
-### Contours
-def environmental_contours(x1, x2, sea_state_duration, return_period,
-                           method, **kwargs):
+mpl_version = tuple(map(int, matplotlib.__version__.split(".")))
+
+
+# Contours
+def environmental_contours(x1, x2, sea_state_duration, return_period, method, **kwargs):
     """
     Returns a Dictionary of x1 and x2 components for each contour
     method passed. A method  may be one of the following:
     Principal Component Analysis, Gaussian, Gumbel, Clayton, Rosenblatt,
     nonparametric Gaussian, nonparametric Clayton,
     nonparametric Gumbel, bivariate KDE, log bivariate KDE
 
     Parameters
     ----------
-    x1: array
+    x1: list, np.ndarray, pd.Series, xr.DataArray
         Component 1 data
-    x2: array
+    x2: list, np.ndarray, pd.Series, xr.DataArray
         Component 2 data
     sea_state_duration : int or float
         `x1` and `x2` averaging period in seconds
     return_period: int, float
         Return period of interest in years
     method: string or list
         Copula method to apply. Options include ['PCA','gaussian',
@@ -68,173 +73,226 @@
             Will return fitting parameters used for each method passed.
             Default False.
 
     Returns
     -------
     copulas: Dictionary
         Dictionary of x1 and x2 copula components for each copula method
-   """
-    try:
-        x1 = np.array(x1)
-    except:
-        pass
-    try:
-        x2 = np.array(x2)
-    except:
-        pass
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(sea_state_duration, (int, float)), (
-        'sea_state_duration must be of type int or float')
-    assert isinstance(return_period, (int, float, np.ndarray)), (
-        'return_period must be of type int, float, or array')
+    """
+    x1 = to_numeric_array(x1, "x1")
+    x2 = to_numeric_array(x2, "x2")
+    if not isinstance(x1, np.ndarray) or x1.ndim == 0:
+        raise TypeError(f"x1 must be a non-scalar array. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray) or x2.ndim == 0:
+        raise TypeError(f"x2 must be a non-scalar array. Got: {type(x2)}")
+    if len(x1) != len(x2):
+        raise ValueError("The lengths of x1 and x2 must be equal.")
+    if not isinstance(sea_state_duration, (int, float)):
+        raise TypeError(
+            f"sea_state_duration must be of type int or float. Got: {type(sea_state_duration)}"
+        )
+    if not isinstance(return_period, (int, float, np.ndarray)):
+        raise TypeError(
+            f"return_period must be of type int, float, or np.ndarray. Got: {type(return_period)}"
+        )
 
     bin_val_size = kwargs.get("bin_val_size", 0.25)
     nb_steps = kwargs.get("nb_steps", 1000)
-    initial_bin_max_val = kwargs.get("initial_bin_max_val", 1.)
+    initial_bin_max_val = kwargs.get("initial_bin_max_val", 1.0)
     min_bin_count = kwargs.get("min_bin_count", 40)
     bandwidth = kwargs.get("bandwidth", None)
     Ndata_bivariate_KDE = kwargs.get("Ndata_bivariate_KDE", 100)
     max_x1 = kwargs.get("max_x1", None)
     max_x2 = kwargs.get("max_x2", None)
     PCA = kwargs.get("PCA", None)
     PCA_bin_size = kwargs.get("PCA_bin_size", 250)
     return_fit = kwargs.get("return_fit", False)
 
-    assert isinstance(PCA, (dict, type(None))), (
-        'If specified PCA must be a dict')
-    assert isinstance(PCA_bin_size, int), 'PCA_bin_size must be of type int'
-    assert isinstance(return_fit, bool), 'return_fit must be of type bool'
-    assert isinstance(bin_val_size, (int, float)), (
-        'bin_val_size must be of type int or float')
-    assert isinstance(nb_steps, int), 'nb_steps must be of type int'
-    assert isinstance(min_bin_count, int), ('min_bin_count must be of '
-                                            + 'type int')
-    assert isinstance(initial_bin_max_val, (int, float)), (
-        'initial_bin_max_val must be of type int or float')
-    if bandwidth == None:
-        assert(not 'bivariate_KDE' in method), (
-            'Must specify keyword bandwidth with bivariate KDE method')
+    if not isinstance(max_x1, (int, float, type(None))):
+        raise TypeError(f"If specified, max_x1 must be a dict. Got: {type(PCA)}")
+    if not isinstance(max_x2, (int, float, type(None))):
+        raise TypeError(f"If specified, max_x2 must be a dict. Got: {type(PCA)}")
+    if not isinstance(PCA, (dict, type(None))):
+        raise TypeError(f"If specified, PCA must be a dict. Got: {type(PCA)}")
+    if not isinstance(PCA_bin_size, int):
+        raise TypeError(f"PCA_bin_size must be of type int. Got: {type(PCA_bin_size)}")
+    if not isinstance(return_fit, bool):
+        raise TypeError(f"return_fit must be of type bool. Got: {type(return_fit)}")
+    if not isinstance(bin_val_size, (int, float)):
+        raise TypeError(
+            f"bin_val_size must be of type int or float. Got: {type(bin_val_size)}"
+        )
+    if not isinstance(nb_steps, int):
+        raise TypeError(f"nb_steps must be of type int. Got: {type(nb_steps)}")
+    if not isinstance(min_bin_count, int):
+        raise TypeError(
+            f"min_bin_count must be of type int. Got: {type(min_bin_count)}"
+        )
+    if not isinstance(initial_bin_max_val, (int, float)):
+        raise TypeError(
+            f"initial_bin_max_val must be of type int or float. Got: {type(initial_bin_max_val)}"
+        )
+    if "bivariate_KDE" in method and bandwidth == None:
+        raise TypeError(
+            f"Must specify keyword bandwidth with bivariate KDE method. Got: {type(bandwidth)}"
+        )
 
     if isinstance(method, str):
         method = [method]
-    assert (len(set(method)) == len(method)), (
-        'Can only pass a unique '
-        + 'method once per function call. Consider wrapping this '
-        + 'function in a for loop to investage variations on the same method')
-
-    method_class = {'PCA': 'parametric',
-                    'gaussian': 'parametric',
-                    'gumbel': 'parametric',
-                    'clayton': 'parametric',
-                    'rosenblatt': 'parametric',
-                    'nonparametric_gaussian': 'nonparametric',
-                    'nonparametric_clayton': 'nonparametric',
-                    'nonparametric_gumbel': 'nonparametric',
-                    'bivariate_KDE': 'KDE',
-                    'bivariate_KDE_log': 'KDE'}
+    if not (len(set(method)) == len(method)):
+        raise ValueError(
+            f"Can only pass a unique "
+            + "method once per function call. Consider wrapping this "
+            + "function in a for loop to investage variations on the same method"
+        )
+
+    method_class = {
+        "PCA": "parametric",
+        "gaussian": "parametric",
+        "gumbel": "parametric",
+        "clayton": "parametric",
+        "rosenblatt": "parametric",
+        "nonparametric_gaussian": "nonparametric",
+        "nonparametric_clayton": "nonparametric",
+        "nonparametric_gumbel": "nonparametric",
+        "bivariate_KDE": "KDE",
+        "bivariate_KDE_log": "KDE",
+    }
 
     classification = []
     methods = method
     for method in methods:
         classification.append(method_class[method])
 
     fit = _iso_prob_and_quantile(sea_state_duration, return_period, nb_steps)
     fit_parametric = None
     fit_nonparametric = None
     component_1 = None
-    if 'parametric' in classification:
-        (para_dist_1, para_dist_2, mean_cond, std_cond) = (
-            _copula_parameters(x1, x2, min_bin_count,
-            initial_bin_max_val, bin_val_size))
+    if "parametric" in classification:
+        (para_dist_1, para_dist_2, mean_cond, std_cond) = _copula_parameters(
+            x1, x2, min_bin_count, initial_bin_max_val, bin_val_size
+        )
 
-        x_quantile = fit['x_quantile']
+        x_quantile = fit["x_quantile"]
         a = para_dist_1[0]
         c = para_dist_1[1]
         loc = para_dist_1[2]
         scale = para_dist_1[3]
 
-        component_1 = stats.exponweib.ppf(
-            x_quantile, a, c, loc=loc, scale=scale)
+        component_1 = stats.exponweib.ppf(x_quantile, a, c, loc=loc, scale=scale)
 
         fit_parametric = fit
-        fit_parametric['para_dist_1'] = para_dist_1
-        fit_parametric['para_dist_2'] = para_dist_2
-        fit_parametric['mean_cond'] = mean_cond
-        fit_parametric['std_cond'] = std_cond
+        fit_parametric["para_dist_1"] = para_dist_1
+        fit_parametric["para_dist_2"] = para_dist_2
+        fit_parametric["mean_cond"] = mean_cond
+        fit_parametric["std_cond"] = std_cond
         if PCA == None:
             PCA = fit_parametric
 
-    if 'nonparametric' in classification:
-        (nonpara_dist_1, nonpara_dist_2, nonpara_pdf_2) = (
-            _nonparametric_copula_parameters(x1, x2, nb_steps=nb_steps))
+    if "nonparametric" in classification:
+        (
+            nonpara_dist_1,
+            nonpara_dist_2,
+            nonpara_pdf_2,
+        ) = _nonparametric_copula_parameters(x1, x2, nb_steps=nb_steps)
         fit_nonparametric = fit
-        fit_nonparametric['nonpara_dist_1'] = nonpara_dist_1
-        fit_nonparametric['nonpara_dist_2'] = nonpara_dist_2
-        fit_nonparametric['nonpara_pdf_2'] = nonpara_pdf_2
-
-    copula_functions = {'PCA':
-                        {'func': PCA_contour,
-                         'vals': (x1, x2, PCA, {'nb_steps': nb_steps,
-                                                'return_fit': return_fit,
-                                                'bin_size': PCA_bin_size})},
-                        'gaussian':
-                        {'func': _gaussian_copula,
-                         'vals': (x1, x2, fit_parametric, component_1,
-                                  {'return_fit': return_fit})},
-                        'gumbel':
-                        {'func': _gumbel_copula,
-                         'vals': (x1, x2, fit_parametric, component_1,
-                                  nb_steps, {'return_fit': return_fit})},
-                        'clayton':
-                        {'func': _clayton_copula,
-                         'vals': (x1, x2, fit_parametric, component_1,
-                                  {'return_fit': return_fit})},
-                        'rosenblatt':
-                        {'func': _rosenblatt_copula,
-                         'vals': (x1, x2, fit_parametric, component_1,
-                                  {'return_fit': return_fit})},
-                        'nonparametric_gaussian':
-                        {'func': _nonparametric_gaussian_copula,
-                         'vals': (x1, x2, fit_nonparametric, nb_steps,
-                                  {'return_fit': return_fit})},
-                        'nonparametric_clayton':
-                        {'func': _nonparametric_clayton_copula,
-                         'vals': (x1, x2, fit_nonparametric, nb_steps,
-                                  {'return_fit': return_fit})},
-                        'nonparametric_gumbel':
-                        {'func': _nonparametric_gumbel_copula,
-                         'vals': (x1, x2, fit_nonparametric, nb_steps,
-                                  {'return_fit': return_fit})},
-                        'bivariate_KDE':
-                        {'func': _bivariate_KDE,
-                         'vals': (x1, x2, bandwidth, fit, nb_steps,
-                                  Ndata_bivariate_KDE,
-                                  {'max_x1': max_x1, 'max_x2': max_x2,
-                                   'return_fit': return_fit})},
-                        'bivariate_KDE_log':
-                        {'func': _bivariate_KDE,
-                         'vals': (x1, x2, bandwidth, fit, nb_steps,
-                                  Ndata_bivariate_KDE,
-                                  {'max_x1': max_x1, 'max_x2': max_x2,
-                                   'log_transform': True,
-                                   'return_fit': return_fit})},
-                        }
+        fit_nonparametric["nonpara_dist_1"] = nonpara_dist_1
+        fit_nonparametric["nonpara_dist_2"] = nonpara_dist_2
+        fit_nonparametric["nonpara_pdf_2"] = nonpara_pdf_2
+
+    copula_functions = {
+        "PCA": {
+            "func": PCA_contour,
+            "vals": (
+                x1,
+                x2,
+                PCA,
+                {
+                    "nb_steps": nb_steps,
+                    "return_fit": return_fit,
+                    "bin_size": PCA_bin_size,
+                },
+            ),
+        },
+        "gaussian": {
+            "func": _gaussian_copula,
+            "vals": (x1, x2, fit_parametric, component_1, {"return_fit": return_fit}),
+        },
+        "gumbel": {
+            "func": _gumbel_copula,
+            "vals": (
+                x1,
+                x2,
+                fit_parametric,
+                component_1,
+                nb_steps,
+                {"return_fit": return_fit},
+            ),
+        },
+        "clayton": {
+            "func": _clayton_copula,
+            "vals": (x1, x2, fit_parametric, component_1, {"return_fit": return_fit}),
+        },
+        "rosenblatt": {
+            "func": _rosenblatt_copula,
+            "vals": (x1, x2, fit_parametric, component_1, {"return_fit": return_fit}),
+        },
+        "nonparametric_gaussian": {
+            "func": _nonparametric_gaussian_copula,
+            "vals": (x1, x2, fit_nonparametric, nb_steps, {"return_fit": return_fit}),
+        },
+        "nonparametric_clayton": {
+            "func": _nonparametric_clayton_copula,
+            "vals": (x1, x2, fit_nonparametric, nb_steps, {"return_fit": return_fit}),
+        },
+        "nonparametric_gumbel": {
+            "func": _nonparametric_gumbel_copula,
+            "vals": (x1, x2, fit_nonparametric, nb_steps, {"return_fit": return_fit}),
+        },
+        "bivariate_KDE": {
+            "func": _bivariate_KDE,
+            "vals": (
+                x1,
+                x2,
+                bandwidth,
+                fit,
+                nb_steps,
+                Ndata_bivariate_KDE,
+                {"max_x1": max_x1, "max_x2": max_x2, "return_fit": return_fit},
+            ),
+        },
+        "bivariate_KDE_log": {
+            "func": _bivariate_KDE,
+            "vals": (
+                x1,
+                x2,
+                bandwidth,
+                fit,
+                nb_steps,
+                Ndata_bivariate_KDE,
+                {
+                    "max_x1": max_x1,
+                    "max_x2": max_x2,
+                    "log_transform": True,
+                    "return_fit": return_fit,
+                },
+            ),
+        },
+    }
     copulas = {}
 
     for method in methods:
-        vals = copula_functions[method]['vals']
+        vals = copula_functions[method]["vals"]
         if return_fit:
-            component_1, component_2, fit = copula_functions[method]['func'](
-                *vals)
-            copulas[f'{method}_fit'] = fit
+            component_1, component_2, fit = copula_functions[method]["func"](*vals)
+            copulas[f"{method}_fit"] = fit
         else:
-            component_1, component_2 = copula_functions[method]['func'](*vals)
-        copulas[f'{method}_x1'] = component_1
-        copulas[f'{method}_x2'] = component_2
+            component_1, component_2 = copula_functions[method]["func"](*vals)
+        copulas[f"{method}_x1"] = component_1
+        copulas[f"{method}_x2"] = component_2
 
     return copulas
 
 
 def PCA_contour(x1, x2, fit, kwargs):
     """
     Calculates environmental contours of extreme sea
@@ -255,17 +313,17 @@
     Neary, V. S. (2016). Application of principal component
     analysis (PCA) and improved joint probability distributions to
     the inverse first-order reliability method (I-FORM) for predicting
     extreme sea states. Ocean Engineering, 112, 307-319.
 
     Parameters
     ----------
-    x1: numpy array
+    x1: list, np.ndarray, pd.Series, xr.DataArray
         Component 1 data
-    x2: numpy array
+    x2: list, np.ndarray, pd.Series, xr.DataArray
         Component 2 data
     fit: dict
         Dictionary of the iso-probability results. May additionally
         contain the principal component analysis (PCA) on x1, x2
         The PCA will be the same for a given x1, x2
         therefore this step may be skipped if multiple calls to
         environmental contours are made for the same x1, x2 pair.
@@ -285,81 +343,86 @@
     x1_contour : numpy array
         Calculated x1 values along the contour boundary following
         return to original input orientation.
     x2_contour : numpy array
         Calculated x2 values along the contour boundary following
         return to original input orientation.
     fit: dict (optional)
-	    principal component analysis dictionary
+            principal component analysis dictionary
         Keys:
         -----
         'principal_axes': sign corrected PCA axes
         'shift'         : The shift applied to x2
         'x1_fit'        : gaussian fit of x1 data
         'mu_param'      : fit to _mu_fcn
         'sigma_param'   : fit to _sig_fits
 
     """
-    try:
-        x1 = np.array(x1)
-    except:
-        pass
-    try:
-        x2 = np.array(x2)
-    except:
-        pass
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
+    x1 = to_numeric_array(x1, "x1")
+    x2 = to_numeric_array(x2, "x2")
+    if not isinstance(x1, np.ndarray) or x1.ndim == 0:
+        raise TypeError(f"x1 must be a non-scalar array. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray) or x2.ndim == 0:
+        raise TypeError(f"x2 must be a non-scalar array. Got: {type(x2)}")
+    if len(x1) != len(x2):
+        raise ValueError("The lengths of x1 and x2 must be equal.")
 
     bin_size = kwargs.get("bin_size", 250)
     nb_steps = kwargs.get("nb_steps", 1000)
     return_fit = kwargs.get("return_fit", False)
 
-    assert isinstance(bin_size, int), 'bin_size must be of type int'
-    assert isinstance(nb_steps, int), 'nb_steps must be of type int'
-    assert isinstance(return_fit, bool), 'return_fit must be of type bool'
+    if not isinstance(bin_size, int):
+        raise TypeError(f"bin_size must be of type int. Got: {type(bin_size)}")
+    if not isinstance(nb_steps, int):
+        raise TypeError(f"nb_steps must be of type int. Got: {type(nb_steps)}")
+    if not isinstance(return_fit, bool):
+        raise TypeError(f"return_fit must be of type bool. Got: {type(return_fit)}")
 
-    if 'x1_fit' not in fit:
+    if "x1_fit" not in fit:
         pca_fit = _principal_component_analysis(x1, x2, bin_size=bin_size)
         for key in pca_fit:
             fit[key] = pca_fit[key]
 
-    x_quantile = fit['x_quantile']
-    y_quantile = fit['y_quantile']
+    x_quantile = fit["x_quantile"]
+    y_quantile = fit["y_quantile"]
 
     # Use the inverse of cdf to calculate component 1 values
-    component_1 = stats.invgauss.ppf(x_quantile,
-                                     mu=fit['x1_fit']['mu'],
-                                     loc=fit['x1_fit']['loc'],
-                                     scale=fit['x1_fit']['scale'])
+    component_1 = stats.invgauss.ppf(
+        x_quantile,
+        mu=fit["x1_fit"]["mu"],
+        loc=fit["x1_fit"]["loc"],
+        scale=fit["x1_fit"]["scale"],
+    )
 
     # Find Component 2 mu using first order linear regression
-    mu_slope = fit['mu_fit'].slope
-    mu_intercept = fit['mu_fit'].intercept
+    mu_slope = fit["mu_fit"].slope
+    mu_intercept = fit["mu_fit"].intercept
     component_2_mu = mu_slope * component_1 + mu_intercept
 
     # Find Componenet 2 sigma using second order polynomial fit
-    sigma_polynomial_coeffcients = fit['sigma_fit'].x
+    sigma_polynomial_coeffcients = fit["sigma_fit"].x
     component_2_sigma = np.polyval(sigma_polynomial_coeffcients, component_1)
 
     # Use calculated mu and sigma values to calculate C2 along the contour
-    component_2 = stats.norm.ppf(y_quantile,
-                                 loc=component_2_mu,
-                                 scale=component_2_sigma)
+    component_2 = stats.norm.ppf(
+        y_quantile, loc=component_2_mu, scale=component_2_sigma
+    )
 
     # Convert contours back to the original reference frame
-    principal_axes = fit['principal_axes']
-    shift = fit['shift']
+    principal_axes = fit["principal_axes"]
+    shift = fit["shift"]
     pa00 = principal_axes[0, 0]
     pa01 = principal_axes[0, 1]
 
-    x1_contour = ((pa00 * component_1 + pa01 * (component_2 - shift)) /
-                  (pa01**2 + pa00**2))
-    x2_contour = ((pa01 * component_1 - pa00 * (component_2 - shift)) /
-                  (pa01**2 + pa00**2))
+    x1_contour = (pa00 * component_1 + pa01 * (component_2 - shift)) / (
+        pa01**2 + pa00**2
+    )
+    x2_contour = (pa01 * component_1 - pa00 * (component_2 - shift)) / (
+        pa01**2 + pa00**2
+    )
 
     # Assign 0 value to any negative x1 contour values
     x1_contour = np.maximum(0, x1_contour)
 
     if return_fit:
         return np.transpose(x1_contour), np.transpose(x2_contour), fit
     return np.transpose(x1_contour), np.transpose(x2_contour)
@@ -406,23 +469,26 @@
        -----
        'principal_axes': sign corrected PCA axes
        'shift'         : The shift applied to x2
        'x1_fit'        : gaussian fit of x1 data
        'mu_param'      : fit to _mu_fcn
        'sigma_param'   : fit to _sig_fits
     """
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(bin_size, int), 'bin_size must be of type int'
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(bin_size, int):
+        raise TypeError(f"bin_size must be of type int. Got: {type(bin_size)}")
+
     # Step 0: Perform Standard PCA
     mean_location = 0
     x1_mean_centered = x1 - x1.mean(axis=0)
     x2_mean_centered = x2 - x2.mean(axis=0)
-    n_samples_by_n_features = np.column_stack((x1_mean_centered,
-                                               x2_mean_centered))
+    n_samples_by_n_features = np.column_stack((x1_mean_centered, x2_mean_centered))
     pca = skPCA(n_components=2)
     pca.fit(n_samples_by_n_features)
     principal_axes = pca.components_
 
     # STEP 1: Transform data into new reference frame
     # Apply correct/expected sign convention
     principal_axes = abs(principal_axes)
@@ -440,37 +506,39 @@
 
     # STEP 2: Fit Component 1 data using a Gaussian Distribution
     x1_sorted_index = x1_components.argsort()
     x1_sorted = x1_components[x1_sorted_index]
     x2_sorted = x2_components[x1_sorted_index]
 
     x1_fit_results = stats.invgauss.fit(x1_sorted, floc=mean_location)
-    x1_fit = {'mu': x1_fit_results[0],
-              'loc': x1_fit_results[1],
-              'scale': x1_fit_results[2]}
+    x1_fit = {
+        "mu": x1_fit_results[0],
+        "loc": x1_fit_results[1],
+        "scale": x1_fit_results[2],
+    }
 
     # Step 3: Bin Data & find order 1 linear relation between x1 & x2 means
     N = len(x1)
-    minimum_4_bins = np.floor(N*0.25)
+    minimum_4_bins = np.floor(N * 0.25)
     if bin_size > minimum_4_bins:
         bin_size = minimum_4_bins
-        msg = ('To allow for a minimum of 4 bins the bin size has been' +
-               f'set to {minimum_4_bins}')
-        print(msg)
+        msg = (
+            "To allow for a minimum of 4 bins, the bin size has been "
+            + f"set to {minimum_4_bins}"
+        )
+        warnings.warn(msg, UserWarning)
 
-    N_multiples = N // bin_size
-    max_N_multiples_index = N_multiples*bin_size
+    N_multiples = int(N // bin_size)
+    max_N_multiples_index = int(N_multiples * bin_size)
 
     x1_integer_multiples_of_bin_size = x1_sorted[0:max_N_multiples_index]
     x2_integer_multiples_of_bin_size = x2_sorted[0:max_N_multiples_index]
 
-    x1_bins = np.split(x1_integer_multiples_of_bin_size,
-                       N_multiples)
-    x2_bins = np.split(x2_integer_multiples_of_bin_size,
-                       N_multiples)
+    x1_bins = np.split(x1_integer_multiples_of_bin_size, N_multiples)
+    x2_bins = np.split(x2_integer_multiples_of_bin_size, N_multiples)
 
     x1_last_bin = x1_sorted[max_N_multiples_index:]
     x2_last_bin = x2_sorted[max_N_multiples_index:]
 
     x1_bins.append(x1_last_bin)
     x2_bins.append(x2_last_bin)
 
@@ -483,37 +551,46 @@
         x2_means = np.append(x2_means, x2_bin.mean())
         x2_sigmas = np.append(x2_sigmas, x2_bin.std())
 
     mu_fit = stats.linregress(x1_means, x2_means)
 
     # STEP 4: Find order 2 relation between x1_mean and x2 standard deviation
     sigma_polynomial_order = 2
-    sig_0 = 0.1 * np.ones(sigma_polynomial_order+1)
+    sig_0 = 0.1 * np.ones(sigma_polynomial_order + 1)
 
     def _objective_function(sig_p, x1_means, x2_sigmas):
         return mean_squared_error(np.polyval(sig_p, x1_means), x2_sigmas)
 
     # Constraint Functions
-    def y_intercept_gt_0(sig_p): return (sig_p[2])
+    def y_intercept_gt_0(sig_p):
+        return sig_p[2]
 
     def sig_polynomial_min_gt_0(sig_p):
-        return (sig_p[2] - (sig_p[1]**2) / (4 * sig_p[0]))
+        return sig_p[2] - (sig_p[1] ** 2) / (4 * sig_p[0])
 
-    constraints = ({'type': 'ineq', 'fun': y_intercept_gt_0},
-                   {'type': 'ineq', 'fun': sig_polynomial_min_gt_0})
-
-    sigma_fit = optim.minimize(_objective_function, x0=sig_0,
-                               args=(x1_means, x2_sigmas),
-                               method='SLSQP', constraints=constraints)
-
-    PCA = {'principal_axes': principal_axes,
-           'shift': shift,
-           'x1_fit': x1_fit,
-           'mu_fit': mu_fit,
-           'sigma_fit': sigma_fit}
+    constraints = (
+        {"type": "ineq", "fun": y_intercept_gt_0},
+        {"type": "ineq", "fun": sig_polynomial_min_gt_0},
+    )
+
+    sigma_fit = optim.minimize(
+        _objective_function,
+        x0=sig_0,
+        args=(x1_means, x2_sigmas),
+        method="SLSQP",
+        constraints=constraints,
+    )
+
+    PCA = {
+        "principal_axes": principal_axes,
+        "shift": shift,
+        "x1_fit": x1_fit,
+        "mu_fit": mu_fit,
+        "sigma_fit": sigma_fit,
+    }
 
     return PCA
 
 
 def _iso_prob_and_quantile(sea_state_duration, return_period, nb_steps):
     """
     Calculates the iso-probability and the x, y quantiles along
@@ -537,45 +614,49 @@
         'exceedance_probability' - probability of exceedance
         'x_component_iso_prob' - x-component of iso probability circle
         'y_component_iso_prob' - y-component of iso probability circle
         'x_quantile' - CDF of x-component
         'y_quantile' - CDF of y-component
     """
 
-    assert isinstance(sea_state_duration, (int, float)
-                      ), 'sea_state_duration must be of type int or float'
-    assert isinstance(return_period, (int, float)), (
-        'return_period must be of type int or float')
-
-    assert isinstance(nb_steps, int), 'nb_steps must be of type int'
+    if not isinstance(sea_state_duration, (int, float)):
+        raise TypeError(
+            f"sea_state_duration must be of type int or float. Got: {type(sea_state_duration)}"
+        )
+    if not isinstance(return_period, (int, float)):
+        raise TypeError(
+            f"return_period must be of type int or float. Got: {type(return_period)}"
+        )
+    if not isinstance(nb_steps, int):
+        raise TypeError(f"nb_steps must be of type int. Got: {type(nb_steps)}")
 
     dt_yrs = sea_state_duration / (3600 * 24 * 365)
     exceedance_probability = 1 / (return_period / dt_yrs)
-    iso_probability_radius = stats.norm.ppf((1 - exceedance_probability),
-                                            loc=0, scale=1)
+    iso_probability_radius = stats.norm.ppf(
+        (1 - exceedance_probability), loc=0, scale=1
+    )
     discretized_radians = np.linspace(0, 2 * np.pi, nb_steps)
 
-    x_component_iso_prob = iso_probability_radius * \
-        np.cos(discretized_radians)
-    y_component_iso_prob = iso_probability_radius * \
-        np.sin(discretized_radians)
+    x_component_iso_prob = iso_probability_radius * np.cos(discretized_radians)
+    y_component_iso_prob = iso_probability_radius * np.sin(discretized_radians)
 
     x_quantile = stats.norm.cdf(x_component_iso_prob, loc=0, scale=1)
     y_quantile = stats.norm.cdf(y_component_iso_prob, loc=0, scale=1)
 
-    results = {'exceedance_probability': exceedance_probability,
-               'x_component_iso_prob': x_component_iso_prob,
-               'y_component_iso_prob': y_component_iso_prob,
-               'x_quantile': x_quantile,
-               'y_quantile': y_quantile}
+    results = {
+        "exceedance_probability": exceedance_probability,
+        "x_component_iso_prob": x_component_iso_prob,
+        "y_component_iso_prob": y_component_iso_prob,
+        "x_quantile": x_quantile,
+        "y_quantile": y_quantile,
+    }
     return results
 
 
-def _copula_parameters(x1, x2, min_bin_count, initial_bin_max_val,
-                       bin_val_size):
+def _copula_parameters(x1, x2, min_bin_count, initial_bin_max_val, bin_val_size):
     """
     Returns an estimate of the Weibull and Lognormal distribution for
     x1 and x2 respectively. Additionally returns the estimates of the
     coefficients from the mean and standard deviation of the Log of x2
     given x1.
 
     Parameters
@@ -598,22 +679,30 @@
     para_dist_2: array
         Lognormal distribution parameters for component 2
     mean_cond: array
         Estimate coefficients of mean of Ln(x2|x1)
     std_cond: array
         Estimate coefficients of the standard deviation of Ln(x2|x1)
     """
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(min_bin_count, int), ('min_bin_count must be of'
-                                            + 'type int')
-    assert isinstance(bin_val_size, (int, float)), (
-        'bin_val_size must be of type int or float')
-    assert isinstance(initial_bin_max_val, (int, float)), (
-        'initial_bin_max_val must be of type int or float')
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(min_bin_count, int):
+        raise TypeError(
+            f"min_bin_count must be of type int. Got: {type(min_bin_count)}"
+        )
+    if not isinstance(bin_val_size, (int, float)):
+        raise TypeError(
+            f"bin_val_size must be of type int or float. Got: {type(bin_val_size)}"
+        )
+    if not isinstance(initial_bin_max_val, (int, float)):
+        raise TypeError(
+            f"initial_bin_max_val must be of type int or float. Got: {type(initial_bin_max_val)}"
+        )
 
     # Binning
     x1_sorted_index = x1.argsort()
     x1_sorted = x1[x1_sorted_index]
     x2_sorted = x2[x1_sorted_index]
 
     # Because x1 is sorted we can find the max index as follows:
@@ -630,18 +719,18 @@
 
     # Add bins until the total number of vals in between bins is
     # < the min bin size
     i = 0
     bin_size_i = np.inf
     while bin_size_i >= min_bin_count:
         i += 1
-        bin_i_max_val = initial_bin_max_val + bin_val_size*(i)
+        bin_i_max_val = initial_bin_max_val + bin_val_size * (i)
         N_vals_lt_limit = sum(x1_sorted <= bin_i_max_val)
         ind = np.append(ind, N_vals_lt_limit)
-        bin_size_i = ind[i]-ind[i-1]
+        bin_size_i = ind[i] - ind[i - 1]
 
     # Weibull distribution parameters for component 1 using MLE
     para_dist_1 = stats.exponweib.fit(x1_sorted, floc=0, fa=1)
     # Lognormal distribution parameters for component 2 using MLE
     para_dist_2 = stats.norm.fit(np.log(x2_sorted))
 
     # Parameters for conditional distribution of T|Hs for each bin
@@ -652,61 +741,59 @@
     # Bin zero special case (lognormal dist over only 1 bin)
     # parameters for zero bin
     ind0 = range(0, int(ind[0]))
     x2_log0 = np.log(x2_sorted[ind0])
     x2_lognormal_dist0 = stats.norm.fit(x2_log0)
     para_dist_cond.append(x2_lognormal_dist0)
     # mean of x1 (component 1 for zero bin)
-    x1_bin0 = x1_sorted[range(0, int(ind[0])-1)]
+    x1_bin0 = x1_sorted[range(0, int(ind[0]) - 1)]
     hss.append(np.mean(x1_bin0))
 
     # Special case 2-bin lognormal Dist
     # parameters for 1 bin
     ind1 = range(0, int(ind[1]))
     x2_log1 = np.log(x2_sorted[ind1])
     x2_lognormal_dist1 = stats.norm.fit(x2_log1)
     para_dist_cond.append(x2_lognormal_dist1)
 
     # mean of Hs (component 1 for bin 1)
-    hss.append(np.mean(x1_sorted[range(0, int(ind[1])-1)]))
+    hss.append(np.mean(x1_sorted[range(0, int(ind[1]) - 1)]))
 
     # lognormal Dist (lognormal dist over only 2 bins)
     for i in range(2, num):
-        ind_i = range(int(ind[i-2]), int(ind[i]))
+        ind_i = range(int(ind[i - 2]), int(ind[i]))
         x2_log_i = np.log(x2_sorted[ind_i])
         x2_lognormal_dist_i = stats.norm.fit(x2_log_i)
         para_dist_cond.append(x2_lognormal_dist_i)
 
         hss.append(np.mean(x1_sorted[ind_i]))
 
     # Estimate coefficient using least square solution (mean: 3rd order,
     # sigma: 2nd order)
-    ind_f = range(int(ind[num-2]), int(len(x1)))
+    ind_f = range(int(ind[num - 2]), int(len(x1)))
     x2_log_f = np.log(x2_sorted[ind_f])
     x2_lognormal_dist_f = stats.norm.fit(x2_log_f)
     para_dist_cond.append(x2_lognormal_dist_f)  # parameters for last bin
 
     # mean of Hs (component 1 for last bin)
     hss.append(np.mean(x1_sorted[ind_f]))
 
     para_dist_cond = np.array(para_dist_cond)
     hss = np.array(hss)
 
     # cubic in Hs: a + bx + cx**2 + dx**3
-    phi_mean = np.column_stack((np.ones(num+1), hss, hss**2, hss**3))
+    phi_mean = np.column_stack((np.ones(num + 1), hss, hss**2, hss**3))
     # quadratic in Hs  a + bx + cx**2
-    phi_std = np.column_stack((np.ones(num+1), hss, hss**2))
+    phi_std = np.column_stack((np.ones(num + 1), hss, hss**2))
 
     # Estimate coefficients of mean of Ln(T|Hs)(vector 4x1) (cubic in Hs)
-    mean_cond = np.linalg.lstsq(phi_mean, para_dist_cond[:, 0],
-                                rcond=None)[0]
+    mean_cond = np.linalg.lstsq(phi_mean, para_dist_cond[:, 0], rcond=None)[0]
     # Estimate coefficients of standard deviation of Ln(T|Hs)
     #    (vector 3x1) (quadratic in Hs)
-    std_cond = np.linalg.lstsq(phi_std, para_dist_cond[:, 1],
-                               rcond=None)[0]
+    std_cond = np.linalg.lstsq(phi_std, para_dist_cond[:, 1], rcond=None)[0]
 
     return para_dist_1, para_dist_2, mean_cond, std_cond
 
 
 def _gaussian_copula(x1, x2, fit, component_1, kwargs):
     """
     Extreme Sea State Gaussian Copula Contour function.
@@ -749,44 +836,50 @@
         x1 = np.array(x1)
     except:
         pass
     try:
         x2 = np.array(x2)
     except:
         pass
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(component_1, np.ndarray), (
-        'x2 must be of type np.ndarray')
-
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(component_1, np.ndarray):
+        raise TypeError(
+            f"component_1 must be of type np.ndarray. Got: {type(component_1)}"
+        )
     return_fit = kwargs.get("return_fit", False)
-    assert isinstance(return_fit, bool), (
-        'If specified return_fit must be a bool')
+    if not isinstance(return_fit, bool):
+        raise TypeError(
+            f"If specified, return_fit must be of type bool. Got: {type(return_fit)}"
+        )
 
-    x_component_iso_prob = fit['x_component_iso_prob']
-    y_component_iso_prob = fit['y_component_iso_prob']
+    x_component_iso_prob = fit["x_component_iso_prob"]
+    y_component_iso_prob = fit["y_component_iso_prob"]
 
     # Calculate Kendall's tau
     tau = stats.kendalltau(x2, x1)[0]
-    rho_gau = np.sin(tau*np.pi/2.)
+    rho_gau = np.sin(tau * np.pi / 2.0)
 
-    z2_Gauss = stats.norm.cdf(y_component_iso_prob*np.sqrt(1.-rho_gau**2.)
-                              + rho_gau*x_component_iso_prob)
+    z2_Gauss = stats.norm.cdf(
+        y_component_iso_prob * np.sqrt(1.0 - rho_gau**2.0)
+        + rho_gau * x_component_iso_prob
+    )
 
-    para_dist_2 = fit['para_dist_2']
+    para_dist_2 = fit["para_dist_2"]
     s = para_dist_2[1]
     loc = 0
     scale = np.exp(para_dist_2[0])
 
     # lognormal inverse
-    component_2_Gaussian = stats.lognorm.ppf(z2_Gauss, s=s, loc=loc,
-                                             scale=scale)
-    fit['tau'] = tau
-    fit['rho'] = rho_gau
-    fit['z2'] = z2_Gauss
+    component_2_Gaussian = stats.lognorm.ppf(z2_Gauss, s=s, loc=loc, scale=scale)
+    fit["tau"] = tau
+    fit["rho"] = rho_gau
+    fit["z2"] = z2_Gauss
 
     if return_fit:
         return component_1, component_2_Gaussian, fit
     return component_1, component_2_Gaussian
 
 
 def _gumbel_density(u, alpha):
@@ -803,26 +896,28 @@
 
     Returns
     -------
     y: np.array
         Copula density function.
     """
 
-    #Ignore divide by 0 warnings and resulting NaN warnings
-    np.seterr(all='ignore')
+    # Ignore divide by 0 warnings and resulting NaN warnings
+    np.seterr(all="ignore")
     v = -np.log(u)
     v = np.sort(v, axis=0)
     vmin = v[0, :]
     vmax = v[1, :]
     nlogC = vmax * (1 + (vmin / vmax) ** alpha) ** (1 / alpha)
-    y = (alpha - 1 + nlogC)*np.exp(
-        -nlogC+np.sum((alpha-1) * np.log(v)+v, axis=0) +
-        (1-2*alpha)*np.log(nlogC))
-    np.seterr(all='warn')
-    return(y)
+    y = (alpha - 1 + nlogC) * np.exp(
+        -nlogC
+        + np.sum((alpha - 1) * np.log(v) + v, axis=0)
+        + (1 - 2 * alpha) * np.log(nlogC)
+    )
+    np.seterr(all="warn")
+    return y
 
 
 def _gumbel_copula(x1, x2, fit, component_1, nb_steps, kwargs):
     """
     This function calculates environmental contours of extreme sea
     states using a Gumbel copula and the inverse first-order reliability
     method.
@@ -865,64 +960,70 @@
         x1 = np.array(x1)
     except:
         pass
     try:
         x2 = np.array(x2)
     except:
         pass
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(component_1, np.ndarray), 'x2 must be of type np.ndarray'
-
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(component_1, np.ndarray):
+        raise TypeError(
+            f"component_1 must be of type np.ndarray. Got: {type(component_1)}"
+        )
     return_fit = kwargs.get("return_fit", False)
-    assert isinstance(
-        return_fit, bool), 'If specified return_fit must be a bool'
-
-    x_quantile = fit['x_quantile']
-    y_quantile = fit['y_quantile']
-    para_dist_2 = fit['para_dist_2']
+    if not isinstance(return_fit, bool):
+        raise TypeError(
+            f"If specified, return_fit must be of type bool. Got: {type(return_fit)}"
+        )
+
+    x_quantile = fit["x_quantile"]
+    y_quantile = fit["y_quantile"]
+    para_dist_2 = fit["para_dist_2"]
 
     # Calculate Kendall's tau
     tau = stats.kendalltau(x2, x1)[0]
-    theta_gum = 1./(1.-tau)
+    theta_gum = 1.0 / (1.0 - tau)
 
     min_limit_2 = 0
-    max_limit_2 = np.ceil(np.amax(x2)*2)
+    max_limit_2 = np.ceil(np.amax(x2) * 2)
     Ndata = 1000
 
     x = np.linspace(min_limit_2, max_limit_2, Ndata)
 
     s = para_dist_2[1]
     scale = np.exp(para_dist_2[0])
     z2 = stats.lognorm.cdf(x, s=s, loc=0, scale=scale)
 
-    fit['tau'] = tau
-    fit['theta'] = theta_gum
-    fit['z2'] = z2
+    fit["tau"] = tau
+    fit["theta"] = theta_gum
+    fit["z2"] = z2
 
     component_2_Gumbel = np.zeros(nb_steps)
     for k in range(nb_steps):
-        z1 = np.array([x_quantile[k]]*Ndata)
+        z1 = np.array([x_quantile[k]] * Ndata)
         Z = np.array((z1, z2))
         Y = _gumbel_density(Z, theta_gum)
         Y = np.nan_to_num(Y)
         # pdf 2|1, f(comp_2|comp_1)=c(z1,z2)*f(comp_2)
-        p_x_x1 = Y*(stats.lognorm.pdf(x, s=s, loc=0, scale=scale))
+        p_x_x1 = Y * (stats.lognorm.pdf(x, s=s, loc=0, scale=scale))
         # Estimate CDF from PDF
         dum = np.cumsum(p_x_x1)
-        cdf = dum/(dum[Ndata-1])
+        cdf = dum / (dum[Ndata - 1])
         # Result of conditional CDF derived based on Gumbel copula
         table = np.array((x, cdf))
         table = table.T
         for j in range(Ndata):
             if y_quantile[k] <= table[0, 1]:
                 component_2_Gumbel[k] = min(table[:, 0])
                 break
             elif y_quantile[k] <= table[j, 1]:
-                component_2_Gumbel[k] = (table[j, 0]+table[j-1, 0])/2
+                component_2_Gumbel[k] = (table[j, 0] + table[j - 1, 0]) / 2
                 break
             else:
                 component_2_Gumbel[k] = table[:, 0].max()
     if return_fit:
         return component_1, component_2_Gumbel, fit
     return component_1, component_2_Gumbel
 
@@ -963,40 +1064,49 @@
     component_2_Clayton: array
         Calculated x2 values along the contour boundary following
         return to original input orientation.
     fit: Dictionary (optional)
         If return_fit=True. Dictionary with iso-probabilities passed
         with additional fit metrics from the copula method.
     """
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(component_1, np.ndarray), 'x2 must be of type np.ndarray'
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(component_1, np.ndarray):
+        raise TypeError(
+            f"component_1 must be of type np.ndarray. Got: {type(component_1)}"
+        )
     return_fit = kwargs.get("return_fit", False)
-    assert isinstance(
-        return_fit, bool), 'If specified return_fit must be a bool'
-
-    x_quantile = fit['x_quantile']
-    y_quantile = fit['y_quantile']
-    para_dist_2 = fit['para_dist_2']
+    if not isinstance(return_fit, bool):
+        raise TypeError(
+            f"If specified, return_fit must be of type bool. Got: {type(return_fit)}"
+        )
+
+    x_quantile = fit["x_quantile"]
+    y_quantile = fit["y_quantile"]
+    para_dist_2 = fit["para_dist_2"]
 
     # Calculate Kendall's tau
     tau = stats.kendalltau(x2, x1)[0]
-    theta_clay = (2.*tau)/(1.-tau)
+    theta_clay = (2.0 * tau) / (1.0 - tau)
 
     s = para_dist_2[1]
     scale = np.exp(para_dist_2[0])
-    z2_Clay = ((1.-x_quantile**(-theta_clay)+x_quantile**(-theta_clay) /
-               y_quantile)**(theta_clay/(1.+theta_clay)))**(-1./theta_clay)
+    z2_Clay = (
+        (1.0 - x_quantile ** (-theta_clay) + x_quantile ** (-theta_clay) / y_quantile)
+        ** (theta_clay / (1.0 + theta_clay))
+    ) ** (-1.0 / theta_clay)
 
     # lognormal inverse
     component_2_Clayton = stats.lognorm.ppf(z2_Clay, s=s, loc=0, scale=scale)
 
-    fit['theta_clay'] = theta_clay
-    fit['tau'] = tau
-    fit['z2_Clay'] = z2_Clay
+    fit["theta_clay"] = theta_clay
+    fit["tau"] = tau
+    fit["z2_Clay"] = z2_Clay
 
     if return_fit:
         return component_1, component_2_Clayton, fit
     return component_1, component_2_Clayton
 
 
 def _rosenblatt_copula(x1, x2, fit, component_1, kwargs):
@@ -1043,44 +1153,55 @@
         x1 = np.array(x1)
     except:
         pass
     try:
         x2 = np.array(x2)
     except:
         pass
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(component_1, np.ndarray), 'x2 must be of type np.ndarray'
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(component_1, np.ndarray):
+        raise TypeError(
+            f"component_1 must be of type np.ndarray. Got: {type(component_1)}"
+        )
     return_fit = kwargs.get("return_fit", False)
-    assert isinstance(
-        return_fit, bool), 'If specified return_fit must be a bool'
-
-    y_quantile = fit['y_quantile']
-    mean_cond = fit['mean_cond']
-    std_cond = fit['std_cond']
+    if not isinstance(return_fit, bool):
+        raise TypeError(
+            f"If specified, return_fit must be of type bool. Got: {type(return_fit)}"
+        )
+
+    y_quantile = fit["y_quantile"]
+    mean_cond = fit["mean_cond"]
+    std_cond = fit["std_cond"]
 
     # mean of Ln(T) as a function of x1
-    lamda_cond = mean_cond[0]+mean_cond[1]*component_1 + \
-        mean_cond[2]*component_1**2+mean_cond[3]*component_1**3
+    lamda_cond = (
+        mean_cond[0]
+        + mean_cond[1] * component_1
+        + mean_cond[2] * component_1**2
+        + mean_cond[3] * component_1**3
+    )
     # Standard deviation of Ln(x2) as a function of x1
-    sigma_cond = std_cond[0]+std_cond[1]*component_1+std_cond[2]*component_1**2
+    sigma_cond = std_cond[0] + std_cond[1] * component_1 + std_cond[2] * component_1**2
     # lognormal inverse
     component_2_Rosenblatt = stats.lognorm.ppf(
-        y_quantile, s=sigma_cond, loc=0, scale=np.exp(lamda_cond))
+        y_quantile, s=sigma_cond, loc=0, scale=np.exp(lamda_cond)
+    )
 
-    fit['lamda_cond'] = lamda_cond
-    fit['sigma_cond'] = sigma_cond
+    fit["lamda_cond"] = lamda_cond
+    fit["sigma_cond"] = sigma_cond
 
     if return_fit:
         return component_1, component_2_Rosenblatt, fit
     return component_1, component_2_Rosenblatt
 
 
-def _nonparametric_copula_parameters(x1, x2, max_x1=None, max_x2=None,
-                                     nb_steps=1000):
+def _nonparametric_copula_parameters(x1, x2, max_x1=None, max_x2=None, nb_steps=1000):
     """
     Calculates nonparametric copula parameters
 
     Parameters
     ----------
     x1: array
         Component 1 data
@@ -1098,23 +1219,28 @@
     nonpara_dist_1:
         x1 points in KDE space and Nonparametric CDF for x1
     nonpara_dist_2:
         x2 points in KDE space and Nonparametric CDF for x2
     nonpara_pdf_2:
         x2 points in KDE space and Nonparametric PDF for x2
     """
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
     if not max_x1:
-        max_x1 = x1.max()*2
+        max_x1 = x1.max() * 2
     if not max_x2:
-        max_x2 = x2.max()*2
-    assert isinstance(max_x1, float), 'max_x1 must be of type float'
-    assert isinstance(max_x2, float), 'max_x2 must be of type float'
-    assert isinstance(nb_steps, int), 'nb_steps must be of type int'
+        max_x2 = x2.max() * 2
+    if not isinstance(max_x1, float):
+        raise TypeError(f"max_x1 must be of type float. Got: {type(max_x1)}")
+    if not isinstance(max_x2, float):
+        raise TypeError(f"max_x2 must be of type float. Got: {type(max_x2)}")
+    if not isinstance(nb_steps, int):
+        raise TypeError(f"nb_steps must be of type int. Got: {type(nb_steps)}")
 
     # Binning
     x1_sorted_index = x1.argsort()
     x1_sorted = x1[x1_sorted_index]
     x2_sorted = x2[x1_sorted_index]
 
     # Calcualte KDE bounds (potential input)
@@ -1124,34 +1250,34 @@
     # Discretize for KDE
     pts_x1 = np.linspace(min_limit_1, max_x1, nb_steps)
     pts_x2 = np.linspace(min_limit_2, max_x2, nb_steps)
 
     # Calculate optimal bandwidth for T and Hs
     sig = stats.median_abs_deviation(x2_sorted)
     num = float(len(x2_sorted))
-    bwT = sig*(4.0/(3.0*num))**(1.0/5.0)
+    bwT = sig * (4.0 / (3.0 * num)) ** (1.0 / 5.0)
 
     sig = stats.median_abs_deviation(x1_sorted)
     num = float(len(x1_sorted))
-    bwHs = sig*(4.0/(3.0*num))**(1.0/5.0)
+    bwHs = sig * (4.0 / (3.0 * num)) ** (1.0 / 5.0)
 
     # Nonparametric PDF for x2
     temp = KDEUnivariate(x2_sorted)
     temp.fit(bw=bwT)
     f_x2 = temp.evaluate(pts_x2)
 
     # Nonparametric CDF for x1
     temp = KDEUnivariate(x1_sorted)
     temp.fit(bw=bwHs)
     tempPDF = temp.evaluate(pts_x1)
-    F_x1 = tempPDF/sum(tempPDF)
+    F_x1 = tempPDF / sum(tempPDF)
     F_x1 = np.cumsum(F_x1)
 
     # Nonparametric CDF for x2
-    F_x2 = f_x2/sum(f_x2)
+    F_x2 = f_x2 / sum(f_x2)
     F_x2 = np.cumsum(F_x2)
 
     nonpara_dist_1 = np.transpose(np.array([pts_x1, F_x1]))
     nonpara_dist_2 = np.transpose(np.array([pts_x2, F_x2]))
     nonpara_pdf_2 = np.transpose(np.array([pts_x2, f_x2]))
 
     return nonpara_dist_1, nonpara_dist_2, nonpara_pdf_2
@@ -1172,24 +1298,25 @@
         copula component calculation.
 
     Returns
     -------
     component: array
         nonparametic component values
     """
-    assert isinstance(nb_steps, int), 'nb_steps must be of type int'
+    if not isinstance(nb_steps, int):
+        raise TypeError(f"nb_steps must be of type int. Got: {type(nb_steps)}")
 
     component = np.zeros(nb_steps)
     for k in range(0, nb_steps):
         for j in range(0, np.size(nonpara_dist, 0)):
             if z[k] <= nonpara_dist[0, 1]:
                 component[k] = min(nonpara_dist[:, 0])
                 break
             elif z[k] <= nonpara_dist[j, 1]:
-                component[k] = (nonpara_dist[j, 0] + nonpara_dist[j-1, 0])/2
+                component[k] = (nonpara_dist[j, 0] + nonpara_dist[j - 1, 0]) / 2
                 break
             else:
                 component[k] = max(nonpara_dist[:, 0])
     return component
 
 
 def _nonparametric_gaussian_copula(x1, x2, fit, nb_steps, kwargs):
@@ -1219,56 +1346,59 @@
         Component 1 nonparametric copula
     component_2_np_gaussian: array
         Component 2 nonparametric Gaussian copula
     fit: Dictionary (optional)
         If return_fit=True. Dictionary with iso-probabilities passed
         with additional fit metrics from the copula method.
     """
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(nb_steps, int), 'nb_steps must be of type int'
-
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(nb_steps, int):
+        raise TypeError(f"nb_steps must be of type int. Got: {type(nb_steps)}")
     return_fit = kwargs.get("return_fit", False)
-    assert isinstance(
-        return_fit, bool), 'If specified return_fit must be a bool'
-
-    x_component_iso_prob = fit['x_component_iso_prob']
-    y_component_iso_prob = fit['y_component_iso_prob']
-    nonpara_dist_1 = fit['nonpara_dist_1']
-    nonpara_dist_2 = fit['nonpara_dist_2']
+    if not isinstance(return_fit, bool):
+        raise TypeError(
+            f"If specified, return_fit must be of type bool. Got: {type(return_fit)}"
+        )
+
+    x_component_iso_prob = fit["x_component_iso_prob"]
+    y_component_iso_prob = fit["y_component_iso_prob"]
+    nonpara_dist_1 = fit["nonpara_dist_1"]
+    nonpara_dist_2 = fit["nonpara_dist_2"]
 
     # Calculate Kendall's tau
     tau = stats.kendalltau(x2, x1)[0]
-    rho_gau = np.sin(tau*np.pi/2.)
+    rho_gau = np.sin(tau * np.pi / 2.0)
 
     # Component 1
     z1 = stats.norm.cdf(x_component_iso_prob)
-    z2 = stats.norm.cdf(y_component_iso_prob*np.sqrt(1. -
-                        rho_gau**2.)+rho_gau*x_component_iso_prob)
-
-    comps = {1: {'z': z1,
-                 'nonpara_dist': nonpara_dist_1
-                 },
-             2: {'z': z2,
-                 'nonpara_dist': nonpara_dist_2
-                 }
-             }
+    z2 = stats.norm.cdf(
+        y_component_iso_prob * np.sqrt(1.0 - rho_gau**2.0)
+        + rho_gau * x_component_iso_prob
+    )
+
+    comps = {
+        1: {"z": z1, "nonpara_dist": nonpara_dist_1},
+        2: {"z": z2, "nonpara_dist": nonpara_dist_2},
+    }
 
     for c in comps:
-        z = comps[c]['z']
-        nonpara_dist = comps[c]['nonpara_dist']
-        comps[c]['comp'] = _nonparametric_component(z, nonpara_dist, nb_steps)
-
-    component_1_np = comps[1]['comp']
-    component_2_np_gaussian = comps[2]['comp']
-
-    fit['tau'] = tau
-    fit['rho'] = rho_gau
-    fit['z1'] = z1
-    fit['z2'] = z2
+        z = comps[c]["z"]
+        nonpara_dist = comps[c]["nonpara_dist"]
+        comps[c]["comp"] = _nonparametric_component(z, nonpara_dist, nb_steps)
+
+    component_1_np = comps[1]["comp"]
+    component_2_np_gaussian = comps[2]["comp"]
+
+    fit["tau"] = tau
+    fit["rho"] = rho_gau
+    fit["z1"] = z1
+    fit["z2"] = z2
 
     if return_fit:
         return component_1_np, component_2_np_gaussian, fit
     return component_1_np, component_2_np_gaussian
 
 
 def _nonparametric_clayton_copula(x1, x2, fit, nb_steps, kwargs):
@@ -1298,59 +1428,61 @@
         Component 1 nonparametric copula
     component_2_np_gaussian: array
         Component 2 nonparametric Clayton copula
     fit: Dictionary (optional)
         If return_fit=True. Dictionary with iso-probabilities passed
         with additional fit metrics from the copula method.
     """
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(nb_steps, int), 'nb_steps must be of type int'
-
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(nb_steps, int):
+        raise TypeError(f"nb_steps must be of type int. Got: {type(nb_steps)}")
     return_fit = kwargs.get("return_fit", False)
-    assert isinstance(return_fit, bool), ('If specified return_fit '
-                                          + 'must be a bool')
-
-    x_component_iso_prob = fit['x_component_iso_prob']
-    x_quantile = fit['x_quantile']
-    y_quantile = fit['y_quantile']
-    nonpara_dist_1 = fit['nonpara_dist_1']
-    nonpara_dist_2 = fit['nonpara_dist_2']
-    nonpara_pdf_2 = fit['nonpara_pdf_2']
+    if not isinstance(return_fit, bool):
+        raise TypeError(
+            f"If specified, return_fit must be of type bool. Got: {type(return_fit)}"
+        )
+
+    x_component_iso_prob = fit["x_component_iso_prob"]
+    x_quantile = fit["x_quantile"]
+    y_quantile = fit["y_quantile"]
+    nonpara_dist_1 = fit["nonpara_dist_1"]
+    nonpara_dist_2 = fit["nonpara_dist_2"]
+    nonpara_pdf_2 = fit["nonpara_pdf_2"]
 
     # Calculate Kendall's tau
     tau = stats.kendalltau(x2, x1)[0]
-    theta_clay = (2.*tau)/(1.-tau)
+    theta_clay = (2.0 * tau) / (1.0 - tau)
 
     # Component 1 (Hs)
     z1 = stats.norm.cdf(x_component_iso_prob)
-    z2_clay = ((1-x_quantile**(-theta_clay)
-                + x_quantile**(-theta_clay)
-                / y_quantile)**(theta_clay/(1.+theta_clay)))**(-1./theta_clay)
-
-    comps = {1: {'z': z1,
-                 'nonpara_dist': nonpara_dist_1
-                 },
-             2: {'z': z2_clay,
-                 'nonpara_dist': nonpara_dist_2
-                 }
-             }
+    z2_clay = (
+        (1 - x_quantile ** (-theta_clay) + x_quantile ** (-theta_clay) / y_quantile)
+        ** (theta_clay / (1.0 + theta_clay))
+    ) ** (-1.0 / theta_clay)
+
+    comps = {
+        1: {"z": z1, "nonpara_dist": nonpara_dist_1},
+        2: {"z": z2_clay, "nonpara_dist": nonpara_dist_2},
+    }
 
     for c in comps:
-        z = comps[c]['z']
-        nonpara_dist = comps[c]['nonpara_dist']
-        comps[c]['comp'] = _nonparametric_component(z, nonpara_dist, nb_steps)
-
-    component_1_np = comps[1]['comp']
-    component_2_np_clayton = comps[2]['comp']
-
-    fit['tau'] = tau
-    fit['theta'] = theta_clay
-    fit['z1'] = z1
-    fit['z2'] = z2_clay
+        z = comps[c]["z"]
+        nonpara_dist = comps[c]["nonpara_dist"]
+        comps[c]["comp"] = _nonparametric_component(z, nonpara_dist, nb_steps)
+
+    component_1_np = comps[1]["comp"]
+    component_2_np_clayton = comps[2]["comp"]
+
+    fit["tau"] = tau
+    fit["theta"] = theta_clay
+    fit["z1"] = z1
+    fit["z2"] = z2_clay
 
     if return_fit:
         return component_1_np, component_2_np_clayton, fit
     return component_1_np, component_2_np_clayton
 
 
 def _nonparametric_gumbel_copula(x1, x2, fit, nb_steps, kwargs):
@@ -1380,71 +1512,75 @@
         Component 1 nonparametric copula
     component_2_np_gumbel: array
         Component 2 nonparametric Gumbel copula
     fit: Dictionary (optional)
         If return_fit=True. Dictionary with iso-probabilities passed
         with additional fit metrics from the copula method.
     """
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(nb_steps, int), 'nb_steps must be of type int'
-
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(nb_steps, int):
+        raise TypeError(f"nb_steps must be of type int. Got: {type(nb_steps)}")
     return_fit = kwargs.get("return_fit", False)
-    assert isinstance(return_fit, bool), ('If specified return_fit '
-                                          + 'must be a bool')
+    if not isinstance(return_fit, bool):
+        raise TypeError(
+            f"If specified, return_fit must be a bool. Got: {type(return_fit)}"
+        )
 
     Ndata = 1000
 
-    x_quantile = fit['x_quantile']
-    y_quantile = fit['y_quantile']
-    nonpara_dist_1 = fit['nonpara_dist_1']
-    nonpara_dist_2 = fit['nonpara_dist_2']
-    nonpara_pdf_2 = fit['nonpara_pdf_2']
+    x_quantile = fit["x_quantile"]
+    y_quantile = fit["y_quantile"]
+    nonpara_dist_1 = fit["nonpara_dist_1"]
+    nonpara_dist_2 = fit["nonpara_dist_2"]
+    nonpara_pdf_2 = fit["nonpara_pdf_2"]
 
     # Calculate Kendall's tau
     tau = stats.kendalltau(x2, x1)[0]
-    theta_gum = 1./(1.-tau)
+    theta_gum = 1.0 / (1.0 - tau)
 
     # Component 1 (Hs)
     z1 = x_quantile
     component_1_np = _nonparametric_component(z1, nonpara_dist_1, nb_steps)
 
     pts_x2 = nonpara_pdf_2[:, 0]
     f_x2 = nonpara_pdf_2[:, 1]
     F_x2 = nonpara_dist_2[:, 1]
 
     component_2_np_gumbel = np.zeros(nb_steps)
     for k in range(nb_steps):
-        z1 = np.array([x_quantile[k]]*Ndata)
+        z1 = np.array([x_quantile[k]] * Ndata)
         Z = np.array((z1.T, F_x2))
         Y = _gumbel_density(Z, theta_gum)
         Y = np.nan_to_num(Y)
         # pdf 2|1
-        p_x2_x1 = Y*f_x2
+        p_x2_x1 = Y * f_x2
         # Estimate CDF from PDF
         dum = np.cumsum(p_x2_x1)
-        cdf = dum/(dum[Ndata-1])
+        cdf = dum / (dum[Ndata - 1])
         table = np.array((pts_x2, cdf))
         table = table.T
         for j in range(Ndata):
             if y_quantile[k] <= table[0, 1]:
                 component_2_np_gumbel[k] = min(table[:, 0])
                 break
             elif y_quantile[k] <= table[j, 1]:
-                component_2_np_gumbel[k] = (table[j, 0]+table[j-1, 0])/2
+                component_2_np_gumbel[k] = (table[j, 0] + table[j - 1, 0]) / 2
                 break
             else:
                 component_2_np_gumbel[k] = max(table[:, 0])
 
-    fit['tau'] = tau
-    fit['theta'] = theta_gum
-    fit['z1'] = z1
-    fit['pts_x2'] = pts_x2
-    fit['f_x2'] = f_x2
-    fit['F_x2'] = F_x2
+    fit["tau"] = tau
+    fit["theta"] = theta_gum
+    fit["z1"] = z1
+    fit["pts_x2"] = pts_x2
+    fit["f_x2"] = f_x2
+    fit["F_x2"] = F_x2
 
     if return_fit:
         return component_1_np, component_2_np_gumbel, fit
     return component_1_np, component_2_np_gumbel
 
 
 def _bivariate_KDE(x1, x2, bw, fit, nb_steps, Ndata_bivariate_KDE, kwargs):
@@ -1462,15 +1598,15 @@
     x2: array
         Component 2 data
     bw: np.array
         Array containing KDE bandwidth for x1 and x2
     fit: Dictionay
         Dictionary of the iso-probability results
     nb_steps: int
-        number of points used to discritize KDE space
+        number of points used to discretize KDE space
     max_x1: float
         Defines the max value of x1 to discretize the KDE space
     max_x2: float
         Defines the max value of x2 to discretize the KDE space
     kwargs : optional
         return_fit: boolean
               Will return fitting parameters used. Default False.
@@ -1483,35 +1619,44 @@
     x2_bivariate_KDE: array
         Calculated x2 values along the contour boundary following
         return to original input orientation.
     fit: Dictionary (optional)
         If return_fit=True. Dictionary with iso-probabilities passed
         with additional fit metrics from the copula method.
     """
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(nb_steps, int), 'nb_steps must be of type int'
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    if not isinstance(nb_steps, int):
+        raise TypeError(f"nb_steps must be of type int. Got: {type(nb_steps)}")
 
     max_x1 = kwargs.get("max_x1", None)
     max_x2 = kwargs.get("max_x2", None)
     log_transform = kwargs.get("log_transform", False)
     return_fit = kwargs.get("return_fit", False)
 
     if isinstance(max_x1, type(None)):
-        max_x1 = x1.max()*2
+        max_x1 = x1.max() * 2
     if isinstance(max_x2, type(None)):
-        max_x2 = x2.max()*2
-    assert isinstance(max_x1, float), 'max_x1 must be of type float'
-    assert isinstance(max_x2, float), 'max_x2 must be of type float'
-    assert isinstance(log_transform, bool), ('If specified log_transform'
-                                             + 'must be a bool')
-    assert isinstance(return_fit, bool), ('If specified return_fit must '
-                                          + 'be a bool')
+        max_x2 = x2.max() * 2
+    if not isinstance(max_x1, float):
+        raise TypeError(f"max_x1 must be of type float. Got: {type(max_x1)}")
+    if not isinstance(max_x2, float):
+        raise TypeError(f"max_x2 must be of type float. Got: {type(max_x2)}")
+    if not isinstance(log_transform, bool):
+        raise TypeError(
+            f"If specified, log_transform must be of type bool. Got: {type(log_transform)}"
+        )
+    if not isinstance(return_fit, bool):
+        raise TypeError(
+            f"If specified, return_fit must be of type bool. Got: {type(return_fit)}"
+        )
 
-    p_f = fit['exceedance_probability']
+    p_f = fit["exceedance_probability"]
 
     min_limit_1 = 0.01
     min_limit_2 = 0.01
     pts_x1 = np.linspace(min_limit_1, max_x1, Ndata_bivariate_KDE)
     pts_x2 = np.linspace(min_limit_2, max_x2, Ndata_bivariate_KDE)
     pt1, pt2 = np.meshgrid(pts_x2, pts_x1)
     mesh_pts_x2 = pt1.flatten()
@@ -1531,67 +1676,79 @@
 
     # Create contour
     f = np.zeros((1, m))
     weight = np.ones((1, n))
     for i in range(0, m):
         ftemp = np.ones((n, 1))
         for j in range(0, d):
-            z = (txi[j][i] - ty[j])/bw[j]
+            z = (txi[j][i] - ty[j]) / bw[j]
             fk = stats.norm.pdf(z)
             if log_transform:
-                fnew = fk*(1/np.transpose(xi[j][i]))
+                fnew = fk * (1 / np.transpose(xi[j][i]))
             else:
                 fnew = fk
             fnew = np.reshape(fnew, (n, 1))
             ftemp = np.multiply(ftemp, fnew)
         f[:, i] = np.dot(weight, ftemp)
 
     fhat = f.reshape(100, 100)
     vals = plt.contour(pt1, pt2, fhat, levels=[p_f])
     plt.clf()
     x1_bivariate_KDE = []
     x2_bivariate_KDE = []
 
-    for i, seg in enumerate(vals.allsegs[0]):
+    if mpl_version < (3, 8):  # For versions before 3.8
+        segments = vals.allsegs[0]
+    else:
+        segments = [path.vertices for path in vals.get_paths()]
+
+    for seg in segments:
         x1_bivariate_KDE.append(seg[:, 1])
         x2_bivariate_KDE.append(seg[:, 0])
 
     x1_bivariate_KDE = np.transpose(np.asarray(x1_bivariate_KDE)[0])
     x2_bivariate_KDE = np.transpose(np.asarray(x2_bivariate_KDE)[0])
 
-    fit['mesh_pts_x1'] = mesh_pts_x1
-    fit['mesh_pts_x2'] = mesh_pts_x2
-    fit['ty'] = ty
-    fit['xi'] = xi
-    fit['contour_vals'] = vals
+    fit["mesh_pts_x1"] = mesh_pts_x1
+    fit["mesh_pts_x2"] = mesh_pts_x2
+    fit["ty"] = ty
+    fit["xi"] = xi
+    fit["contour_vals"] = vals
 
     if return_fit:
         return x1_bivariate_KDE, x2_bivariate_KDE, fit
     return x1_bivariate_KDE, x2_bivariate_KDE
 
 
-### Sampling
-def samples_full_seastate(x1, x2, points_per_interval, return_periods,
-                          sea_state_duration, method="PCA", bin_size=250):
+# Sampling
+def samples_full_seastate(
+    x1,
+    x2,
+    points_per_interval,
+    return_periods,
+    sea_state_duration,
+    method="PCA",
+    bin_size=250,
+):
     """
     Sample a sea state between contours of specified return periods.
 
     This function is used for the full sea state approach for the
     extreme load. See Coe et al. 2018 for more details. It was
     originally part of WDRT.
 
     Coe, R. G., Michelen, C., Eckert-Gallup, A., &
     Sallaberry, C. (2018). Full long-term design response analysis of a
     wave energy converter. Renewable Energy, 116, 356-366.
 
     Parameters
     ----------
-    x1: np.array
+    x1: list, np.ndarray, pd.Series, xr.DataArray
         Component 1 data
-    x2: np.array
+    x2: list, np.ndarray, pd.Series, xr.DataArray
         Component 2 data
     points_per_interval : int
         Number of sample points to be calculated per contour interval.
     return_periods: np.array
         Vector of return periods that define the contour intervals in
         which samples will be taken. Values must be greater than zero
         and must be in increasing order.
@@ -1608,63 +1765,71 @@
         Vector of Hs values for each sample point.
     Te_Samples: np.array
         Vector of Te values for each sample point.
     weight_points: np.array
         Vector of probabilistic weights for each sampling point
         to be used in risk calculations.
     """
-    if method != 'PCA':
+    if method != "PCA":
         raise NotImplementedError(
-            "Full sea state sampling is currently only implemented using " +
-            "the 'PCA' method.")
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(points_per_interval,
-                      int), 'points_per_interval must be of int'
-    assert isinstance(return_periods, np.ndarray
-                      ), 'return_periods must be of type np.ndarray'
-    assert isinstance(sea_state_duration, (int, float)
-                      ), 'sea_state_duration must be of int or float'
-    assert isinstance(method, (str, list)
-                      ), 'method must be of type string or list'
-    assert isinstance(bin_size, int), 'bin_size must be of int'
+            "Full sea state sampling is currently only implemented using "
+            + "the 'PCA' method."
+        )
+    x1 = to_numeric_array(x1, "x1")
+    x2 = to_numeric_array(x2, "x2")
+    if not isinstance(points_per_interval, int):
+        raise TypeError(
+            f"points_per_interval must be of int. Got: {type(points_per_interval)}"
+        )
+    if not isinstance(return_periods, np.ndarray):
+        raise TypeError(
+            f"return_periods must be of type np.ndarray. Got: {type(return_periods)}"
+        )
+    if not isinstance(sea_state_duration, (int, float)):
+        raise TypeError(
+            f"sea_state_duration must be of int or float. Got: {type(sea_state_duration)}"
+        )
+    if not isinstance(method, (str, list)):
+        raise TypeError(f"method must be of type string or list. Got: {type(method)}")
+    if not isinstance(bin_size, int):
+        raise TypeError(f"bin_size must be of int. Got: {type(bin_size)}")
 
     pca_fit = _principal_component_analysis(x1, x2, bin_size)
 
     # Calculate line where Hs = 0 to avoid sampling Hs in negative space
     t_zeroline = np.linspace(2.5, 30, 1000)
     t_zeroline = np.transpose(t_zeroline)
     h_zeroline = np.zeros(len(t_zeroline))
 
     # Transform zero line into principal component space
-    coeff = pca_fit['principal_axes']
-    shift = pca_fit['shift']
-    comp_zeroline = np.dot(np.transpose(np.vstack([h_zeroline, t_zeroline])),
-                           coeff)
+    coeff = pca_fit["principal_axes"]
+    shift = pca_fit["shift"]
+    comp_zeroline = np.dot(np.transpose(np.vstack([h_zeroline, t_zeroline])), coeff)
     comp_zeroline[:, 1] = comp_zeroline[:, 1] + shift
 
-    comp1 = pca_fit['x1_fit']
+    comp1 = pca_fit["x1_fit"]
     c1_zeroline_prob = stats.invgauss.cdf(
-        comp_zeroline[:, 0], mu=comp1['mu'], loc=0, scale=comp1['scale'])
+        comp_zeroline[:, 0], mu=comp1["mu"], loc=0, scale=comp1["scale"]
+    )
 
-    mu_slope = pca_fit['mu_fit'].slope
-    mu_intercept = pca_fit['mu_fit'].intercept
+    mu_slope = pca_fit["mu_fit"].slope
+    mu_intercept = pca_fit["mu_fit"].intercept
     mu_zeroline = mu_slope * comp_zeroline[:, 0] + mu_intercept
 
-    sigma_polynomial_coeffcients = pca_fit['sigma_fit'].x
-    sigma_zeroline = np.polyval(
-        sigma_polynomial_coeffcients, comp_zeroline[:, 0])
-    c2_zeroline_prob = stats.norm.cdf(comp_zeroline[:, 1],
-                                      loc=mu_zeroline, scale=sigma_zeroline)
+    sigma_polynomial_coeffcients = pca_fit["sigma_fit"].x
+    sigma_zeroline = np.polyval(sigma_polynomial_coeffcients, comp_zeroline[:, 0])
+    c2_zeroline_prob = stats.norm.cdf(
+        comp_zeroline[:, 1], loc=mu_zeroline, scale=sigma_zeroline
+    )
 
     c1_normzeroline = stats.norm.ppf(c1_zeroline_prob, 0, 1)
     c2_normzeroline = stats.norm.ppf(c2_zeroline_prob, 0, 1)
 
     return_periods = np.asarray(return_periods)
-    contour_probs = 1 / (365*24*60*60/sea_state_duration * return_periods)
+    contour_probs = 1 / (365 * 24 * 60 * 60 / sea_state_duration * return_periods)
 
     # Reliability contour generation
     # Calculate reliability
     beta_lines = stats.norm.ppf((1 - contour_probs), 0, 1)
     # Add zero as lower bound to first contour
     beta_lines = np.hstack((0, beta_lines))
     # Discretize the circle
@@ -1682,130 +1847,128 @@
     mask = c1_normzeroline > minval
     c1_normzeroline = c1_normzeroline[mask]
     c2_normzeroline = c2_normzeroline[mask]
 
     # Transform to polar coordinates
     theta_zeroline = np.arctan2(c2_normzeroline, c1_normzeroline)
     rho_zeroline = np.sqrt(c1_normzeroline**2 + c2_normzeroline**2)
-    theta_zeroline[theta_zeroline < 0] = theta_zeroline[
-        theta_zeroline < 0] + 2 * np.pi
+    theta_zeroline[theta_zeroline < 0] = theta_zeroline[theta_zeroline < 0] + 2 * np.pi
 
     sample_alpha, sample_beta, weight_points = _generate_sample_data(
-        beta_lines, rho_zeroline, theta_zeroline, points_per_interval,
-        contour_probs)
+        beta_lines, rho_zeroline, theta_zeroline, points_per_interval, contour_probs
+    )
 
     # Sample transformation to principal component space
     sample_u1 = sample_beta * np.cos(sample_alpha)
     sample_u2 = sample_beta * np.sin(sample_alpha)
 
     comp1_sample = stats.invgauss.ppf(
         stats.norm.cdf(sample_u1, loc=0, scale=1),
-        mu=comp1['mu'], loc=0, scale=comp1['scale'])
+        mu=comp1["mu"],
+        loc=0,
+        scale=comp1["scale"],
+    )
     mu_sample = mu_slope * comp1_sample + mu_intercept
 
     # Calculate sigma values at each point on the circle
     sigma_sample = np.polyval(sigma_polynomial_coeffcients, comp1_sample)
 
     # Use calculated mu and sigma values to calculate C2 along the contour
-    comp2_sample = stats.norm.ppf(stats.norm.cdf(sample_u2, loc=0, scale=1),
-                                  loc=mu_sample, scale=sigma_sample)
+    comp2_sample = stats.norm.ppf(
+        stats.norm.cdf(sample_u2, loc=0, scale=1), loc=mu_sample, scale=sigma_sample
+    )
 
     # Sample transformation into Hs-T space
-    h_sample, t_sample = _princomp_inv(
-        comp1_sample, comp2_sample, coeff, shift)
+    h_sample, t_sample = _princomp_inv(comp1_sample, comp2_sample, coeff, shift)
 
     return h_sample, t_sample, weight_points
 
 
 def samples_contour(t_samples, t_contour, hs_contour):
     """
     Get Hs points along a specified environmental contour using
     user-defined T values.
 
     Parameters
     ----------
-    t_samples : np.array
+    t_samples : list, np.ndarray, pd.Series, xr.DataArray
         Points for sampling along return contour
-    t_contour : np.array
+    t_contour : list, np.ndarray, pd.Series, xr.DataArray
         T values along contour
-    hs_contour : np.array
+    hs_contour : list, np.ndarray, pd.Series, xr.DataArray
         Hs values along contour
 
     Returns
     -------
-    hs_samples : nparray
+    hs_samples : np.ndarray
         points sampled along return contour
     """
-    assert isinstance(
-        t_samples, np.ndarray), 't_samples must be of type np.ndarray'
-    assert isinstance(
-        t_contour, np.ndarray), 't_contour must be of type np.ndarray'
-    assert isinstance(
-        hs_contour, np.ndarray), 'hs_contour must be of type np.ndarray'
+    t_samples = to_numeric_array(t_samples, "t_samples")
+    t_contour = to_numeric_array(t_contour, "t_contour")
+    hs_contour = to_numeric_array(hs_contour, "hs_contour")
 
-    #finds minimum and maximum energy period values
+    # finds minimum and maximum energy period values
     amin = np.argmin(t_contour)
     amax = np.argmax(t_contour)
     aamin = np.min([amin, amax])
     aamax = np.max([amin, amax])
-    #finds points along the contour
+    # finds points along the contour
     w1 = hs_contour[aamin:aamax]
     w2 = np.concatenate((hs_contour[aamax:], hs_contour[:aamin]))
-    if (np.max(w1) > np.max(w2)):
+    if np.max(w1) > np.max(w2):
         x1 = t_contour[aamin:aamax]
         y1 = hs_contour[aamin:aamax]
     else:
         x1 = np.concatenate((t_contour[aamax:], t_contour[:aamin]))
         y1 = np.concatenate((hs_contour[aamax:], hs_contour[:aamin]))
-    #sorts data based on the max and min energy period values
+    # sorts data based on the max and min energy period values
     ms = np.argsort(x1)
     x = x1[ms]
     y = y1[ms]
-    #interpolates the sorted data
+    # interpolates the sorted data
     si = interp.interp1d(x, y)
-    #finds the wave height based on the user specified energy period values
+    # finds the wave height based on the user specified energy period values
     hs_samples = si(t_samples)
 
     return hs_samples
 
 
-def _generate_sample_data(beta_lines, rho_zeroline, theta_zeroline,
-                          points_per_interval, contour_probs):
+def _generate_sample_data(
+    beta_lines, rho_zeroline, theta_zeroline, points_per_interval, contour_probs
+):
     """
     Calculate radius, angle, and weight for each sample point
 
     Parameters
     ----------
-    beta_lines: np.array
+    beta_lines: list, np.ndarray, pd.Series, xr.DataArray
         Array of mu fitting function parameters.
-    rho_zeroline: np.array
+    rho_zeroline: list, np.ndarray, pd.Series, xr.DataArray
         Array of radii
-    theta_zeroline: np.array
+    theta_zeroline: list, np.ndarray, pd.Series, xr.DataArray
     points_per_interval: int
-    contour_probs: np.array
+    contour_probs: list, np.ndarray, pd.Series, xr.DataArray
 
     Returns
     -------
     sample_alpha: np.array
         Array of fitted sample angle values.
     sample_beta: np.array
         Array of fitted sample radius values.
     weight_points: np.array
         Array of weights for each point.
     """
-    assert isinstance(
-        beta_lines, np.ndarray), 'beta_lines must be of type np.ndarray'
-    assert isinstance(
-        rho_zeroline, np.ndarray), 'rho_zeroline must be of type np.ndarray'
-    assert isinstance(theta_zeroline, np.ndarray
-                      ), 'theta_zeroline must be of type np.ndarray'
-    assert isinstance(points_per_interval, int
-                      ), 'points_per_interval must be of type int'
-    assert isinstance(
-        contour_probs, np.ndarray), 'contour_probs must be of type np.ndarray'
+    beta_lines = to_numeric_array(beta_lines, "beta_lines")
+    rho_zeroline = to_numeric_array(rho_zeroline, "rho_zeroline")
+    theta_zeroline = to_numeric_array(theta_zeroline, "theta_zeroline")
+    contour_probs = to_numeric_array(contour_probs, "contour_probs")
+    if not isinstance(points_per_interval, int):
+        raise TypeError(
+            f"points_per_interval must be of type int. Got: {type(points_per_interval)}"
+        )
 
     num_samples = (len(beta_lines) - 1) * points_per_interval
     alpha_bounds = np.zeros((len(beta_lines) - 1, 2))
     angular_dist = np.zeros(len(beta_lines) - 1)
     angular_ratio = np.zeros(len(beta_lines) - 1)
     alpha = np.zeros((len(beta_lines) - 1, points_per_interval + 1))
     weight = np.zeros(len(beta_lines) - 1)
@@ -1818,43 +1981,49 @@
         # Check if any of the radii for the Hs=0, line are smaller than
         # the radii of the contour, meaning that these lines intersect
         r = rho_zeroline - beta_lines[i + 1] + 0.01
         if any(r < 0):
             left = np.amin(np.where(r < 0))
             right = np.amax(np.where(r < 0))
             # Save sampling bounds
-            alpha_bounds[i, :] = (theta_zeroline[left], theta_zeroline[right] -
-                                  2 * np.pi)
+            alpha_bounds[i, :] = (
+                theta_zeroline[left],
+                theta_zeroline[right] - 2 * np.pi,
+            )
         else:
             alpha_bounds[i, :] = np.array((0, 2 * np.pi))
         # Find the angular distance that will be covered by sampling the disc
         angular_dist[i] = sum(abs(alpha_bounds[i]))
         # Calculate ratio of area covered for each contour
         angular_ratio[i] = angular_dist[i] / (2 * np.pi)
         # Discretize the remaining portion of the disc into 10 equally spaced
         # areas to be sampled
         alpha[i, :] = np.arange(
             min(alpha_bounds[i]),
-            max(alpha_bounds[i]) + 0.1, angular_dist[i] / points_per_interval)
+            max(alpha_bounds[i]) + 0.1,
+            angular_dist[i] / points_per_interval,
+        )
         # Calculate the weight of each point sampled per contour
-        weight[i] = ((contour_probs[i] - contour_probs[i + 1]) *
-                     angular_ratio[i] / points_per_interval)
+        weight[i] = (
+            (contour_probs[i] - contour_probs[i + 1])
+            * angular_ratio[i]
+            / points_per_interval
+        )
         for j in range(points_per_interval):
             # Generate sample radius by adding a randomly sampled distance to
             # the 'disc' lower bound
-            sample_beta[(i) * points_per_interval + j] = (
-                beta_lines[i] +
-                np.random.random_sample() * (beta_lines[i + 1] - beta_lines[i])
-            )
+            sample_beta[(i) * points_per_interval + j] = beta_lines[
+                i
+            ] + np.random.random_sample() * (beta_lines[i + 1] - beta_lines[i])
             # Generate sample angle by adding a randomly sampled distance to
             # the lower bound of the angle defining a discrete portion of the
             # 'disc'
-            sample_alpha[(i) * points_per_interval + j] = (
-                alpha[i, j] +
-                np.random.random_sample() * (alpha[i, j + 1] - alpha[i, j]))
+            sample_alpha[(i) * points_per_interval + j] = alpha[
+                i, j
+            ] + np.random.random_sample() * (alpha[i, j + 1] - alpha[i, j])
             # Save the weight for each sample point
             weight_points[i * points_per_interval + j] = weight[i]
 
     return sample_alpha, sample_beta, weight_points
 
 
 def _princomp_inv(princip_data1, princip_data2, coeff, shift):
@@ -1876,24 +2045,32 @@
     Returns
     -------
     original1: np.array
         Hs values following rotation from principal component space.
     original2: np.array
         T values following rotation from principal component space.
     """
-    assert isinstance(
-        princip_data1, np.ndarray), 'princip_data1 must be of type np.ndarray'
-    assert isinstance(
-        princip_data2, np.ndarray), 'princip_data2 must be of type np.ndarray'
-    assert isinstance(coeff, np.ndarray), 'coeff must be of type np.ndarray'
-    assert isinstance(shift, float), 'float must be of type float'
+    if not isinstance(princip_data1, np.ndarray):
+        raise TypeError(
+            f"princip_data1 must be of type np.ndarray. Got: {type(princip_data1)}"
+        )
+    if not isinstance(princip_data2, np.ndarray):
+        raise TypeError(
+            f"princip_data2 must be of type np.ndarray. Got: {type(princip_data2)}"
+        )
+    if not isinstance(coeff, np.ndarray):
+        raise TypeError(f"coeff must be of type np.ndarray. Got: {type(coeff)}")
+    if not isinstance(shift, float):
+        raise TypeError(f"shift must be of type float. Got: {type(shift)}")
 
     original1 = np.zeros(len(princip_data1))
     original2 = np.zeros(len(princip_data1))
     for i in range(len(princip_data2)):
-        original1[i] = (((coeff[0, 1] * (princip_data2[i] - shift)) +
-                        (coeff[0, 0] * princip_data1[i])) / (coeff[0, 1]**2 +
-                                                             coeff[0, 0]**2))
-        original2[i] = (((coeff[0, 1] * princip_data1[i]) -
-                        (coeff[0, 0] * (princip_data2[i] - shift))) /
-                        (coeff[0, 1]**2 + coeff[0, 0]**2))
+        original1[i] = (
+            (coeff[0, 1] * (princip_data2[i] - shift))
+            + (coeff[0, 0] * princip_data1[i])
+        ) / (coeff[0, 1] ** 2 + coeff[0, 0] ** 2)
+        original2[i] = (
+            (coeff[0, 1] * princip_data1[i])
+            - (coeff[0, 0] * (princip_data2[i] - shift))
+        ) / (coeff[0, 1] ** 2 + coeff[0, 0] ** 2)
     return original1, original2
```

### Comparing `mhkit-0.7.0/mhkit/wave/graphics.py` & `mhkit-0.8.0/mhkit/wave/graphics.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-
 from mhkit.river.resource import exceedance_probability
 from mhkit.river.graphics import _xy_plot
+from mhkit.utils import convert_to_dataset
 import matplotlib.patheffects as pe
 import matplotlib.pyplot as plt
 from matplotlib import gridspec
 import pandas as pd
 import xarray as xr
 import numpy as np
 import matplotlib
@@ -13,72 +13,73 @@
 
 def plot_spectrum(S, ax=None):
     """
     Plots wave amplitude spectrum versus omega
 
     Parameters
     ------------
-    S: pandas DataFrame
+    S: pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Spectral density [m^2/Hz] indexed frequency [Hz]
     ax : matplotlib axes object
         Axes for plotting.  If None, then a new figure is created.
 
     Returns
     ---------
     ax : matplotlib pyplot axes
     """
-    assert isinstance(S, pd.DataFrame), 'S must be of type pd.DataFrame'
+    S = convert_to_dataset(S)
 
-    f = S.index
-    for key in S.keys():
-        ax = _xy_plot(f*2*np.pi, S[key]/(2*np.pi), fmt='-', xlabel='omega [rad/s]',
-             ylabel='Spectral density [m$^2$s/rad]', ax=ax)
+    frequency_dimension = list(S.dims)[0]
+    f = S[frequency_dimension]
+    for var in S.data_vars:
+        ax = _xy_plot(
+            f * 2 * np.pi,
+            S[var] / (2 * np.pi),
+            fmt="-",
+            xlabel="omega [rad/s]",
+            ylabel="Spectral density [m$^2$s/rad]",
+            ax=ax,
+        )
 
     return ax
 
 
 def plot_elevation_timeseries(eta, ax=None):
     """
     Plot wave surface elevation time-series
 
     Parameters
     ----------
-    eta: pandas DataFrame
+    eta: pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Wave surface elevation [m] indexed by time [datetime or s]
     ax : matplotlib axes object
         Axes for plotting.  If None, then a new figure is created.
 
     Returns
     -------
     ax : matplotlib pyplot axes
     """
+    eta = convert_to_dataset(eta)
 
-    assert isinstance(eta, pd.DataFrame), 'eta must be of type pd.DataFrame'
+    time_dimension = list(eta.dims)[0]
+    t = eta[time_dimension]
 
-    for key in eta.keys():
-        ax = _xy_plot(eta.index, eta[key], fmt='-', xlabel='Time',
-            ylabel='$\eta$ [m]', ax=ax)
+    for var in eta.data_vars:
+        ax = _xy_plot(t, eta[var], fmt="-", xlabel="Time", ylabel="$\eta$ [m]", ax=ax)
 
     return ax
 
 
-def plot_matrix(
-        M, 
-        xlabel='Te', 
-        ylabel='Hm0', 
-        zlabel=None, 
-        show_values=True,
-        ax=None
-    ):
+def plot_matrix(M, xlabel="Te", ylabel="Hm0", zlabel=None, show_values=True, ax=None):
     """
     Plots values in the matrix as a scatter diagram
 
     Parameters
     ------------
-    M: pandas DataFrame
+    M: pandas Series, pandas DataFrame, xarray DataArray
         Matrix with numeric labels for x and y axis, and numeric entries.
         An example would be the average capture length matrix generated by
         mhkit.device.wave, or something similar.
     xlabel: string (optional)
         Title of the x-axis
     ylabel: string (optional)
         Title of the y-axis
@@ -89,235 +90,295 @@
     ax : matplotlib axes object
         Axes for plotting.  If None, then a new figure is created.
     Returns
     ---------
     ax : matplotlib pyplot axes
     """
 
-    assert isinstance(M, pd.DataFrame), 'M must be of type pd.DataFrame'
+    try:
+        M = pd.DataFrame(M)
+    except:
+        pass
+    if not isinstance(M, pd.DataFrame):
+        raise TypeError(f"M must be of type pd.DataFrame. Got: {type(M)}")
 
     if ax is None:
         plt.figure()
         ax = plt.gca()
 
-    im = ax.imshow(M, origin='lower', aspect='auto')
+    im = ax.imshow(M, origin="lower", aspect="auto")
 
     # Add colorbar
     cbar = plt.colorbar(im)
     if zlabel:
         cbar.set_label(zlabel, rotation=270, labelpad=15)
 
     # Set x and y label
     ax.set_xlabel(xlabel)
     ax.set_ylabel(ylabel)
 
     # Show values in the plot
     if show_values:
         for i, col in enumerate(M.columns):
             for j, index in enumerate(M.index):
-                if not np.isnan(M.loc[index,col]):
-                    ax.text(i, j, format(M.loc[index,col], '.2f'), ha="center", va="center")
+                if not np.isnan(M.loc[index, col]):
+                    ax.text(
+                        i, j, format(M.loc[index, col], ".2f"), ha="center", va="center"
+                    )
 
     # Reset x and y ticks
     ax.set_xticks(np.arange(len(M.columns)))
     ax.set_yticks(np.arange(len(M.index)))
     ax.set_xticklabels(M.columns)
     ax.set_yticklabels(M.index)
 
     return ax
 
 
 def plot_chakrabarti(H, lambda_w, D, ax=None):
     """
-    Plots, in the style of Chakrabart (2005), relative importance of viscous,
+    Plots, in the style of Chakrabarti (2005), relative importance of viscous,
     inertia, and diffraction phemonena
     Chakrabarti, Subrata. Handbook of Offshore Engineering (2-volume set).
     Elsevier, 2005.
 
-    Parameters
-    ----------
-    H: float or numpy array or pandas Series
-        Wave height [m]
-    lambda_w: float or numpy array or pandas Series
-        Wave length [m]
-    D: float or numpy array or pandas Series
-        Characteristic length [m]
-    ax : matplotlib axes object (optional)
-        Axes for plotting.  If None, then a new figure is created.
+    Examples:
 
-    Returns
-    -------
-    ax : matplotlib pyplot axes
-
-    Examples
-    --------
     **Using floats**
+
     >>> plt.figure()
     >>> D = 5
     >>> H = 8
     >>> lambda_w = 200
     >>> wave.graphics.plot_chakrabarti(H, lambda_w, D)
+
     **Using numpy array**
+
     >>> plt.figure()
     >>> D = np.linspace(5,15,5)
     >>> H = 8*np.ones_like(D)
     >>> lambda_w = 200*np.ones_like(D)
     >>> wave.graphics.plot_chakrabarti(H, lambda_w, D)
+
     **Using pandas DataFrame**
+
     >>> plt.figure()
     >>> D = np.linspace(5,15,5)
     >>> H = 8*np.ones_like(D)
     >>> lambda_w = 200*np.ones_like(D)
-    >>> df = pd.DataFrame([H.flatten(),lambda_w.flatten(),D.flatten()], \
-                              index=['H','lambda_w','D']).transpose()
+    >>> df = pd.DataFrame([H.flatten(),lambda_w.flatten(),D.flatten()], index=['H','lambda_w','D']).transpose()
     >>> wave.graphics.plot_chakrabarti(df.H, df.lambda_w, df.D)
+
+    Parameters
+    ----------
+    H: int, float, numpy array, pandas Series, or xarray DataArray
+        Wave height [m]
+    lambda_w: int, float, numpy array, pandas Series, or xarray DataArray
+        Wave length [m]
+    D: int, float, numpy array, pandas Series, or xarray DataArray
+        Characteristic length [m]
+    ax : matplotlib axes object (optional)
+        Axes for plotting.  If None, then a new figure is created.
+
+    Returns
+    -------
+    ax : matplotlib pyplot axes
     """
-    assert isinstance(H, (np.ndarray, float, int, np.int64,pd.Series)), \
-           'H must be a real numeric type'
-    assert isinstance(lambda_w, (np.ndarray, float, int, np.int64,pd.Series)), \
-           'lambda_w must be a real numeric type'
-    assert isinstance(D, (np.ndarray, float, int, np.int64,pd.Series)), \
-           'D must be a real numeric type'
-
-    if any([(isinstance(H, np.ndarray) or isinstance(H, pd.Series)),        \
-            (isinstance(lambda_w, np.ndarray) or isinstance(H, pd.Series)), \
-            (isinstance(D, np.ndarray) or isinstance(H, pd.Series))\
-           ]):
-        errMsg = 'D, H, and lambda_w must be same shape'
+    if not isinstance(H, (np.ndarray, float, int, np.int64, pd.Series, xr.DataArray)):
+        raise TypeError(
+            f"H must be of type float, int, np.int64, np.ndarray, pd.Series, or xr.DataArray. Got: {type(H)}"
+        )
+    if not isinstance(
+        lambda_w, (np.ndarray, float, int, np.int64, pd.Series, xr.DataArray)
+    ):
+        raise TypeError(
+            f"lambda_w must be of type float, int, np.int64, np.ndarray, pd.Series, or xr.DataArray. Got: {type(lambda_w)}"
+        )
+    if not isinstance(D, (np.ndarray, float, int, np.int64, pd.Series, xr.DataArray)):
+        raise TypeError(
+            f"D must be of type float, int, np.int64, np.ndarray, pd.Series, or xr.DataArray. Got: {type(D)}"
+        )
+
+    if any(
+        [
+            isinstance(H, (np.ndarray, pd.Series, xr.DataArray)),
+            isinstance(lambda_w, (np.ndarray, pd.Series, xr.DataArray)),
+            isinstance(D, (np.ndarray, pd.Series, xr.DataArray)),
+        ]
+    ):
         n_H = H.squeeze().shape
         n_lambda_w = lambda_w.squeeze().shape
         n_D = D.squeeze().shape
-        assert n_H == n_lambda_w and n_H == n_D, errMsg
+        if not (n_H == n_lambda_w and n_H == n_D):
+            raise ValueError("D, H, and lambda_w must be same shape")
 
         if isinstance(H, np.ndarray):
-            mvals = pd.DataFrame(H.reshape(len(H),1), columns=['H'])
-            mvals['lambda_w'] = lambda_w
-            mvals['D'] = D
-        elif isinstance(H, pd.Series):
+            mvals = pd.DataFrame(H.reshape(len(H), 1), columns=["H"])
+            mvals["lambda_w"] = lambda_w
+            mvals["D"] = D
+        elif isinstance(H, (pd.Series, xr.DataArray)):
             mvals = pd.DataFrame(H)
-            mvals['lambda_w'] = lambda_w
-            mvals['D'] = D
+            mvals["lambda_w"] = lambda_w
+            mvals["D"] = D
 
     else:
         H = np.array([H])
         lambda_w = np.array([lambda_w])
         D = np.array([D])
-        mvals = pd.DataFrame(H.reshape(len(H),1), columns=['H'])
-        mvals['lambda_w'] = lambda_w
-        mvals['D'] = D
+        mvals = pd.DataFrame(H.reshape(len(H), 1), columns=["H"])
+        mvals["lambda_w"] = lambda_w
+        mvals["D"] = D
 
     if ax is None:
         plt.figure()
         ax = plt.gca()
 
-    ax.set_xscale('log')
-    ax.set_yscale('log')
+    ax.set_xscale("log")
+    ax.set_yscale("log")
 
     for index, row in mvals.iterrows():
         H = row.H
         D = row.D
         lambda_w = row.lambda_w
 
         KC = H / D
-        Diffraction = np.pi*D / lambda_w
-        label = f'$H$ = {H:g}, $\lambda_w$ = {lambda_w:g}, $D$ = {D:g}'
-        ax.plot(Diffraction, KC, 'o', label=label)
-
-    if np.any(KC>=10 or KC<=.02) or np.any(Diffraction>=50) or \
-        np.any(lambda_w >= 1000) :
-        ax.autoscale(enable=True, axis='both', tight=True)
+        Diffraction = np.pi * D / lambda_w
+        label = f"$H$ = {H:g}, $\lambda_w$ = {lambda_w:g}, $D$ = {D:g}"
+        ax.plot(Diffraction, KC, "o", label=label)
+
+    if (
+        np.any(KC >= 10 or KC <= 0.02)
+        or np.any(Diffraction >= 50)
+        or np.any(lambda_w >= 1000)
+    ):
+        ax.autoscale(enable=True, axis="both", tight=True)
     else:
         ax.set_xlim((0.01, 10))
         ax.set_ylim((0.01, 50))
 
     graphScale = list(ax.get_xlim())
-    if graphScale[0] >= .01:
-        graphScale[0] =.01
+    if graphScale[0] >= 0.01:
+        graphScale[0] = 0.01
 
     # deep water breaking limit (H/lambda_w = 0.14)
-    x = np.logspace(1,np.log10(graphScale[0]), 2)
+    x = np.logspace(1, np.log10(graphScale[0]), 2)
     y_breaking = 0.14 * np.pi / x
-    ax.plot(x, y_breaking, 'k-')
+    ax.plot(x, y_breaking, "k-")
     graphScale = list(ax.get_xlim())
 
-    ax.text(1, 7,
-            'wave\nbreaking\n$H/\lambda_w > 0.14$',
-            ha='center', va='center', fontstyle='italic',
-            fontsize='small',clip_on='True')
+    ax.text(
+        1,
+        7,
+        "wave\nbreaking\n$H/\lambda_w > 0.14$",
+        ha="center",
+        va="center",
+        fontstyle="italic",
+        fontsize="small",
+        clip_on="True",
+    )
 
     # upper bound of low drag region
     ldv = 20
-    y_small_drag = 20*np.ones_like(graphScale)
+    y_small_drag = 20 * np.ones_like(graphScale)
     graphScale[1] = 0.14 * np.pi / ldv
-    ax.plot(graphScale, y_small_drag,'k--')
-    ax.text(0.0125, 30,
-            'drag',
-            ha='center', va='top', fontstyle='italic',
-            fontsize='small',clip_on='True')
+    ax.plot(graphScale, y_small_drag, "k--")
+    ax.text(
+        0.0125,
+        30,
+        "drag",
+        ha="center",
+        va="top",
+        fontstyle="italic",
+        fontsize="small",
+        clip_on="True",
+    )
 
     # upper bound of small drag region
     sdv = 1.5
-    y_small_drag = sdv*np.ones_like(graphScale)
+    y_small_drag = sdv * np.ones_like(graphScale)
     graphScale[1] = 0.14 * np.pi / sdv
-    ax.plot(graphScale, y_small_drag,'k--')
-    ax.text(0.02, 7,
-            'inertia \n& drag',
-            ha='center', va='center', fontstyle='italic',
-            fontsize='small',clip_on='True')
+    ax.plot(graphScale, y_small_drag, "k--")
+    ax.text(
+        0.02,
+        7,
+        "inertia \n& drag",
+        ha="center",
+        va="center",
+        fontstyle="italic",
+        fontsize="small",
+        clip_on="True",
+    )
 
     # upper bound of negligible drag region
     ndv = 0.25
     graphScale[1] = 0.14 * np.pi / ndv
-    y_small_drag = ndv*np.ones_like(graphScale)
-    ax.plot(graphScale, y_small_drag,'k--')
-    ax.text(8e-2, 0.7,
-            'large\ninertia',
-            ha='center', va='center', fontstyle='italic',
-            fontsize='small',clip_on='True')
-
-
-    ax.text(8e-2, 6e-2,
-            'all\ninertia',
-            ha='center', va='center', fontstyle='italic',
-            fontsize='small', clip_on='True')
+    y_small_drag = ndv * np.ones_like(graphScale)
+    ax.plot(graphScale, y_small_drag, "k--")
+    ax.text(
+        8e-2,
+        0.7,
+        "large\ninertia",
+        ha="center",
+        va="center",
+        fontstyle="italic",
+        fontsize="small",
+        clip_on="True",
+    )
+
+    ax.text(
+        8e-2,
+        6e-2,
+        "all\ninertia",
+        ha="center",
+        va="center",
+        fontstyle="italic",
+        fontsize="small",
+        clip_on="True",
+    )
 
     # left bound of diffraction region
     drv = 0.5
     graphScale = list(ax.get_ylim())
     graphScale[1] = 0.14 * np.pi / drv
-    x_diff_reg = drv*np.ones_like(graphScale)
-    ax.plot(x_diff_reg, graphScale, 'k--')
-    ax.text(2, 6e-2,
-            'diffraction',
-            ha='center', va='center', fontstyle='italic',
-            fontsize='small',clip_on='True')
-
+    x_diff_reg = drv * np.ones_like(graphScale)
+    ax.plot(x_diff_reg, graphScale, "k--")
+    ax.text(
+        2,
+        6e-2,
+        "diffraction",
+        ha="center",
+        va="center",
+        fontstyle="italic",
+        fontsize="small",
+        clip_on="True",
+    )
 
     if index > 0:
-        ax.legend(fontsize='xx-small', ncol=2)
+        ax.legend(fontsize="xx-small", ncol=2)
 
-    ax.set_xlabel('Diffraction parameter, $\\frac{\\pi D}{\\lambda_w}$')
-    ax.set_ylabel('KC parameter, $\\frac{H}{D}$')
+    ax.set_xlabel("Diffraction parameter, $\\frac{\\pi D}{\\lambda_w}$")
+    ax.set_ylabel("KC parameter, $\\frac{H}{D}$")
 
     plt.tight_layout()
 
 
 def plot_environmental_contour(x1, x2, x1_contour, x2_contour, **kwargs):
-    '''
+    """
     Plots an overlay of the x1 and x2 variables to the calculate
     environmental contours.
+
     Parameters
     ----------
-    x1: numpy array
+    x1: list, np.ndarray, pd.Series, xr.DataArray
         x-axis data
-    x2: numpy array
+    x2: list, np.ndarray, pd.Series, xr.DataArray
         x-axis data
-    x1_contour: numpy array or list
+    x1_contour: list, np.ndarray, pd.Series, xr.DataArray
         Calculated x1 contour values
-    x2_contour: numpy array or list
+    x2_contour: list, np.ndarray, pd.Series, xr.DataArray
         Calculated x2 contour values
     **kwargs : optional
         x_label: string (optional)
             x-axis label. Default None.
         y_label: string (optional)
             y-axis label. Default None.
         data_label: string (optional)
@@ -327,98 +388,130 @@
             Legend label for x1_contour, x2_contour countor data
             (e.g. '100-year contour'). Default None.
         ax : matplotlib axes object (optional)
             Axes for plotting.  If None, then a new figure is created.
             Default None.
         markers: string
             string or list of strings to use as marker types
+
     Returns
     -------
     ax : matplotlib pyplot axes
-    '''
-    try: x1 = x1.values
-    except: pass
-    try: x2 = x2.values
-    except: pass
-    assert isinstance(x1, np.ndarray), 'x1 must be of type np.ndarray'
-    assert isinstance(x2, np.ndarray), 'x2 must be of type np.ndarray'
-    assert isinstance(x1_contour, (np.ndarray,list)), ('x1_contour must be of '
-                                                'type np.ndarray or list')
-    assert isinstance(x2_contour, (np.ndarray,list)), ('x2_contour must be of '
-                                               'type np.ndarray or list')
+    """
+    try:
+        x1 = x1.values
+    except:
+        pass
+    try:
+        x2 = x2.values
+    except:
+        pass
+    if not isinstance(x1, np.ndarray):
+        raise TypeError(f"x1 must be of type np.ndarray. Got: {type(x1)}")
+    if not isinstance(x2, np.ndarray):
+        raise TypeError(f"x2 must be of type np.ndarray. Got: {type(x2)}")
+    try:
+        x1_contour = x1_contour.values
+    except:
+        pass
+    try:
+        x2_contour = x2_contour.values
+    except:
+        pass
+    if not isinstance(x1_contour, (np.ndarray, list)):
+        raise TypeError(
+            f"x1_contour must be of type np.ndarray or list. Got: {type(x1_contour)}"
+        )
+    if not isinstance(x2_contour, (np.ndarray, list)):
+        raise TypeError(
+            f"x2_contour must be of type np.ndarray or list. Got: {type(x2_contour)}"
+        )
+
     x_label = kwargs.get("x_label", None)
     y_label = kwargs.get("y_label", None)
-    data_label=kwargs.get("data_label", None)
-    contour_label=kwargs.get("contour_label", None)
-    ax=kwargs.get("ax", None)
-    markers=kwargs.get("markers", '-')
-    assert isinstance(data_label, (str,type(None))), 'data_label must be of type str'
-    assert isinstance(contour_label, (str,list, type(None))), ('contour_label be of '
-                                                  'type str')
-
-    if isinstance(markers, list):
-        assert all( [isinstance(marker, (str)) for marker in markers] )
-    elif isinstance(markers, str):
-        markers=[markers]
-        assert all( [isinstance(marker, (str)) for marker in markers] )
-    else:
-        assert isinstance(markers, (str,list)), ('markers must be of type str or list of strings')
+    data_label = kwargs.get("data_label", None)
+    contour_label = kwargs.get("contour_label", None)
+    ax = kwargs.get("ax", None)
+    markers = kwargs.get("markers", "-")
+    if not isinstance(data_label, (str, type(None))):
+        raise TypeError(
+            f"If specified, data_label must be of type str. Got: {type(data_label)}"
+        )
+    if not isinstance(contour_label, (str, list, type(None))):
+        raise TypeError(
+            f"If specified, contour_label be of type str. Got: {type(contour_label)}"
+        )
 
-    assert len(x2_contour) == len(x1_contour),  ('contour must be of'
-            f'equal dimesion got {len(x2_contour)} and {len(x1_contour)}')
+    if isinstance(markers, str):
+        markers = [markers]
+    if not isinstance(markers, list) or not all(
+        [isinstance(marker, (str)) for marker in markers]
+    ):
+        raise TypeError(
+            f"markers must be of type str or list of strings. Got: {markers}"
+        )
 
+    if not len(x2_contour) == len(x1_contour):
+        raise ValueError(
+            f"contour must be of equal dimension got {len(x2_contour)} and {len(x1_contour)}"
+        )
 
     if isinstance(x1_contour, np.ndarray):
-        N_contours=1
-        x2_contour  = [x2_contour]
+        N_contours = 1
+        x2_contour = [x2_contour]
         x1_contour = [x1_contour]
     elif isinstance(x1_contour, list):
-        N_contours=len(x1_contour)
+        N_contours = len(x1_contour)
 
     if contour_label != None:
         if isinstance(contour_label, str):
             contour_label = [contour_label]
         N_c_labels = len(contour_label)
-        assert  N_c_labels == N_contours, ('If specified, the '
-             'number of contour lables must be equal to number the '
-            f'number of contour years. Got {N_c_labels} and {N_contours}')
+        if not N_c_labels == N_contours:
+            raise ValueError(
+                "If specified, the number of contour labels must"
+                " be equal to number the number of contour years."
+                f" Got: {N_c_labels} and {N_contours}"
+            )
     else:
         contour_label = [None] * N_contours
 
-    if len(markers)==1:
-        markers=markers*N_contours
-    assert len(markers) == N_contours,  ('Markers must be same length'
-            f'as N contours specified. Got: {len(markers)} and {len(x1_contour)}')
+    if len(markers) == 1:
+        markers = markers * N_contours
+    if not len(markers) == N_contours:
+        raise ValueError(
+            "Markers must be same length as N contours specified."
+            f"Got: {len(markers)} and {len(x1_contour)}"
+        )
 
     for i in range(N_contours):
         contour1 = np.array(x1_contour[i]).T
         contour2 = np.array(x2_contour[i]).T
-        ax = _xy_plot(contour1, contour2, markers[i],
-                      label=contour_label[i], ax=ax)
+        ax = _xy_plot(contour1, contour2, markers[i], label=contour_label[i], ax=ax)
 
-    plt.plot(x1, x2, 'bo', alpha=0.1, label=data_label)
+    plt.plot(x1, x2, "bo", alpha=0.1, label=data_label)
 
-    plt.legend(loc='lower right')
+    plt.legend(loc="lower right")
     plt.xlabel(x_label)
     plt.ylabel(y_label)
     plt.tight_layout()
     return ax
 
 
 def plot_avg_annual_energy_matrix(
-        Hm0, 
-        Te, 
-        J, 
-        time_index=None, 
-        Hm0_bin_size=None, 
-        Te_bin_size=None, 
-        Hm0_edges=None, 
-        Te_edges=None
-    ):
-    '''
+    Hm0,
+    Te,
+    J,
+    time_index=None,
+    Hm0_bin_size=None,
+    Te_bin_size=None,
+    Hm0_edges=None,
+    Te_edges=None,
+):
+    """
     Creates an average annual energy matrix with frequency of occurance.
 
     Parameters
     ----------
     Hm0: array-like
         Significant wave height
     Te: array-like
@@ -436,130 +529,144 @@
     Te_edges: array-like
         Defines the Te bin edges to use. Optional default None.
 
     Returns
     -------
     fig: Figure
         Average annual energy table plot
-    '''
+    """
 
     fig = plt.figure()
     if isinstance(time_index, type(None)):
         data = pd.DataFrame(dict(Hm0=Hm0, Te=Te, J=J))
     else:
-        data= pd.DataFrame(dict(Hm0=Hm0, Te=Te, J=J), index=time_index)
-    years=data.index.year.unique()
+        data = pd.DataFrame(dict(Hm0=Hm0, Te=Te, J=J), index=time_index)
+    years = data.index.year.unique()
 
     if isinstance(Hm0_edges, type(None)):
         Hm0_max = data.Hm0.max()
-        Hm0_edges = np.arange(0,Hm0_max+Hm0_bin_size,Hm0_bin_size)
+        Hm0_edges = np.arange(0, Hm0_max + Hm0_bin_size, Hm0_bin_size)
     if isinstance(Te_edges, type(None)):
         Te_max = data.Te.max()
-        Te_edges = np.arange(0, Te_max+Te_bin_size,Te_bin_size)
+        Te_edges = np.arange(0, Te_max + Te_bin_size, Te_bin_size)
 
     # Dict for number of hours each sea state occurs
-    hist_counts={}
-    hist_J={}
+    hist_counts = {}
+    hist_J = {}
 
     # Create hist of counts, and weghted by J for each year
     for year in years:
         year_data = data.loc[str(year)].copy(deep=True)
 
         # Get the counts of each bin
-        counts, xedges, yedges= np.histogram2d(
+        counts, xedges, yedges = np.histogram2d(
             year_data.Te,
             year_data.Hm0,
-            bins = (Te_edges,Hm0_edges),
+            bins=(Te_edges, Hm0_edges),
         )
 
         # Get centers for number of counts plot location
-        xcenters = xedges[:-1]+ np.diff(xedges)
-        ycenters = yedges[:-1]+ np.diff(yedges)
+        xcenters = xedges[:-1] + np.diff(xedges)
+        ycenters = yedges[:-1] + np.diff(yedges)
 
-        year_data['xbins'] = np.digitize(year_data.Te, xcenters)
-        year_data['ybins'] = np.digitize(year_data.Hm0, ycenters)
+        year_data["xbins"] = np.digitize(year_data.Te, xcenters)
+        year_data["ybins"] = np.digitize(year_data.Hm0, ycenters)
 
         total_year_J = year_data.J.sum()
 
-        H=counts.copy()
+        H = counts.copy()
 
         for i in range(len(xcenters)):
             for j in range(len(ycenters)):
-                bin_J = year_data[(year_data.xbins == i) & (year_data.ybins == j)].J.sum()
+                bin_J = year_data[
+                    (year_data.xbins == i) & (year_data.ybins == j)
+                ].J.sum()
                 H[i][j] = bin_J / total_year_J
 
         # Save in results dict
         hist_counts[year] = counts
         hist_J[year] = H
 
     # Calculate avg annual
-    avg_annual_counts_hist = sum(hist_counts.values())/len(years)
-    avg_annual_J_hist = sum(hist_J.values())/len(years)
+    avg_annual_counts_hist = sum(hist_counts.values()) / len(years)
+    avg_annual_J_hist = sum(hist_J.values()) / len(years)
 
     # Create a mask of non-zero weights to hide from imshow
-    Hmasked = np.ma.masked_where(~(avg_annual_J_hist>0),avg_annual_J_hist)
-    plt.imshow(Hmasked.T, interpolation = 'none', vmin = 0.005, origin='lower', aspect='auto',
-               extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])
+    Hmasked = np.ma.masked_where(~(avg_annual_J_hist > 0), avg_annual_J_hist)
+    plt.imshow(
+        Hmasked.T,
+        interpolation="none",
+        vmin=0.005,
+        origin="lower",
+        aspect="auto",
+        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],
+    )
 
     # Plot number of counts as text on the hist of annual avg J
     for xi in range(len(xcenters)):
         for yi in range(len(ycenters)):
             if avg_annual_counts_hist[xi][yi] != 0:
                 plt.text(
-                    xedges[xi], 
-                    yedges[yi], 
-                    int(np.ceil(avg_annual_counts_hist[xi][yi])), 
-                    fontsize=10, 
-                    color='white', 
-                    path_effects=[pe.withStroke(linewidth=1, foreground="k")]
-                ) 
-    plt.xlabel('Wave Energy Period (s)')
-    plt.ylabel('Significant Wave Height (m)')
+                    xedges[xi],
+                    yedges[yi],
+                    int(np.ceil(avg_annual_counts_hist[xi][yi])),
+                    fontsize=10,
+                    color="white",
+                    path_effects=[pe.withStroke(linewidth=1, foreground="k")],
+                )
+    plt.xlabel("Wave Energy Period (s)")
+    plt.ylabel("Significant Wave Height (m)")
 
-    cbar=plt.colorbar()
-    cbar.set_label('Mean Normalized Annual Energy')
+    cbar = plt.colorbar()
+    cbar.set_label("Mean Normalized Annual Energy")
 
     plt.tight_layout()
     return fig
 
 
 def monthly_cumulative_distribution(J):
-    '''
+    """
     Creates a cumulative distribution of energy flux as described in
     IEC TS 62600-101.
 
     Parameters
     ----------
-    J: Series
+    J: pd.Series, xr.DataArray
         Energy Flux with DateTime index
 
     Returns
     -------
     ax: axes
         Figure of monthly cumulative distribution
-    '''
-    assert isinstance(J, pd.Series), 'J must be of type pd.Series'
-    cumSum={}
-    months=J.index.month.unique()
+    """
+    J = pd.Series(J)
+    cumSum = {}
+    months = J.index.month.unique()
     for month in months:
-        F = exceedance_probability(J[J.index.month==month])
-        cumSum[month] = 1-F/100
-        cumSum[month].sort_values('F', inplace=True)
-    plt.figure(figsize=(12,8) )
+        F = exceedance_probability(J[J.index.month == month])
+        cumSum[month] = 1 - F / 100
+        cumSum[month].sort_values("F", inplace=True)
+    plt.figure(figsize=(12, 8))
     for month in months:
-        plt.semilogx(J.loc[cumSum[month].index], cumSum[month].F, '--',
-            label=calendar.month_abbr[month])
+        plt.semilogx(
+            J.loc[cumSum[month].index],
+            cumSum[month].F,
+            "--",
+            label=calendar.month_abbr[month],
+        )
 
     F = exceedance_probability(J)
-    F.sort_values('F', inplace=True)
-    ax = plt.semilogx(J.loc[F.index], 1-F['F']/100, 'k-', fillstyle='none', label='All')
+    F.sort_values("F", inplace=True)
+    ax = plt.semilogx(
+        J.loc[F.index], 1 - F["F"] / 100, "k-", fillstyle="none", label="All"
+    )
 
     plt.grid()
-    plt.xlabel('Energy Flux')
-    plt.ylabel('Cumulative Distribution')
+    plt.xlabel("Energy Flux")
+    plt.ylabel("Cumulative Distribution")
     plt.legend()
     return ax
 
 
 def plot_compendium(Hs, Tp, Dp, buoy_title=None, ax=None):
     """
     Create subplots showing: Significant Wave Height (Hs), Peak Period (Tp),
@@ -567,79 +674,86 @@
 
     See http://cdip.ucsd.edu/themes/cdip?pb=1&bl=cdip?pb=1&d2=p70&u3=s:100:st:1:v:compendium:dt:201204 for example Compendium plot.
 
     Developed based on: http://cdip.ucsd.edu/themes/media/docs/documents/html_pages/compendium.html
 
     Parameters
     ----------
-    Hs: pandas Series
+    Hs: pandas Series or xarray DataArray
         significant wave height
-    Tp: pandas Series
+    Tp: pandas Series or xarray DataArray
         significant wave height
-    Dp: pandas Series
+    Dp: pandas Series or xarray DataArray
         significant wave height
     buoy_title: string (optional)
         Buoy title from the CDIP THREDDS Server
     ax : matplotlib axes object (optional)
         Axes for plotting.  If None, then a new figure is created.
     Returns
     -------
     ax : matplotlib pyplot axes
 
     """
-    assert isinstance(Hs, pd.Series), 'Hs must be of type pd.Series'
-    assert isinstance(Tp, pd.Series), 'Tp must be of type pd.Series'
-    assert isinstance(Dp, pd.Series), 'Dp must be of type pd.Series'
-    assert isinstance(buoy_title, (str, type(None))), 'buoy_title must be of type string'
-
-    f, (pHs, pTp, pDp) = plt.subplots(3, 1, sharex=True, figsize=(15,10))
-
-    pHs.plot(Hs.index,Hs,'b')
-    pTp.plot(Tp.index,Tp,'b')
-    pDp.scatter(Dp.index,Dp,color='blue',s=5)
-
-    pHs.tick_params(axis='x', which='major', labelsize=12, top='off')
-    pHs.set_ylim(0,8)
-    pHs.tick_params(axis='y', which='major', labelsize=12, right='off')
-    pHs.set_ylabel('Hs [m]', fontsize=18)
-    pHs.grid(color='b', linestyle='--')
+    Hs = pd.Series(Hs)
+    Tp = pd.Series(Tp)
+    Dp = pd.Series(Dp)
+    if not isinstance(Hs, pd.Series):
+        raise TypeError(f"Hs must be of type pd.Series. Got: {type(Hs)}")
+    if not isinstance(Tp, pd.Series):
+        raise TypeError(f"Tp must be of type pd.Series. Got: {type(Tp)}")
+    if not isinstance(Dp, pd.Series):
+        raise TypeError(f"Dp must be of type pd.Series. Got: {type(Dp)}")
+    if not isinstance(buoy_title, (str, type(None))):
+        raise TypeError(
+            f"If specified, buoy_title must be of type string. Got: {type(buoy_title)}"
+        )
 
-    pHs2 = pHs.twinx()
-    pHs2.set_ylim(0,25)
-    pHs2.set_ylabel('Hs [ft]', fontsize=18)
+    f, (pHs, pTp, pDp) = plt.subplots(3, 1, sharex=True, figsize=(15, 10))
 
+    pHs.plot(Hs.index, Hs, "b")
+    pTp.plot(Tp.index, Tp, "b")
+    pDp.scatter(Dp.index, Dp, color="blue", s=5)
+
+    pHs.tick_params(axis="x", which="major", labelsize=12, top="off")
+    pHs.set_ylim(0, 8)
+    pHs.tick_params(axis="y", which="major", labelsize=12, right="off")
+    pHs.set_ylabel("Hs [m]", fontsize=18)
+    pHs.grid(color="b", linestyle="--")
 
-    # Peak Period, Tp
-    pTp.set_ylim(0,28)
-    pTp.set_ylabel('Tp [s]', fontsize=18)
-    pTp.grid(color='b', linestyle='--')
+    pHs2 = pHs.twinx()
+    pHs2.set_ylim(0, 25)
+    pHs2.set_ylabel("Hs [ft]", fontsize=18)
 
+    # Peak Period, Tp
+    pTp.set_ylim(0, 28)
+    pTp.set_ylabel("Tp [s]", fontsize=18)
+    pTp.grid(color="b", linestyle="--")
 
     # Direction, Dp
-    pDp.set_ylim(0,360)
-    pDp.set_ylabel('Dp [deg]', fontsize=18)
-    pDp.grid(color='b', linestyle='--')
-    pDp.set_xlabel('Day', fontsize=18)
+    pDp.set_ylim(0, 360)
+    pDp.set_ylabel("Dp [deg]", fontsize=18)
+    pDp.grid(color="b", linestyle="--")
+    pDp.set_xlabel("Day", fontsize=18)
 
     # Set x-axis tick interval to every 5 days
     degrees = 70
     days = matplotlib.dates.DayLocator(interval=5)
-    daysFmt = matplotlib.dates.DateFormatter('%Y-%m-%d')
+    daysFmt = matplotlib.dates.DateFormatter("%Y-%m-%d")
     plt.gca().xaxis.set_major_locator(days)
     plt.gca().xaxis.set_major_formatter(daysFmt)
-    plt.setp( pDp.xaxis.get_majorticklabels(), rotation=degrees )
+    plt.setp(pDp.xaxis.get_majorticklabels(), rotation=degrees)
 
     # Set Titles
     month_name_start = Hs.index.month_name()[0][:3]
     year_start = Hs.index.year[0]
     month_name_end = Hs.index.month_name()[-1][:3]
     year_end = Hs.index.year[-1]
     plt.suptitle(buoy_title, fontsize=30)
 
-    plt.title(f'{Hs.index[0].date()} to {Hs.index[-1].date()}', fontsize=20)
+    plt.title(f"{Hs.index[0].date()} to {Hs.index[-1].date()}", fontsize=20)
 
     ax = f
 
     return ax
 
 
 def plot_boxplot(Hs, buoy_title=None):
@@ -648,160 +762,187 @@
     data.
 
     Developed based on:
         http://cdip.ucsd.edu/themes/media/docs/documents/html_pages/annualHs_plot.html
 
     Parameters
     ------------
-    data: pandas DataFrame
+    Hs: pandas Series or xarray DataArray
         Spectral density [m^2/Hz] indexed frequency [Hz]
     buoy_title: string (optional)
         Buoy title from the CDIP THREDDS Server
     ax : matplotlib axes object (optional)
         Axes for plotting.  If None, then a new figure is created.
     Returns
     ---------
     ax : matplotlib pyplot axes
     """
-    assert isinstance(Hs, pd.Series), 'Hs must be of type pd.Series'
-    assert isinstance(buoy_title, (str, type(None))), 'buoy_title must be of type string'
+    Hs = pd.Series(Hs)
+    if not isinstance(Hs, pd.Series):
+        raise TypeError(f"Hs must be of type pd.Series. Got: {type(Hs)}")
+    if not isinstance(buoy_title, (str, type(None))):
+        raise TypeError(
+            f"If specified, buoy_title must be of type string. Got: {type(buoy_title)}"
+        )
 
     months = Hs.index.month
     means = Hs.groupby(months).mean()
     monthlengths = Hs.groupby(months).count()
 
-    fig = plt.figure(figsize=(10,12))
-    gs = gridspec.GridSpec(2,1, height_ratios=[4,1])
+    fig = plt.figure(figsize=(10, 12))
+    gs = gridspec.GridSpec(2, 1, height_ratios=[4, 1])
 
-    boxprops = dict(color='k')
-    whiskerprops = dict(linestyle='--', color='k')
-    flierprops = dict(marker='+', color='r',markeredgecolor='r',markerfacecolor='r')
-    medianprops = dict(linewidth=2.5,color='firebrick')
-    meanprops = dict(linewidth=2.5, marker='_',  markersize=25)
+    boxprops = dict(color="k")
+    whiskerprops = dict(linestyle="--", color="k")
+    flierprops = dict(marker="+", color="r", markeredgecolor="r", markerfacecolor="r")
+    medianprops = dict(linewidth=2.5, color="firebrick")
+    meanprops = dict(linewidth=2.5, marker="_", markersize=25)
 
-    bp = plt.subplot(gs[0,:])
+    bp = plt.subplot(gs[0, :])
 
     Hs_months = Hs.to_frame().groupby(months)
-    bp = Hs_months.boxplot(subplots=False, boxprops=boxprops,
-        whiskerprops=whiskerprops, flierprops=flierprops,
-        medianprops=medianprops, showmeans=True, meanprops=meanprops)
+    bp = Hs_months.boxplot(
+        subplots=False,
+        boxprops=boxprops,
+        whiskerprops=whiskerprops,
+        flierprops=flierprops,
+        medianprops=medianprops,
+        showmeans=True,
+        meanprops=meanprops,
+    )
 
     # Add values of monthly means as text
     for i, mean in enumerate(means):
-        bp.annotate(np.round(mean,2), (means.index[i],mean),fontsize=12,
-                    horizontalalignment='center',verticalalignment='bottom',
-                    color='g')
+        bp.annotate(
+            np.round(mean, 2),
+            (means.index[i], mean),
+            fontsize=12,
+            horizontalalignment="center",
+            verticalalignment="bottom",
+            color="g",
+        )
 
     # Create a second row of x-axis labels for top subplot
     newax = bp.twiny()
-    newax.tick_params(which='major', direction='in', pad=-18)
+    newax.tick_params(which="major", direction="in", pad=-18)
     newax.set_xlim(bp.get_xlim())
-    newax.xaxis.set_ticks_position('top')
-    newax.xaxis.set_label_position('top')
-    newax.set_xticks(np.arange(1,13,1))
-    newax.set_xticklabels(monthlengths,fontsize=10)
-
+    newax.xaxis.set_ticks_position("top")
+    newax.xaxis.set_label_position("top")
+    newax.set_xticks(np.arange(1, 13, 1))
+    newax.set_xticklabels(monthlengths, fontsize=10)
 
     # Sample 'legend' boxplot, to go underneath actual boxplot
-    bp_sample2 = np.random.normal(2.5,0.5,500)
-    bp2 = plt.subplot(gs[1,:])
-    meanprops = dict(linewidth=2.5, marker='|',  markersize=25)
-    bp2_example = bp2.boxplot(bp_sample2,vert=False,flierprops=flierprops,
-                        medianprops=medianprops)
-    sample_mean=2.3
-    bp2.scatter(sample_mean,1,marker="|",color='g',linewidths=1.0,s=200)
+    bp_sample2 = np.random.normal(2.5, 0.5, 500)
+    bp2 = plt.subplot(gs[1, :])
+    meanprops = dict(linewidth=2.5, marker="|", markersize=25)
+    bp2_example = bp2.boxplot(
+        bp_sample2, vert=False, flierprops=flierprops, medianprops=medianprops
+    )
+    sample_mean = 2.3
+    bp2.scatter(sample_mean, 1, marker="|", color="g", linewidths=1.0, s=200)
 
-    for line in bp2_example['medians']:
+    for line in bp2_example["medians"]:
         xm, ym = line.get_xydata()[0]
-    for line in bp2_example['boxes']:
+    for line in bp2_example["boxes"]:
         xb, yb = line.get_xydata()[0]
-    for line in bp2_example['whiskers']:
+    for line in bp2_example["whiskers"]:
         xw, yw = line.get_xydata()[0]
 
-    bp2.annotate("Median",[xm-0.1,ym-0.3*ym],fontsize=10,color='firebrick')
-    bp2.annotate("Mean",[sample_mean-0.1,0.65],fontsize=10,color='g')
-    bp2.annotate("25%ile",[xb-0.05*xb,yb-0.15*yb],fontsize=10)
-    bp2.annotate("75%ile",[xb+0.26*xb,yb-0.15*yb],fontsize=10)
-    bp2.annotate("Outliers",[xw+0.3*xw,yw-0.3*yw],fontsize=10,color='r')
+    bp2.annotate("Median", [xm - 0.1, ym - 0.3 * ym], fontsize=10, color="firebrick")
+    bp2.annotate("Mean", [sample_mean - 0.1, 0.65], fontsize=10, color="g")
+    bp2.annotate("25%ile", [xb - 0.05 * xb, yb - 0.15 * yb], fontsize=10)
+    bp2.annotate("75%ile", [xb + 0.26 * xb, yb - 0.15 * yb], fontsize=10)
+    bp2.annotate("Outliers", [xw + 0.3 * xw, yw - 0.3 * yw], fontsize=10, color="r")
 
     if buoy_title:
         plt.suptitle(buoy_title, fontsize=30, y=0.97)
     bp.set_title("Significant Wave Height by Month", fontsize=20, y=1.01)
     bp2.set_title("Sample Boxplot", fontsize=10, y=1.02)
 
     # Set axes labels and ticks
-    months_text = [ m[:3] for m in Hs.index.month_name().unique()]
-    bp.set_xticklabels(months_text,fontsize=12)
-    bp.set_ylabel('Significant Wave Height, Hs (m)', fontsize=14)
-    bp.tick_params(axis='y', which='major', labelsize=12, right='off')
-    bp.tick_params(axis='x', which='major', labelsize=12, top='off')
+    months_text = [m[:3] for m in Hs.index.month_name().unique()]
+    bp.set_xticklabels(months_text, fontsize=12)
+    bp.set_ylabel("Significant Wave Height, Hs (m)", fontsize=14)
+    bp.tick_params(axis="y", which="major", labelsize=12, right="off")
+    bp.tick_params(axis="x", which="major", labelsize=12, top="off")
 
     # Plot horizontal gridlines onto top subplot
-    bp.grid(axis='x', color='b', linestyle='-', alpha=0.25)
+    bp.grid(axis="x", color="b", linestyle="-", alpha=0.25)
 
     # Remove tickmarks from bottom subplot
     bp2.axes.get_xaxis().set_visible(False)
     bp2.axes.get_yaxis().set_visible(False)
 
     ax = fig
 
     return ax
 
 
 def plot_directional_spectrum(
-        spectrum, 
-        min=None, 
-        fill=True, 
-        nlevels=11,
-        name="Elevation Variance", 
-        units="m^2"
-    ):
+    spectrum,
+    color_level_min=None,
+    fill=True,
+    nlevels=11,
+    name="Elevation Variance",
+    units="m^2",
+):
     """
     Create a contour polar plot of a directional spectrum.
 
     Parameters
     ------------
     spectrum: xarray.DataArray
         Spectral data indexed frequency [Hz] and wave direction [deg].
-    min: float (optional)
-        Minimum value to plot.
+    color_level_min: float (optional)
+        Minimum color bar level.
     fill: bool
         Whether to use `contourf` (filled) instead of `contour` (lines).
     nlevels: int
         Number of contour levels to plot.
     name: str
         Name of the (integral) spectrum variable.
     units: str
         Units of the (integral) spectrum variable.
 
     Returns
     ---------
     ax : matplotlib pyplot axes
     """
-    assert isinstance(spectrum, xr.DataArray), 'spectrum must be a DataArray'
-    if min is not None:
-        assert isinstance(min, float), 'min must be a float'
-    assert isinstance(fill, bool), 'fill must be a bool'
-    assert isinstance(nlevels, int), 'nlevels must be an int'
-    assert isinstance(name, str), 'name must be a string'
-    assert isinstance(units, str), 'units must be a string'
-
-    a,f = np.meshgrid(np.deg2rad(spectrum.direction), spectrum.frequency)
-    _, ax = plt.subplots(subplot_kw=dict(projection='polar'))
-    tmp = np.floor(np.min(spectrum.data)*10)/10
-    min = tmp if (min is None) else min
-    max = np.ceil(np.max(spectrum.data)*10)/10
-    levels = np.linspace(min, max, nlevels)
+    if not isinstance(spectrum, xr.DataArray):
+        raise TypeError(f"spectrum must be of type xr.DataArray. Got: {type(spectrum)}")
+    if not isinstance(color_level_min, (type(None), float)):
+        raise TypeError(
+            f"If specified, color_level_min must be of type float. Got: {type(color_level_min)}"
+        )
+    if not isinstance(fill, bool):
+        raise TypeError(f"If specified, fill must be of type bool. Got: {type(fill)}")
+    if not isinstance(nlevels, int):
+        raise TypeError(
+            f"If specified, nlevels must be of type int. Got: {type(nlevels)}"
+        )
+    if not isinstance(name, str):
+        raise TypeError(f"If specified, name must be of type string. Got: {type(name)}")
+    if not isinstance(units, str):
+        raise TypeError(
+            f"If specified, units must be of type string. Got: {type(units)}"
+        )
+
+    a, f = np.meshgrid(np.deg2rad(spectrum.direction), spectrum.frequency)
+    _, ax = plt.subplots(subplot_kw=dict(projection="polar"))
+    tmp = np.floor(np.min(spectrum.data) * 10) / 10
+    color_level_min = tmp if (color_level_min is None) else color_level_min
+    color_level_max = np.ceil(np.max(spectrum.data) * 10) / 10
+    levels = np.linspace(color_level_min, color_level_max, nlevels)
     if fill:
         c = ax.contourf(a, f, spectrum, levels=levels)
     else:
         c = ax.contour(a, f, spectrum, levels=levels)
     cbar = plt.colorbar(c)
-    cbar.set_label(f'Spectrum [{units}/Hz/deg]', rotation=270, labelpad=20)
-    ax.set_title(f'{name} Spectrum')
+    cbar.set_label(f"Spectrum [{units}/Hz/deg]", rotation=270, labelpad=20)
+    ax.set_title(f"{name} Spectrum")
     ylabels = ax.get_yticklabels()
     ylabels = [ilabel.get_text() for ilabel in ax.get_yticklabels()]
     ylabels = [ilabel + "Hz" for ilabel in ylabels]
     ticks_loc = ax.get_yticks()
     ax.set_yticks(ticks_loc)
     ax.set_yticklabels(ylabels)
     return ax
```

### Comparing `mhkit-0.7.0/mhkit/wave/io/cdip.py` & `mhkit-0.8.0/mhkit/wave/io/cdip.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,476 +1,630 @@
-from datetime import timezone
+import os
 import pandas as pd
 import numpy as np
 import datetime
 import netCDF4
-import time
 import pytz
+from mhkit.utils.cache import handle_caching
+from mhkit.utils import convert_nested_dict_and_pandas
+
 
 def _validate_date(date_text):
-    '''
+    """
     Checks date format to ensure YYYY-MM-DD format and return date in
     datetime format.
-    
+
     Parameters
     ----------
     date_text: string
         Date string format to check
-        
+
     Returns
     -------
     dt: datetime
-    '''  
-    assert isinstance(date_text, str), (f'date_text must be' / 
-                                              'of type string')
+    """
+
+    if not isinstance(date_text, str):
+        raise ValueError("date_text must be of type string. Got: {date_text}")
+
     try:
-        dt = datetime.datetime.strptime(date_text, '%Y-%m-%d')
+        dt = datetime.datetime.strptime(date_text, "%Y-%m-%d")
     except ValueError:
         raise ValueError("Incorrect data format, should be YYYY-MM-DD")
     else:
-        dt = dt.replace(tzinfo=timezone.utc)
-        
+        dt = dt.replace(tzinfo=datetime.timezone.utc)
+
     return dt
 
 
 def _start_and_end_of_year(year):
-    '''
+    """
     Returns a datetime start and end for a given year
-    
+
     Parameters
     ----------
     year: int
         Year to get start and end dates
-        
+
     Returns
     -------
     start_year: datetime object
         start of the year
     end_year: datetime object
-        end of the year    
-    '''
-    
-    assert isinstance(year, (type(None),int,list)), 'year must be of type int'
-    
+        end of the year
+    """
+
+    if not isinstance(year, (type(None), int, list)):
+        raise ValueError("year must be of type int, list, or None. Got: {type(year)}")
+
     try:
         year = str(year)
-        start_year = datetime.datetime.strptime(year, '%Y')
-    except ValueError:
-        raise ValueError("Incorrect years format, should be YYYY")
-    else:            
-        next_year = datetime.datetime.strptime(f'{int(year)+1}', '%Y')
+        start_year = datetime.datetime.strptime(year, "%Y")
+    except ValueError as exc:
+        raise ValueError("Incorrect years format, should be YYYY") from exc
+    else:
+        next_year = datetime.datetime.strptime(f"{int(year)+1}", "%Y")
         end_year = next_year - datetime.timedelta(days=1)
     return start_year, end_year
 
 
 def _dates_to_timestamp(nc, start_date=None, end_date=None):
-    '''
-    Returns timestamps from dates. 
-    
+    """
+    Returns timestamps from dates.
+
     Parameters
     ----------
     nc: netCDF Object
-        netCDF data for the given station number and data type   
-    start_date: string 
+        netCDF data for the given station number and data type
+    start_date: string
         Start date in YYYY-MM-DD, e.g. '2012-04-01'
-    end_date: string 
-        End date in YYYY-MM-DD, e.g. '2012-04-30'        
-        
+    end_date: string
+        End date in YYYY-MM-DD, e.g. '2012-04-30'
+
     Returns
     -------
     start_stamp: float
-         seconds since the Epoch to start_date    
+         seconds since the Epoch to start_date
     end_stamp: float
          seconds since the Epoch to end_date
-    '''
-    
-    assert isinstance(start_date, (str, type(None))), ('start_date' /
-        'must be of type str')
-    assert isinstance(end_date, (str, type(None))), ('end_date must be' / 
-        'of type str')
-        
-    time_all = nc.variables['waveTime'][:].compressed()
-    t_i=(datetime.datetime.fromtimestamp(time_all[0])
-          .astimezone(pytz.timezone('UTC')))
-    t_f=(datetime.datetime.fromtimestamp(time_all[-1])
-          .astimezone(pytz.timezone('UTC')))
+    """
+
+    if start_date and not isinstance(start_date, datetime.datetime):
+        raise ValueError(
+            f"start_date must be of type datetime.datetime or None. Got: {type(start_date)}"
+        )
+
+    if end_date and not isinstance(end_date, datetime.datetime):
+        raise ValueError(
+            f"end_date must be of type datetime.datetime or None. Got: {type(end_date)}"
+        )
+
+    time_all = nc.variables["waveTime"][:].compressed()
+    t_i = datetime.datetime.fromtimestamp(time_all[0]).astimezone(pytz.timezone("UTC"))
+    t_f = datetime.datetime.fromtimestamp(time_all[-1]).astimezone(pytz.timezone("UTC"))
     time_range_all = [t_i, t_f]
-    
-    if start_date:        
-        start_datetime = _validate_date(start_date)   
-    if end_date:
-        end_datetime = _validate_date(end_date)   
-        if start_datetime > end_datetime:
-            raise Exception(f'start_date ({start_datetime}) must be'+
-                f'before end_date ({end_datetime})')
-        elif start_datetime == end_datetime:
-            raise Exception(f'start_date ({start_datetime}) cannot be'+
-                f'the same as end_date ({end_datetime})')
-                
-    def to_timestamp(time):
-        stamp = (pd.to_datetime(time)
-                  .astimezone(pytz.timezone('UTC'))
-                  .timestamp())
-        return stamp
-    
+
     if start_date:
-        if start_datetime > time_range_all[0] and start_datetime < time_range_all[1]:
-            start_stamp = start_datetime.astimezone(pytz.timezone('UTC')).timestamp()
+        start_date = start_date.astimezone(pytz.UTC)
+        if start_date > time_range_all[0] and start_date < time_range_all[1]:
+            start_stamp = start_date.timestamp()
         else:
-            print(f'WARNING: Provided start_date ({start_datetime}) is ' 
-            f'not in the returned data range {time_range_all} \n' 
-            f'Setting start_date to the earliest date in range '
-            f'{time_range_all[0]}')
-            start_stamp = to_timestamp(time_range_all[0])
-    
+            print(
+                f"WARNING: Provided start_date ({start_date}) is "
+                f"not in the returned data range {time_range_all} \n"
+                f"Setting start_date to the earliest date in range "
+                f"{time_range_all[0]}"
+            )
+            start_stamp = time_range_all[0].timestamp()
+
     if end_date:
-        if end_datetime > time_range_all[0] and end_datetime < time_range_all[1]:
-            end_stamp = end_datetime.astimezone(pytz.timezone('UTC')).timestamp()
+        end_date = end_date.astimezone(pytz.UTC)
+        if end_date > time_range_all[0] and end_date < time_range_all[1]:
+            end_stamp = end_date.timestamp()
         else:
-            print(f'WARNING: Provided end_date ({end_datetime}) is ' 
-            f'not in the returned data range {time_range_all} \n' 
-            f'Setting end_date to the latest date in range '
-            f'{time_range_all[1]}')
-            end_stamp = to_timestamp(time_range_all[1])
-    
-    
+            print(
+                f"WARNING: Provided end_date ({end_date}) is "
+                f"not in the returned data range {time_range_all} \n"
+                f"Setting end_date to the latest date in range "
+                f"{time_range_all[1]}"
+            )
+            end_stamp = time_range_all[1].timestamp()
+
     if start_date and not end_date:
-        end_stamp = to_timestamp(time_range_all[1])
+        end_stamp = time_range_all[1].timestamp()
 
     elif end_date and not start_date:
-        start_stamp = to_timestamp(time_range_all[0])
-        
+        start_stamp = time_range_all[0].timestamp()
+
     if not start_date:
-        start_stamp = to_timestamp(time_range_all[0])
+        start_stamp = time_range_all[0].timestamp()
     if not end_date:
-        end_stamp = to_timestamp(time_range_all[1])
+        end_stamp = time_range_all[1].timestamp()
+
+    return start_stamp, end_stamp
 
-    return start_stamp, end_stamp 
 
-    
 def request_netCDF(station_number, data_type):
-    '''
+    """
     Returns historic or realtime data from CDIP THREDDS server
-   
+
     Parameters
     ----------
     station_number: string
         CDIP station number of interest
     data_type: string
         'historic' or 'realtime'
-   
+
     Returns
     -------
-    nc: netCDF Object
+    nc: xarray Dataset
         netCDF data for the given station number and data type
-    '''
-    assert isinstance(station_number, str), (f'station_number must be ' + 
-                                              f'of type string. Got: {station_number}')
-    assert isinstance(data_type, str), (f'data_type must be' / 
-                                              'of type string')
-    assert data_type in ['historic', 'realtime'], ('data_type must be'\
-        f' "historic" or "realtime". Got: {data_type}')                                              
-    if data_type == 'historic':
-        cdip_archive= 'http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/archive'
-        data_url =  f'{cdip_archive}/{station_number}p1/{station_number}p1_historic.nc'
-    elif data_type == 'realtime':
-        cdip_realtime = 'http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/realtime'
-        data_url = f'{cdip_realtime}/{station_number}p1_rt.nc'
-    
+    """
+
+    if not isinstance(station_number, (str, type(None))):
+        raise ValueError(
+            f"station_number must be of type string. Got: {type(station_number)}"
+        )
+
+    if not isinstance(data_type, str):
+        raise ValueError(f"data_type must be of type string. Got: {type(data_type)}")
+
+    if data_type not in ["historic", "realtime"]:
+        raise ValueError('data_type must be "historic" or "realtime". Got: {data_type}')
+
+    BASE_URL = "http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/"
+
+    if data_type == "historic":
+        data_url = (
+            f"{BASE_URL}archive/{station_number}p1/{station_number}p1_historic.nc"
+        )
+    else:  # data_type == 'realtime'
+        data_url = f"{BASE_URL}realtime/{station_number}p1_rt.nc"
+
     nc = netCDF4.Dataset(data_url)
-    
+
     return nc
 
-    
-def request_parse_workflow(nc=None, station_number=None, parameters=None, 
-               years=None, start_date=None, end_date=None, 
-               data_type='historic', all_2D_variables=False):
-    '''
-    Parses a passed CDIP netCDF file or requests a station number 
-    from http://cdip.ucsd.edu/) and parses. This function can return specific 
+
+def request_parse_workflow(
+    nc=None,
+    station_number=None,
+    parameters=None,
+    years=None,
+    start_date=None,
+    end_date=None,
+    data_type="historic",
+    all_2D_variables=False,
+    silent=False,
+    to_pandas=True,
+):
+    """
+    Parses a passed CDIP netCDF file or requests a station number
+    from http://cdip.ucsd.edu/) and parses. This function can return specific
     parameters is passed. Years may be non-consecutive e.g. [2001, 2010].
     Time may be sliced by dates (start_date or end date in YYYY-MM-DD).
     data_type defaults to historic but may also be set to 'realtime'.
     By default 2D variables are not parsed if all 2D varaibles are needed. See
-    the MHKiT CDiP example Jupyter notbook for information on available parameters. 
-    
-    
+    the MHKiT CDiP example Jupyter notbook for information on available parameters.
+
+
     Parameters
     ----------
     nc: netCDF Object
-        netCDF data for the given station number and data type. Can be the output of 
-        request_netCDF   
+        netCDF data for the given station number and data type. Can be the output of
+        request_netCDF
     station_number: string
         Station number of CDIP wave buoy
-    parameters: string or list of stings
+    parameters: string or list of strings
         Parameters to return. If None will return all varaibles except
-        2D-variables.        
+        2D-variables.
     years: int or list of int
-        Year date, e.g. 2001 or [2001, 2010]        
-    start_date: string 
+        Year date, e.g. 2001 or [2001, 2010]
+    start_date: string
         Start date in YYYY-MM-DD, e.g. '2012-04-01'
-    end_date: string 
+    end_date: string
         End date in YYYY-MM-DD, e.g. '2012-04-30'
     data_type: string
-        Either 'historic' or 'realtime'   
+        Either 'historic' or 'realtime'
     all_2D_variables: boolean
-        Will return all 2D data. Enabling this will add significant 
+        Will return all 2D data. Enabling this will add significant
         processing time. If all 2D variables are not needed it is
-        recomended to pass 2D parameters of interest using the 
+        recomended to pass 2D parameters of interest using the
         'parameters' keyword and leave this set to False. Default False.
-    
+    silent: boolean
+        Set to True to prevent the print statement that announces when 2D
+        variable processing begins. Default False.
+    to_pandas: bool (optional)
+        Flag to output a dictionary of pandas objects instead of a dictionary
+        of xarray objects. Default = True.
+
+
     Returns
     -------
     data: dictionary
-        'vars1D': DataFrame
-            1D variables indexed by time    
+        'data': dictionary of variables
+            'vars': pandas DataFrame or xarray Dataset
+                1D variables indexed by time
+            'vars2D': dictionary of DataFrames or Datasets, optional
+                If 2D-vars are passed in the 'parameters key' or if run
+                with all_2D_variables=True, then this key will appear
+                with a dictonary of DataFrames of 2D variables.
         'metadata': dictionary
             Anything not of length time
-        'vars2D': dictionary of DataFrames, optional
-            If 2D-vars are passed in the 'parameters key' or if run 
-            with all_2D_variables=True, then this key will appear 
-            with a dictonary of DataFrames of 2D variables.     
-    '''
-    assert isinstance(station_number, (str, type(None))), (f'station_number must be '+     
-                                              'of type string')
-    assert isinstance(parameters, (str, type(None), list)), ('parameters' /
-        'must be of type str or list of strings')
-    assert isinstance(start_date, (str, type(None))), ('start_date' /
-        'must be of type str')
-    assert isinstance(end_date, (str, type(None))), ('end_date must be' / 
-        'of type str')
-    assert isinstance(years, (type(None),int,list)), ('years must be of'/
-        'type int or list of ints')
-    assert isinstance(data_type, str), (f'data_type must be' / 
-                                              'of type string')        
-    assert data_type in ['historic', 'realtime'], 'data_type must be'\
-        f' "historic" or "realtime". Got: {data_type}'
+    """
+    if not isinstance(station_number, (str, type(None))):
+        raise TypeError(
+            f"station_number must be of type string. Got: {type(station_number)}"
+        )
+
+    if not isinstance(parameters, (str, type(None), list)):
+        raise TypeError(
+            f"parameters must be of type str or list of strings. Got: {type(parameters)}"
+        )
+
+    if start_date is not None:
+        if isinstance(start_date, str):
+            try:
+                start_date = datetime.datetime.strptime(start_date, "%Y-%m-%d")
+                start_date = start_date.replace(tzinfo=pytz.UTC)
+            except ValueError as exc:
+                raise ValueError("Incorrect data format, should be YYYY-MM-DD") from exc
+        else:
+            raise TypeError(f"start_date must be of type str. Got: {type(start_date)}")
+
+    if end_date is not None:
+        if isinstance(end_date, str):
+            try:
+                end_date = datetime.datetime.strptime(end_date, "%Y-%m-%d")
+                end_date = end_date.replace(tzinfo=pytz.UTC)
+            except ValueError as exc:
+                raise ValueError("Incorrect data format, should be YYYY-MM-DD") from exc
+        else:
+            raise TypeError(f"end_date must be of type str. Got: {type(end_date)}")
+
+    if not isinstance(years, (type(None), int, list)):
+        raise TypeError(
+            f"years must be of type int or list of ints. Got: {type(years)}"
+        )
+
+    if not isinstance(data_type, str):
+        raise TypeError(f"data_type must be of type string. Got: {type(data_type)}")
+
+    if data_type not in ["historic", "realtime"]:
+        raise ValueError(
+            f'data_type must be "historic" or "realtime". Got: {data_type}'
+        )
 
-  
     if not any([nc, station_number]):
-        raise Exception('Must provide either a CDIP netCDF file or a station '+ 
-            'number')
-   
+        raise ValueError("Must provide either a CDIP netCDF file or a station number.")
+
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
     if not nc:
         nc = request_netCDF(station_number, data_type)
-    
-    buoy_name = nc.variables['metaStationName'][:].compressed().tobytes().decode("utf-8")
-    
-    
-    multiyear=False
+
+    # Define the path to the cache directory
+    cache_dir = os.path.join(os.path.expanduser("~"), ".cache", "mhkit", "cdip")
+
+    buoy_name = (
+        nc.variables["metaStationName"][:].compressed().tobytes().decode("utf-8")
+    )
+
+    multiyear = False
     if years:
-        if isinstance(years,int):
-            start_date = f'{years}-01-01'
-            end_date = f'{years+1}-01-01'            
-        elif isinstance(years,list):
-            if len(years)==1:
-                start_date = f'{years[0]}-01-01'
-                end_date = f'{years[0]+1}-01-01'
+        if isinstance(years, int):
+            start_date = datetime.datetime(years, 1, 1, tzinfo=pytz.UTC)
+            end_date = datetime.datetime(years + 1, 1, 1, tzinfo=pytz.UTC)
+        elif isinstance(years, list):
+            if len(years) == 1:
+                start_date = datetime.datetime(years[0], 1, 1, tzinfo=pytz.UTC)
+                end_date = datetime.datetime(years[0] + 1, 1, 1, tzinfo=pytz.UTC)
             else:
-                multiyear=True
-
+                multiyear = True
     if not multiyear:
-        data = get_netcdf_variables(nc, 
-                       start_date=start_date, end_date=end_date, 
-                       parameters=parameters, 
-                       all_2D_variables=all_2D_variables)  
-
-    elif multiyear:
-        data={'data':{},'metadata':{}}
-        multiyear_data={}
-        multiyear_data_2D={}
-        for year in years: 
-            start_date = f'{year}-01-01'
-            end_date = f'{year+1}-01-01'
-            
-            year_data = get_netcdf_variables(nc, 
-                       start_date=start_date, end_date=end_date,  
-                       parameters=parameters, 
-                       all_2D_variables=all_2D_variables) 
-            multiyear_data[year] = year_data['data']
-          
-        for data_key in year_data['data'].keys():
-            if data_key.endswith('2D'):
-                data['data'][data_key]={}
-                for data_key2D in year_data['data'][data_key].keys():
-                    data_list=[]
-                    for year in years:    
+        # Check the cache first
+        hash_params = f"{station_number}-{parameters}-{start_date}-{end_date}"
+        data = handle_caching(hash_params, cache_dir)
+
+        if data[:2] == (None, None):
+            data = get_netcdf_variables(
+                nc,
+                start_date=start_date,
+                end_date=end_date,
+                parameters=parameters,
+                all_2D_variables=all_2D_variables,
+                silent=silent,
+            )
+            handle_caching(hash_params, cache_dir, data=data)
+        else:
+            data = data[0]
+
+    else:
+        data = {"data": {}, "metadata": {}}
+        multiyear_data = {}
+        for year in years:
+            start_date = datetime.datetime(year, 1, 1, tzinfo=pytz.UTC)
+            end_date = datetime.datetime(year + 1, 1, 1, tzinfo=pytz.UTC)
+
+            # Check the cache for each individual year
+            hash_params = f"{station_number}-{parameters}-{start_date}-{end_date}"
+            year_data = handle_caching(hash_params, cache_dir)
+            if year_data[:2] == (None, None):
+                year_data = get_netcdf_variables(
+                    nc,
+                    start_date=start_date,
+                    end_date=end_date,
+                    parameters=parameters,
+                    all_2D_variables=all_2D_variables,
+                    silent=silent,
+                )
+                # Cache the individual year's data
+                handle_caching(hash_params, cache_dir, data=year_data)
+            else:
+                year_data = year_data[0]
+            multiyear_data[year] = year_data["data"]
+
+        for data_key in year_data["data"].keys():
+            if data_key.endswith("2D"):
+                data["data"][data_key] = {}
+                for data_key2D in year_data["data"][data_key].keys():
+                    data_list = []
+                    for year in years:
                         data2D = multiyear_data[year][data_key][data_key2D]
                         data_list.append(data2D)
-                    data['data'][data_key][data_key2D]=pd.concat(data_list)
-            else:                
+                    data["data"][data_key][data_key2D] = pd.concat(data_list)
+            else:
                 data_list = [multiyear_data[year][data_key] for year in years]
-                data['data'][data_key] = pd.concat(data_list)
-
+                data["data"][data_key] = pd.concat(data_list)
 
-                
+    if buoy_name:
+        try:
+            data.setdefault("metadata", {})["name"] = buoy_name
+        except:
+            pass
 
-        data['metadata'] = year_data['metadata']
-    data['metadata']['name'] = buoy_name    
+    if not to_pandas:
+        data = convert_nested_dict_and_pandas(data)
 
     return data
-    
-    
-def get_netcdf_variables(nc, start_date=None, end_date=None, 
-                         parameters=None, all_2D_variables=False):
-    '''
+
+
+def get_netcdf_variables(
+    nc,
+    start_date=None,
+    end_date=None,
+    parameters=None,
+    all_2D_variables=False,
+    silent=False,
+    to_pandas=True,
+):
+    """
     Iterates over and extracts variables from CDIP bouy data. See
-    the MHKiT CDiP example Jupyter notbook for information on available 
-    parameters. 
-    
-    
+    the MHKiT CDiP example Jupyter notbook for information on available
+    parameters.
+
     Parameters
     ----------
     nc: netCDF Object
         netCDF data for the given station number and data type
     start_stamp: float
         Data of interest start in seconds since epoch
     end_stamp: float
-        Data of interest end in seconds since epoch  
-    parameters: string or list of stings
+        Data of interest end in seconds since epoch
+    parameters: string or list of strings
         Parameters to return. If None will return all varaibles except
         2D-variables. Default None.
     all_2D_variables: boolean
-        Will return all 2D data. Enabling this will add significant 
+        Will return all 2D data. Enabling this will add significant
         processing time. If all 2D variables are not needed it is
-        recomended to pass 2D parameters of interest using the 
+        recomended to pass 2D parameters of interest using the
         'parameters' keyword and leave this set to False. Default False.
+    silent: boolean
+        Set to True to prevent the print statement that announces when 2D
+        variable processing begins. Default False.
+    to_pandas: bool (optional)
+        Flag to output a dictionary of pandas objects instead of a dictionary
+        of xarray objects. Default = True.
+
 
     Returns
     -------
     results: dictionary
-        'vars1D': DataFrame
-            1D variables indexed by time    
+        'data': dictionary of variables
+            'vars': pandas DataFrame or xarray Dataset
+                1D variables indexed by time
+            'vars2D': dictionary of DataFrames or Datasets, optional
+                If 2D-vars are passed in the 'parameters key' or if run
+                with all_2D_variables=True, then this key will appear
+                with a dictonary of DataFrames/Datasets of 2D variables.
         'metadata': dictionary
             Anything not of length time
-        'vars2D': dictionary of DataFrames, optional
-            If 2D-vars are passed in the 'parameters key' or if run 
-            with all_2D_variables=True, then this key will appear 
-            with a dictonary of DataFrames of 2D variables.
-    '''
-    
-    assert isinstance(nc, netCDF4.Dataset), 'nc must be netCDF4 dataset'
-    assert isinstance(start_date, (str, type(None))), ('start_date' /
-        'must be of type str')
-    assert isinstance(end_date, (str, type(None))), ('end_date must be' / 
-        'of type str')
-    assert isinstance(parameters, (str, type(None), list)), ('parameters' /
-        'must be of type str or list of strings')        
-    assert isinstance(all_2D_variables, bool), ('all_2D_variables'/
-        'must be a boolean')
+    """
 
-    if parameters:
-        if isinstance(parameters,str):
-            parameters = [parameters]        
-        assert all([isinstance(param , str) for param in parameters]), ('All'/
-           'elements of parameters must be strings')
+    if not isinstance(nc, netCDF4.Dataset):
+        raise TypeError("nc must be netCDF4 dataset. Got: {type(nc)}")
+
+    if start_date and isinstance(start_date, str):
+        start_date = datetime.datetime.strptime(start_date, "%Y-%m-%d")
 
+    if end_date and isinstance(end_date, str):
+        end_date = datetime.datetime.strptime(end_date, "%Y-%m-%d")
+
+    if not isinstance(parameters, (str, type(None), list)):
+        raise TypeError(
+            "parameters must be of type str or list of strings. Got: {type(parameters)}"
+        )
+
+    if not isinstance(all_2D_variables, bool):
+        raise TypeError(
+            "all_2D_variables must be a boolean. Got: {type(all_2D_variables)}"
+        )
+
+    if parameters:
+        if isinstance(parameters, str):
+            parameters = [parameters]
+        for param in parameters:
+            if not isinstance(param, str):
+                raise TypeError("All elements of parameters must be strings.")
+
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
+    buoy_name = (
+        nc.variables["metaStationName"][:].compressed().tobytes().decode("utf-8")
+    )
 
-    buoy_name = nc.variables['metaStationName'][:].compressed().tobytes().decode("utf-8")           
     allVariables = [var for var in nc.variables]
-    
-    include_2D_variables=False
-    twoDimensionalVars = [ 'waveEnergyDensity', 'waveMeanDirection', 
-                           'waveA1Value', 'waveB1Value', 'waveA2Value', 
-                           'waveB2Value', 'waveCheckFactor', 'waveSpread', 
-                           'waveM2Value', 'waveN2Value']  
-    
+    allVariableSet = set(allVariables)
+
+    twoDimensionalVars = [
+        "waveEnergyDensity",
+        "waveMeanDirection",
+        "waveA1Value",
+        "waveB1Value",
+        "waveA2Value",
+        "waveB2Value",
+        "waveCheckFactor",
+        "waveSpread",
+        "waveM2Value",
+        "waveN2Value",
+    ]
+    twoDimensionalVarsSet = set(twoDimensionalVars)
+
+    # If parameters are provided, convert them into a set
     if parameters:
         params = set(parameters)
-        include_params = params.intersection(set(allVariables))            
-        if params != include_params:
-           not_found = params.difference(include_params)
-           print(f'WARNING: {not_found} was not found in data.\n' \
-                 f'Possible parameters are:\n {allVariables}')
-                
-        include_params_2D = include_params.intersection(
-                                set(twoDimensionalVars))                
-        include_params = include_params.difference(include_params_2D)
-        
-        if include_params_2D:
-            include_2D_variables=True
-            include_params.add('waveFrequency')
-            include_2D_vars = sorted(include_params_2D)
-        
-        include_vars = sorted(include_params)
-            
     else:
-        include_vars = allVariables
-        
-        for var in twoDimensionalVars:
-            include_vars.remove(var)
-            
-        if all_2D_variables:
-            include_2D_variables=True
-            include_2D_vars = twoDimensionalVars                 
-
-    
-    start_stamp, end_stamp =_dates_to_timestamp(nc, start_date=start_date, 
-                                                 end_date=end_date)
-    
-    variables_by_type={}       
-    prefixs = ['wave', 'sst', 'gps', 'dwr', 'meta']
-    remainingVariables = set(include_vars)
-    for prefix in prefixs:
-        variables_by_type[prefix] = [var for var in include_vars 
-            if var.startswith(prefix)]
-        remainingVariables -= set(variables_by_type[prefix])
-        if not variables_by_type[prefix]:
-            del variables_by_type[prefix]
+        params = set()
+
+    # If all_2D_variables is True, add all 2D variables to params
+    if all_2D_variables:
+        params.update(twoDimensionalVarsSet)
+
+    include_params = params & allVariableSet
+    if params != include_params:
+        not_found = params - include_params
+        print(
+            f"WARNING: {not_found} was not found in data.\n"
+            f"Possible parameters are:\n {allVariables}"
+        )
+
+    include_params_2D = include_params & twoDimensionalVarsSet
+    include_params -= include_params_2D
+
+    include_2D_variables = bool(include_params_2D)
+    if include_2D_variables:
+        include_params.add("waveFrequency")
+
+    include_vars = include_params
+
+    # when parameters is None and all_2D_variables is False
+    if not parameters and not all_2D_variables:
+        include_vars = allVariableSet - twoDimensionalVarsSet
+
+    start_stamp, end_stamp = _dates_to_timestamp(
+        nc, start_date=start_date, end_date=end_date
+    )
+
+    prefixs = ["wave", "sst", "gps", "dwr", "meta"]
+    variables_by_type = {
+        prefix: [var for var in include_vars if var.startswith(prefix)]
+        for prefix in prefixs
+    }
+    variables_by_type = {
+        prefix: vars for prefix, vars in variables_by_type.items() if vars
+    }
 
-    results={'data':{}, 'metadata':{}}
+    results = {"data": {}, "metadata": {}}
     for prefix in variables_by_type:
-        var_results={}
-        time_variables={}
-        metadata={}
-        
-        if prefix != 'meta':
-            prefixTime = nc.variables[f'{prefix}Time'][:]
-            
-            masked_time = np.ma.masked_outside(prefixTime, start_stamp,
-            end_stamp)
-            mask = masked_time.mask                               
-            var_time = masked_time.compressed() 
+        time_variables = {}
+        metadata = {}
+
+        if prefix != "meta":
+            prefixTime = nc.variables[f"{prefix}Time"][:]
+
+            masked_time = np.ma.masked_outside(prefixTime, start_stamp, end_stamp)
+            mask = masked_time.mask
+            var_time = masked_time.compressed()
             N_time = masked_time.size
-        else:
-            N_time= np.nan
-    
-        for var in variables_by_type[prefix]:   
-            variable = np.ma.filled(nc.variables[var])
-            if variable.size == N_time:              
-                variable = np.ma.masked_array(variable, mask).astype(float)
-                time_variables[var] = variable.compressed()
-            else:
-                metadata[var] = nc.variables[var][:].compressed()
 
-        time_slice = pd.to_datetime(var_time, unit='s')
-        data = pd.DataFrame(time_variables, index=time_slice)        
-         
-        if prefix != 'meta':      
-            results['data'][prefix] = data
-            results['data'][prefix].name = buoy_name
-        results['metadata'][prefix] = metadata
-            
-        if (prefix == 'wave') and (include_2D_variables):
-            
-            print('Processing 2D Variables:')
-            vars2D={}
-            columns=metadata['waveFrequency']
-            N_time= len(time_slice)
+            for var in variables_by_type[prefix]:
+                variable = np.ma.filled(nc.variables[var])
+                if variable.size == N_time:
+                    variable = np.ma.masked_array(variable, mask).astype(float)
+                    time_variables[var] = variable.compressed()
+                else:
+                    metadata[var] = nc.variables[var][:].compressed()
+
+            time_slice = pd.to_datetime(var_time, unit="s")
+            data = pd.DataFrame(time_variables, index=time_slice)
+            results["data"][prefix] = data
+            results["data"][prefix].name = buoy_name
+
+        results["metadata"][prefix] = metadata
+
+        if (prefix == "wave") and (include_2D_variables):
+            if not silent:
+                print("Processing 2D Variables:")
+
+            vars2D = {}
+            columns = metadata["waveFrequency"]
+            N_time = len(time_slice)
             N_frequency = len(columns)
             try:
                 l = len(mask)
             except:
                 mask = np.array([False] * N_time)
-                
-            mask2D= np.tile(mask, (len(columns),1)).T
-            for var in include_2D_vars:
+
+            mask2D = np.tile(mask, (len(columns), 1)).T
+            for var in include_params_2D:
                 variable2D = nc.variables[var][:].data
                 variable2D = np.ma.masked_array(variable2D, mask2D)
-                variable2D = variable2D.compressed().reshape(N_time, N_frequency)            
-                variable = pd.DataFrame(variable2D,index=time_slice,
-                                        columns=columns)
+                variable2D = variable2D.compressed().reshape(N_time, N_frequency)
+                variable = pd.DataFrame(variable2D, index=time_slice, columns=columns)
                 vars2D[var] = variable
-            results['data']['wave2D'] = vars2D
-    results['metadata']['name'] = buoy_name
-        
+            results["data"]["wave2D"] = vars2D
+    results["metadata"]["name"] = buoy_name
+
+    if not to_pandas:
+        results = convert_nested_dict_and_pandas(results)
+
     return results
+
+
+def _process_multiyear_data(nc, years, parameters, all_2D_variables):
+    """
+    A helper function to process multiyear data.
+
+    Parameters
+    ----------
+    nc : netCDF4.Dataset
+        netCDF file containing the data
+    years : list of int
+        A list of years to process
+    parameters : list of str
+        A list of parameters to return
+    all_2D_variables : bool
+        Whether to return all 2D variables
+
+    Returns
+    -------
+    data : dict
+        A dictionary containing the processed data
+    """
+
+    data = {}
+    for year in years:
+        start_date = datetime.datetime(year, 1, 1)
+        end_date = datetime.datetime(year + 1, 1, 1)
+
+        year_data = get_netcdf_variables(
+            nc,
+            start_date=start_date,
+            end_date=end_date,
+            parameters=parameters,
+            all_2D_variables=all_2D_variables,
+        )
+        data[year] = year_data
+
+    return data
```

### Comparing `mhkit-0.7.0/mhkit/wave/io/swan.py` & `mhkit-0.8.0/mhkit/wave/io/swan.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,295 +1,355 @@
 from scipy.io import loadmat
 from os.path import isfile
 import pandas as pd
+import xarray as xr
 import numpy as np
-import re 
-  
+import re
+from mhkit.utils import convert_to_dataset, convert_nested_dict_and_pandas
 
-def read_table(swan_file):
-    '''
+
+def read_table(swan_file, to_pandas=True):
+    """
     Reads in SWAN table format output
-    
+
     Parameters
     ----------
     swan_file: str
         filename to import
-        
+    to_pandas: bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
+
     Returns
     -------
-    swan_data: DataFrame
+    swan_data: pandas DataFrame or xarray Dataset
         Dataframe of swan output
     metaDict: Dictionary
         Dictionary of metaData
-    '''
-    assert isinstance(swan_file, str), 'swan_file must be of type str'
-    assert isfile(swan_file)==True, f'File not found: {swan_file}'
-    
-    f = open(swan_file,'r')
+    """
+    if not isinstance(swan_file, str):
+        raise TypeError(f"swan_file must be of type str. Got: {type(swan_file)}")
+    if not isfile(swan_file):
+        raise ValueError(f"File not found: {swan_file}")
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
+    f = open(swan_file, "r")
     header_line_number = 4
-    for i in range(header_line_number+2):
+    for i in range(header_line_number + 2):
         line = f.readline()
-        if line.startswith('% Run'):
+        if line.startswith("% Run"):
             metaDict = _parse_line_metadata(line)
-            if metaDict['Table'].endswith('SWAN'):
-                metaDict['Table'] = metaDict['Table'].split(' SWAN')[:-1]    
-        if  i == header_line_number:         
-            header = re.split("\s+",line.rstrip().strip('%').lstrip())
-            metaDict['header'] = header
-        if i == header_line_number+1:
-            units = re.split('\s+',line.strip(' %\n').replace('[','').replace(']',''))
-            metaDict['units'] = units
-    f.close()    
-    
-    swan_data = pd.read_csv(swan_file, sep='\s+', comment='%', 
-                            names=metaDict['header'])                
-    return swan_data, metaDict    
-
-
-def read_block(swan_file):
-    '''
-    Reads in SWAN block output with headers and creates a dictionary 
-    of DataFrames for each SWAN output variable in the output file.
-    
+            if metaDict["Table"].endswith("SWAN"):
+                metaDict["Table"] = metaDict["Table"].split(" SWAN")[:-1]
+        if i == header_line_number:
+            header = re.split("\s+", line.rstrip().strip("%").lstrip())
+            metaDict["header"] = header
+        if i == header_line_number + 1:
+            units = re.split(
+                "\s+", line.strip(" %\n").replace("[", "").replace("]", "")
+            )
+            metaDict["units"] = units
+    f.close()
+
+    swan_data = pd.read_csv(swan_file, sep="\s+", comment="%", names=metaDict["header"])
+
+    if not to_pandas:
+        swan_data = convert_to_dataset(swan_data)
+
+    return swan_data, metaDict
+
+
+def read_block(swan_file, to_pandas=True):
+    """
+    Reads in SWAN block output with headers and creates a dictionary
+    of DataFrames or Datasets for each SWAN output variable in the output file.
+
     Parameters
     ----------
     swan_file: str
         swan block file to import
-        
+    to_pandas: bool (optional)
+        Flag to output a dictionary of pandas objects instead of a dictionary
+        of xarray objects. Default = True.
+
     Returns
     -------
     data: Dictionary
-        Dictionary of DataFrame of swan output variables  
+        Dictionary of DataFrames or Datasets of swan output variables
     metaDict: Dictionary
-        Dictionary of metaData dependent on file type    
-    '''
-    assert isinstance(swan_file, str), 'swan_file must be of type str'
-    assert isfile(swan_file)==True, f'File not found: {swan_file}'
-    
-    extension = swan_file.split('.')[1].lower()
-    if extension == 'mat':
+        Dictionary of metaData dependent on file type
+    """
+    if not isinstance(swan_file, str):
+        raise TypeError(f"swan_file must be of type str. Got: {type(swan_file)}")
+    if not isfile(swan_file):
+        raise ValueError(f"File not found: {swan_file}")
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
+    extension = swan_file.split(".")[1].lower()
+    if extension == "mat":
         dataDict = _read_block_mat(swan_file)
-        metaData = {'filetype': 'mat',
-                    'variables': [var for var in dataDict.keys()]}
+        metaData = {"filetype": "mat", "variables": [var for var in dataDict.keys()]}
     else:
         dataDict, metaData = _read_block_txt(swan_file)
+
+    if not to_pandas:
+        dataDict = convert_nested_dict_and_pandas(dataDict)
+
     return dataDict, metaData
-    
+
 
 def _read_block_txt(swan_file):
-    '''
-    Reads in SWAN block output with headers and creates a dictionary 
+    """
+    Reads in SWAN block output with headers and creates a dictionary
     of DataFrames for each SWAN output variable in the output file.
-    
+
     Parameters
     ----------
     swan_file: str
         swan block file to import (must be written with headers)
-        
+
     Returns
     -------
     dataDict: Dictionary
         Dictionary of DataFrame of swan output variables
     metaDict: Dictionary
-        Dictionary of metaData dependent on file type    
-    '''
-    assert isinstance(swan_file, str), 'swan_file must be of type str'
-    assert isfile(swan_file)==True, f'File not found: {swan_file}'
-    
-    f = open(swan_file) 
-    runLines=[]
+        Dictionary of metaData dependent on file type
+    """
+    if not isinstance(swan_file, str):
+        raise TypeError(f"swan_file must be of type str. Got: {type(swan_file)}")
+    if not isfile(swan_file):
+        raise ValueError(f"File not found: {swan_file}")
+
+    f = open(swan_file)
+    runLines = []
     metaDict = {}
     column_position = None
-    dataDict={}
+    dataDict = {}
     for position, line in enumerate(f):
-        
-        if line.startswith('% Run'):
+        if line.startswith("% Run"):
             varPosition = position
             runLines.extend([position])
-            column_position = position + 5                       
-            varDict = _parse_line_metadata(line)         
-            varDict['unitMultiplier'] = float(varDict['Unit'].split(' ')[0])
-            
-            metaDict[varPosition] = varDict            
-            variable = varDict['vars']
+            column_position = position + 5
+            varDict = _parse_line_metadata(line)
+            varDict["unitMultiplier"] = float(varDict["Unit"].split(" ")[0])
+
+            metaDict[varPosition] = varDict
+            variable = varDict["vars"]
             dataDict[variable] = {}
-            
-        if position==column_position and column_position!=None:
-           columns = line.strip('% \n').split()
-           metaDict[varPosition]['cols'] = columns
-           N_columns = len(columns)
-           columns_position = None
-           
-        
-        if not line.startswith('%'):
-            raw_data = ' '.join(re.split(' |\.', line.strip(' \n'))).split()
+
+        if position == column_position and column_position != None:
+            columns = line.strip("% \n").split()
+            metaDict[varPosition]["cols"] = columns
+            N_columns = len(columns)
+            columns_position = None
+
+        if not line.startswith("%"):
+            raw_data = " ".join(re.split(" |\.", line.strip(" \n"))).split()
             index_number = int(raw_data[0])
             columns_data = raw_data[1:]
-            data=[]
-            possibleNaNs = ['****']
+            data = []
+            possibleNaNs = ["****"]
             NNaNsTotal = sum([line.count(nanVal) for nanVal in possibleNaNs])
-            
-            if NNaNsTotal>0:
+
+            if NNaNsTotal > 0:
                 for vals in columns_data:
-                    NNaNs = 0                                      
+                    NNaNs = 0
                     for nanVal in possibleNaNs:
                         NNaNs += vals.count(nanVal)
                     if NNaNs > 0:
                         for i in range(NNaNs):
-                            data.extend([np.nan]) 
+                            data.extend([np.nan])
                     else:
                         data.extend([float(vals)])
-            else:                
-                data.extend([float(val) for val in columns_data])             
-                
+            else:
+                data.extend([float(val) for val in columns_data])
+
             dataDict[variable][index_number] = data
-                
-    metaData = pd.DataFrame(metaDict).T        
+
+    metaData = pd.DataFrame(metaDict).T
     f.close()
-    
-    for var in metaData.vars.values: 
-        df = pd.DataFrame(dataDict[var]).T        
-        varCols =  metaData[metaData.vars == var].cols.values.tolist()[0]
+
+    for var in metaData.vars.values:
+        df = pd.DataFrame(dataDict[var]).T
+        varCols = metaData[metaData.vars == var].cols.values.tolist()[0]
         colsDict = dict(zip(df.columns.values.tolist(), varCols))
         df.rename(columns=colsDict)
         unitMultiplier = metaData[metaData.vars == var].unitMultiplier.values[0]
-        dataDict[var] = df * unitMultiplier 
-    
-    metaData.pop('cols')
-    metaData = metaData.set_index('vars').T.to_dict()           
-    return dataDict, metaData              
-    
+        dataDict[var] = df * unitMultiplier
+
+    metaData.pop("cols")
+    metaData = metaData.set_index("vars").T.to_dict()
+    return dataDict, metaData
+
 
 def _read_block_mat(swan_file):
-    '''
+    """
     Reads in SWAN matlab output and creates a dictionary of DataFrames
     for each swan output variable.
-    
+
     Parameters
     ----------
     swan_file: str
         filename to import
-        
+
     Returns
     -------
     dataDict: Dictionary
         Dictionary of DataFrame of swan output variables
-    '''
-    assert isinstance(swan_file, str), 'swan_file must be of type str'
-    assert isfile(swan_file)==True, f'File not found: {swan_file}'
-        
+    """
+    if not isinstance(swan_file, str):
+        raise TypeError(f"swan_file must be of type str. Got: {type(swan_file)}")
+    if not isfile(swan_file):
+        raise ValueError(f"File not found: {swan_file}")
+
     dataDict = loadmat(swan_file, struct_as_record=False, squeeze_me=True)
-    removeKeys = ['__header__', '__version__', '__globals__']
+    removeKeys = ["__header__", "__version__", "__globals__"]
     for key in removeKeys:
         dataDict.pop(key, None)
     for key in dataDict.keys():
         dataDict[key] = pd.DataFrame(dataDict[key])
     return dataDict
-   
-    
+
+
 def _parse_line_metadata(line):
-    '''
+    """
     Parses the variable metadata into a dictionary
-    
+
     Parameters
     ----------
     line: str
         line from block swan data to parse
-        
+
     Returns
     -------
     metaDict: Dictionary
         Dictionary of variable metadata
-    '''
-    assert isinstance(line, str), 'line must be of type str'
-    
-    metaDict={}        
-    meta=re.sub('\s+', " ", line.replace(',', ' ').strip('% \n').replace('**', 'vars:'))
-    mList = meta.split(':')
-    elms = [elm.split(' ') for elm in mList]
+    """
+    if not isinstance(line, str):
+        raise TypeError(f"line must be of type str. Got: {type(line)}")
+
+    metaDict = {}
+    meta = re.sub(
+        "\s+", " ", line.replace(",", " ").strip("% \n").replace("**", "vars:")
+    )
+    mList = meta.split(":")
+    elms = [elm.split(" ") for elm in mList]
     for elm in elms:
         try:
-            elm.remove('')
+            elm.remove("")
         except:
-            pass                
-    for i in range(len(elms)-1):
+            pass
+    for i in range(len(elms) - 1):
         elm = elms[i]
         key = elm[-1]
-        val = ' '.join(elms[i+1][:-1])
+        val = " ".join(elms[i + 1][:-1])
         metaDict[key] = val
-    metaDict[key] = ' '.join(elms[-1]) 
-        
-    return metaDict    
+    metaDict[key] = " ".join(elms[-1])
 
+    return metaDict
+
+
+def dictionary_of_block_to_table(dictionary_of_DataFrames, names=None, to_pandas=True):
+    """
+    Converts a dictionary of structured 2D grid SWAN block format
+    x (columns),y (index) to SWAN table format x (column),y (column),
+    values (column) DataFrame or Dataset.
 
-def dictionary_of_block_to_table(dictionary_of_DataFrames, names=None):
-    '''
-    Converts a dictionary of structured 2D grid SWAN block format 
-    x (columns),y (index) to SWAN table format x (column),y (column), 
-    values (column)  DataFrame.
-    
     Parameters
     ----------
-    dictionary_of_DataFrames: Dictionary 
+    dictionary_of_DataFrames: Dictionary
         Dictionary of DataFrames in with columns as X indicie and Y as index.
     names: List (Optional)
         Name of data column in returned table. Default=Dictionary.keys()
+    to_pandas: bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
+
     Returns
     -------
-    swanTables: DataFrame
-        DataFrame with columns x,y,values where values = Dictionary.keys()
-        or names        
-    '''
-    assert isinstance(dictionary_of_DataFrames, dict), (
-                          'dictionary_of_DataFrames must be of type Dict')
-    assert bool(dictionary_of_DataFrames), 'dictionary_of_DataFrames is empty'
-    for key in dictionary_of_DataFrames:  
-        assert isinstance(dictionary_of_DataFrames[key],pd.DataFrame), (
-                          f'Dictionary key:{key} must be of type pd.DataFrame')
+    swanTables: pandas DataFrame or xarray Dataset
+        DataFrame/Dataset with columns x,y,values where values = Dictionary.keys()
+        or names
+    """
+    if not isinstance(dictionary_of_DataFrames, dict):
+        raise TypeError(
+            f"dictionary_of_DataFrames must be of type dict. Got: {type(dictionary_of_DataFrames)}"
+        )
+    if not bool(dictionary_of_DataFrames):
+        raise ValueError(
+            f"dictionary_of_DataFrames is empty. Got: {dictionary_of_DataFrames}"
+        )
+    for key in dictionary_of_DataFrames:
+        if not isinstance(dictionary_of_DataFrames[key], pd.DataFrame):
+            raise TypeError(
+                f"Dictionary key:{key} must be of type pd.DataFrame. Got: {type(dictionary_of_DataFrames[key])}"
+            )
     if not isinstance(names, type(None)):
-        assert isinstance(names, list), (
-                'If specified names must be of type list')         
-        assert all([isinstance(elm, str) for elm in names]), (
-                'If specified all elements in names must be of type string')
-        assert len(names) == len(dictionary_of_DataFrames), (
-                'If specified names must the same length as dictionary_of_DataFrames')
-    
+        if not isinstance(names, list):
+            raise TypeError(
+                f"If specified, names must be of type list. Got: {type(names)}"
+            )
+        if not all([isinstance(elm, str) for elm in names]):
+            raise ValueError(
+                f"If specified, all elements in names must be of type string. Got: {names}"
+            )
+        if not len(names) == len(dictionary_of_DataFrames):
+            raise ValueError(
+                "If specified, names must the same length as dictionary_of_DataFrames"
+            )
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
     if names == None:
-        variables = [var for var in  dictionary_of_DataFrames.keys() ]
+        variables = [var for var in dictionary_of_DataFrames.keys()]
     else:
         variables = names
-    
+
     var0 = variables[0]
     swanTables = block_to_table(dictionary_of_DataFrames[var0], name=var0)
-    for var in variables[1:]:    
+    for var in variables[1:]:
         tmp_dat = block_to_table(dictionary_of_DataFrames[var], name=var)
         swanTables[var] = tmp_dat[var]
-    
+
+    if not to_pandas:
+        swanTables = convert_to_dataset(swanTables)
+
     return swanTables
-        
 
-def block_to_table(data, name='values'):
-    '''
-    Converts structured 2D grid SWAN block format x (columns), y (index) 
-    to SWAN table format x (column),y (column), values (column) 
+
+def block_to_table(data, name="values", to_pandas=True):
+    """
+    Converts structured 2D grid SWAN block format x (columns), y (index)
+    to SWAN table format x (column),y (column), values (column)
     DataFrame.
-    
+
     Parameters
     ----------
-    data: DataFrame
+    data: pandas DataFrame or xarray Dataset
         DataFrame in with columns as X indicie and Y as index.
     name: string (Optional)
         Name of data column in returned table. Default='values'
+    to_pandas: bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
+
     Returns
     -------
-    table: DataFrame
-        DataFrame with columns x,y,values           
-    '''
-    assert isinstance(data,pd.DataFrame), 'data must be of type pd.DataFrame'
-    assert isinstance(name, str), 'Name must be of type str'
-    
+    table: pandas DataFrame or xarray Dataset
+        DataFrame with columns x,y,values
+    """
+    if isinstance(data, xr.Dataset):
+        data = data.to_pandas()
+    if not isinstance(data, pd.DataFrame):
+        raise TypeError(f"data must be of type pd.DataFrame. Got: {type(data)}")
+    if not isinstance(name, str):
+        raise TypeError(f"If specified, name must be of type str. Got: {type(name)}")
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
     table = data.unstack().reset_index(name=name)
-    table = table.rename(columns={'level_0':'x', 'level_1': 'y'})
-    table.sort_values(['x', 'y'], ascending=[True, True],  inplace=True)
+    table = table.rename(columns={"level_0": "x", "level_1": "y"})
+    table.sort_values(["x", "y"], ascending=[True, True], inplace=True)
 
-    return table
+    if not to_pandas:
+        table = convert_to_dataset(table)
 
+    return table
```

### Comparing `mhkit-0.7.0/mhkit/wave/io/wecsim.py` & `mhkit-0.8.0/mhkit/wave/io/wecsim.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,143 +1,161 @@
 import pandas as pd
 import numpy as np
 import scipy.io as sio
+from os.path import isfile
+from mhkit.utils import convert_nested_dict_and_pandas
 
 
-def read_output(file_name):
+def read_output(file_name, to_pandas=True):
     """
-    Loads the wecSim response class once 'output' has been saved to a `.mat` 
-    structure. 
-    
-    NOTE: Python is unable to import MATLAB objects. 
-    MATLAB must be used to save the wecSim object as a structure. 
-        
+    Loads the wecSim response class once 'output' has been saved to a `.mat`
+    structure.
+
+    NOTE: Python is unable to import MATLAB objects.
+    MATLAB must be used to save the wecSim object as a structure.
+
     Parameters
     ------------
     file_name: string
         Name of wecSim output file saved as a `.mat` structure
-        
-        
+    to_pandas: bool (optional)
+        Flag to output a dictionary of pandas objects instead of a dictionary
+        of xarray objects. Default = True.
+
     Returns
     ---------
-    ws_output: dict 
-        Dictionary of pandas DataFrames, indexed by time (s)      
-              
+    ws_output: dict
+        Dictionary of pandas DataFrames or xarray Datasets, indexed by time (s)
+
     """
-    
+    if not isinstance(file_name, str):
+        raise TypeError(f"file_name must be of type str. Got: {type(file_name)}")
+    if not isfile(file_name):
+        raise ValueError(f"File not found: {file_name}")
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
     ws_data = sio.loadmat(file_name)
-    output = ws_data['output']
+    output = ws_data["output"]
 
     ######################################
     ## import wecSim wave class
     #         type: ''
     #         time: [iterations x 1 double]
     #    elevation: [iterations x 1 double]
     ######################################
-    try:              
-        wave = output['wave']
-        wave_type = wave[0][0][0][0][0][0]  
-        time = wave[0][0]['time'][0][0].squeeze()
-        elevation = wave[0][0]['elevation'][0][0].squeeze()
-        
+    try:
+        wave = output["wave"]
+        wave_type = wave[0][0][0][0][0][0]
+        time = wave[0][0]["time"][0][0].squeeze()
+        elevation = wave[0][0]["elevation"][0][0].squeeze()
+
         ######################################
         ## create wave_output DataFrame
         ######################################
-        wave_output = pd.DataFrame(data = time,columns=['time'])   
-        wave_output = wave_output.set_index('time') 
-        wave_output['elevation'] = elevation        
+        wave_output = pd.DataFrame(data=time, columns=["time"])
+        wave_output = wave_output.set_index("time")
+        wave_output["elevation"] = elevation
         wave_output.name = wave_type
-    
+
     except:
-        print("wave class not used") 
-        wave_output = []      
-    
-    
+        print("wave class not used")
+        wave_output = []
+
     ######################################
     ## import wecSim body class
     #                       name: ''
     #                       time: [iterations x 1 double]
     #                   position: [iterations x 6 double]
     #                   velocity: [iterations x 6 double]
     #               acceleration: [iterations x 6 double]
     #                 forceTotal: [iterations x 6 double]
     #            forceExcitation: [iterations x 6 double]
     #      forceRadiationDamping: [iterations x 6 double]
     #             forceAddedMass: [iterations x 6 double]
     #             forceRestoring: [iterations x 6 double]
     #     forceMorisonAndViscous: [iterations x 6 double]
     #         forceLinearDamping: [iterations x 6 double]
-    ######################################    
+    ######################################
     try:
-        bodies = output['bodies']
-        num_bodies = len(bodies[0][0]['name'][0])  
-        name = []   
+        bodies = output["bodies"]
+        num_bodies = len(bodies[0][0]["name"][0])
+        name = []
         time = []
         position = []
         velocity = []
         acceleration = []
         forceTotal = []
         forceExcitation = []
         forceRadiationDamping = []
         forceAddedMass = []
         forceRestoring = []
         forceMorisonAndViscous = []
         forceLinearDamping = []
         for body in range(num_bodies):
-            name.append(bodies[0][0]['name'][0][body][0])   
-            time.append(bodies[0][0]['time'][0][body])
-            position.append(bodies[0][0]['position'][0][body])
-            velocity.append(bodies[0][0]['velocity'][0][body])
-            acceleration.append(bodies[0][0]['acceleration'][0][body])
-            forceTotal.append(bodies[0][0]['forceTotal'][0][body])
-            forceExcitation.append(bodies[0][0]['forceExcitation'][0][body])
-            forceRadiationDamping.append(bodies[0][0]['forceRadiationDamping'][0][body])
-            forceAddedMass.append(bodies[0][0]['forceAddedMass'][0][body])
-            forceRestoring.append(bodies[0][0]['forceRestoring'][0][body])
+            name.append(bodies[0][0]["name"][0][body][0])
+            time.append(bodies[0][0]["time"][0][body])
+            position.append(bodies[0][0]["position"][0][body])
+            velocity.append(bodies[0][0]["velocity"][0][body])
+            acceleration.append(bodies[0][0]["acceleration"][0][body])
+            forceTotal.append(bodies[0][0]["forceTotal"][0][body])
+            forceExcitation.append(bodies[0][0]["forceExcitation"][0][body])
+            forceRadiationDamping.append(bodies[0][0]["forceRadiationDamping"][0][body])
+            forceAddedMass.append(bodies[0][0]["forceAddedMass"][0][body])
+            forceRestoring.append(bodies[0][0]["forceRestoring"][0][body])
             try:
-                # Format in WEC-Sim responseClass >= v4.2 
-                forceMorisonAndViscous.append(bodies[0][0]['forceMorisonAndViscous'][0][body])
+                # Format in WEC-Sim responseClass >= v4.2
+                forceMorisonAndViscous.append(
+                    bodies[0][0]["forceMorisonAndViscous"][0][body]
+                )
             except:
                 # Format in WEC-Sim responseClass <= v4.1
-                forceMorisonAndViscous.append(bodies[0][0]['forceMorrisonAndViscous'][0][body])
-            forceLinearDamping.append(bodies[0][0]['forceLinearDamping'][0][body])    
+                forceMorisonAndViscous.append(
+                    bodies[0][0]["forceMorrisonAndViscous"][0][body]
+                )
+            forceLinearDamping.append(bodies[0][0]["forceLinearDamping"][0][body])
     except:
-        num_bodies = 0         
-        
+        num_bodies = 0
+
     ######################################
     ## create body_output DataFrame
-    ######################################            
+    ######################################
     def _write_body_output(body):
-        for dof in range(6):                
-            tmp_body[f'position_dof{dof+1}'] = position[body][:,dof]
-            tmp_body[f'velocity_dof{dof+1}'] = velocity[body][:,dof]
-            tmp_body[f'acceleration_dof{dof+1}'] = acceleration[body][:,dof]            
-            tmp_body[f'forceTotal_dof{dof+1}'] = forceTotal[body][:,dof]
-            tmp_body[f'forceExcitation_dof{dof+1}'] = forceExcitation[body][:,dof]
-            tmp_body[f'forceRadiationDamping_dof{dof+1}'] = forceRadiationDamping[body][:,dof]
-            tmp_body[f'forceAddedMass_dof{dof+1}'] = forceAddedMass[body][:,dof]
-            tmp_body[f'forceRestoring_dof{dof+1}'] = forceRestoring[body][:,dof]
-            tmp_body[f'forceMorisonAndViscous_dof{dof+1}'] = forceMorisonAndViscous[body][:,dof]
-            tmp_body[f'forceLinearDamping_dof{dof+1}'] = forceLinearDamping[body][:,dof]                            
+        for dof in range(6):
+            tmp_body[f"position_dof{dof+1}"] = position[body][:, dof]
+            tmp_body[f"velocity_dof{dof+1}"] = velocity[body][:, dof]
+            tmp_body[f"acceleration_dof{dof+1}"] = acceleration[body][:, dof]
+            tmp_body[f"forceTotal_dof{dof+1}"] = forceTotal[body][:, dof]
+            tmp_body[f"forceExcitation_dof{dof+1}"] = forceExcitation[body][:, dof]
+            tmp_body[f"forceRadiationDamping_dof{dof+1}"] = forceRadiationDamping[body][
+                :, dof
+            ]
+            tmp_body[f"forceAddedMass_dof{dof+1}"] = forceAddedMass[body][:, dof]
+            tmp_body[f"forceRestoring_dof{dof+1}"] = forceRestoring[body][:, dof]
+            tmp_body[f"forceMorisonAndViscous_dof{dof+1}"] = forceMorisonAndViscous[
+                body
+            ][:, dof]
+            tmp_body[f"forceLinearDamping_dof{dof+1}"] = forceLinearDamping[body][
+                :, dof
+            ]
         return tmp_body
 
     if num_bodies >= 1:
         body_output = {}
         for body in range(num_bodies):
-            tmp_body = pd.DataFrame(data = time[0],columns=['time'])   
-            tmp_body = tmp_body.set_index('time') 
+            tmp_body = pd.DataFrame(data=time[0], columns=["time"])
+            tmp_body = tmp_body.set_index("time")
             tmp_body.name = name[body]
             if num_bodies == 1:
                 body_output = _write_body_output(body)
             elif num_bodies > 1:
-                body_output[f'body{body+1}'] = _write_body_output(body) 
+                body_output[f"body{body+1}"] = _write_body_output(body)
     else:
-        print("body class not used") 
-        body_output = []    
-
+        print("body class not used")
+        body_output = []
 
     ######################################
     ## import wecSim pto class
     #                      name: ''
     #                      time: [iterations x 1 double]
     #                  position: [iterations x 6 double]
     #                  velocity: [iterations x 6 double]
@@ -145,314 +163,322 @@
     #                forceTotal: [iterations x 6 double]
     #            forceActuation: [iterations x 6 double]
     #           forceConstraint: [iterations x 6 double]
     #    forceInternalMechanics: [iterations x 6 double]
     #    powerInternalMechanics: [iterations x 6 double]
     ######################################
     try:
-        ptos = output['ptos']
-        num_ptos = len(ptos[0][0]['name'][0]) 
-        name = []   
+        ptos = output["ptos"]
+        num_ptos = len(ptos[0][0]["name"][0])
+        name = []
         time = []
         position = []
         velocity = []
         acceleration = []
         forceTotal = []
         forceActuation = []
         forceConstraint = []
         forceInternalMechanics = []
-        powerInternalMechanics= []
+        powerInternalMechanics = []
         for pto in range(num_ptos):
-            name.append(ptos[0][0]['name'][0][pto][0])  
-            time.append(ptos[0][0]['time'][0][pto])
-            position.append(ptos[0][0]['position'][0][pto])
-            velocity.append(ptos[0][0]['velocity'][0][pto])
-            acceleration.append(ptos[0][0]['acceleration'][0][pto])
-            forceTotal.append(ptos[0][0]['forceTotal'][0][pto])        
-            forceActuation.append(ptos[0][0]['forceActuation'][0][pto])        
-            forceConstraint.append(ptos[0][0]['forceConstraint'][0][pto])        
-            forceInternalMechanics.append(ptos[0][0]['forceInternalMechanics'][0][pto])        
-            powerInternalMechanics.append(ptos[0][0]['powerInternalMechanics'][0][pto])        
+            name.append(ptos[0][0]["name"][0][pto][0])
+            time.append(ptos[0][0]["time"][0][pto])
+            position.append(ptos[0][0]["position"][0][pto])
+            velocity.append(ptos[0][0]["velocity"][0][pto])
+            acceleration.append(ptos[0][0]["acceleration"][0][pto])
+            forceTotal.append(ptos[0][0]["forceTotal"][0][pto])
+            forceActuation.append(ptos[0][0]["forceActuation"][0][pto])
+            forceConstraint.append(ptos[0][0]["forceConstraint"][0][pto])
+            forceInternalMechanics.append(ptos[0][0]["forceInternalMechanics"][0][pto])
+            powerInternalMechanics.append(ptos[0][0]["powerInternalMechanics"][0][pto])
     except:
-        num_ptos = 0         
-        
+        num_ptos = 0
+
     ######################################
     ## create pto_output DataFrame
-    ######################################      
+    ######################################
     def _write_pto_output(pto):
-        for dof in range(6):                
-            tmp_pto[f'position_dof{dof+1}'] = position[pto][:,dof]
-            tmp_pto[f'velocity_dof{dof+1}'] = velocity[pto][:,dof]
-            tmp_pto[f'acceleration_dof{dof+1}'] = acceleration[pto][:,dof]                 
-            tmp_pto[f'forceTotal_dof{dof+1}'] = forceTotal[pto][:,dof]            
-            tmp_pto[f'forceTotal_dof{dof+1}'] = forceTotal[pto][:,dof]     
-            tmp_pto[f'forceActuation_dof{dof+1}'] = forceActuation[pto][:,dof]                 
-            tmp_pto[f'forceConstraint_dof{dof+1}'] = forceConstraint[pto][:,dof]            
-            tmp_pto[f'forceInternalMechanics_dof{dof+1}'] = forceInternalMechanics[pto][:,dof]     
-            tmp_pto[f'powerInternalMechanics_dof{dof+1}'] = powerInternalMechanics[pto][:,dof]
+        for dof in range(6):
+            tmp_pto[f"position_dof{dof+1}"] = position[pto][:, dof]
+            tmp_pto[f"velocity_dof{dof+1}"] = velocity[pto][:, dof]
+            tmp_pto[f"acceleration_dof{dof+1}"] = acceleration[pto][:, dof]
+            tmp_pto[f"forceTotal_dof{dof+1}"] = forceTotal[pto][:, dof]
+            tmp_pto[f"forceTotal_dof{dof+1}"] = forceTotal[pto][:, dof]
+            tmp_pto[f"forceActuation_dof{dof+1}"] = forceActuation[pto][:, dof]
+            tmp_pto[f"forceConstraint_dof{dof+1}"] = forceConstraint[pto][:, dof]
+            tmp_pto[f"forceInternalMechanics_dof{dof+1}"] = forceInternalMechanics[pto][
+                :, dof
+            ]
+            tmp_pto[f"powerInternalMechanics_dof{dof+1}"] = powerInternalMechanics[pto][
+                :, dof
+            ]
         return tmp_pto
 
     if num_ptos >= 1:
-        pto_output = {}     
+        pto_output = {}
         for pto in range(num_ptos):
-            tmp_pto = pd.DataFrame(data = time[0],columns=['time'])   
-            tmp_pto = tmp_pto.set_index('time') 
+            tmp_pto = pd.DataFrame(data=time[0], columns=["time"])
+            tmp_pto = tmp_pto.set_index("time")
             tmp_pto.name = name[pto]
-            if num_ptos == 1:  
+            if num_ptos == 1:
                 pto_output = _write_pto_output(pto)
             elif num_ptos > 1:
-                pto_output[f'pto{pto+1}'] = _write_pto_output(pto)
+                pto_output[f"pto{pto+1}"] = _write_pto_output(pto)
     else:
-        print("pto class not used") 
+        print("pto class not used")
         pto_output = []
 
-
     ######################################
     ## import wecSim constraint class
-    #                       
+    #
     #            name: ''
     #            time: [iterations x 1 double]
     #        position: [iterations x 6 double]
     #        velocity: [iterations x 6 double]
     #    acceleration: [iterations x 6 double]
     # forceConstraint: [iterations x 6 double]
-    ######################################    
+    ######################################
     try:
-        constraints = output['constraints']
-        num_constraints = len(constraints[0][0]['name'][0])   
-        name = []   
+        constraints = output["constraints"]
+        num_constraints = len(constraints[0][0]["name"][0])
+        name = []
         time = []
         position = []
         velocity = []
         acceleration = []
         forceConstraint = []
         for constraint in range(num_constraints):
-            name.append(constraints[0][0]['name'][0][constraint][0])   
-            time.append(constraints[0][0]['time'][0][constraint])
-            position.append(constraints[0][0]['position'][0][constraint])
-            velocity.append(constraints[0][0]['velocity'][0][constraint])
-            acceleration.append(constraints[0][0]['acceleration'][0][constraint])
-            forceConstraint.append(constraints[0][0]['forceConstraint'][0][constraint])        
+            name.append(constraints[0][0]["name"][0][constraint][0])
+            time.append(constraints[0][0]["time"][0][constraint])
+            position.append(constraints[0][0]["position"][0][constraint])
+            velocity.append(constraints[0][0]["velocity"][0][constraint])
+            acceleration.append(constraints[0][0]["acceleration"][0][constraint])
+            forceConstraint.append(constraints[0][0]["forceConstraint"][0][constraint])
     except:
-        num_constraints = 0 
-        
+        num_constraints = 0
+
     ######################################
     ## create constraint_output DataFrame
-    ######################################    
+    ######################################
     def _write_constraint_output(constraint):
-        for dof in range(6):                
-            tmp_constraint[f'position_dof{dof+1}'] = position[constraint][:,dof]
-            tmp_constraint[f'velocity_dof{dof+1}'] = velocity[constraint][:,dof]
-            tmp_constraint[f'acceleration_dof{dof+1}'] = acceleration[constraint][:,dof]            
-            tmp_constraint[f'forceConstraint_dof{dof+1}'] = forceConstraint[constraint][:,dof]
+        for dof in range(6):
+            tmp_constraint[f"position_dof{dof+1}"] = position[constraint][:, dof]
+            tmp_constraint[f"velocity_dof{dof+1}"] = velocity[constraint][:, dof]
+            tmp_constraint[f"acceleration_dof{dof+1}"] = acceleration[constraint][
+                :, dof
+            ]
+            tmp_constraint[f"forceConstraint_dof{dof+1}"] = forceConstraint[constraint][
+                :, dof
+            ]
         return tmp_constraint
 
     if num_constraints >= 1:
         constraint_output = {}
         for constraint in range(num_constraints):
-            tmp_constraint = pd.DataFrame(data = time[0],columns=['time'])   
-            tmp_constraint = tmp_constraint.set_index('time') 
+            tmp_constraint = pd.DataFrame(data=time[0], columns=["time"])
+            tmp_constraint = tmp_constraint.set_index("time")
             tmp_constraint.name = name[constraint]
             if num_constraints == 1:
                 constraint_output = _write_constraint_output(constraint)
             elif num_constraints > 1:
-                constraint_output[f'constraint{constraint+1}'] = _write_constraint_output(constraint)         
+                constraint_output[f"constraint{constraint+1}"] = (
+                    _write_constraint_output(constraint)
+                )
     else:
-        print("constraint class not used") 
+        print("constraint class not used")
         constraint_output = []
 
-
     ######################################
     ## import wecSim mooring class
-    # 
+    #
     #         name: ''
     #         time: [iterations x 1 double]
     #     position: [iterations x 6 double]
     #     velocity: [iterations x 6 double]
     # forceMooring: [iterations x 6 double]
     ######################################
     try:
-        moorings = output['mooring']
-        num_moorings = len(moorings[0][0]['name'][0])   
-        name = []   
+        moorings = output["mooring"]
+        num_moorings = len(moorings[0][0]["name"][0])
+        name = []
         time = []
         position = []
         velocity = []
         forceMooring = []
         for mooring in range(num_moorings):
-            name.append(moorings[0][0]['name'][0][mooring][0])   
-            time.append(moorings[0][0]['time'][0][mooring])
-            position.append(moorings[0][0]['position'][0][mooring])
-            velocity.append(moorings[0][0]['velocity'][0][mooring])
-            forceMooring.append(moorings[0][0]['forceMooring'][0][mooring])    
+            name.append(moorings[0][0]["name"][0][mooring][0])
+            time.append(moorings[0][0]["time"][0][mooring])
+            position.append(moorings[0][0]["position"][0][mooring])
+            velocity.append(moorings[0][0]["velocity"][0][mooring])
+            forceMooring.append(moorings[0][0]["forceMooring"][0][mooring])
     except:
-        num_moorings = 0 
+        num_moorings = 0
 
     ######################################
     ## create mooring_output DataFrame
-    ######################################    
+    ######################################
     def _write_mooring_output(mooring):
-        for dof in range(6):                
-            tmp_mooring[f'position_dof{dof+1}'] = position[mooring][:,dof]
-            tmp_mooring[f'velocity_dof{dof+1}'] = velocity[mooring][:,dof]
-            tmp_mooring[f'forceMooring_dof{dof+1}'] = forceMooring[mooring][:,dof]
+        for dof in range(6):
+            tmp_mooring[f"position_dof{dof+1}"] = position[mooring][:, dof]
+            tmp_mooring[f"velocity_dof{dof+1}"] = velocity[mooring][:, dof]
+            tmp_mooring[f"forceMooring_dof{dof+1}"] = forceMooring[mooring][:, dof]
         return tmp_mooring
 
-    if num_moorings >= 1:   
+    if num_moorings >= 1:
         mooring_output = {}
         for mooring in range(num_moorings):
-            tmp_mooring = pd.DataFrame(data = time[0],columns=['time'])   
-            tmp_mooring = tmp_mooring.set_index('time') 
+            tmp_mooring = pd.DataFrame(data=time[0], columns=["time"])
+            tmp_mooring = tmp_mooring.set_index("time")
             tmp_mooring.name = name[mooring]
-            if num_moorings == 1:   
+            if num_moorings == 1:
                 mooring_output = _write_mooring_output(mooring)
-            elif num_moorings > 1:   
-                mooring_output[f'mooring{mooring+1}'] = _write_mooring_output(mooring)
+            elif num_moorings > 1:
+                mooring_output[f"mooring{mooring+1}"] = _write_mooring_output(mooring)
     else:
-        print("mooring class not used") 
+        print("mooring class not used")
         mooring_output = []
-        
-    
+
     ######################################
     ## import wecSim moorDyn class
     #
     #    Lines: [11 struct]
     #    Line1: [11 struct]
     #    Line2: [11 struct]
     #    Line3: [11 struct]
     #    Line4: [11 struct]
     #    Line5: [11 struct]
-    #    Line6: [11 struct]  
+    #    Line6: [11 struct]
     ######################################
     try:
-        moorDyn = output['moorDyn']       
-        num_lines = len(moorDyn[0][0][0].dtype) - 1    # number of moorDyn lines
-      
-        Lines =  moorDyn[0][0]['Lines'][0][0][0]      
+        moorDyn = output["moorDyn"]
+        num_lines = len(moorDyn[0][0][0].dtype) - 1  # number of moorDyn lines
+
+        Lines = moorDyn[0][0]["Lines"][0][0][0]
         signals = Lines.dtype.names
         num_signals = len(Lines.dtype.names)
-        data = Lines[0]      
+        data = Lines[0]
         time = data[0]
-        Lines = pd.DataFrame(data = time,columns=['time'])   
-        Lines = Lines.set_index('time')       
-        for signal in range(1,num_signals):
-            Lines[signals[signal]] = data[signal]        
-        moorDyn_output= {'Lines': Lines}
-
-        Line_num_output = {}  
-        for line_num in range(1,num_lines+1):
-          tmp_moordyn =  moorDyn[0][0][f'Line{line_num}'][0][0][0]
-          signals = tmp_moordyn.dtype.names
-          num_signals = len(tmp_moordyn.dtype.names)
-          data = tmp_moordyn[0]
-          time = data[0]
-          tmp_moordyn = pd.DataFrame(data = time,columns=['time'])   
-          tmp_moordyn = tmp_moordyn.set_index('time')       
-          for signal in range(1,num_signals):
-            tmp_moordyn[signals[signal]] = data[signal]              
-          Line_num_output[f'Line{line_num}'] = tmp_moordyn
-        
+        Lines = pd.DataFrame(data=time, columns=["time"])
+        Lines = Lines.set_index("time")
+        for signal in range(1, num_signals):
+            Lines[signals[signal]] = data[signal]
+        moorDyn_output = {"Lines": Lines}
+
+        Line_num_output = {}
+        for line_num in range(1, num_lines + 1):
+            tmp_moordyn = moorDyn[0][0][f"Line{line_num}"][0][0][0]
+            signals = tmp_moordyn.dtype.names
+            num_signals = len(tmp_moordyn.dtype.names)
+            data = tmp_moordyn[0]
+            time = data[0]
+            tmp_moordyn = pd.DataFrame(data=time, columns=["time"])
+            tmp_moordyn = tmp_moordyn.set_index("time")
+            for signal in range(1, num_signals):
+                tmp_moordyn[signals[signal]] = data[signal]
+            Line_num_output[f"Line{line_num}"] = tmp_moordyn
+
         moorDyn_output.update(Line_num_output)
-      
+
     except:
-        print("moorDyn class not used") 
+        print("moorDyn class not used")
         moorDyn_output = []
 
-
     ######################################
     ## import wecSim ptosim class
-    # 
+    #
     #                 name: ''
     #             pistonCF: [11 struct]
     #            pistonNCF: [11 struct]
     #           checkValve: [11 struct]
     #                valve: [11 struct]
     #          accumulator: [12 struct]
     #       hydraulicMotor: [11 struct]
     #      rotaryGenerator: [11 struct]
     #    pmLinearGenerator: [11 struct]
     #    pmRotaryGenerator: [11 struct]
     #      motionMechanism: [11 struct]
-    ######################################    
+    ######################################
     try:
-        ptosim = output['ptosim']  
-        num_ptosim = len(ptosim[0][0]['name'][0])   # number of ptosim  
-        print("ptosim class output not supported at this time") 
+        ptosim = output["ptosim"]
+        num_ptosim = len(ptosim[0][0]["name"][0])  # number of ptosim
+        print("ptosim class output not supported at this time")
     except:
-        print("ptosim class not used") 
+        print("ptosim class not used")
         ptosim_output = []
-    
-    
+
     ######################################
     ## import wecSim cable class
-    # 
+    #
     #       name: ''
     #       time: [iterations x 1 double]
     #   position: [iterations x 6 double]
     #   velocity: [iterations x 6 double]
     # forcecable: [iterations x 6 double]
     ######################################
     try:
-        cables = output['cables']
-        num_cables = len(cables[0][0]['name'][0])   
-        name = []   
+        cables = output["cables"]
+        num_cables = len(cables[0][0]["name"][0])
+        name = []
         time = []
         position = []
         velocity = []
         acceleration = []
         forcetotal = []
         forceactuation = []
         forceconstraint = []
         for cable in range(num_cables):
-            name.append(cables[0][0]['name'][0][cable][0])   
-            time.append(cables[0][0]['time'][0][cable])
-            position.append(cables[0][0]['position'][0][cable])
-            velocity.append(cables[0][0]['velocity'][0][cable])
-            acceleration.append(cables[0][0]['acceleration'][0][cable])
-            forcetotal.append(cables[0][0]['forceTotal'][0][cable])
-            forceactuation.append(cables[0][0]['forceActuation'][0][cable])
-            forceconstraint.append(cables[0][0]['forceConstraint'][0][cable])
+            name.append(cables[0][0]["name"][0][cable][0])
+            time.append(cables[0][0]["time"][0][cable])
+            position.append(cables[0][0]["position"][0][cable])
+            velocity.append(cables[0][0]["velocity"][0][cable])
+            acceleration.append(cables[0][0]["acceleration"][0][cable])
+            forcetotal.append(cables[0][0]["forceTotal"][0][cable])
+            forceactuation.append(cables[0][0]["forceActuation"][0][cable])
+            forceconstraint.append(cables[0][0]["forceConstraint"][0][cable])
     except:
-        num_cables = 0 
+        num_cables = 0
 
     ######################################
     ## create cable_output DataFrame
-    ######################################    
+    ######################################
     def _write_cable_output(cable):
-        for dof in range(6):                
-            tmp_cable[f'position_dof{dof+1}'] = position[cable][:,dof]
-            tmp_cable[f'velocity_dof{dof+1}'] = velocity[cable][:,dof]
-            tmp_cable[f'acceleration_dof{dof+1}'] = acceleration[cable][:,dof]
-            tmp_cable[f'forcetotal_dof{dof+1}'] = forcetotal[cable][:,dof]
-            tmp_cable[f'forceactuation_dof{dof+1}'] = forceactuation[cable][:,dof]
-            tmp_cable[f'forceconstraint_dof{dof+1}'] = forceconstraint[cable][:,dof]
+        for dof in range(6):
+            tmp_cable[f"position_dof{dof+1}"] = position[cable][:, dof]
+            tmp_cable[f"velocity_dof{dof+1}"] = velocity[cable][:, dof]
+            tmp_cable[f"acceleration_dof{dof+1}"] = acceleration[cable][:, dof]
+            tmp_cable[f"forcetotal_dof{dof+1}"] = forcetotal[cable][:, dof]
+            tmp_cable[f"forceactuation_dof{dof+1}"] = forceactuation[cable][:, dof]
+            tmp_cable[f"forceconstraint_dof{dof+1}"] = forceconstraint[cable][:, dof]
         return tmp_cable
 
-    if num_cables >= 1:   
+    if num_cables >= 1:
         cable_output = {}
         for cable in range(num_cables):
-            tmp_cable = pd.DataFrame(data = time[0],columns=['time'])   
-            tmp_cable = tmp_cable.set_index('time') 
+            tmp_cable = pd.DataFrame(data=time[0], columns=["time"])
+            tmp_cable = tmp_cable.set_index("time")
             tmp_cable.name = name[cable]
-            if num_cables == 1:   
+            if num_cables == 1:
                 cable_output = _write_cable_output(cable)
-            elif num_cables > 1:   
-                cable_output[f'cable{cable+1}'] = _write_cable_output(cable)
+            elif num_cables > 1:
+                cable_output[f"cable{cable+1}"] = _write_cable_output(cable)
     else:
-        print("cable class not used") 
+        print("cable class not used")
         cable_output = []
 
+    ############################################
+    ## create wecSim output - Dict of DataFrames
+    ############################################
+    ws_output = {
+        "wave": wave_output,
+        "bodies": body_output,
+        "ptos": pto_output,
+        "constraints": constraint_output,
+        "mooring": mooring_output,
+        "moorDyn": moorDyn_output,
+        "ptosim": ptosim_output,
+        "cables": cable_output,
+    }
 
+    if not to_pandas:
+        ws_output = convert_nested_dict_and_pandas(ws_output)
 
-    ######################################
-    ## create wecSim output DataFrame of Dict
-    ######################################
-    ws_output = {'wave' : wave_output, 
-                 'bodies' : body_output,
-                 'ptos' : pto_output,
-                 'constraints' : constraint_output,                 
-                 'mooring' : mooring_output,
-                  'moorDyn': moorDyn_output, 
-                  'ptosim' : ptosim_output,
-                  'cables': cable_output
-                 }
-    return ws_output 
+    return ws_output
```

### Comparing `mhkit-0.7.0/mhkit/wave/performance.py` & `mhkit-0.8.0/mhkit/wave/performance.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,62 +1,89 @@
 import numpy as np
 import pandas as pd
-import xarray 
+import xarray as xr
 import types
 from scipy.stats import binned_statistic_2d as _binned_statistic_2d
 from mhkit import wave
 import matplotlib.pylab as plt
 from os.path import join
+from mhkit.utils import convert_to_dataarray, convert_to_dataset
 
-def capture_length(P, J):
+
+def capture_length(P, J, to_pandas=True):
     """
     Calculates the capture length (often called capture width).
 
     Parameters
     ------------
-    P: numpy array or pandas Series
+    P: numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Power [W]
-    J: numpy array or pandas Series
+    J: numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Omnidirectional wave energy flux [W/m]
+    to_pandas: bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
 
     Returns
     ---------
-    L: numpy array or pandas Series
+    L: pandas Series or xarray DataArray
         Capture length [m]
     """
-    assert isinstance(P, (np.ndarray, pd.Series)), 'P must be of type np.ndarray or pd.Series'
-    assert isinstance(J, (np.ndarray, pd.Series)), 'J must be of type np.ndarray or pd.Series'
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
+    P = convert_to_dataarray(P)
+    J = convert_to_dataarray(J)
 
-    L = P/J
+    L = P / J
+
+    if to_pandas:
+        L = L.to_pandas()
 
     return L
 
 
-def statistics(X):
+def statistics(X, to_pandas=True):
     """
     Calculates statistics, including count, mean, standard
     deviation (std), min, percentiles (25%, 50%, 75%), and max.
 
     Note that std uses a degree of freedom of 1 in accordance with
     IEC/TS 62600-100.
 
     Parameters
     ------------
-    X: numpy array or pandas Series
+    X: numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Data
+    to_pandas: bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
 
     Returns
     ---------
-    stats: pandas Series
+    stats: pandas Series or xarray DataArray
         Statistics
     """
-    assert isinstance(X, (np.ndarray, pd.Series)), 'X must be of type np.ndarray or pd.Series'
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
+
+    X = convert_to_dataarray(X)
 
-    stats = pd.Series(X).describe()
-    stats['std'] = _std_ddof1(X)
+    count = X.count().item()
+    mean = X.mean().item()
+    std = _std_ddof1(X)
+    q = X.quantile([0.0, 0.25, 0.5, 0.75, 1.0]).values
+    variables = ["count", "mean", "std", "min", "25%", "50%", "75%", "max"]
+
+    stats = xr.DataArray(
+        data=[count, mean, std, q[0], q[1], q[2], q[3], q[4]],
+        dims="index",
+        coords={"index": variables},
+    )
+
+    if to_pandas:
+        stats = stats.to_pandas()
 
     return stats
 
 
 def _std_ddof1(a):
     # Standard deviation with degree of freedom equal to 1
     if len(a) == 0:
@@ -67,217 +94,273 @@
         return np.std(a, ddof=1)
 
 
 def _performance_matrix(X, Y, Z, statistic, x_centers, y_centers):
     # General performance matrix function
 
     # Convert bin centers to edges
-    xi = [np.mean([x_centers[i], x_centers[i+1]]) for i in range(len(x_centers)-1)]
-    xi.insert(0,-np.inf)
+    xi = [np.mean([x_centers[i], x_centers[i + 1]]) for i in range(len(x_centers) - 1)]
+    xi.insert(0, -np.inf)
     xi.append(np.inf)
 
-    yi = [np.mean([y_centers[i], y_centers[i+1]]) for i in range(len(y_centers)-1)]
-    yi.insert(0,-np.inf)
+    yi = [np.mean([y_centers[i], y_centers[i + 1]]) for i in range(len(y_centers) - 1)]
+    yi.insert(0, -np.inf)
     yi.append(np.inf)
 
     # Override standard deviation with degree of freedom equal to 1
-    if statistic == 'std':
+    if statistic == "std":
         statistic = _std_ddof1
 
     # Provide function to compute frequency
     def _frequency(a):
-        return len(a)/len(Z)
-    if statistic == 'frequency':
-        statistic = _frequency
+        return len(a) / len(Z)
 
-    zi, x_edge, y_edge, binnumber = _binned_statistic_2d(X, Y, Z, statistic,
-                        bins=[xi,yi], expand_binnumbers=False)
+    if statistic == "frequency":
+        statistic = _frequency
 
-    M = pd.DataFrame(zi, index=x_centers, columns=y_centers)
+    zi, x_edge, y_edge, binnumber = _binned_statistic_2d(
+        X, Y, Z, statistic, bins=[xi, yi], expand_binnumbers=False
+    )
+
+    M = xr.DataArray(
+        data=zi,
+        dims=["x_centers", "y_centers"],
+        coords={"x_centers": x_centers, "y_centers": y_centers},
+    )
 
     return M
 
 
-def capture_length_matrix(Hm0, Te, L, statistic, Hm0_bins, Te_bins):
+def capture_length_matrix(Hm0, Te, L, statistic, Hm0_bins, Te_bins, to_pandas=True):
     """
     Generates a capture length matrix for a given statistic
 
     Note that IEC/TS 62600-100 requires capture length matrices for
     the mean, std, count, min, and max.
 
     Parameters
     ------------
-    Hm0: numpy array or pandas Series
+    Hm0: numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Significant wave height from spectra [m]
-    Te: numpy array or pandas Series
+    Te: numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Energy period from spectra [s]
-    L : numpy array or pandas Series
+    L : numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Capture length [m]
     statistic: string
         Statistic for each bin, options include: 'mean', 'std', 'median',
         'count', 'sum', 'min', 'max', and 'frequency'.  Note that 'std' uses
         a degree of freedom of 1 in accordance with IEC/TS 62600-100.
     Hm0_bins: numpy array
         Bin centers for Hm0 [m]
     Te_bins: numpy array
         Bin centers for Te [s]
+    to_pandas: bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
 
     Returns
     ---------
-    LM: pandas DataFrames
+    LM: pandas DataFrame or xarray DataArray
         Capture length matrix with index equal to Hm0_bins and columns
         equal to Te_bins
 
     """
-    assert isinstance(Hm0, (np.ndarray, pd.Series)), 'Hm0 must be of type np.ndarray or pd.Series'
-    assert isinstance(Te, (np.ndarray, pd.Series)), 'Te must be of type np.ndarray or pd.Series'
-    assert isinstance(L, (np.ndarray, pd.Series)), 'L must be of type np.ndarray or pd.Series'
-    assert isinstance(statistic, (str, types.FunctionType)), 'statistic must be of type str or callable'
-    assert isinstance(Hm0_bins, np.ndarray), 'Hm0_bins must be of type np.ndarray'
-    assert isinstance(Te_bins, np.ndarray), 'Te_bins must be of type np.ndarray'
+    Hm0 = convert_to_dataarray(Hm0)
+    Te = convert_to_dataarray(Te)
+    L = convert_to_dataarray(L)
+
+    if not isinstance(statistic, (str, types.FunctionType)):
+        raise TypeError(
+            f"statistic must be of type str or callable. Got: {type(statistic)}"
+        )
+    if not isinstance(Hm0_bins, np.ndarray):
+        raise TypeError(f"Hm0_bins must be of type np.ndarray. Got: {type(Hm0_bins)}")
+    if not isinstance(Te_bins, np.ndarray):
+        raise TypeError(f"Te_bins must be of type np.ndarray. Got: {type(Te_bins)}")
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
 
     LM = _performance_matrix(Hm0, Te, L, statistic, Hm0_bins, Te_bins)
 
+    if to_pandas:
+        LM = LM.to_pandas()
+
     return LM
 
 
-def wave_energy_flux_matrix(Hm0, Te, J, statistic, Hm0_bins, Te_bins):
+def wave_energy_flux_matrix(Hm0, Te, J, statistic, Hm0_bins, Te_bins, to_pandas=True):
     """
     Generates a wave energy flux matrix for a given statistic
 
     Parameters
     ------------
-    Hm0: numpy array or pandas Series
+    Hm0: numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Significant wave height from spectra [m]
-    Te: numpy array or pandas Series
+    Te: numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Energy period from spectra [s]
-    J : numpy array or pandas Series
+    J : numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Wave energy flux from spectra [W/m]
     statistic: string
         Statistic for each bin, options include: 'mean', 'std', 'median',
         'count', 'sum', 'min', 'max', and 'frequency'.  Note that 'std' uses a degree of freedom
         of 1 in accordance of IEC/TS 62600-100.
     Hm0_bins: numpy array
         Bin centers for Hm0 [m]
     Te_bins: numpy array
         Bin centers for Te [s]
+    to_pandas: bool (optional)
+        Flag to output pandas instead of xarray. Default = True.
 
     Returns
     ---------
-    JM: pandas DataFrames
+    JM: pandas DataFrame or xarray DataArray
         Wave energy flux matrix with index equal to Hm0_bins and columns
         equal to Te_bins
 
     """
-    assert isinstance(Hm0, (np.ndarray, pd.Series)), 'Hm0 must be of type np.ndarray or pd.Series'
-    assert isinstance(Te, (np.ndarray, pd.Series)), 'Te must be of type np.ndarray or pd.Series'
-    assert isinstance(J, (np.ndarray, pd.Series)), 'J must be of type np.ndarray or pd.Series'
-    assert isinstance(statistic, (str, callable)), 'statistic must be of type str or callable'
-    assert isinstance(Hm0_bins, np.ndarray), 'Hm0_bins must be of type np.ndarray'
-    assert isinstance(Te_bins, np.ndarray), 'Te_bins must be of type np.ndarray'
+    Hm0 = convert_to_dataarray(Hm0)
+    Te = convert_to_dataarray(Te)
+    J = convert_to_dataarray(J)
+    if not isinstance(statistic, (str, callable)):
+        raise TypeError(
+            f"statistic must be of type str or callable. Got: {type(statistic)}"
+        )
+    if not isinstance(Hm0_bins, np.ndarray):
+        raise TypeError(f"Hm0_bins must be of type np.ndarray. Got: {type(Hm0_bins)}")
+    if not isinstance(Te_bins, np.ndarray):
+        raise TypeError(f"Te_bins must be of type np.ndarray. Got: {type(Te_bins)}")
+    if not isinstance(to_pandas, bool):
+        raise TypeError(f"to_pandas must be of type bool. Got: {type(to_pandas)}")
 
     JM = _performance_matrix(Hm0, Te, J, statistic, Hm0_bins, Te_bins)
 
+    if to_pandas:
+        JM = JM.to_pandas()
+
     return JM
 
+
 def power_matrix(LM, JM):
     """
     Generates a power matrix from a capture length matrix and wave energy
     flux matrix
 
     Parameters
     ------------
-    LM: pandas DataFrame
+    LM: pandas DataFrame or xarray Dataset
         Capture length matrix
-    JM: pandas DataFrame
+    JM: pandas DataFrame or xarray Dataset
         Wave energy flux matrix
 
     Returns
     ---------
-    PM: pandas DataFrames
+    PM: pandas DataFrame or xarray Dataset
         Power matrix
 
     """
-    assert isinstance(LM, pd.DataFrame), 'LM must be of type pd.DataFrame'
-    assert isinstance(JM, pd.DataFrame), 'JM must be of type pd.DataFrame'
+    if not isinstance(LM, (pd.DataFrame, xr.Dataset)):
+        raise TypeError(
+            f"LM must be of type pd.DataFrame or xr.Dataset. Got: {type(LM)}"
+        )
+    if not isinstance(JM, (pd.DataFrame, xr.Dataset)):
+        raise TypeError(
+            f"JM must be of type pd.DataFrame or xr.Dataset. Got: {type(JM)}"
+        )
 
-    PM = LM*JM
+    PM = LM * JM
 
     return PM
 
+
 def mean_annual_energy_production_timeseries(L, J):
     """
     Calculates mean annual energy production (MAEP) from time-series
 
     Parameters
     ------------
-    L: numpy array or pandas Series
+    L: numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Capture length
-    J: numpy array or pandas Series
+    J: numpy array, pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Wave energy flux
 
     Returns
     ---------
     maep: float
         Mean annual energy production
 
     """
-    assert isinstance(L, (np.ndarray, pd.Series)), 'L must be of type np.ndarray or pd.Series'
-    assert isinstance(J, (np.ndarray, pd.Series)), 'J must be of type np.ndarray or pd.Series'
+    L = convert_to_dataarray(L)
+    J = convert_to_dataarray(J)
 
-    T = 8766 # Average length of a year (h)
+    T = 8766  # Average length of a year (h)
     n = len(L)
 
-    maep = T/n * np.sum(L * J)
+    maep = T / n * (L * J).sum().item()
 
     return maep
 
+
 def mean_annual_energy_production_matrix(LM, JM, frequency):
     """
     Calculates mean annual energy production (MAEP) from matrix data
     along with data frequency in each bin
 
     Parameters
     ------------
-    LM: pandas DataFrame
+    LM: pandas DataFrame or xarray Dataset
         Capture length
-    JM: pandas DataFrame
+    JM: pandas DataFrame or xarray Dataset
         Wave energy flux
-    frequency: pandas DataFrame
+    frequency: pandas DataFrame or xarray Dataset
         Data frequency for each bin
 
     Returns
     ---------
     maep: float
         Mean annual energy production
 
     """
-    assert isinstance(LM, pd.DataFrame), 'LM must be of type pd.DataFrame'
-    assert isinstance(JM, pd.DataFrame), 'JM must be of type pd.DataFrame'
-    assert isinstance(frequency, pd.DataFrame), 'frequency must be of type pd.DataFrame'
-    assert LM.shape == JM.shape == frequency.shape, 'LM, JM, and frequency must be of the same size'
-    #assert frequency.sum().sum() == 1
+    LM = convert_to_dataarray(LM)
+    JM = convert_to_dataarray(JM)
+    frequency = convert_to_dataarray(frequency)
+
+    if not LM.shape == JM.shape == frequency.shape:
+        raise ValueError("LM, JM, and frequency must be of the same size")
+    if not np.abs(frequency.sum() - 1) < 1e-6:
+        raise ValueError("Frequency components must sum to one.")
 
-    T = 8766 # Average length of a year (h)
+    T = 8766  # Average length of a year (h)
     maep = T * np.nansum(LM * JM * frequency)
 
     return maep
 
-def power_performance_workflow(S, h, P, statistic, frequency_bins=None, deep=False, rho=1205, g=9.80665, ratio=2, show_values=False, savepath=""):
+
+def power_performance_workflow(
+    S,
+    h,
+    P,
+    statistic,
+    frequency_bins=None,
+    deep=False,
+    rho=1205,
+    g=9.80665,
+    ratio=2,
+    show_values=False,
+    savepath="",
+):
     """
     High-level function to compute power performance quantities of
     interest following IEC TS 62600-100 for given wave spectra.
 
     Parameters
     ------------
-    S: pandas DataFrame or Series
+    S:  pandas Series, pandas DataFrame, xarray DataArray, or xarray Dataset
         Spectral density [m^2/Hz] indexed by frequency [Hz]
     h: float
         Water depth [m]
-    P: numpy array or pandas Series
+    P: numpy ndarray, pandas DataFrame, pandas Series, xarray DataArray, or xarray Dataset
         Power [W]
     statistic: string or list of strings
         Statistics for plotting capture length matrices,
         options include: "mean", "std", "median",
         "count", "sum", "min", "max", and "frequency".
         Note that "std" uses a degree of freedom of 1 in accordance with IEC/TS 62600-100.
         To output capture length matrices for multiple binning parameters,
@@ -305,63 +388,99 @@
     ---------
     LM: xarray dataset
         Capture length matrices
 
     maep_matrix: float
         Mean annual energy production
     """
-    assert isinstance(S, (pd.DataFrame,pd.Series)), 'S must be of type pd.DataFrame or pd.Series'
-    assert isinstance(h, (int,float)), 'h must be of type int or float'
-    assert isinstance(P, (np.ndarray, pd.Series)), 'P must be of type np.ndarray or pd.Series'
-    assert isinstance(deep, bool), 'deep must be of type bool'
-    assert isinstance(rho, (int,float)), 'rho must be of type int or float'
-    assert isinstance(g, (int,float)), 'g must be of type int or float'
-    assert isinstance(ratio, (int,float)), 'ratio must be of type int or float'
+    S = convert_to_dataset(S)
+    if not isinstance(h, (int, float)):
+        raise TypeError(f"h must be of type int or float. Got: {type(h)}")
+    P = convert_to_dataarray(P)
+    if not isinstance(deep, bool):
+        raise TypeError(f"deep must be of type bool. Got: {type(deep)}")
+    if not isinstance(rho, (int, float)):
+        raise TypeError(f"rho must be of type int or float. Got: {type(rho)}")
+    if not isinstance(g, (int, float)):
+        raise TypeError(f"g must be of type int or float. Got: {type(g)}")
+    if not isinstance(ratio, (int, float)):
+        raise TypeError(f"ratio must be of type int or float. Got: {type(ratio)}")
 
     # Compute the enegy periods from the spectra data
-    Te = wave.resource.energy_period(S, frequency_bins=frequency_bins)
-    Te = Te['Te']
+    Te = wave.resource.energy_period(S, frequency_bins=frequency_bins, to_pandas=False)
+    Te = Te["Te"]
 
     # Compute the significant wave height from the NDBC spectra data
-    Hm0 = wave.resource.significant_wave_height(S, frequency_bins=frequency_bins)
-    Hm0 = Hm0['Hm0']
+    Hm0 = wave.resource.significant_wave_height(
+        S, frequency_bins=frequency_bins, to_pandas=False
+    )
+    Hm0 = Hm0["Hm0"]
 
     # Compute the energy flux from spectra data and water depth
-    J = wave.resource.energy_flux(S, h, deep=deep, rho=rho, g=g, ratio=ratio)
-    J = J['J']
+    J = wave.resource.energy_flux(
+        S, h, deep=deep, rho=rho, g=g, ratio=ratio, to_pandas=False
+    )
+    J = J["J"]
 
     # Calculate capture length from power and energy flux
-    L = wave.performance.capture_length(P,J)
+    L = wave.performance.capture_length(P, J, to_pandas=False)
 
     # Generate bins for Hm0 and Te, input format (start, stop, step_size)
-    Hm0_bins = np.arange(0, Hm0.values.max() + .5, .5)
+    Hm0_bins = np.arange(0, Hm0.values.max() + 0.5, 0.5)
     Te_bins = np.arange(0, Te.values.max() + 1, 1)
 
     # Create capture length matrices for each statistic based on IEC/TS 62600-100
     # Median, sum, frequency additionally provided
-    LM = xarray.Dataset()
-    LM['mean'] = wave.performance.capture_length_matrix(Hm0, Te, L, 'mean', Hm0_bins, Te_bins)
-    LM['std'] = wave.performance.capture_length_matrix(Hm0, Te, L, 'std', Hm0_bins, Te_bins)
-    LM['median'] = wave.performance.capture_length_matrix(Hm0, Te, L, 'median', Hm0_bins, Te_bins)
-    LM['count'] = wave.performance.capture_length_matrix(Hm0, Te, L, 'count', Hm0_bins, Te_bins)
-    LM['sum'] = wave.performance.capture_length_matrix(Hm0, Te, L, 'sum', Hm0_bins, Te_bins)
-    LM['min'] = wave.performance.capture_length_matrix(Hm0, Te, L, 'min', Hm0_bins, Te_bins)
-    LM['max'] = wave.performance.capture_length_matrix(Hm0, Te, L, 'max', Hm0_bins, Te_bins)
-    LM['freq'] = wave.performance.capture_length_matrix(Hm0, Te, L,'frequency', Hm0_bins, Te_bins)
+    LM = xr.Dataset()
+    LM["mean"] = wave.performance.capture_length_matrix(
+        Hm0, Te, L, "mean", Hm0_bins, Te_bins, to_pandas=False
+    )
+    LM["std"] = wave.performance.capture_length_matrix(
+        Hm0, Te, L, "std", Hm0_bins, Te_bins, to_pandas=False
+    )
+    LM["median"] = wave.performance.capture_length_matrix(
+        Hm0, Te, L, "median", Hm0_bins, Te_bins, to_pandas=False
+    )
+    LM["count"] = wave.performance.capture_length_matrix(
+        Hm0, Te, L, "count", Hm0_bins, Te_bins, to_pandas=False
+    )
+    LM["sum"] = wave.performance.capture_length_matrix(
+        Hm0, Te, L, "sum", Hm0_bins, Te_bins, to_pandas=False
+    )
+    LM["min"] = wave.performance.capture_length_matrix(
+        Hm0, Te, L, "min", Hm0_bins, Te_bins, to_pandas=False
+    )
+    LM["max"] = wave.performance.capture_length_matrix(
+        Hm0, Te, L, "max", Hm0_bins, Te_bins, to_pandas=False
+    )
+    LM["freq"] = wave.performance.capture_length_matrix(
+        Hm0, Te, L, "frequency", Hm0_bins, Te_bins, to_pandas=False
+    )
 
     # Create wave energy flux matrix using mean
-    JM = wave.performance.wave_energy_flux_matrix(Hm0, Te, J, 'mean', Hm0_bins, Te_bins)
+    JM = wave.performance.wave_energy_flux_matrix(
+        Hm0, Te, J, "mean", Hm0_bins, Te_bins, to_pandas=False
+    )
 
     # Calculate maep from matrix
-    maep_matrix = wave.performance.mean_annual_energy_production_matrix(LM['mean'].to_pandas(), JM, LM['freq'].to_pandas())
+    maep_matrix = wave.performance.mean_annual_energy_production_matrix(
+        LM["mean"], JM, LM["freq"]
+    )
 
     # Plot capture length matrices using statistic
     for str in statistic:
         if str not in list(LM.data_vars):
-            print('ERROR: Invalid Statistics passed')
+            print("ERROR: Invalid Statistics passed")
             continue
-        plt.figure(figsize=(12,12), num='Capture Length Matrix ' + str)
+        plt.figure(figsize=(12, 12), num="Capture Length Matrix " + str)
         ax = plt.gca()
-        wave.graphics.plot_matrix(LM[str].to_pandas(), xlabel='Te (s)', ylabel='Hm0 (m)', zlabel= str + ' of Capture Length', show_values=show_values, ax=ax)
-        plt.savefig(join(savepath,'Capture Length Matrix ' + str + '.png'))
+        wave.graphics.plot_matrix(
+            LM[str],
+            xlabel="Te (s)",
+            ylabel="Hm0 (m)",
+            zlabel=str + " of Capture Length",
+            show_values=show_values,
+            ax=ax,
+        )
+        plt.savefig(join(savepath, "Capture Length Matrix " + str + ".png"))
 
     return LM, maep_matrix
```

### Comparing `mhkit-0.7.0/mhkit.egg-info/SOURCES.txt` & `mhkit-0.8.0/mhkit.egg-info/SOURCES.txt`

 * *Files 20% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 LICENSE.md
 README.md
 setup.py
 mhkit/__init__.py
-mhkit/utils.py
 mhkit.egg-info/PKG-INFO
 mhkit.egg-info/SOURCES.txt
 mhkit.egg-info/dependency_links.txt
 mhkit.egg-info/not-zip-safe
 mhkit.egg-info/requires.txt
 mhkit.egg-info/top_level.txt
 mhkit/dolfyn/__init__.py
@@ -40,17 +39,21 @@
 mhkit/dolfyn/rotate/rdi.py
 mhkit/dolfyn/rotate/signature.py
 mhkit/dolfyn/rotate/vector.py
 mhkit/dolfyn/tools/__init__.py
 mhkit/dolfyn/tools/fft.py
 mhkit/dolfyn/tools/misc.py
 mhkit/loads/__init__.py
-mhkit/loads/extreme.py
 mhkit/loads/general.py
 mhkit/loads/graphics.py
+mhkit/loads/extreme/__init__.py
+mhkit/loads/extreme/extremes.py
+mhkit/loads/extreme/mler.py
+mhkit/loads/extreme/peaks.py
+mhkit/loads/extreme/sample.py
 mhkit/mooring/__init__.py
 mhkit/mooring/graphics.py
 mhkit/mooring/io.py
 mhkit/mooring/main.py
 mhkit/power/__init__.py
 mhkit/power/characteristics.py
 mhkit/power/quality.py
@@ -76,26 +79,30 @@
 mhkit/tests/dolfyn/test_rotate_adp.py
 mhkit/tests/dolfyn/test_rotate_adv.py
 mhkit/tests/dolfyn/test_shortcuts.py
 mhkit/tests/dolfyn/test_time.py
 mhkit/tests/dolfyn/test_tools.py
 mhkit/tests/dolfyn/test_vs_nortek.py
 mhkit/tests/loads/__init__.py
+mhkit/tests/loads/test_extreme.py
 mhkit/tests/loads/test_loads.py
 mhkit/tests/power/__init__.py
 mhkit/tests/power/test_power.py
 mhkit/tests/river/__init__.py
-mhkit/tests/river/test_io.py
+mhkit/tests/river/test_io_d3d.py
+mhkit/tests/river/test_io_usgs.py
 mhkit/tests/river/test_performance.py
 mhkit/tests/river/test_resource.py
 mhkit/tests/tidal/__init__.py
 mhkit/tests/tidal/test_io.py
 mhkit/tests/tidal/test_performance.py
 mhkit/tests/tidal/test_resource.py
 mhkit/tests/utils/__init__.py
+mhkit/tests/utils/test_cache.py
+mhkit/tests/utils/test_upcrossing.py
 mhkit/tests/utils/test_utils.py
 mhkit/tests/wave/__init__.py
 mhkit/tests/wave/test_contours.py
 mhkit/tests/wave/test_performance.py
 mhkit/tests/wave/test_resource_metrics.py
 mhkit/tests/wave/test_resource_spectrum.py
 mhkit/tests/wave/io/__init__.py
@@ -103,20 +110,26 @@
 mhkit/tests/wave/io/test_ndbc.py
 mhkit/tests/wave/io/test_swan.py
 mhkit/tests/wave/io/test_wecsim.py
 mhkit/tests/wave/io/hindcast/__init__.py
 mhkit/tests/wave/io/hindcast/test_hindcast.py
 mhkit/tests/wave/io/hindcast/test_wind_toolkit.py
 mhkit/tidal/__init__.py
-mhkit/tidal/d3d.py
 mhkit/tidal/graphics.py
 mhkit/tidal/performance.py
 mhkit/tidal/resource.py
 mhkit/tidal/io/__init__.py
+mhkit/tidal/io/d3d.py
 mhkit/tidal/io/noaa.py
+mhkit/utils/__init__.py
+mhkit/utils/cache.py
+mhkit/utils/stat_utils.py
+mhkit/utils/time_utils.py
+mhkit/utils/type_handling.py
+mhkit/utils/upcrossing.py
 mhkit/wave/__init__.py
 mhkit/wave/contours.py
 mhkit/wave/graphics.py
 mhkit/wave/performance.py
 mhkit/wave/resource.py
 mhkit/wave/io/__init__.py
 mhkit/wave/io/cdip.py
```

### Comparing `mhkit-0.7.0/setup.py` & `mhkit-0.8.0/setup.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,45 +1,47 @@
-from setuptools import setup, find_packages
-from distutils.core import Extension
 import os
 import re
+from setuptools import setup, find_packages
 
-DISTNAME = 'mhkit'
+DISTNAME = "mhkit"
 PACKAGES = find_packages()
 EXTENSIONS = []
-DESCRIPTION = 'Marine and Hydrokinetic Toolkit'
-AUTHOR = 'MHKiT developers'
-MAINTAINER_EMAIL = ''
-LICENSE = 'Revised BSD'
-URL = 'https://github.com/MHKiT-Software/mhkit-python'
-CLASSIFIERS = ['Development Status :: 3 - Alpha',
-               'Programming Language :: Python :: 3',
-               'Topic :: Scientific/Engineering',
-               'Intended Audience :: Science/Research',
-               'Operating System :: OS Independent',
-               ]
-DEPENDENCIES = ['pandas>=1.0.0, <=1.5.0',
-                'numpy>=1.21.0',
-                'scipy',
-                'matplotlib',
-                'requests',
-                'pecos>=0.1.9',
-                'fatpack',
-                'lxml',
-                'scikit-learn',
-                'NREL-rex>=0.2.63',
-                'six>=1.13.0',
-                'h5py>=3.6.0',
-                'h5pyd >=0.7.0, <=0.10.3',
-                'netCDF4<=1.5.8',
-                'xarray<=2022.9.0',
-                'statsmodels',
-                'pytz',
-                'bottleneck',
-                'beautifulsoup4',]
+DESCRIPTION = "Marine and Hydrokinetic Toolkit"
+AUTHOR = "MHKiT developers"
+MAINTAINER_EMAIL = ""
+LICENSE = "Revised BSD"
+URL = "https://github.com/MHKiT-Software/mhkit-python"
+CLASSIFIERS = [
+    "Development Status :: 3 - Alpha",
+    "Programming Language :: Python :: 3",
+    "Topic :: Scientific/Engineering",
+    "Intended Audience :: Science/Research",
+    "Operating System :: OS Independent",
+]
+DEPENDENCIES = [
+    "pandas>=1.0.0",
+    "numpy>=1.21.0",
+    "scipy",
+    "matplotlib",
+    "requests",
+    "pecos>=0.3.0",
+    "fatpack",
+    "lxml",
+    "scikit-learn",
+    "NREL-rex>=0.2.63",
+    "six>=1.13.0",
+    "h5py>=3.6.0",
+    "h5pyd >=0.7.0",
+    "netCDF4",
+    "xarray",
+    "statsmodels",
+    "pytz",
+    "bottleneck",
+    "beautifulsoup4",
+]
 
 LONG_DESCRIPTION = """
 MHKiT-Python is a Python package designed for marine renewable energy applications to assist in 
 data processing and visualization.  The software package includes functionality for:
 
 * Data processing
 * Data visualization
@@ -66,33 +68,33 @@
 The software is distributed under the Revised BSD License.
 See [copyright and license](LICENSE.md) for more information.
 """
 
 
 # get version from __init__.py
 file_dir = os.path.abspath(os.path.dirname(__file__))
-with open(os.path.join(file_dir, 'mhkit', '__init__.py')) as f:
+with open(os.path.join(file_dir, "mhkit", "__init__.py")) as f:
     version_file = f.read()
-    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]",
-                              version_file, re.M)
+    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]", version_file, re.M)
     if version_match:
         VERSION = version_match.group(1)
     else:
         raise RuntimeError("Unable to find version string.")
 
-setup(name=DISTNAME,
-      version=VERSION,
-      packages=PACKAGES,
-      ext_modules=EXTENSIONS,
-      description=DESCRIPTION,
-      long_description_content_type="text/markdown",
-      long_description=LONG_DESCRIPTION,
-      author=AUTHOR,
-      maintainer_email=MAINTAINER_EMAIL,
-      license=LICENSE,
-      url=URL,
-      classifiers=CLASSIFIERS,
-      zip_safe=False,
-      install_requires=DEPENDENCIES,
-      scripts=[],
-      include_package_data=True
-      )
+setup(
+    name=DISTNAME,
+    version=VERSION,
+    packages=PACKAGES,
+    ext_modules=EXTENSIONS,
+    description=DESCRIPTION,
+    long_description_content_type="text/markdown",
+    long_description=LONG_DESCRIPTION,
+    author=AUTHOR,
+    maintainer_email=MAINTAINER_EMAIL,
+    license=LICENSE,
+    url=URL,
+    classifiers=CLASSIFIERS,
+    zip_safe=False,
+    install_requires=DEPENDENCIES,
+    scripts=[],
+    include_package_data=True,
+)
```


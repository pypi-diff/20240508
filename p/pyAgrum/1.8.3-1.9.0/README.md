# Comparing `tmp/pyAgrum-1.8.3-cp39-cp39-win_amd64.whl.zip` & `tmp/pyAgrum-1.9.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,46 +1,46 @@
-Zip file size: 2601417 bytes, number of entries: 44
--rw-rw-rw-  2.0 fat     9039 b- defN 23-Jun-11 20:42 pyAgrum/config.py
--rw-rw-rw-  2.0 fat     3555 b- defN 23-Jun-11 20:42 pyAgrum/defaults.ini
--rw-rw-rw-  2.0 fat     9098 b- defN 23-Jun-11 20:44 pyAgrum/deprecated.py
--rw-rw-rw-  2.0 fat   724210 b- defN 23-Jun-11 20:42 pyAgrum/pyAgrum.py
--rw-rw-rw-  2.0 fat  7369216 b- defN 23-Jun-11 20:51 pyAgrum/_pyAgrum.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat    27046 b- defN 23-Jun-11 20:44 pyAgrum/__init__.py
--rw-rw-rw-  2.0 fat     4185 b- defN 23-Jun-11 20:42 pyAgrum/causal/notebook.py
--rw-rw-rw-  2.0 fat     6933 b- defN 23-Jun-11 20:42 pyAgrum/causal/_CausalFormula.py
--rw-rw-rw-  2.0 fat    13335 b- defN 23-Jun-11 20:42 pyAgrum/causal/_causalImpact.py
--rw-rw-rw-  2.0 fat    12429 b- defN 23-Jun-11 20:42 pyAgrum/causal/_CausalModel.py
--rw-rw-rw-  2.0 fat    19845 b- defN 23-Jun-11 20:42 pyAgrum/causal/_doAST.py
--rw-rw-rw-  2.0 fat    10387 b- defN 23-Jun-11 20:42 pyAgrum/causal/_doCalculus.py
--rw-rw-rw-  2.0 fat     8731 b- defN 23-Jun-11 20:42 pyAgrum/causal/_doorCriteria.py
--rw-rw-rw-  2.0 fat    14451 b- defN 23-Jun-11 20:42 pyAgrum/causal/_dSeparation.py
--rw-rw-rw-  2.0 fat     2032 b- defN 23-Jun-11 20:42 pyAgrum/causal/_exceptions.py
--rw-rw-rw-  2.0 fat     1560 b- defN 23-Jun-11 20:42 pyAgrum/causal/_types.py
--rw-rw-rw-  2.0 fat     2310 b- defN 23-Jun-11 20:42 pyAgrum/causal/__init__.py
--rw-rw-rw-  2.0 fat     9230 b- defN 23-Jun-11 20:42 pyAgrum/lib/bn2graph.py
--rw-rw-rw-  2.0 fat    15908 b- defN 23-Jun-11 20:42 pyAgrum/lib/bn2roc.py
--rw-rw-rw-  2.0 fat     4839 b- defN 23-Jun-11 20:42 pyAgrum/lib/bn2scores.py
--rw-rw-rw-  2.0 fat    19114 b- defN 23-Jun-11 20:42 pyAgrum/lib/bn_vs_bn.py
--rw-rw-rw-  2.0 fat     1281 b- defN 23-Jun-11 20:42 pyAgrum/lib/classifier.py
--rw-rw-rw-  2.0 fat     9505 b- defN 23-Jun-11 20:42 pyAgrum/lib/cn2graph.py
--rw-rw-rw-  2.0 fat    11095 b- defN 23-Jun-11 20:42 pyAgrum/lib/dynamicBN.py
--rw-rw-rw-  2.0 fat    34130 b- defN 23-Jun-11 20:42 pyAgrum/lib/explain.py
--rw-rw-rw-  2.0 fat     3474 b- defN 23-Jun-11 20:42 pyAgrum/lib/export.py
--rw-rw-rw-  2.0 fat     8640 b- defN 23-Jun-11 20:42 pyAgrum/lib/id2graph.py
--rw-rw-rw-  2.0 fat    12628 b- defN 23-Jun-11 20:42 pyAgrum/lib/image.py
--rw-rw-rw-  2.0 fat     9370 b- defN 23-Jun-11 20:42 pyAgrum/lib/ipython.py
--rw-rw-rw-  2.0 fat      234 b- defN 23-Jun-11 20:42 pyAgrum/lib/mn2graph.py
--rw-rw-rw-  2.0 fat    17129 b- defN 23-Jun-11 20:42 pyAgrum/lib/mrf2graph.py
--rw-rw-rw-  2.0 fat    54833 b- defN 23-Jun-11 20:42 pyAgrum/lib/notebook.py
--rw-rw-rw-  2.0 fat    12649 b- defN 23-Jun-11 20:42 pyAgrum/lib/proba_histogram.py
--rw-rw-rw-  2.0 fat     4969 b- defN 23-Jun-11 20:42 pyAgrum/lib/_colors.py
--rw-rw-rw-  2.0 fat     1108 b- defN 23-Jun-11 20:42 pyAgrum/lib/__init__.py
--rw-rw-rw-  2.0 fat    38297 b- defN 23-Jun-11 20:42 pyAgrum/skbn/bnclassifier.py
--rw-rw-rw-  2.0 fat    39166 b- defN 23-Jun-11 20:42 pyAgrum/skbn/discretizer.py
--rw-rw-rw-  2.0 fat    11326 b- defN 23-Jun-11 20:42 pyAgrum/skbn/_learningMethods.py
--rw-rw-rw-  2.0 fat     6619 b- defN 23-Jun-11 20:42 pyAgrum/skbn/_MBCalcul.py
--rw-rw-rw-  2.0 fat    10287 b- defN 23-Jun-11 20:42 pyAgrum/skbn/_utils.py
--rw-rw-rw-  2.0 fat     1480 b- defN 23-Jun-11 20:42 pyAgrum/skbn/__init__.py
--rw-rw-rw-  2.0 fat     2281 b- defN 23-Jun-11 20:44 pyAgrum-1.8.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat     3623 b- defN 23-Jun-11 20:51 pyAgrum-1.8.3.dist-info/RECORD
--rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-11 20:51 pyAgrum-1.8.3.dist-info/WHEEL
-44 files, 8581669 bytes uncompressed, 2595837 bytes compressed:  69.8%
+Zip file size: 2603736 bytes, number of entries: 44
+-rw-rw-rw-  2.0 fat     9039 b- defN 23-Jul-25 16:08 pyAgrum/config.py
+-rw-rw-rw-  2.0 fat     3663 b- defN 23-Jul-25 16:08 pyAgrum/defaults.ini
+-rw-rw-rw-  2.0 fat     8631 b- defN 23-Jul-25 16:11 pyAgrum/deprecated.py
+-rw-rw-rw-  2.0 fat   725587 b- defN 23-Jul-25 16:08 pyAgrum/pyAgrum.py
+-rw-rw-rw-  2.0 fat  7369216 b- defN 23-Jul-25 16:17 pyAgrum/_pyAgrum.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat    27046 b- defN 23-Jul-25 16:11 pyAgrum/__init__.py
+-rw-rw-rw-  2.0 fat     4185 b- defN 23-Jul-25 16:08 pyAgrum/causal/notebook.py
+-rw-rw-rw-  2.0 fat     6933 b- defN 23-Jul-25 16:08 pyAgrum/causal/_CausalFormula.py
+-rw-rw-rw-  2.0 fat    13335 b- defN 23-Jul-25 16:08 pyAgrum/causal/_causalImpact.py
+-rw-rw-rw-  2.0 fat    12429 b- defN 23-Jul-25 16:08 pyAgrum/causal/_CausalModel.py
+-rw-rw-rw-  2.0 fat    19845 b- defN 23-Jul-25 16:08 pyAgrum/causal/_doAST.py
+-rw-rw-rw-  2.0 fat    10387 b- defN 23-Jul-25 16:08 pyAgrum/causal/_doCalculus.py
+-rw-rw-rw-  2.0 fat     8731 b- defN 23-Jul-25 16:08 pyAgrum/causal/_doorCriteria.py
+-rw-rw-rw-  2.0 fat    14451 b- defN 23-Jul-25 16:08 pyAgrum/causal/_dSeparation.py
+-rw-rw-rw-  2.0 fat     2032 b- defN 23-Jul-25 16:08 pyAgrum/causal/_exceptions.py
+-rw-rw-rw-  2.0 fat     1560 b- defN 23-Jul-25 16:08 pyAgrum/causal/_types.py
+-rw-rw-rw-  2.0 fat     2310 b- defN 23-Jul-25 16:08 pyAgrum/causal/__init__.py
+-rw-rw-rw-  2.0 fat     9164 b- defN 23-Jul-25 16:08 pyAgrum/lib/bn2graph.py
+-rw-rw-rw-  2.0 fat    25054 b- defN 23-Jul-25 16:08 pyAgrum/lib/bn2roc.py
+-rw-rw-rw-  2.0 fat     4839 b- defN 23-Jul-25 16:08 pyAgrum/lib/bn2scores.py
+-rw-rw-rw-  2.0 fat    19143 b- defN 23-Jul-25 16:08 pyAgrum/lib/bn_vs_bn.py
+-rw-rw-rw-  2.0 fat     1281 b- defN 23-Jul-25 16:08 pyAgrum/lib/classifier.py
+-rw-rw-rw-  2.0 fat     9437 b- defN 23-Jul-25 16:08 pyAgrum/lib/cn2graph.py
+-rw-rw-rw-  2.0 fat    11095 b- defN 23-Jul-25 16:08 pyAgrum/lib/dynamicBN.py
+-rw-rw-rw-  2.0 fat    33987 b- defN 23-Jul-25 16:08 pyAgrum/lib/explain.py
+-rw-rw-rw-  2.0 fat     3484 b- defN 23-Jul-25 16:08 pyAgrum/lib/export.py
+-rw-rw-rw-  2.0 fat     8611 b- defN 23-Jul-25 16:08 pyAgrum/lib/id2graph.py
+-rw-rw-rw-  2.0 fat    12628 b- defN 23-Jul-25 16:08 pyAgrum/lib/image.py
+-rw-rw-rw-  2.0 fat     9370 b- defN 23-Jul-25 16:08 pyAgrum/lib/ipython.py
+-rw-rw-rw-  2.0 fat      235 b- defN 23-Jul-25 16:08 pyAgrum/lib/mn2graph.py
+-rw-rw-rw-  2.0 fat    17045 b- defN 23-Jul-25 16:08 pyAgrum/lib/mrf2graph.py
+-rw-rw-rw-  2.0 fat    54788 b- defN 23-Jul-25 16:08 pyAgrum/lib/notebook.py
+-rw-rw-rw-  2.0 fat    13848 b- defN 23-Jul-25 16:08 pyAgrum/lib/proba_histogram.py
+-rw-rw-rw-  2.0 fat     4984 b- defN 23-Jul-25 16:08 pyAgrum/lib/_colors.py
+-rw-rw-rw-  2.0 fat     1108 b- defN 23-Jul-25 16:08 pyAgrum/lib/__init__.py
+-rw-rw-rw-  2.0 fat    37942 b- defN 23-Jul-25 16:08 pyAgrum/skbn/bnclassifier.py
+-rw-rw-rw-  2.0 fat    41630 b- defN 23-Jul-25 16:08 pyAgrum/skbn/discretizer.py
+-rw-rw-rw-  2.0 fat    11255 b- defN 23-Jul-25 16:08 pyAgrum/skbn/_learningMethods.py
+-rw-rw-rw-  2.0 fat     6742 b- defN 23-Jul-25 16:08 pyAgrum/skbn/_MBCalcul.py
+-rw-rw-rw-  2.0 fat    10347 b- defN 23-Jul-25 16:08 pyAgrum/skbn/_utils.py
+-rw-rw-rw-  2.0 fat     1480 b- defN 23-Jul-25 16:08 pyAgrum/skbn/__init__.py
+-rw-rw-rw-  2.0 fat     2281 b- defN 23-Jul-25 16:11 pyAgrum-1.9.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat     3623 b- defN 23-Jul-25 16:17 pyAgrum-1.9.0.dist-info/RECORD
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-25 16:17 pyAgrum-1.9.0.dist-info/WHEEL
+44 files, 8594873 bytes uncompressed, 2598156 bytes compressed:  69.8%
```

## zipnote {}

```diff
@@ -117,17 +117,17 @@
 
 Filename: pyAgrum/skbn/_utils.py
 Comment: 
 
 Filename: pyAgrum/skbn/__init__.py
 Comment: 
 
-Filename: pyAgrum-1.8.3.dist-info/METADATA
+Filename: pyAgrum-1.9.0.dist-info/METADATA
 Comment: 
 
-Filename: pyAgrum-1.8.3.dist-info/RECORD
+Filename: pyAgrum-1.9.0.dist-info/RECORD
 Comment: 
 
-Filename: pyAgrum-1.8.3.dist-info/WHEEL
+Filename: pyAgrum-1.9.0.dist-info/WHEEL
 Comment: 
 
 Zip file comment:
```

## pyAgrum/defaults.ini

```diff
@@ -32,192 +32,198 @@
 000001f0: 6869 7374 6f67 7261 6d5f 686f 7269 7a6f  histogram_horizo
 00000200: 6e74 616c 5f74 6872 6573 686f 6c64 203d  ntal_threshold =
 00000210: 2038 0d0a 2020 6869 7374 6f67 7261 6d5f   8..  histogram_
 00000220: 6c69 6e65 5f74 6872 6573 686f 6c64 203d  line_threshold =
 00000230: 2034 300d 0a20 2068 6973 746f 6772 616d   40..  histogram
 00000240: 5f63 6f6c 6f72 3d64 6172 6b73 6561 6772  _color=darkseagr
 00000250: 6565 6e0d 0a20 2068 6973 746f 6772 616d  een..  histogram
-00000260: 5f75 7365 5f70 6572 6365 6e74 203d 2054  _use_percent = T
-00000270: 7275 650d 0a0d 0a20 2023 2063 6f6d 7061  rue....  # compa
-00000280: 6374 207c 2020 636c 6173 7369 6361 6c0d  ct |  classical.
-00000290: 0a20 2068 6973 746f 6772 616d 5f6d 6f64  .  histogram_mod
-000002a0: 653d 636f 6d70 6163 740d 0a0d 0a20 2023  e=compact....  #
-000002b0: 206d 6572 6765 207c 2072 6576 6d65 7267   merge | revmerg
-000002c0: 6520 7c20 6e6f 6d65 7267 650d 0a20 2070  e | nomerge..  p
-000002d0: 6f74 656e 7469 616c 5f70 6172 656e 745f  otential_parent_
-000002e0: 7661 6c75 6573 203d 206d 6572 6765 0d0a  values = merge..
-000002f0: 0d0a 2020 2320 7365 6520 6d61 7470 6c6f  ..  # see matplo
-00000300: 746c 6962 0d0a 2020 6669 6775 7265 5f66  tlib..  figure_f
-00000310: 6163 6563 6f6c 6f72 203d 2023 4530 4530  acecolor = #E0E0
-00000320: 4530 0d0a 0d0a 2020 2320 666c 6f77 2063  E0....  # flow c
-00000330: 6f6e 6669 6775 7261 7469 6f6e 0d0a 2020  onfiguration..  
-00000340: 666c 6f77 5f62 6163 6b67 726f 756e 645f  flow_background_
-00000350: 636f 6c6f 7220 3d20 7472 616e 7370 6172  color = transpar
-00000360: 656e 740d 0a20 2066 6c6f 775f 626f 7264  ent..  flow_bord
-00000370: 6572 5f63 6f6c 6f72 203d 2074 7261 6e73  er_color = trans
-00000380: 7061 7265 6e74 0d0a 2020 666c 6f77 5f62  parent..  flow_b
-00000390: 6f72 6465 725f 7769 6474 6820 3d20 300d  order_width = 0.
-000003a0: 0a0d 0a20 2023 2073 7667 206f 7220 706e  ...  # svg or pn
-000003b0: 670d 0a20 2067 7261 7068 5f66 6f72 6d61  g..  graph_forma
-000003c0: 7420 3d20 7376 670d 0a0d 0a20 2073 686f  t = svg....  sho
-000003d0: 775f 696e 6665 7265 6e63 655f 7469 6d65  w_inference_time
-000003e0: 203d 2054 7275 650d 0a0d 0a20 2023 7468   = True....  #th
-000003f0: 656d 696e 670d 0a20 2064 6566 6175 6c74  eming..  default
-00000400: 5f61 7263 5f63 6f6c 6f72 203d 2023 3441  _arc_color = #4A
-00000410: 3441 3441 0d0a 2020 6465 6661 756c 745f  4A4A..  default_
-00000420: 6e6f 6465 5f62 6763 6f6c 6f72 203d 2023  node_bgcolor = #
-00000430: 3430 3430 3430 0d0a 2020 6465 6661 756c  404040..  defaul
-00000440: 745f 6e6f 6465 5f66 6763 6f6c 6f72 203d  t_node_fgcolor =
-00000450: 2077 6869 7465 0d0a 2020 6576 6964 656e   white..  eviden
-00000460: 6365 5f62 6763 6f6c 6f72 203d 2073 616e  ce_bgcolor = san
-00000470: 6479 6272 6f77 6e0d 0a20 2065 7669 6465  dybrown..  evide
-00000480: 6e63 655f 6667 636f 6c6f 7220 3d20 626c  nce_fgcolor = bl
-00000490: 6163 6b0d 0a20 2064 6566 6175 6c74 5f6e  ack..  default_n
-000004a0: 6f64 655f 636d 6170 203d 2050 6173 7465  ode_cmap = Paste
-000004b0: 6c31 0d0a 2020 6465 6661 756c 745f 6172  l1..  default_ar
-000004c0: 635f 636d 6170 203d 2042 7547 6e0d 0a20  c_cmap = BuGn.. 
-000004d0: 2064 6566 6175 6c74 5f65 6467 655f 636d   default_edge_cm
-000004e0: 6170 203d 2042 7547 6e0d 0a0d 0a20 2064  ap = BuGn....  d
-000004f0: 6566 6175 6c74 5f67 7261 7068 5f73 697a  efault_graph_siz
-00000500: 6520 3d20 350d 0a20 2064 6566 6175 6c74  e = 5..  default
-00000510: 5f67 7261 7068 5f69 6e66 6572 656e 6365  _graph_inference
-00000520: 5f73 697a 6520 3d20 380d 0a0d 0a20 2023  _size = 8....  #
-00000530: 2067 7261 7068 207c 2066 6163 746f 7267   graph | factorg
-00000540: 7261 7068 0d0a 2020 6465 6661 756c 745f  raph..  default_
-00000550: 6d61 726b 6f76 6e65 7477 6f72 6b5f 7669  markovnetwork_vi
-00000560: 6577 203d 2066 6163 746f 7267 7261 7068  ew = factorgraph
-00000570: 0d0a 0d0a 2020 2320 6a75 6e63 7469 6f6e  ....  # junction
-00000580: 7472 6565 202f 2063 6c69 7175 6567 7261  tree / cliquegra
-00000590: 7068 0d0a 2020 6a75 6e63 7469 6f6e 7472  ph..  junctiontr
-000005a0: 6565 5f67 7261 7068 5f73 697a 6520 3d20  ee_graph_size = 
-000005b0: 3130 0d0a 2020 6a75 6e63 7469 6f6e 7472  10..  junctiontr
-000005c0: 6565 5f77 6974 685f 6e61 6d65 733d 5472  ee_with_names=Tr
-000005d0: 7565 0d0a 2020 6a75 6e63 7469 6f6e 7472  ue..  junctiontr
-000005e0: 6565 5f73 6570 6172 6174 6f72 5f62 6763  ee_separator_bgc
-000005f0: 6f6c 6f72 3d70 616c 6567 7265 656e 0d0a  olor=palegreen..
-00000600: 2020 6a75 6e63 7469 6f6e 7472 6565 5f73    junctiontree_s
-00000610: 6570 6172 6174 6f72 5f66 6763 6f6c 6f72  eparator_fgcolor
-00000620: 3d62 6c61 636b 0d0a 2020 6a75 6e63 7469  =black..  juncti
-00000630: 6f6e 7472 6565 5f73 6570 6172 6174 6f72  ontree_separator
-00000640: 5f66 6f6e 7473 697a 653d 380d 0a20 206a  _fontsize=8..  j
-00000650: 756e 6374 696f 6e74 7265 655f 636c 6971  unctiontree_cliq
-00000660: 7565 5f62 6763 6f6c 6f72 3d62 7572 6c79  ue_bgcolor=burly
-00000670: 776f 6f64 0d0a 2020 6a75 6e63 7469 6f6e  wood..  junction
-00000680: 7472 6565 5f63 6c69 7175 655f 6667 636f  tree_clique_fgco
-00000690: 6c6f 723d 626c 6163 6b0d 0a20 206a 756e  lor=black..  jun
-000006a0: 6374 696f 6e74 7265 655f 636c 6971 7565  ctiontree_clique
-000006b0: 5f66 6f6e 7473 697a 653d 3130 0d0a 2020  _fontsize=10..  
-000006c0: 6a75 6e63 7469 6f6e 7472 6565 5f6d 6170  junctiontree_map
-000006d0: 5f63 6c69 7175 6573 6361 6c65 3d30 2e33  _cliquescale=0.3
+00000260: 5f65 6467 655f 636f 6c6f 723d 6461 726b  _edge_color=dark
+00000270: 6772 6565 6e0d 0a20 2068 6973 746f 6772  green..  histogr
+00000280: 616d 5f75 7365 5f70 6572 6365 6e74 203d  am_use_percent =
+00000290: 2054 7275 650d 0a20 2023 2068 6973 746f   True..  # histo
+000002a0: 6772 616d 207c 2062 6172 0d0a 2020 6869  gram | bar..  hi
+000002b0: 7374 6f67 7261 6d5f 6469 7363 7265 7469  stogram_discreti
+000002c0: 7a65 645f 7669 7375 616c 6973 6174 696f  zed_visualisatio
+000002d0: 6e20 3d20 6869 7374 6f67 7261 6d0d 0a0d  n = histogram...
+000002e0: 0a0d 0a20 2023 2063 6f6d 7061 6374 207c  ...  # compact |
+000002f0: 2020 636c 6173 7369 6361 6c0d 0a20 2068    classical..  h
+00000300: 6973 746f 6772 616d 5f6d 6f64 653d 636f  istogram_mode=co
+00000310: 6d70 6163 740d 0a0d 0a20 2023 206d 6572  mpact....  # mer
+00000320: 6765 207c 2072 6576 6d65 7267 6520 7c20  ge | revmerge | 
+00000330: 6e6f 6d65 7267 650d 0a20 2070 6f74 656e  nomerge..  poten
+00000340: 7469 616c 5f70 6172 656e 745f 7661 6c75  tial_parent_valu
+00000350: 6573 203d 206d 6572 6765 0d0a 0d0a 2020  es = merge....  
+00000360: 2320 7365 6520 6d61 7470 6c6f 746c 6962  # see matplotlib
+00000370: 0d0a 2020 6669 6775 7265 5f66 6163 6563  ..  figure_facec
+00000380: 6f6c 6f72 203d 2023 4530 4530 4530 0d0a  olor = #E0E0E0..
+00000390: 0d0a 2020 2320 666c 6f77 2063 6f6e 6669  ..  # flow confi
+000003a0: 6775 7261 7469 6f6e 0d0a 2020 666c 6f77  guration..  flow
+000003b0: 5f62 6163 6b67 726f 756e 645f 636f 6c6f  _background_colo
+000003c0: 7220 3d20 7472 616e 7370 6172 656e 740d  r = transparent.
+000003d0: 0a20 2066 6c6f 775f 626f 7264 6572 5f63  .  flow_border_c
+000003e0: 6f6c 6f72 203d 2074 7261 6e73 7061 7265  olor = transpare
+000003f0: 6e74 0d0a 2020 666c 6f77 5f62 6f72 6465  nt..  flow_borde
+00000400: 725f 7769 6474 6820 3d20 300d 0a0d 0a20  r_width = 0.... 
+00000410: 2023 2073 7667 206f 7220 706e 670d 0a20   # svg or png.. 
+00000420: 2067 7261 7068 5f66 6f72 6d61 7420 3d20   graph_format = 
+00000430: 7376 670d 0a0d 0a20 2073 686f 775f 696e  svg....  show_in
+00000440: 6665 7265 6e63 655f 7469 6d65 203d 2054  ference_time = T
+00000450: 7275 650d 0a0d 0a20 2023 7468 656d 696e  rue....  #themin
+00000460: 670d 0a20 2064 6566 6175 6c74 5f61 7263  g..  default_arc
+00000470: 5f63 6f6c 6f72 203d 2023 3441 3441 3441  _color = #4A4A4A
+00000480: 0d0a 2020 6465 6661 756c 745f 6e6f 6465  ..  default_node
+00000490: 5f62 6763 6f6c 6f72 203d 2023 3430 3430  _bgcolor = #4040
+000004a0: 3430 0d0a 2020 6465 6661 756c 745f 6e6f  40..  default_no
+000004b0: 6465 5f66 6763 6f6c 6f72 203d 2077 6869  de_fgcolor = whi
+000004c0: 7465 0d0a 2020 6576 6964 656e 6365 5f62  te..  evidence_b
+000004d0: 6763 6f6c 6f72 203d 2073 616e 6479 6272  gcolor = sandybr
+000004e0: 6f77 6e0d 0a20 2065 7669 6465 6e63 655f  own..  evidence_
+000004f0: 6667 636f 6c6f 7220 3d20 626c 6163 6b0d  fgcolor = black.
+00000500: 0a20 2064 6566 6175 6c74 5f6e 6f64 655f  .  default_node_
+00000510: 636d 6170 203d 2050 6173 7465 6c31 0d0a  cmap = Pastel1..
+00000520: 2020 6465 6661 756c 745f 6172 635f 636d    default_arc_cm
+00000530: 6170 203d 2042 7547 6e0d 0a20 2064 6566  ap = BuGn..  def
+00000540: 6175 6c74 5f65 6467 655f 636d 6170 203d  ault_edge_cmap =
+00000550: 2042 7547 6e0d 0a0d 0a20 2064 6566 6175   BuGn....  defau
+00000560: 6c74 5f67 7261 7068 5f73 697a 6520 3d20  lt_graph_size = 
+00000570: 350d 0a20 2064 6566 6175 6c74 5f67 7261  5..  default_gra
+00000580: 7068 5f69 6e66 6572 656e 6365 5f73 697a  ph_inference_siz
+00000590: 6520 3d20 380d 0a0d 0a20 2023 2067 7261  e = 8....  # gra
+000005a0: 7068 207c 2066 6163 746f 7267 7261 7068  ph | factorgraph
+000005b0: 0d0a 2020 6465 6661 756c 745f 6d61 726b  ..  default_mark
+000005c0: 6f76 6e65 7477 6f72 6b5f 7669 6577 203d  ovnetwork_view =
+000005d0: 2066 6163 746f 7267 7261 7068 0d0a 0d0a   factorgraph....
+000005e0: 2020 2320 6a75 6e63 7469 6f6e 7472 6565    # junctiontree
+000005f0: 202f 2063 6c69 7175 6567 7261 7068 0d0a   / cliquegraph..
+00000600: 2020 6a75 6e63 7469 6f6e 7472 6565 5f67    junctiontree_g
+00000610: 7261 7068 5f73 697a 6520 3d20 3130 0d0a  raph_size = 10..
+00000620: 2020 6a75 6e63 7469 6f6e 7472 6565 5f77    junctiontree_w
+00000630: 6974 685f 6e61 6d65 733d 5472 7565 0d0a  ith_names=True..
+00000640: 2020 6a75 6e63 7469 6f6e 7472 6565 5f73    junctiontree_s
+00000650: 6570 6172 6174 6f72 5f62 6763 6f6c 6f72  eparator_bgcolor
+00000660: 3d70 616c 6567 7265 656e 0d0a 2020 6a75  =palegreen..  ju
+00000670: 6e63 7469 6f6e 7472 6565 5f73 6570 6172  nctiontree_separ
+00000680: 6174 6f72 5f66 6763 6f6c 6f72 3d62 6c61  ator_fgcolor=bla
+00000690: 636b 0d0a 2020 6a75 6e63 7469 6f6e 7472  ck..  junctiontr
+000006a0: 6565 5f73 6570 6172 6174 6f72 5f66 6f6e  ee_separator_fon
+000006b0: 7473 697a 653d 380d 0a20 206a 756e 6374  tsize=8..  junct
+000006c0: 696f 6e74 7265 655f 636c 6971 7565 5f62  iontree_clique_b
+000006d0: 6763 6f6c 6f72 3d62 7572 6c79 776f 6f64  gcolor=burlywood
 000006e0: 0d0a 2020 6a75 6e63 7469 6f6e 7472 6565  ..  junctiontree
-000006f0: 5f6d 6170 5f73 6570 7363 616c 653d 302e  _map_sepscale=0.
-00000700: 310d 0a20 206a 756e 6374 696f 6e74 7265  1..  junctiontre
-00000710: 655f 6d61 705f 6564 6765 6c65 6e3d 310d  e_map_edgelen=1.
-00000720: 0a0d 0a20 2023 2073 7472 7563 7475 7261  ...  # structura
-00000730: 6c20 6469 6666 0d0a 2020 6772 6170 6864  l diff..  graphd
-00000740: 6966 665f 6d69 7373 696e 675f 7374 796c  iff_missing_styl
-00000750: 653d 6461 7368 6564 0d0a 2020 6772 6170  e=dashed..  grap
-00000760: 6864 6966 665f 6d69 7373 696e 675f 636f  hdiff_missing_co
-00000770: 6c6f 723d 7265 640d 0a20 2067 7261 7068  lor=red..  graph
-00000780: 6469 6666 5f6f 7665 7266 6c6f 775f 7374  diff_overflow_st
-00000790: 796c 653d 6461 7368 6564 0d0a 2020 6772  yle=dashed..  gr
-000007a0: 6170 6864 6966 665f 6f76 6572 666c 6f77  aphdiff_overflow
-000007b0: 5f63 6f6c 6f72 3d70 7572 706c 650d 0a20  _color=purple.. 
-000007c0: 2067 7261 7068 6469 6666 5f72 6576 6572   graphdiff_rever
-000007d0: 7365 645f 7374 796c 653d 736f 6c69 640d  sed_style=solid.
-000007e0: 0a20 2067 7261 7068 6469 6666 5f72 6576  .  graphdiff_rev
-000007f0: 6572 7365 645f 636f 6c6f 723d 7075 7270  ersed_color=purp
-00000800: 6c65 0d0a 2020 6772 6170 6864 6966 665f  le..  graphdiff_
-00000810: 636f 7272 6563 745f 7374 796c 653d 736f  correct_style=so
-00000820: 6c69 640d 0a20 2067 7261 7068 6469 6666  lid..  graphdiff
-00000830: 5f63 6f72 7265 6374 5f63 6f6c 6f72 3d67  _correct_color=g
-00000840: 7265 790d 0a0d 0a0d 0a5b 424e 5d0d 0a20  rey......[BN].. 
-00000850: 2061 6c6c 6f77 5f6d 6f64 6966 6963 6174   allow_modificat
-00000860: 696f 6e5f 7768 656e 5f73 6176 696e 6720  ion_when_saving 
-00000870: 3d20 4661 6c73 650d 0a0d 0a5b 6661 6374  = False....[fact
-00000880: 6f72 6772 6170 685d 0d0a 2020 2374 6865  orgraph]..  #the
-00000890: 6d69 6e67 0d0a 2020 6465 6661 756c 745f  ming..  default_
-000008a0: 6e6f 6465 5f62 6763 6f6c 6f72 203d 2063  node_bgcolor = c
-000008b0: 6f72 616c 0d0a 2020 6465 6661 756c 745f  oral..  default_
-000008c0: 6e6f 6465 5f66 6763 6f6c 6f72 203d 2062  node_fgcolor = b
-000008d0: 6c61 636b 0d0a 2020 6465 6661 756c 745f  lack..  default_
-000008e0: 6661 6374 6f72 5f62 6763 6f6c 6f72 203d  factor_bgcolor =
-000008f0: 2062 7572 6c79 776f 6f64 0d0a 2020 6564   burlywood..  ed
-00000900: 6765 5f6c 656e 6774 6820 3d20 302e 370d  ge_length = 0.7.
-00000910: 0a20 2065 6467 655f 6c65 6e67 7468 5f69  .  edge_length_i
-00000920: 6e66 6572 656e 6365 203d 2030 2e39 0d0a  nference = 0.9..
-00000930: 0d0a 5b64 796e 616d 6963 424e 5d0d 0a20  ..[dynamicBN].. 
-00000940: 2023 7468 656d 696e 670d 0a20 2064 6566   #theming..  def
-00000950: 6175 6c74 5f67 7261 7068 5f73 697a 6520  ault_graph_size 
-00000960: 3d20 360d 0a0d 0a5b 696e 666c 7565 6e63  = 6....[influenc
-00000970: 6544 6961 6772 616d 5d0d 0a20 2023 7468  eDiagram]..  #th
-00000980: 656d 696e 670d 0a20 2064 6566 6175 6c74  eming..  default
-00000990: 5f67 7261 7068 5f73 697a 6520 3d20 360d  _graph_size = 6.
-000009a0: 0a20 2064 6566 6175 6c74 5f63 6861 6e63  .  default_chanc
-000009b0: 655f 6267 636f 6c6f 7220 3d20 2338 3038  e_bgcolor = #808
-000009c0: 3038 300d 0a20 2064 6566 6175 6c74 5f63  080..  default_c
-000009d0: 6861 6e63 655f 6667 636f 6c6f 7220 3d20  hance_fgcolor = 
-000009e0: 7768 6974 650d 0a20 2064 6566 6175 6c74  white..  default
-000009f0: 5f75 7469 6c69 7479 5f62 6763 6f6c 6f72  _utility_bgcolor
-00000a00: 203d 2023 3530 3530 3841 0d0a 2020 6465   = #50508A..  de
-00000a10: 6661 756c 745f 7574 696c 6974 795f 6667  fault_utility_fg
-00000a20: 636f 6c6f 7220 3d20 7768 6974 650d 0a20  color = white.. 
-00000a30: 2064 6566 6175 6c74 5f64 6563 6973 696f   default_decisio
-00000a40: 6e5f 6267 636f 6c6f 7220 3d20 2339 4135  n_bgcolor = #9A5
-00000a50: 3035 300d 0a20 2064 6566 6175 6c74 5f64  050..  default_d
-00000a60: 6563 6973 696f 6e5f 6667 636f 6c6f 7220  ecision_fgcolor 
-00000a70: 3d20 7768 6974 650d 0a0d 0a20 2063 6861  = white....  cha
-00000a80: 6e63 655f 7368 6170 6520 3d20 656c 6c69  nce_shape = elli
-00000a90: 7073 650d 0a20 2075 7469 6c69 7479 5f73  pse..  utility_s
-00000aa0: 6861 7065 203d 2068 6578 6167 6f6e 0d0a  hape = hexagon..
-00000ab0: 2020 6465 6369 7369 6f6e 5f73 6861 7065    decision_shape
-00000ac0: 203d 2062 6f78 0d0a 0d0a 2020 6465 6369   = box....  deci
-00000ad0: 7369 6f6e 5f61 7263 5f73 7479 6c65 203d  sion_arc_style =
-00000ae0: 2074 6170 6572 6564 2c20 626f 6c64 2c20   tapered, bold, 
-00000af0: 646f 7474 6564 0d0a 2020 7574 696c 6974  dotted..  utilit
-00000b00: 795f 6172 635f 7374 796c 6520 3d20 6461  y_arc_style = da
-00000b10: 7368 6564 0d0a 0d0a 2020 6465 6661 756c  shed....  defaul
-00000b20: 745f 6964 5f73 697a 653d 360d 0a20 2064  t_id_size=6..  d
-00000b30: 6566 6175 6c74 5f69 645f 696e 6665 7265  efault_id_infere
-00000b40: 6e63 655f 7369 7a65 3d36 0d0a 0d0a 2020  nce_size=6....  
-00000b50: 7574 696c 6974 795f 7669 7369 626c 655f  utility_visible_
-00000b60: 6469 6769 7473 203d 2032 0d0a 2020 7574  digits = 2..  ut
-00000b70: 696c 6974 795f 7368 6f77 5f73 7464 6576  ility_show_stdev
-00000b80: 203d 2054 7275 650d 0a0d 0a20 2023 7368   = True....  #sh
-00000b90: 6f77 2075 7469 6c69 7479 2028 4661 6c73  ow utility (Fals
-00000ba0: 6529 206f 7220 2d75 7469 6c69 7479 2028  e) or -utility (
-00000bb0: 4c6f 7373 290d 0a20 2075 7469 6c69 7479  Loss)..  utility
-00000bc0: 5f73 686f 775f 6c6f 7373 203d 2046 616c  _show_loss = Fal
-00000bd0: 7365 0d0a 0d0a 5b63 7265 6461 6c6e 6574  se....[credalnet
-00000be0: 5d0d 0a20 2023 7468 656d 696e 670d 0a20  ]..  #theming.. 
-00000bf0: 2064 6566 6175 6c74 5f6e 6f64 655f 6267   default_node_bg
-00000c00: 636f 6c6f 7220 3d20 2334 3034 3034 300d  color = #404040.
-00000c10: 0a20 2064 6566 6175 6c74 5f6e 6f64 655f  .  default_node_
-00000c20: 6667 636f 6c6f 7220 3d20 7768 6974 650d  fgcolor = white.
-00000c30: 0a20 2068 6973 746f 5f6d 6178 5f63 6f6c  .  histo_max_col
-00000c40: 6f72 203d 2023 4242 4646 4141 0d0a 0d0a  or = #BBFFAA....
-00000c50: 5b63 6175 7361 6c5d 0d0a 2020 7368 6f77  [causal]..  show
-00000c60: 5f6c 6174 656e 745f 6e61 6d65 7320 3d20  _latent_names = 
-00000c70: 4661 6c73 650d 0a20 2023 206c 6174 6578  False..  # latex
-00000c80: 2063 6f6d 6d61 6e64 2066 6f72 206e 6f74   command for not
-00000c90: 6174 696f 6e20 6f66 2069 6e74 6572 7665  ation of interve
-00000ca0: 6e74 696f 6e20 696e 2066 6f72 6d75 6c61  ntion in formula
-00000cb0: 0d0a 2020 6c61 7465 785f 646f 5f70 7265  ..  latex_do_pre
-00000cc0: 6669 7820 3d20 5c68 6f6f 6b72 6967 6874  fix = \hookright
-00000cd0: 6172 726f 775c 6d6b 6572 6e2d 362e 356d  arrow\mkern-6.5m
-00000ce0: 750d 0a20 206c 6174 6578 5f64 6f5f 7375  u..  latex_do_su
-00000cf0: 6666 6978 203d 0d0a 0d0a 2020 2374 6865  ffix =....  #the
-00000d00: 6d69 6e67 0d0a 2020 6465 6661 756c 745f  ming..  default_
-00000d10: 6772 6170 685f 7369 7a65 203d 2032 2e35  graph_size = 2.5
-00000d20: 0d0a 2020 6465 6661 756c 745f 6e6f 6465  ..  default_node
-00000d30: 5f62 6763 6f6c 6f72 203d 2023 3430 3430  _bgcolor = #4040
-00000d40: 3430 0d0a 2020 6465 6661 756c 745f 6e6f  40..  default_no
-00000d50: 6465 5f66 6763 6f6c 6f72 203d 2077 6869  de_fgcolor = whi
-00000d60: 7465 0d0a 2020 6465 6661 756c 745f 6c61  te..  default_la
-00000d70: 7465 6e74 5f62 6763 6f6c 6f72 203d 2023  tent_bgcolor = #
-00000d80: 4130 3830 3830 0d0a 2020 6465 6661 756c  A08080..  defaul
-00000d90: 745f 6c61 7465 6e74 5f66 6763 6f6c 6f72  t_latent_fgcolor
-00000da0: 203d 2062 6c61 636b 0d0a 0d0a 5b52 4f43   = black....[ROC
-00000db0: 5d0d 0a20 2064 7261 775f 636f 6c6f 7220  ]..  draw_color 
-00000dc0: 3d20 2330 3038 3830 300d 0a20 2066 696c  = #008800..  fil
-00000dd0: 6c5f 636f 6c6f 7220 3d20 2341 4145 4541  l_color = #AAEEA
-00000de0: 410d 0a                                  A..
+000006f0: 5f63 6c69 7175 655f 6667 636f 6c6f 723d  _clique_fgcolor=
+00000700: 626c 6163 6b0d 0a20 206a 756e 6374 696f  black..  junctio
+00000710: 6e74 7265 655f 636c 6971 7565 5f66 6f6e  ntree_clique_fon
+00000720: 7473 697a 653d 3130 0d0a 2020 6a75 6e63  tsize=10..  junc
+00000730: 7469 6f6e 7472 6565 5f6d 6170 5f63 6c69  tiontree_map_cli
+00000740: 7175 6573 6361 6c65 3d30 2e33 0d0a 2020  quescale=0.3..  
+00000750: 6a75 6e63 7469 6f6e 7472 6565 5f6d 6170  junctiontree_map
+00000760: 5f73 6570 7363 616c 653d 302e 310d 0a20  _sepscale=0.1.. 
+00000770: 206a 756e 6374 696f 6e74 7265 655f 6d61   junctiontree_ma
+00000780: 705f 6564 6765 6c65 6e3d 310d 0a0d 0a20  p_edgelen=1.... 
+00000790: 2023 2073 7472 7563 7475 7261 6c20 6469   # structural di
+000007a0: 6666 0d0a 2020 6772 6170 6864 6966 665f  ff..  graphdiff_
+000007b0: 6d69 7373 696e 675f 7374 796c 653d 6461  missing_style=da
+000007c0: 7368 6564 0d0a 2020 6772 6170 6864 6966  shed..  graphdif
+000007d0: 665f 6d69 7373 696e 675f 636f 6c6f 723d  f_missing_color=
+000007e0: 7265 640d 0a20 2067 7261 7068 6469 6666  red..  graphdiff
+000007f0: 5f6f 7665 7266 6c6f 775f 7374 796c 653d  _overflow_style=
+00000800: 6461 7368 6564 0d0a 2020 6772 6170 6864  dashed..  graphd
+00000810: 6966 665f 6f76 6572 666c 6f77 5f63 6f6c  iff_overflow_col
+00000820: 6f72 3d70 7572 706c 650d 0a20 2067 7261  or=purple..  gra
+00000830: 7068 6469 6666 5f72 6576 6572 7365 645f  phdiff_reversed_
+00000840: 7374 796c 653d 736f 6c69 640d 0a20 2067  style=solid..  g
+00000850: 7261 7068 6469 6666 5f72 6576 6572 7365  raphdiff_reverse
+00000860: 645f 636f 6c6f 723d 7075 7270 6c65 0d0a  d_color=purple..
+00000870: 2020 6772 6170 6864 6966 665f 636f 7272    graphdiff_corr
+00000880: 6563 745f 7374 796c 653d 736f 6c69 640d  ect_style=solid.
+00000890: 0a20 2067 7261 7068 6469 6666 5f63 6f72  .  graphdiff_cor
+000008a0: 7265 6374 5f63 6f6c 6f72 3d67 7265 790d  rect_color=grey.
+000008b0: 0a0d 0a0d 0a5b 424e 5d0d 0a20 2061 6c6c  .....[BN]..  all
+000008c0: 6f77 5f6d 6f64 6966 6963 6174 696f 6e5f  ow_modification_
+000008d0: 7768 656e 5f73 6176 696e 6720 3d20 4661  when_saving = Fa
+000008e0: 6c73 650d 0a0d 0a5b 6661 6374 6f72 6772  lse....[factorgr
+000008f0: 6170 685d 0d0a 2020 2374 6865 6d69 6e67  aph]..  #theming
+00000900: 0d0a 2020 6465 6661 756c 745f 6e6f 6465  ..  default_node
+00000910: 5f62 6763 6f6c 6f72 203d 2063 6f72 616c  _bgcolor = coral
+00000920: 0d0a 2020 6465 6661 756c 745f 6e6f 6465  ..  default_node
+00000930: 5f66 6763 6f6c 6f72 203d 2062 6c61 636b  _fgcolor = black
+00000940: 0d0a 2020 6465 6661 756c 745f 6661 6374  ..  default_fact
+00000950: 6f72 5f62 6763 6f6c 6f72 203d 2062 7572  or_bgcolor = bur
+00000960: 6c79 776f 6f64 0d0a 2020 6564 6765 5f6c  lywood..  edge_l
+00000970: 656e 6774 6820 3d20 302e 370d 0a20 2065  ength = 0.7..  e
+00000980: 6467 655f 6c65 6e67 7468 5f69 6e66 6572  dge_length_infer
+00000990: 656e 6365 203d 2030 2e39 0d0a 0d0a 5b64  ence = 0.9....[d
+000009a0: 796e 616d 6963 424e 5d0d 0a20 2023 7468  ynamicBN]..  #th
+000009b0: 656d 696e 670d 0a20 2064 6566 6175 6c74  eming..  default
+000009c0: 5f67 7261 7068 5f73 697a 6520 3d20 360d  _graph_size = 6.
+000009d0: 0a0d 0a5b 696e 666c 7565 6e63 6544 6961  ...[influenceDia
+000009e0: 6772 616d 5d0d 0a20 2023 7468 656d 696e  gram]..  #themin
+000009f0: 670d 0a20 2064 6566 6175 6c74 5f67 7261  g..  default_gra
+00000a00: 7068 5f73 697a 6520 3d20 360d 0a20 2064  ph_size = 6..  d
+00000a10: 6566 6175 6c74 5f63 6861 6e63 655f 6267  efault_chance_bg
+00000a20: 636f 6c6f 7220 3d20 2338 3038 3038 300d  color = #808080.
+00000a30: 0a20 2064 6566 6175 6c74 5f63 6861 6e63  .  default_chanc
+00000a40: 655f 6667 636f 6c6f 7220 3d20 7768 6974  e_fgcolor = whit
+00000a50: 650d 0a20 2064 6566 6175 6c74 5f75 7469  e..  default_uti
+00000a60: 6c69 7479 5f62 6763 6f6c 6f72 203d 2023  lity_bgcolor = #
+00000a70: 3530 3530 3841 0d0a 2020 6465 6661 756c  50508A..  defaul
+00000a80: 745f 7574 696c 6974 795f 6667 636f 6c6f  t_utility_fgcolo
+00000a90: 7220 3d20 7768 6974 650d 0a20 2064 6566  r = white..  def
+00000aa0: 6175 6c74 5f64 6563 6973 696f 6e5f 6267  ault_decision_bg
+00000ab0: 636f 6c6f 7220 3d20 2339 4135 3035 300d  color = #9A5050.
+00000ac0: 0a20 2064 6566 6175 6c74 5f64 6563 6973  .  default_decis
+00000ad0: 696f 6e5f 6667 636f 6c6f 7220 3d20 7768  ion_fgcolor = wh
+00000ae0: 6974 650d 0a0d 0a20 2063 6861 6e63 655f  ite....  chance_
+00000af0: 7368 6170 6520 3d20 656c 6c69 7073 650d  shape = ellipse.
+00000b00: 0a20 2075 7469 6c69 7479 5f73 6861 7065  .  utility_shape
+00000b10: 203d 2068 6578 6167 6f6e 0d0a 2020 6465   = hexagon..  de
+00000b20: 6369 7369 6f6e 5f73 6861 7065 203d 2062  cision_shape = b
+00000b30: 6f78 0d0a 0d0a 2020 6465 6369 7369 6f6e  ox....  decision
+00000b40: 5f61 7263 5f73 7479 6c65 203d 2074 6170  _arc_style = tap
+00000b50: 6572 6564 2c20 626f 6c64 2c20 646f 7474  ered, bold, dott
+00000b60: 6564 0d0a 2020 7574 696c 6974 795f 6172  ed..  utility_ar
+00000b70: 635f 7374 796c 6520 3d20 6461 7368 6564  c_style = dashed
+00000b80: 0d0a 0d0a 2020 6465 6661 756c 745f 6964  ....  default_id
+00000b90: 5f73 697a 653d 360d 0a20 2064 6566 6175  _size=6..  defau
+00000ba0: 6c74 5f69 645f 696e 6665 7265 6e63 655f  lt_id_inference_
+00000bb0: 7369 7a65 3d36 0d0a 0d0a 2020 7574 696c  size=6....  util
+00000bc0: 6974 795f 7669 7369 626c 655f 6469 6769  ity_visible_digi
+00000bd0: 7473 203d 2032 0d0a 2020 7574 696c 6974  ts = 2..  utilit
+00000be0: 795f 7368 6f77 5f73 7464 6576 203d 2054  y_show_stdev = T
+00000bf0: 7275 650d 0a0d 0a20 2023 7368 6f77 2075  rue....  #show u
+00000c00: 7469 6c69 7479 2028 4661 6c73 6529 206f  tility (False) o
+00000c10: 7220 2d75 7469 6c69 7479 2028 4c6f 7373  r -utility (Loss
+00000c20: 290d 0a20 2075 7469 6c69 7479 5f73 686f  )..  utility_sho
+00000c30: 775f 6c6f 7373 203d 2046 616c 7365 0d0a  w_loss = False..
+00000c40: 0d0a 5b63 7265 6461 6c6e 6574 5d0d 0a20  ..[credalnet].. 
+00000c50: 2023 7468 656d 696e 670d 0a20 2064 6566   #theming..  def
+00000c60: 6175 6c74 5f6e 6f64 655f 6267 636f 6c6f  ault_node_bgcolo
+00000c70: 7220 3d20 2334 3034 3034 300d 0a20 2064  r = #404040..  d
+00000c80: 6566 6175 6c74 5f6e 6f64 655f 6667 636f  efault_node_fgco
+00000c90: 6c6f 7220 3d20 7768 6974 650d 0a20 2068  lor = white..  h
+00000ca0: 6973 746f 5f6d 6178 5f63 6f6c 6f72 203d  isto_max_color =
+00000cb0: 2023 4242 4646 4141 0d0a 0d0a 5b63 6175   #BBFFAA....[cau
+00000cc0: 7361 6c5d 0d0a 2020 7368 6f77 5f6c 6174  sal]..  show_lat
+00000cd0: 656e 745f 6e61 6d65 7320 3d20 4661 6c73  ent_names = Fals
+00000ce0: 650d 0a20 2023 206c 6174 6578 2063 6f6d  e..  # latex com
+00000cf0: 6d61 6e64 2066 6f72 206e 6f74 6174 696f  mand for notatio
+00000d00: 6e20 6f66 2069 6e74 6572 7665 6e74 696f  n of interventio
+00000d10: 6e20 696e 2066 6f72 6d75 6c61 0d0a 2020  n in formula..  
+00000d20: 6c61 7465 785f 646f 5f70 7265 6669 7820  latex_do_prefix 
+00000d30: 3d20 5c68 6f6f 6b72 6967 6874 6172 726f  = \hookrightarro
+00000d40: 775c 6d6b 6572 6e2d 362e 356d 750d 0a20  w\mkern-6.5mu.. 
+00000d50: 206c 6174 6578 5f64 6f5f 7375 6666 6978   latex_do_suffix
+00000d60: 203d 0d0a 0d0a 2020 2374 6865 6d69 6e67   =....  #theming
+00000d70: 0d0a 2020 6465 6661 756c 745f 6772 6170  ..  default_grap
+00000d80: 685f 7369 7a65 203d 2032 2e35 0d0a 2020  h_size = 2.5..  
+00000d90: 6465 6661 756c 745f 6e6f 6465 5f62 6763  default_node_bgc
+00000da0: 6f6c 6f72 203d 2023 3430 3430 3430 0d0a  olor = #404040..
+00000db0: 2020 6465 6661 756c 745f 6e6f 6465 5f66    default_node_f
+00000dc0: 6763 6f6c 6f72 203d 2077 6869 7465 0d0a  gcolor = white..
+00000dd0: 2020 6465 6661 756c 745f 6c61 7465 6e74    default_latent
+00000de0: 5f62 6763 6f6c 6f72 203d 2023 4130 3830  _bgcolor = #A080
+00000df0: 3830 0d0a 2020 6465 6661 756c 745f 6c61  80..  default_la
+00000e00: 7465 6e74 5f66 6763 6f6c 6f72 203d 2062  tent_fgcolor = b
+00000e10: 6c61 636b 0d0a 0d0a 5b52 4f43 5d0d 0a20  lack....[ROC].. 
+00000e20: 2064 7261 775f 636f 6c6f 7220 3d20 2330   draw_color = #0
+00000e30: 3038 3830 300d 0a20 2066 696c 6c5f 636f  08800..  fill_co
+00000e40: 6c6f 7220 3d20 2341 4145 4541 410d 0a    lor = #AAEEAA..
```

## pyAgrum/deprecated.py

```diff
@@ -99,29 +99,14 @@
   """
   warnings.warn(""""
 ** pyAgrum.MarkovNet is deprecated in pyAgrum>1.5.2.
 ** A pyAgrum.MarkovRandomField has been created.
 """, DeprecationWarning, stacklevel=2)
   return MarkovRandomField(*args, **kwargs)
 
-
-########################################################################################################
-def deprecated_adjacents(mixed_graph, n):
-  """
-  Deprecated methods in MixedGraph for pyAgrum>1.3.1
-  """
-  warnings.warn("""
-** pyAgrum.MixedGraph.adjacents() is deprecated from pyAgrum>1.3.1. Please use boundary() instead.
-""", DeprecationWarning, stacklevel=2)
-  return mixed_graph.boundary(n)
-
-
-MixedGraph.adjacents = deprecated_adjacents
-
-
 ########################################################################################################
 def deprecated_learnMixedGraph(learner):
   """
   Deprecated methods in BNLearner for pyAgrum>1.5.2
   """
   warnings.warn("""
 ** pyAgrum.BNLearner.learnMixedGraph() is deprecated from pyAgrum>1.5.2. Please use learnPDAG() methods instead.
```

## pyAgrum/pyAgrum.py

```diff
@@ -8551,15 +8551,14 @@
         pyAgrum.SizeError
           If v size's does not matches the domain size.
         pyAgrum.ArgumentError
           If anything wrong with the arguments.
 
         """
 
-        # test
         if len(args)>1:
           d=args[1]
           if type(d)==dict:
             if set(d.keys())==set(self.names):
               return self.fillWith(args[0],[d[s] for s in self.names])
             else:
               raise pyAgrum.ArgumentError(f"[pyAgrum] keys in dict {tuple(d.keys())} does not match the Potential's variables {self.names}")
@@ -8825,21 +8824,21 @@
         Parameters
         ----------
         func : function(Dict[str,int])->float
             A function that takes a single argument, representing the value of a python representation of a `gum.Instantiation` (as a dictionary), and returns a float.
 
         Warnings
         --------
-        The `gum.Potential` is assumed to contain a joint distribution.
+        The `pyAgrum.Potential` is assumed to contain a joint distribution.
 
-        Example
-        -------
-        def log2cptA(x):
-         return -math.log2(bn.cpt('A')[x])
-        entropy_of_A=bn.cpt('A').expectedValue(log2cptA) # OK it A has no parents.
+        Examples
+        --------
+        >>> def log2cptA(x):
+        ...   return -math.log2(bn.cpt('A')[x])
+        >>> entropy_of_A=bn.cpt('A').expectedValue(log2cptA) # OK it A has no parents.
 
         Returns
         -------
         float
             The mathematical expected value of the random variable calculated using the given function as an argument.
 
         """
@@ -25790,36 +25789,40 @@
 
 # Register ShaferShenoyLIMIDInference in _pyAgrum:
 _pyAgrum.ShaferShenoyLIMIDInference_swigregister(ShaferShenoyLIMIDInference)
 class BNLearner(object):
     r"""
 
     BNLearner(filename,inducedTypes=True) -> BNLearner
-        Parameters:
+        Parameters
+        ----------
             - **source** (*str* or *pandas.DataFrame*) -- the data to learn from
-            - **missingSymbols** (*List[str]*) -- list of string that will be interpreted as missing values (by default : ['?'])
+            - **missingSymbols** (*List[str]*) -- list of string that will be interpreted as missing values (by default : `['?']`)
             - **inducedTypes** (*Bool*) -- whether BNLearner should try to automatically find the type of each variable
 
     BNLearner(filename,src) -> BNLearner
-        Parameters:
+        Parameters
+        ----------
             - **source** (*str* or *pandas.DataFrame) -- the data to learn from
             - **src** (*pyAgrum.BayesNet*) -- the Bayesian network used to find those modalities
-            - **missingSymbols** (*List[str]*) -- list of string that will be interpreted as missing values (by default : ['?'])
+            - **missingSymbols** (*List[str]*) -- list of string that will be interpreted as missing values (by default : `['?']`)
 
     BNLearner(learner) -> BNLearner
-        Parameters:
+        Parameters
+        ----------
             - **learner** (*pyAgrum.BNLearner*) -- the BNLearner to copy
 
+
     """
 
     thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc="The membership flag")
 
     def __init__(self, *args):
 
-        if not type(args[0]) is str:
+        if type(args[0]) is not str:
           if hasattr(args[0],"to_csv"):
               import tempfile
               csvfile = tempfile.NamedTemporaryFile(delete=False)
               tmpfilename = csvfile.name
               csvfilename = tmpfilename + ".csv"
               csvfile.close()
               args[0].to_csv(csvfilename,na_rep="?",index=False)
@@ -25844,38 +25847,52 @@
 
         """
         return _pyAgrum.BNLearner_learnBN(self)
 
     def learnParameters(self, *args) -> "pyAgrum.BayesNet":
         r"""
 
-        learns a BN (its parameters) when its structure is known.
+        Create a new BN copying its structure from the argument (dag or BN) and learning its parameters from the database w.r.t the BNLearner's state (priors, etc.).
+
+        Warnings
+        --------
+        When using a `pyAgrum.DAG` as input parameter, NodeIds in the dag and index of rows in the database must fit in order to coherently fix the structure of the BN.
+        Generally, it is safer to use a `pyAgrum.BayesianNet` as input or even to use `pyAgrum.BNLearner.fitParameters`.
 
         Parameters
         ----------
         dag : pyAgrum.DAG
         bn : pyAgrum.BayesNet
         take_into_account_score : bool
-        	The dag passed in argument may have been learnt from a structure learning. In this case, if the score used to learn the structure has an implicit prior (like K2 which has a 1-smoothing prior), it is important to also take into account this implicit prior for parameter learning. By default, if a score exists, we will learn parameters by taking into account the prior specified by methods usePriorXXX () + the implicit prior of the score, else we just take into account the prior specified by usePriorXXX ()
+        	The dag passed in argument may have been learnt from a structure learning. In this case, if the score used to learn the structure has an implicit prior (like K2 which has a 1-smoothing prior), it is important to also take into account this implicit prior for parameter learning. By default (`take_into_account_score=True`), we will learn parameters by taking into account the prior specified by methods usePriorXXX () + the implicit prior of the score (if any). If `take_into_account_score=False`, we just take into account the prior specified by `usePriorXXX()`.
 
         Returns
         -------
         pyAgrum.BayesNet
         	the learned BayesNet
 
         Raises
         ------
         pyAgrum.MissingVariableInDatabase
         	If a variable of the BN is not found in the database
         pyAgrum.UnknownLabelInDatabase
         	If a label is found in the database that do not correspond to the variable
 
         """
+
+        if type(args[0])==pyAgrum.BayesNet:
+            res=pyAgrum.BayesNet(args[0])
+            self.fitParameters(res)
+            return res
+
+
+
         return _pyAgrum.BNLearner_learnParameters(self, *args)
 
+
     def setInitialDAG(self, dag: "DAG") -> "pyAgrum.BNLearner":
         r"""
 
         Parameters
         ----------
         dag : pyAgrum.DAG
         	an initial DAG structure
@@ -26355,45 +26372,48 @@
             else:
                 name=self.nameFromId(i)
             p.add(RangeVariable(name,name,0,self.domainSize(i)-1))
             lv.append(name)
         p.fillWith(self.rawPseudoCount(lv))
         return p
 
-    def fitParameters(self,bn):
+    def fitParameters(self,bn,take_into_account_score=True):
       """
-      Easy shortcut to LearnParameters method. fitParameters uses self to direcuptly populate the CPTs of bn.0
+      Easy shortcut to LearnParameters method. fitParameters directly populates the CPTs of the argument.
 
       Parameters
       ----------
       bn : pyAgrum.BayesNet
-        a BN which will directly have its parameters learned.
+        a BN which will directly have its parameters learned inplace.
+
+      take_into_account_score : bool
+    	The dag passed in argument may have been learnt from a structure learning. In this case, if the score used to learn the structure has an implicit prior (like K2 which has a 1-smoothing prior), it is important to also take into account this implicit prior for parameter learning. By default (`take_into_account_score=True`), we will learn parameters by taking into account the prior specified by methods usePriorXXX () + the implicit prior of the score (if any). If `take_into_account_score=False`, we just take into account the prior specified by `usePriorXXX()`.
 
       """
       if set(self.names())!=bn.names():
         raise Exception("Not the same variable names in the database and in the BN")
 
       d=DAG()
       for n in bn.names():
         d.addNodeWithId(self.idFromName(n))
       for i1,i2 in bn.arcs():
         d.addArc(self.idFromName(bn.variable(i1).name()),self.idFromName(bn.variable(i2).name()))
-      tmp=self.learnParameters(d)
+      tmp=self.learnParameters(d,take_into_account_score)
       for n in tmp.names():
         bn.cpt(n).fillWith(tmp.cpt(n))
       return self
 
     def learnEssentialGraph(self):
       bn=BayesNet()
       for i in range(len(self.names())):
         bn.add(self.nameFromId(i),2)
       ge=EssentialGraph(bn,self.learnPDAG())
       ge._bn=bn
-      return ge
 
+      return ge
 
 
     def setVerbosity(self, v: bool) -> None:
         r"""
 
         Parameters
         ----------
```

## pyAgrum/__init__.py

```diff
@@ -23,15 +23,15 @@
 # *   GNU General Public License for more details.                          *
 # *                                                                         *
 # *   You should have received a copy of the GNU General Public License     *
 # *   along with this program; if not, write to the                         *
 # *   Free Software Foundation, Inc.,                                       *
 # *   59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
 
-__version__ = '1.8.3'
+__version__ = '1.9.0'
 __license__ = __doc__
 __project_url__ = 'http://agrum.org'
 
 from typing import List
 import warnings
 import numpy as np
 
@@ -144,15 +144,15 @@
 
 
 def about():
   """
   about() for pyAgrum
 
   """
-  print("pyAgrum 1.8.3")
+  print("pyAgrum 1.9.0")
   print("(c) 2015-2023 Pierre-Henri Wuillemin, Christophe Gonzales")
   print("""
     This is free software; see the source code for copying conditions.
     There is ABSOLUTELY NO WARRANTY; not even for MERCHANTABILITY or
     FITNESS FOR A PARTICULAR PURPOSE.  For details, see 'pyAgrum.warranty'.
     """)
```

## pyAgrum/lib/bn2graph.py

```diff
@@ -26,17 +26,18 @@
 
 
 import time
 import hashlib
 
 from tempfile import mkdtemp
 import matplotlib.pyplot as plt
-import pyAgrum as gum
 import pydot as dot
 
+import pyAgrum as gum
+
 from pyAgrum.lib import proba_histogram
 import pyAgrum.lib._colors as gumcols
 
 
 def BN2dot(bn, size=None, nodeColor=None, arcWidth=None, arcLabel=None, arcColor=None, cmapNode=None, cmapArc=None, showMsg=None):
   """
   create a pydot representation of the BN
@@ -103,27 +104,24 @@
   for a in bn.arcs():
     (n,j) = a
     pw = 1
     av =f"{n}&nbsp;&rarr;&nbsp;{j}"
     col = gumcols.getBlackInTheme()
     lb=""
 
-    if arcWidth is not None:
-      if a in arcWidth:
-        if maxarcs != minarcs:
-          pw = 0.1 + 5 * (arcWidth[a] - minarcs) / (maxarcs - minarcs)
-        av = f"{n}&nbsp;&rarr;&nbsp;{j} : {arcWidth[a]}"
-
-    if arcColor is not None:
-      if a in arcColor:
-        col = gumcols.proba2color(arcColor[a], cmapArc)
-
-    if arcLabel is not None:
-        if a in arcLabel:
-            lb=arcLabel[a]
+    if arcWidth is not None and a in arcWidth:
+      if maxarcs != minarcs:
+        pw = 0.1 + 5 * (arcWidth[a] - minarcs) / (maxarcs - minarcs)
+      av = f"{n}&nbsp;&rarr;&nbsp;{j} : {arcWidth[a]}"
+
+    if arcColor is not None and a in arcColor:
+      col = gumcols.proba2color(arcColor[a], cmapArc)
+
+    if arcLabel is not None and a in arcLabel:
+      lb=arcLabel[a]
 
     edge = dot.Edge('"' + bn.variable(a[0]).name() + '"', '"' + bn.variable(a[1]).name() + '"',
                     label=lb, fontsize="10",
                     penwidth=pw, color=col,
                     tooltip=av
                     )
     dotobj.add_edge(edge)
@@ -221,18 +219,17 @@
 
     # defaults
     bgcol = gum.config["notebook", "default_node_bgcolor"]
     fgcol = gum.config["notebook", "default_node_fgcolor"]
     if len(targets) == 0 or name in targets or nid in targets:
       bgcol = gum.config["notebook", "figure_facecolor"]
 
-    if nodeColor is not None:
-      if name in nodeColor or nid in nodeColor:
-        bgcol = gumcols.proba2bgcolor(nodeColor[name], cmapNode)
-        fgcol = gumcols.proba2fgcolor(nodeColor[name], cmapNode)
+    if nodeColor is not None and (name in nodeColor or nid in nodeColor):
+      bgcol = gumcols.proba2bgcolor(nodeColor[name], cmapNode)
+      fgcol = gumcols.proba2fgcolor(nodeColor[name], cmapNode)
 
     # 'hard' colour for evidence (?)
     if name in evs or nid in evs:
       bgcol = gum.config["notebook", "evidence_bgcolor"]
       fgcol = gum.config["notebook", "evidence_fgcolor"]
 
     colorattribute = f'fillcolor="{bgcol}", fontcolor="{fgcol}", color="#000000"'
@@ -247,23 +244,21 @@
 
   for a in showdag.arcs():
     (n, j) = a
     pw = 1
     av = f"{n}&nbsp;&rarr;&nbsp;{j}"
     col = gumcols.getBlackInTheme()
 
-    if arcWidth is not None:
-      if a in arcWidth:
-        if maxarcs != minarcs:
-          pw = 0.1 + 5 * (arcWidth[a] - minarcs) / (maxarcs - minarcs)
-        av = f"{n}&nbsp;&rarr;&nbsp;{j} : {arcWidth[a]}"
-
-    if arcColor is not None:
-      if a in arcColor:
-        col = gumcols.proba2color(arcColor[a], cmapArc)
+    if arcWidth is not None and a in arcWidth:
+      if maxarcs != minarcs:
+        pw = 0.1 + 5 * (arcWidth[a] - minarcs) / (maxarcs - minarcs)
+      av = f"{n}&nbsp;&rarr;&nbsp;{j} : {arcWidth[a]}"
+
+    if arcColor is not None and a in arcColor:
+      col = gumcols.proba2color(arcColor[a], cmapArc)
 
     dotstr += f' "{bn.variable(n).name()}"->"{bn.variable(j).name()}" [penwidth="{pw}",tooltip="{av}",color="{col}"];'
 
   dotstr += '}'
 
   g = dot.graph_from_dot_data(dotstr)[0]
```

## pyAgrum/lib/bn2roc.py

```diff
@@ -30,14 +30,24 @@
 import numpy as np
 
 from matplotlib import pylab
 
 import pyAgrum as gum
 from pyAgrum import skbn
 
+CSV_TMP_SUFFIX = ".x.csv"
+
+
+def _getFilename(datasrc):
+  "*.CSV_TMP_SUFFIXcsv is the signature of a temp csv file"
+  if datasrc[-len(CSV_TMP_SUFFIX):] == ".x.csv":
+    return "dataframe"
+
+  return datasrc
+
 
 def _lines_count(filename):
   """
   Parameters
   ----------
   filename : str
     a filename
@@ -52,37 +62,38 @@
   with open(filename) as f:
     for _ in f.readlines():
       numlines += 1
 
   return numlines
 
 
-def _checkCompatibility(bn, fields, csv_name):
+@gum.deprecated_arg(newA="datasrc", oldA="csvname", version="1.8.3")
+def _checkCompatibility(bn, fields, datasrc):
   """
   check if variables of the bn are in the fields
 
   Parameters
   ----------
   bn : pyAgrum.BayesNet
     a Bayesian network
   fields : list
     a list of fields
-  csv_name : str
-    the name of the csv file
+  datasrc : str|DataFrame
+    a csv filename or a pandas.DataFrame
 
   Returns
   -------
   list
     a list of position for variables in fields, None otherwise.
   """
   res = {}
   isOK = True
   for field in bn.names():
     if field not in fields:
-      print(f"** field '{field}' is missing in {csv_name}")
+      print(f"** field '{field}' is missing.")
       isOK = False
     else:
       res[bn.idFromName(field)] = fields[field]
 
   if not isOK:
     res = None
 
@@ -109,41 +120,46 @@
   for i in range(1, len(points)):
     somme += (points[i][0] - points[i - 1][0]) * \
              (points[i - 1][1] + points[i][1])
 
   return somme / 2
 
 
-def _computeF1(points, ind):
-  return 2 * points[ind][0] * points[ind][1] / (points[ind][0] + points[ind][1])
+def _computeFbeta(points, ind, beta=1):
+  return (1 + beta ** 2) * points[ind][1] * points[ind][0] / ((beta ** 2 * points[ind][1]) + points[ind][0])
 
 
-def _computepoints(bn, csv_name, target, label, show_progress=True, with_labels=True, significant_digits=10):
+@gum.deprecated_arg(newA="datasrc", oldA="csvname", version="1.8.3")
+def _computePoints(bn, datasrc, target, label, *, beta=1, show_progress=True, with_labels=True, significant_digits=10):
   """
-  Compute the ROC curve points.
+  Compute the ROC points.
 
   Parameters
   ----------
   bn : pyAgrum.BayesNet
     a Bayesian network
-  csv_name : str
-    a csv filename
+  datasrc : str|DataFrame
+    a csv filename or a pandas.DataFrame
   target : str
     the target
   label : str
-    the target's label
+    the target's label or id
+  beta : float
+    the value of beta for the F-beta score
   show_progress : bool
     indicates if the resulting curve must be printed
+  with_labels: bool
+    whether we use label or id (especially for parameter label)
   significant_digits:
     number of significant digits when computing probabilities
 
   Returns
   -------
   tuple (res, totalP, totalN)
-    where res is a list of (proba,isWellClassified) for each line of csv_name.
+    where res is a list of (proba,isWellClassified) for each line of datasrc.
 
   """
   idTarget = bn.idFromName(target)
   label = str(label)
 
   if not with_labels:
     idLabel = -1
@@ -151,26 +167,27 @@
       if bn.variable(idTarget).label(i) == label:
         idLabel = i
         break
     assert idLabel >= 0
   else:
     idLabel = label
 
-  Classifier = skbn.BNClassifier(significant_digit=significant_digits)
+  Classifier = skbn.BNClassifier(beta=beta, significant_digit=significant_digits)
 
   if show_progress:
     # tqdm is optional:
     # pylint: disable=import-outside-toplevel
+    filename = _getFilename(datasrc)
     from tqdm import tqdm
-    pbar = tqdm(total=_lines_count(csv_name) - 1, desc=csv_name,
+    pbar = tqdm(total=_lines_count(datasrc) - 1, desc=filename,
                 bar_format='{desc}: {percentage:3.0f}%|{bar}|')
 
   Classifier.fromTrainedModel(bn, target, idLabel)
   # as a Binary classifier, y will be a list of True (good classification) and False (bad one)
-  X, y = Classifier.XYfromCSV(csv_name, with_labels=with_labels, target=target)
+  X, y = Classifier.XYfromCSV(datasrc, with_labels=with_labels, target=target)
   predictions = Classifier.predict_proba(X)
 
   totalP = np.count_nonzero(y)
   totalN = len(y) - totalP
   res = []
   for i in range(len(X)):
     px = predictions[i][1]
@@ -181,24 +198,26 @@
 
   if show_progress:
     pbar.close()
 
   return res, totalP, totalN
 
 
-def _computeROC_PR(values, totalP, totalN):
+def _computeROC_PR(values, totalP, totalN, beta):
   """
   Parameters
   ----------
   values :
     the curve values
   totalP : int
     the number of positive values
   totalN : int
     the number of negative values
+  beta : float
+    the value of beta for the F-beta score
 
   Returns
   -------
   tuple
     (points_ROC, ind_ROC, threshold_ROC,AUC_ROC,points_PR, ind_PR, threshold_PR, AUC_PR,thresholds)
   """
 
@@ -232,15 +251,16 @@
       d = fpr * fpr + (1 - tpr) * (1 - tpr)
       if d < dmin_ROC:
         dmin_ROC = d
         ind_ROC = len(pointsROC)
         threshopt_ROC = (cur_threshold + old_threshold) / 2
 
       if prec + tpr > 0:
-        f = 2 * prec * tpr / (prec + tpr)  # f1
+        f = (1 + beta ** 2) * prec * tpr / ((beta ** 2 * prec) + tpr)
+
         if f > fmax_PR:
           fmax_PR = f
           ind_PR = len(pointsPR)
           threshopt_PR = (cur_threshold + old_threshold) / 2
 
       pointsROC.append((fpr, tpr))
       pointsPR.append((tpr, prec))
@@ -258,22 +278,118 @@
   thresholds.append(0)
   pointsROC.append((1, 1))
   pointsPR.append((1, 0))
 
   AUC_ROC = _computeAUC(pointsROC)
   AUC_PR = _computeAUC(pointsPR)
 
-  f1_ROC = _computeF1(pointsPR, ind_ROC)
-  f1_PR = _computeF1(pointsPR, ind_PR)
+  fbeta_ROC = _computeFbeta(pointsPR, ind_ROC, beta)
+  fbeta_PR = _computeFbeta(pointsPR, ind_PR, beta)
 
-  return (pointsROC, ind_ROC, threshopt_ROC, AUC_ROC, f1_ROC,
-          pointsPR, ind_PR, threshopt_PR, AUC_PR, f1_PR,
+  return (pointsROC, ind_ROC, threshopt_ROC, AUC_ROC, fbeta_ROC,
+          pointsPR, ind_PR, threshopt_PR, AUC_PR, fbeta_PR,
           thresholds)
 
 
+@gum.deprecated_arg(newA="datasrc", oldA="csvname", version="1.8.3")
+def getROCpoints(bn, datasrc, target, label, with_labels=True, significant_digits=10):
+  """
+  Compute the points of the ROC curve
+
+  Parameters
+  ----------
+  bn : pyAgrum.BayesNet
+    a Bayesian network
+  datasrc : str | DataFrame
+    a csv filename or a DataFrame
+  target : str
+    the target
+  label : str
+    the target's label
+  with_labels: bool
+    whether we use label or id (especially for parameter label)
+  significant_digits:
+    number of significant digits when computing probabilities
+
+  Returns
+  -------
+    List[Tuple[int,int]]
+      the list of points (FalsePositifRate,TruePositifRate)
+  """
+  if type(datasrc) is not str and hasattr(datasrc, "to_csv"):
+    import tempfile
+    csvfile = tempfile.NamedTemporaryFile(delete=False)
+    tmpfilename = csvfile.name
+    csvfilename = tmpfilename + CSV_TMP_SUFFIX
+    csvfile.close()
+    datasrc.to_csv(csvfilename, na_rep="?", index=False)
+
+    l = getROCpoints(bn, csvfilename, target, label, with_labels=with_labels, significant_digits=significant_digits)
+
+    os.remove(csvfilename)
+    return l
+
+  (res, totalP, totalN) = _computePoints(bn, datasrc, target,
+                                         label, show_progress=False, with_labels=with_labels,
+                                         significant_digits=significant_digits)
+  (pointsROC, ind_ROC, thresholdROC, AUC_ROC, fbeta_ROC, pointsPR, ind_PR,
+   thresholdPR, AUC_PR, fbeta_PR, thresholds) = _computeROC_PR(res, totalP, totalN, beta=1)
+
+  return pointsROC
+
+
+@gum.deprecated_arg(newA="datasrc", oldA="csvname", version="1.8.3")
+def getPRpoints(bn, datasrc, target, label, with_labels=True, significant_digits=10):
+  """
+  Compute the points of the PR curve
+
+  Parameters
+  ----------
+  bn : pyAgrum.BayesNet
+    a Bayesian network
+  datasrc : str|DataFrame
+    a csv filename or a pandas.DataFrame
+  target : str
+    the target
+  label : str
+    the target's label
+  with_labels: bool
+    whether we use label or id (especially for parameter label)
+  significant_digits:
+    number of significant digits when computing probabilities
+
+  Returns
+  -------
+    List[Tuple[float,float]]
+      the list of points (precision,recall)
+  """
+  if type(datasrc) is not str and hasattr(datasrc, "to_csv"):
+    import tempfile
+    csvfile = tempfile.NamedTemporaryFile(delete=False)
+    tmpfilename = csvfile.name
+    csvfilename = tmpfilename + CSV_TMP_SUFFIX
+    csvfile.close()
+
+    datasrc.to_csv(csvfilename, na_rep="?", index=False)
+
+    l = getPRpoints(bn, csvfilename, target, label, with_labels=with_labels, significant_digits=significant_digits)
+
+    os.remove(csvfilename)
+    return l
+
+  show_progress = False
+  (res, totalP, totalN) = _computePoints(bn, datasrc, target,
+                                         label, show_progress=show_progress, with_labels=with_labels,
+                                         significant_digits=significant_digits)
+  (pointsROC, ind_ROC, thresholdROC, AUC_ROC, fbeta_ROC, pointsPR, ind_PR,
+   thresholdPR, AUC_PR, fbeta_PR, thresholds) = _computeROC_PR(res, totalP, totalN, beta=1)
+
+  return pointsPR
+
+
 def _getPoint(threshold: float, thresholds: List[float], points: List[Tuple[float, float]]) -> Tuple[float, float]:
   """
 
   Find the point corresponding to threshold in points (annotated by thresholds)
 
   Parameters
   ----------
@@ -304,16 +420,16 @@
   ind = _dichot(0, len(thresholds), thresholds, threshold)
   if ind == len(points) - 1:
     return points[ind]
   else:  # a threshold is between 2 points
     return (points[ind][0] + points[ind + 1][0]) / 2, (points[ind][1] + points[ind + 1][1]) / 2
 
 
-def _basicDraw(ax, points, thresholds, f1, AUC, main_color, secondary_color, last_color="black",
-               thresholds_to_show=None, align_threshold="left"):
+def _basicDraw(ax, points, thresholds, fbeta, beta, AUC, main_color, secondary_color,
+               last_color="black", thresholds_to_show=None, align_threshold="left"):
   ax.grid(color='#aaaaaa', linestyle='-', linewidth=1, alpha=0.5)
 
   ax.plot([x[0] for x in points], [y[1] for y in points], '-',
           linewidth=3, color=gum.config["ROC", "draw_color"], zorder=3
           )
   ax.fill_between([x[0] for x in points],
                   [y[1] for y in points], 0, color=gum.config["ROC", "fill_color"]
@@ -362,59 +478,69 @@
   if align_threshold == "left":
     AUC_x = 0.95
     AUC_halign = "right"
   else:
     AUC_x = 0.05
     AUC_halign = "left"
 
-  ax.text(AUC_x, 0.0, f'AUC={AUC:.4f}\nf1={f1:.4f}', {'color': main_color, 'fontsize': 18},
-          horizontalalignment=AUC_halign,
-          verticalalignment='bottom',
-          fontsize=18)
+  if beta == 1:
+    ax.text(AUC_x, 0.0, f'AUC={AUC:.4f}\nF1={fbeta:.4f}', {'color': main_color, 'fontsize': 18},
+            horizontalalignment=AUC_halign,
+            verticalalignment='bottom',
+            fontsize=18)
+  else:
+    ax.text(AUC_x, 0.0, f'AUC={AUC:.4f}\nF-{beta:g}={fbeta:.4f}', {'color': main_color, 'fontsize': 18},
+            horizontalalignment=AUC_halign,
+            verticalalignment='bottom',
+            fontsize=18)
 
 
-def _drawROC(points, zeTitle, f1_ROC, AUC_ROC, thresholds, thresholds_to_show, ax=None):
+def _drawROC(points, zeTitle, fbeta_ROC, beta, AUC_ROC, thresholds, thresholds_to_show, ax=None):
   ax = ax or pylab.gca()
 
-  _basicDraw(ax, points, thresholds, f1=f1_ROC, AUC=AUC_ROC, main_color='#DD5555',
+  _basicDraw(ax, points, thresholds, fbeta=fbeta_ROC, beta=beta, AUC=AUC_ROC, main_color='#DD5555',
              secondary_color='#120af7', thresholds_to_show=thresholds_to_show, align_threshold="left")
   ax.plot([0.0, 1.0], [0.0, 1.0], '-', color="#AAAAAA")
   ax.set_xlabel('False positive rate')
   ax.set_ylabel('True positive rate')
 
   ax.set_title(zeTitle)
 
 
-def _drawPR(points, zeTitle, f1_PR, AUC_PR, thresholds, thresholds_to_show, rate, ax=None):
+def _drawPR(points, zeTitle, fbeta_PR, beta, AUC_PR, thresholds, thresholds_to_show, rate, ax=None):
   ax = ax or pylab.gca()
 
-  _basicDraw(ax, points, thresholds, f1=f1_PR, AUC=AUC_PR, main_color='#120af7', secondary_color='#DD5555',
+  _basicDraw(ax, points, thresholds, fbeta=fbeta_PR, beta=beta, AUC=AUC_PR, main_color='#120af7',
+             secondary_color='#DD5555',
              thresholds_to_show=thresholds_to_show, align_threshold="right")
   ax.plot([0.0, 1.0], [rate, rate], '-', color="#AAAAAA")
-  ax.set_xlabel('Precision')
-  ax.set_ylabel('Recall')
+  ax.set_xlabel('Recall')
+  ax.set_ylabel('Precision')
 
   ax.set_title(zeTitle)
 
 
-def showROC_PR(bn, csv_name, target, label, show_progress=True, show_fig=True, save_fig=False, with_labels=True,
-               show_ROC=True, show_PR=True, significant_digits=10):
+@gum.deprecated_arg(newA="datasrc", oldA="csvname", version="1.8.3")
+def showROC_PR(bn, datasrc, target, label, *, beta=1, show_progress=True, show_fig=True, save_fig=False,
+               with_labels=True, show_ROC=True, show_PR=True, significant_digits=10):
   """
   Compute the ROC curve and save the result in the folder of the csv file.
 
   Parameters
   ----------
   bn : pyAgrum.BayesNet
     a Bayesian network
-  csv_name : str
-    a csv filename
+  datasrc : str|DataFrame
+    a csv filename or a pandas.DataFrame
   target : str
     the target
   label : str
     the target label
+  beta : float
+    the value of beta for the F-beta score
   show_progress : bool
     indicates if the progress bar must be printed
   save_fig:
     save the result
   show_fig:
     plot the resuls
   with_labels:
@@ -429,72 +555,90 @@
   Returns
   -------
   tuple
     (pointsROC, thresholdROC, pointsPR, thresholdPR)
 
   """
 
-  (res, totalP, totalN) = _computepoints(bn, csv_name, target,
-                                         label, show_progress, with_labels, significant_digits)
-  (pointsROC, ind_ROC, thresholdROC, AUC_ROC, f1_ROC, pointsPR, ind_PR,
-   thresholdPR, AUC_PR, f1_PR, thresholds) = _computeROC_PR(res, totalP, totalN)
+  if type(datasrc) is not str and hasattr(datasrc, "to_csv"):
+    import tempfile
+    csvfile = tempfile.NamedTemporaryFile(delete=False)
+    tmpfilename = csvfile.name
+    csvfilename = tmpfilename + CSV_TMP_SUFFIX
+    csvfile.close()
+    datasrc.to_csv(csvfilename, na_rep="?", index=False)
+
+    showROC_PR(bn, csvfilename, target, label, beta=beta, show_progress=show_progress, show_fig=show_fig,
+               save_fig=save_fig, with_labels=with_labels, show_ROC=show_ROC, show_PR=show_PR,
+               significant_digits=significant_digits)
+
+    os.remove(csvfilename)
+    return
+
+  filename = _getFilename(datasrc)
+  (res, totalP, totalN) = _computePoints(bn, datasrc, target,
+                                         label, beta=beta, show_progress=show_progress, with_labels=with_labels,
+                                         significant_digits=significant_digits)
+  (pointsROC, ind_ROC, thresholdROC, AUC_ROC, fbeta_ROC, pointsPR, ind_PR,
+   thresholdPR, AUC_PR, fbeta_PR, thresholds) = _computeROC_PR(res, totalP, totalN, beta)
   try:
     shortname = os.path.basename(bn.property("name"))
   except gum.NotFound:
     shortname = "unnamed"
-  title = shortname + " vs " + csv_name + " - " + target + "=" + str(label)
+  title = shortname + " vs " + filename + " - " + target + "=" + str(label)
 
   rate = totalP / (totalP + totalN)
 
   if show_ROC and show_PR:
-    figname = f"{csv_name}-ROCandPR_{shortname}-{target}-{label}.png"
+    figname = f"{filename}-ROCandPR_{shortname}-{target}-{label}.png"
     fig = pylab.figure(figsize=(10, 4))
     fig.suptitle(title)
     pylab.gcf().subplots_adjust(wspace=0.1)
 
     ax1 = fig.add_subplot(1, 2, 1)
-    _drawROC(points=pointsROC, zeTitle="ROC", f1_ROC=f1_ROC, AUC_ROC=AUC_ROC, thresholds=thresholds,
+    _drawROC(points=pointsROC, zeTitle="ROC", fbeta_ROC=fbeta_ROC, beta=beta, AUC_ROC=AUC_ROC, thresholds=thresholds,
              thresholds_to_show=[thresholdROC, thresholdPR],
              ax=ax1)
 
     ax2 = fig.add_subplot(1, 2, 2)
     ax2.yaxis.tick_right()
     ax2.yaxis.set_label_position("right")
-    _drawPR(points=pointsPR, zeTitle="Precision-Recall", f1_PR=f1_PR, AUC_PR=AUC_PR,
+    _drawPR(points=pointsPR, zeTitle="Precision-Recall", fbeta_PR=fbeta_PR, beta=beta, AUC_PR=AUC_PR,
             thresholds=thresholds, thresholds_to_show=[thresholdPR, thresholdROC], rate=rate, ax=ax2)
   elif show_ROC:
-    figname = f"{csv_name}-ROC_{shortname}-{target}-{label}.png"
+    figname = f"{filename}-ROC_{shortname}-{target}-{label}.png"
 
-    _drawROC(points=pointsROC, zeTitle=title, f1_ROC=f1_ROC, AUC_ROC=AUC_ROC, thresholds=thresholds,
+    _drawROC(points=pointsROC, zeTitle=title, fbeta_ROC=fbeta_ROC, beta=beta, AUC_ROC=AUC_ROC, thresholds=thresholds,
              thresholds_to_show=[thresholdROC])
   elif show_PR:
-    figname = f"{csv_name}-PR_{shortname}-{target}-{label}.png"
-    _drawPR(points=pointsPR, zeTitle=title, f1_PR=f1_PR, AUC_PR=AUC_PR, thresholds=thresholds,
+    figname = f"{filename}-PR_{shortname}-{target}-{label}.png"
+    _drawPR(points=pointsPR, zeTitle=title, fbeta_PR=fbeta_PR, beta=beta, AUC_PR=AUC_PR, thresholds=thresholds,
             thresholds_to_show=[thresholdPR], rate=rate)
 
   if save_fig:
     pylab.savefig(figname, dpi=300)
 
   if show_fig:
     pylab.show()
 
   return AUC_ROC, thresholdROC, AUC_PR, thresholdPR
 
 
-def showROC(bn, csv_name, target, label, show_progress=True, show_fig=True, save_fig=False, with_labels=True,
+@gum.deprecated_arg(newA="datasrc", oldA="csvname", version="1.8.3")
+def showROC(bn, datasrc, target, label, show_progress=True, show_fig=True, save_fig=False, with_labels=True,
             significant_digits=10):
   """
   Compute the ROC curve and save the result in the folder of the csv file.
 
   Parameters
   ----------
   bn : pyAgrum.BayesNet
     a Bayesian network
-  csv_name : str
-    a csv filename
+  datasrc : str|DataFrame
+    a csv filename or a pandas.DataFrame
   target : str
     the target
   label : str
     the target label
   show_progress : bool
     indicates if the progress bar must be printed
   save_fig:
@@ -503,30 +647,30 @@
     plot the resuls
   with_labels:
     labels in csv
   significant_digits:
     number of significant digits when computing probabilities
   """
 
-  return showROC_PR(bn, csv_name, target, label, show_progress=show_progress, show_fig=show_fig, save_fig=save_fig,
+  return showROC_PR(bn, datasrc, target, label, show_progress=show_progress, show_fig=show_fig, save_fig=save_fig,
                     with_labels=with_labels, show_ROC=True, show_PR=False, significant_digits=significant_digits)
 
 
-def showPR(bn, csv_name, target, label, show_progress=True, show_fig=True, save_fig=False, with_labels=True,
-           significant_digits=10
-           ):
+@gum.deprecated_arg(newA="datasrc", oldA="csvname", version="1.8.3")
+def showPR(bn, datasrc, target, label, *, beta=1, show_progress=True, show_fig=True, save_fig=False,
+           with_labels=True, significant_digits=10):
   """
   Compute the ROC curve and save the result in the folder of the csv file.
 
   Parameters
   ----------
   bn : pyAgrum.BayesNet
     a Bayesian network
-  csv_name : str
-    a csv filename
+  datasrc : str|DataFrame
+    a csv filename or a pandas.DataFrame
   target : str
     the target
   label : str
     the target label
   show_progress : bool
     indicates if the progress bar must be printed
   save_fig:
@@ -535,9 +679,125 @@
     plot the resuls ?
   with_labels:
     labels in csv ?
   significant_digits:
     number of significant digits when computing probabilities
   """
 
-  return showROC_PR(bn, csv_name, target, label, show_progress=show_progress, show_fig=show_fig, save_fig=save_fig,
+  return showROC_PR(bn, datasrc, target, label, show_progress=show_progress, show_fig=show_fig, save_fig=save_fig,
                     with_labels=with_labels, show_ROC=False, show_PR=True, significant_digits=significant_digits)
+
+
+def animROC(bn, datasrc, target="Y", label="1"):
+  """
+  Interactive selection of a threshold using TPR and FPR for BN and data
+
+  Parameters
+  ----------
+  bn : pyAgrum.BayesNet
+    a Bayesian network
+  datasrc : str|DataFrame
+    a csv filename or a pandas.DataFrame
+  target : str
+    the target
+  label : str
+    the target label
+  """
+  import ipywidgets as widgets
+  import matplotlib.pyplot as plt
+  import matplotlib.ticker as mtick
+
+  class DisplayROC:
+    def __init__(self, points):
+      self._x = [i / len(points) for i in range(len(points))]
+      self._y1, self._y2 = zip(*points)
+      self._points = points
+
+    def display(self, threshold):
+      rate = threshold / 100.0
+      indexes = int((len(self._points) - 1) * rate)
+
+      plt.rcParams["figure.figsize"] = (4, 3)
+
+      fig, (ax1, ax2) = plt.subplots(nrows=2)
+      ax1.plot(viewer._x, viewer._y1, "g")
+      ax1.plot(viewer._x, viewer._y2, "r")
+      ax1.plot([rate, rate], [0, 1])
+      ax1.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))
+
+      ax2.barh([0, 1], self._points[indexes], color=["g", "r"])
+      ax2.set_yticks(ticks=[0, 1], labels=["FPR", "TPR"])
+      ax2.annotate(f" {self._points[indexes][0]:.1%}", xy=(1, 0), xytext=(1, -0.2))
+      ax2.annotate(f" {self._points[indexes][1]:.1%}", xy=(1, 1), xytext=(1, 0.8))
+      ax2.set_xlim(0, 1)
+
+      plt.tight_layout()
+      plt.show()
+
+  viewer = DisplayROC(getROCpoints(bn, datasrc, target=target, label=label))
+
+  def interactive_view(rate: float):
+    viewer.display(rate)
+
+  # widgets.interact(interactive_view, rate=(0,100,1))
+  interactive_plot = widgets.interactive(interactive_view, rate=(0, 100, 1))
+  output = interactive_plot.children[-1]
+  output.layout.height = '250px'
+  return interactive_plot
+
+
+def animPR(bn, datasrc, target="Y", label="1"):
+  """
+  Interactive selection of a threshold using TPR and FPR for BN and data
+
+  Parameters
+  ----------
+  bn : pyAgrum.BayesNet
+    a Bayesian network
+  datasrc : str|DataFrame
+    a csv filename or a pandas.DataFrame
+  target : str
+    the target
+  label : str
+    the target label
+  """
+  import ipywidgets as widgets
+  import matplotlib.pyplot as plt
+  import matplotlib.ticker as mtick
+
+  class DisplayPR:
+    def __init__(self, points):
+      self._x = [i / len(points) for i in range(len(points))]
+      self._y1, self._y2 = zip(*points)
+      self._points = points
+
+    def display(self, threshold):
+      rate = threshold / 100.0
+      indexes = int((len(self._points) - 1) * rate)
+
+      plt.rcParams["figure.figsize"] = (4, 3)
+
+      fig, (ax1, ax2) = plt.subplots(nrows=2)
+      ax1.plot(viewer._x, viewer._y1, "r")
+      ax1.plot(viewer._x, viewer._y2, "g")
+      ax1.plot([rate, rate], [0, 1])
+      ax1.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))
+
+      ax2.barh([1, 0], self._points[indexes], color=["r", "g"])
+      ax2.set_yticks(ticks=[0, 1], labels=["Precision", "Recall"])
+      ax2.annotate(f" {self._points[indexes][1]:.1%}", xy=(1, 0), xytext=(1, -0.2))
+      ax2.annotate(f" {self._points[indexes][0]:.1%}", xy=(1, 1), xytext=(1, 0.8))
+      ax2.set_xlim(0, 1)
+
+      plt.tight_layout()
+      plt.show()
+
+  viewer = DisplayPR(getPRpoints(bn, datasrc, target=target, label=label))
+
+  def interactive_view(rate: float):
+    viewer.display(rate)
+
+  # widgets.interact(interactive_view, rate=(0,100,1))
+  interactive_plot = widgets.interactive(interactive_view, rate=(0, 100, 1))
+  output = interactive_plot.children[-1]
+  output.layout.height = '250px'
+  return interactive_plot
```

## pyAgrum/lib/bn2scores.py

```diff
@@ -58,15 +58,15 @@
   Returns
   -------
   Dict[int,str]
     return a dictionary of position for BN variables in fields
   """
   res = {}
   for field in bn.names():
-    if not field in fields:
+    if field not in fields:
       raise gum.DatabaseError(f"** At least, field '{field}' is missing in {csv_name}")
     res[bn.idFromName(field)] = fields[field]
 
   return res
 
 
 def computeScores(bn_name, csv_name, visible=False):
```

## pyAgrum/lib/bn_vs_bn.py

```diff
@@ -28,14 +28,17 @@
 import math
 from itertools import product, combinations
 
 import pyAgrum as gum
 import pyAgrum.lib._colors as gumcols
 
 
+STRUCTURAL_HAMMING="structural hamming"
+PURE_HAMMING="hamming"
+
 class GraphicalBNComparator:
   """
   BNGraphicalComparator allows to compare in multiple way 2 BNs...The smallest assumption is that the names of the variables are the same in the 2 BNs. But some comparisons will have also to check the type and domainSize of the variables. The bns have not exactly the  same role : _bn1 is rather the referent model for the comparison whereas _bn2 is the compared one to the referent model.
 
   Parameters
   ----------
   name1 : str or pyAgrum.BayesNet
@@ -267,15 +270,15 @@
 
   def scores(self):
     """
     Compute Precision, Recall, F-score for self._bn2 compared to self._bn1
 
     precision and recall are computed considering BN1 as the reference
 
-    Fscor is 2*(recall* precision)/(recall+precision) and is the weighted average of Precision and Recall.
+    Fscore is 2*(recall* precision)/(recall+precision) and is the weighted average of Precision and Recall.
 
     dist2opt=square root of (1-precision)^2+(1-recall)^2 and represents the euclidian distance to the ideal point (precision=1, recall=1)
 
     Returns
     -------
     dict[str,double]
       A dictionnary containing 'precision', 'recall', 'fscore', 'dist2opt' and so on.
@@ -336,66 +339,65 @@
 
     Hamming distance is the difference of edges comparing the 2 skeletons, and Structural Hamming difference is the
     difference comparing the cpdags,	including the arcs' orientation.
 
     Returns
     -------
     dict[double,double]
-      A dictionnary containing 'hamming','structural hamming'
+      A dictionary containing PURE_HAMMING,STRUCTURAL_HAMMING
     """
     # convert graphs to cpdags
-
     cpdag1 = gum.EssentialGraph(self._bn1).pdag()
     cpdag2 = gum.EssentialGraph(self._bn2).pdag()
 
     # We look at all combinations
     listVariables = self._bn1.names()
-    hamming_dico = {'hamming': 0, 'structural hamming': 0}
+    hamming_dico = {PURE_HAMMING: 0, STRUCTURAL_HAMMING: 0}
 
     for head, tail in combinations(listVariables, 2):
       idHead_1 = self._bn1.idFromName(head)
       idTail_1 = self._bn1.idFromName(tail)
 
       idHead_2 = self._bn2.idFromName(head)
       idTail_2 = self._bn2.idFromName(tail)
 
       if cpdag1.existsArc(idHead_1, idTail_1):  # Check arcs head->tail
         if cpdag2.existsArc(idTail_2, idHead_2) or cpdag2.existsEdge(idTail_2, idHead_2):
-          hamming_dico["structural hamming"] += 1
+          hamming_dico[STRUCTURAL_HAMMING] += 1
         elif not cpdag2.existsArc(idTail_2, idHead_2) and not cpdag2.existsArc(idHead_2,
                                                                                idTail_2) and not cpdag2.existsEdge(
           idTail_2, idHead_2):
 
-          hamming_dico["structural hamming"] += 1
-          hamming_dico["hamming"] += 1
+          hamming_dico[STRUCTURAL_HAMMING] += 1
+          hamming_dico[PURE_HAMMING] += 1
 
       elif cpdag1.existsArc(idTail_1, idHead_1):  # Check arcs tail->head
         if cpdag2.existsArc(idHead_2, idTail_2) or cpdag2.existsEdge(idTail_2, idHead_2):
-          hamming_dico["structural hamming"] += 1
+          hamming_dico[STRUCTURAL_HAMMING] += 1
         elif not cpdag2.existsArc(idTail_2, idHead_2) and \
            not cpdag2.existsArc(idHead_2, idTail_2) and \
            not cpdag2.existsEdge(idTail_2, idHead_2):
-          hamming_dico["structural hamming"] += 1
-          hamming_dico["hamming"] += 1
+          hamming_dico[STRUCTURAL_HAMMING] += 1
+          hamming_dico[PURE_HAMMING] += 1
 
       elif cpdag1.existsEdge(idTail_1, idHead_1):  # Check edge
         if cpdag2.existsArc(idHead_2, idTail_2) or cpdag2.existsArc(idTail_2, idHead_2):
-          hamming_dico["structural hamming"] += 1
+          hamming_dico[STRUCTURAL_HAMMING] += 1
         elif not cpdag2.existsArc(idTail_2, idHead_2) and \
            not cpdag2.existsArc(idHead_2, idTail_2) and \
            not cpdag2.existsEdge(idTail_2, idHead_2):
-          hamming_dico["structural hamming"] += 1
-          hamming_dico["hamming"] += 1
+          hamming_dico[STRUCTURAL_HAMMING] += 1
+          hamming_dico[PURE_HAMMING] += 1
           # check no edge or arc on the ref graph, and yes on the other graph
 
       elif cpdag2.existsArc(idHead_2, idTail_2) or \
          cpdag2.existsEdge(idHead_2, idTail_2) or \
          cpdag2.existsArc(idTail_2, idHead_2):
-        hamming_dico["structural hamming"] += 1
-        hamming_dico["hamming"] += 1
+        hamming_dico[STRUCTURAL_HAMMING] += 1
+        hamming_dico[PURE_HAMMING] += 1
 
     return hamming_dico
 
 
 def graphDiff(bnref, bncmp,noStyle=False):
   """ Return a pydot graph that compares the arcs of bnref to bncmp.
   graphDiff allows bncmp to have less nodes than bnref. (this is not the case in GraphicalBNComparator.dotDiff())
@@ -470,15 +472,14 @@
       # a node is missing
       if not (n1 in bncmp.names() and n2 in bncmp.names()):
         res.add_edge(dot.Edge(f'"{n1}"', f'"{n2}"',
                               style=gum.config["notebook", "graphdiff_missing_style"],
                               color=gum.config["notebook", "graphdiff_missing_color"]))
         continue
 
-      keyarc = "graphdiff_correct"
       if bncmp.existsArc(n1, n2):  # arc is OK in BN2
         res.add_edge(dot.Edge(f'"{n1}"', f'"{n2}"',
                               style=gum.config["notebook", "graphdiff_correct_style"],
                               color=gum.config["notebook", "graphdiff_correct_color"]))
       elif bncmp.existsArc(n2, n1):  # arc is reversed in BN2
         res.add_edge(dot.Edge(f'"{n1}"', f'"{n2}"',
                               style="invis"))
```

## pyAgrum/lib/cn2graph.py

```diff
@@ -107,27 +107,24 @@
   for a in bn.arcs():
     (n, j) = a
     pw = 1
     av = f"{n}&nbsp;&rarr;&nbsp;{j}"
     col = gumcols.getBlackInTheme()
     lb = ""
 
-    if arcWidth is not None:
-      if a in arcWidth:
-        if maxarcs != minarcs:
-          pw = 0.1 + 5 * (arcWidth[a] - minarcs) / (maxarcs - minarcs)
-        av = f"{n}&nbsp;&rarr;&nbsp;{j} : {arcWidth[a]}"
-
-    if arcColor is not None:
-      if a in arcColor:
-        col = gumcols.proba2color(arcColor[a], cmapArc)
-
-    if arcLabel is not None:
-        if a in arcLabel:
-            lb=arcLabel[a]
+    if arcWidth is not None and a in arcWidth:
+      if maxarcs != minarcs:
+        pw = 0.1 + 5 * (arcWidth[a] - minarcs) / (maxarcs - minarcs)
+      av = f"{n}&nbsp;&rarr;&nbsp;{j} : {arcWidth[a]}"
+
+    if arcColor is not None and a in arcColor:
+      col = gumcols.proba2color(arcColor[a], cmapArc)
+
+    if arcLabel is not None and a in arcLabel:
+      lb=arcLabel[a]
 
     edge = dot.Edge('"' + bn.variable(a[0]).name() + '"', '"' + bn.variable(a[1]).name() + '"',
                     label=lb, fontsize="10",
                     penwidth=pw, color=col,
                     tooltip=av
                     )
     graph.add_edge(edge)
@@ -222,18 +219,17 @@
 
     # defaults
     bgcol = gum.config["notebook", "default_node_bgcolor"]
     fgcol = gum.config["notebook", "default_node_fgcolor"]
     if len(targets) == 0 or name in targets or nid in targets:
       bgcol = gum.config["notebook", "figure_facecolor"]
 
-    if nodeColor is not None:
-      if name in nodeColor or nid in nodeColor:
-        bgcol = gumcols.proba2bgcolor(nodeColor[name], cmapNode)
-        fgcol = gumcols.proba2fgcolor(nodeColor[name], cmapNode)
+    if nodeColor is not None and (name in nodeColor or nid in nodeColor):
+      bgcol = gumcols.proba2bgcolor(nodeColor[name], cmapNode)
+      fgcol = gumcols.proba2fgcolor(nodeColor[name], cmapNode)
 
     # 'hard' colour for evidence (?)
     if name in evs or nid in evs:
       bgcol = gum.config["notebook", "evidence_bgcolor"]
       fgcol = gum.config["notebook", "evidence_fgcolor"]
 
     colorattribute = f'fillcolor="{bgcol}", fontcolor="{fgcol}", color="#000000"'
@@ -248,23 +244,21 @@
 
   for a in showdag.arcs():
     (n, j) = a
     pw = 1
     av = f"{n}&nbsp;&rarr;&nbsp;{j}"
     col = gumcols.getBlackInTheme()
 
-    if arcWidth is not None:
-      if a in arcWidth:
-        if maxarcs != minarcs:
-          pw = 0.1 + 5 * (arcWidth[n, j] - minarcs) / (maxarcs - minarcs)
-        av = f"{n}&nbsp;&rarr;&nbsp;{j} : {arcWidth[a]}"
-
-    if arcColor is not None:
-      if a in arcColor:
-        col = gumcols.proba2color(arcColor[n, j], cmapArc)
+    if arcWidth is not None and a in arcWidth:
+      if maxarcs != minarcs:
+        pw = 0.1 + 5 * (arcWidth[n, j] - minarcs) / (maxarcs - minarcs)
+      av = f"{n}&nbsp;&rarr;&nbsp;{j} : {arcWidth[a]}"
+
+    if arcColor is not None and a in arcColor:
+      col = gumcols.proba2color(arcColor[n, j], cmapArc)
 
     dotstr += f' "{bn.variable(n).name()}"->"{bn.variable(j).name()}" [penwidth="{pw}",tooltip="{av}",color="{col}"];'
 
   dotstr += '}'
 
   g = dot.graph_from_dot_data(dotstr)[0]
```

## pyAgrum/lib/explain.py

```diff
@@ -710,15 +710,15 @@
       if True, the importance plot is shown in percent.
 
     Returns
     -------
       a dictionary Dict[str,float]
     """
     results, v = self._causal(train)
-    n_feats = len(self.feats_names)
+
     res = {}
     for col in results.columns:
       res[col] = abs(results[col]).mean()
 
     self._plotResults(results, v, plot, plot_importance, percentage)
 
     return res
@@ -757,21 +757,17 @@
 
     Returns
     -------
     matplotlib.pyplot.scatter
     """
     min_value = np.min(data, axis=(0, 1))
     max_value = np.max(data, axis=(0, 1))
-    # horiz_shift = (max_value - min_value)/100
-    # N=150
-    # K=0
     bin_size = (max_value - min_value) / N
     horiz_shift = K * bin_size
 
-    # fig, axs = plt.subplots()
     if ax is None:
       _, ax = plt.subplots()
     if cmap is None:
       # Set Color Map
       ## Define the hex colors
       color1 = gum.config["notebook", "potential_color_0"]
       color2 = gum.config["notebook", "potential_color_1"]
@@ -1054,9 +1050,9 @@
   Dict[str,int]
     the list of names and their depth.
   """
   if type(x) == str:
     nx = bn.idFromName(x)
   else:
     nx = x
-  nodes, arcs, visited = _buildMB(bn, nx, k)
+  nodes, _ , visited = _buildMB(bn, nx, k)
   return {bn.variable(node).name(): k - visited[node] for node in nodes}
```

## pyAgrum/lib/export.py

```diff
@@ -70,31 +70,31 @@
   bn :
     the Bayesian network to export
   filename : Optional[str]
     the name of the file (including the prefix), if None , use sys.stdout
   """
   def _toFastBN(bn,zefile):
     print('bn=gum.fastBN("""', end="",file=zefile)
-    vars = set()
+    sovars = set()
     first=True
     for x, y in bn.arcs():
       if not first:
         print('\n                 ', end="", file=zefile)
       else:
         first=False
-      if x in vars:
+      if x in sovars:
         print(bn.variable(x).name(), end="", file=zefile)
       else:
         print(bn.variable(x).toFast(), end="", file=zefile)
-        vars.add(x)
+        sovars.add(x)
       print("->", end="", file=zefile)
-      if y in vars:
+      if y in sovars:
         print(bn.variable(y).name(), end=";", file=zefile)
       else:
         print(bn.variable(y).toFast(), end=";", file=zefile)
-        vars.add(y)
+        sovars.add(y)
     print('""")')
   if filename is None:
     _toFastBN(bn,sys.stdout)
   else:
     with open(filename, "w") as pyfile:
       _toFastBN(bn,pyfile)
```

## pyAgrum/lib/id2graph.py

```diff
@@ -187,15 +187,14 @@
       fgcolor = gum.config["notebook", "evidence_fgcolor"]
 
     styleattribute = 'style=filled, height=0,margin=0.1'
     colorattribute = f'fillcolor="{bgcolor}", fontcolor="{fgcolor}", color="#000000"'
 
     if not diag.isUtilityNode(nid):
       if len(targets) == 0 or name in targets or nid in targets:
-        # shape="rectangle"
         filename = temp_dir + \
                    hashlib.md5(name.encode()).hexdigest() + "." + \
                    gum.config["notebook", "graph_format"]
         saveFigProba(ie.posterior(name), filename, bgcolor=bgcolor, util=ie.posteriorUtility(nid), txtcolor=fgcolor)
         dotstr += f' "{name}" [shape=rectangle,image="{filename}",label="", {colorattribute}];\n'
       else:
         dotstr += f' "{name}" [{colorattribute},shape={shape},{styleattribute}]'
```

## pyAgrum/lib/image.py

```diff
@@ -57,15 +57,15 @@
       os.remove(tmp.name)
     except PermissionError:  # probably windows error : file still 'used' ... grrr...
       pass
     return img
 
   fmt_image = filename.split(".")[-1]
   if fmt_image not in ['pdf', 'png', 'fig', 'jpg', 'svg', 'ps']:
-    raise Exception(
+    raise NameError(
       f"{filename} in not a correct filename for export : extension '{fmt_image}' not in [pdf,png,fig,jpg,svg]."
     )
 
   if isinstance(model, gum.BayesNet):
     fig = BN2dot(model, **kwargs)
   elif isinstance(model, gum.MarkovRandomField):
     if gum.config["notebook", "default_markovnetwork_view"] == "graph":
@@ -312,15 +312,15 @@
       os.remove(tmp.name)
     except PermissionError:  # probably windows error : file still 'used' ... grrr...
       pass
     return img
 
   fmt_image = filename.split(".")[-1]
   if fmt_image not in ['pdf', 'png', 'ps']:
-    raise Exception(
+    raise NameError(
       f"{filename} in not a correct filename for export : extension '{fmt_image}' not in [pdf,png,ps]."
     )
 
   import cairosvg
 
   if "size" in kwargs:
     size = kwargs['size']
```

## pyAgrum/lib/mn2graph.py

```diff
@@ -1,8 +1,8 @@
 import warnings
 
 warnings.warn(
-            f"""
+            """
 ** pyAgrum : module 'pyAgrum.lib.mn2graph' is deprecated. Please import pyAgrum.lib.mrf2graph instead.
 """, DeprecationWarning, stacklevel=2)
 
-from pyAgrum.lib.mrf2graph import *
+from pyAgrum.lib.mrf2graph import *
```

## pyAgrum/lib/mrf2graph.py

```diff
@@ -113,27 +113,24 @@
   for a in mrf.edges():
     (n, j) = a
     pw = 1
     av = f"{n}&nbsp;&mdash;&nbsp;{j}"
     col = gumcols.getBlackInTheme()
     lb = ""
 
-    if edgeWidth is not None:
-      if a in edgeWidth:
-        if maxedges != minedges:
-          pw = 0.1 + 5 * (edgeWidth[a] - minedges) / (maxedges - minedges)
-        av = f"{n}&nbsp;&mdash;&nbsp;{j} : {edgeWidth[a]}"
-
-    if edgeColor is not None:
-      if a in edgeColor:
-        col = gumcols.proba2color(edgeColor[a], cmapEdge)
-
-    if edgeLabel is not None:
-      if a in edgeLabel:
-        lb = edgeLabel[a]
+    if edgeWidth is not None and a in edgeWidth:
+      if maxedges != minedges:
+        pw = 0.1 + 5 * (edgeWidth[a] - minedges) / (maxedges - minedges)
+      av = f"{n}&nbsp;&mdash;&nbsp;{j} : {edgeWidth[a]}"
+
+    if edgeColor is not None and a in edgeColor:
+      col = gumcols.proba2color(edgeColor[a], cmapEdge)
+
+    if edgeLabel is not None and a in edgeLabel:
+      lb = edgeLabel[a]
 
     edge = dot.Edge('"' + mrf.variable(a[0]).name() + '"',
                     '"' + mrf.variable(a[1]).name() + '"',
                     label=lb, fontsize="10",
                     penwidth=pw, color=col,
                     tooltip=av
                     )
@@ -304,18 +301,17 @@
 
     # defaults
     bgcol = gum.config["notebook", "default_node_bgcolor"]
     fgcol = gum.config["notebook", "default_node_fgcolor"]
     if len(targets) == 0 or name in targets or nid in targets:
       bgcol = gum.config["notebook", "figure_facecolor"]
 
-    if nodeColor is not None:
-      if name in nodeColor or nid in nodeColor:
-        bgcol = gumcols.proba2bgcolor(nodeColor[name], cmapNode)
-        fgcol = gumcols.proba2fgcolor(nodeColor[name], cmapNode)
+    if nodeColor is not None and (name in nodeColor or nid in nodeColor):
+      bgcol = gumcols.proba2bgcolor(nodeColor[name], cmapNode)
+      fgcol = gumcols.proba2fgcolor(nodeColor[name], cmapNode)
 
     # 'hard' colour for evidence (?)
     if name in evs or nid in evs:
       bgcol = gum.config["notebook", "evidence_bgcolor"]
       fgcol = gum.config["notebook", "evidence_fgcolor"]
 
     colorattribute = f'fillcolor="{bgcol}", fontcolor="{fgcol}", color="#000000"'
@@ -330,25 +326,22 @@
       dotstr += f' "{name}" [{colorattribute}]'
 
   for a in mrf.edges():
     (n, j) = a
     pw = 1
     av = f"{n}&nbsp;&mdash;&nbsp;{j}"
     col = gumcols.getBlackInTheme()
-    lb = ""
 
-    if arcWidth is not None:
-      if (n, j) in arcWidth:
-        if maxarcs != minarcs:
-          pw = 0.1 + 5 * (arcWidth[a] - minarcs) / (maxarcs - minarcs)
-        av = f"{n}&nbsp;&mdash;&nbsp;{j} : {arcWidth[a]}"
-
-    if arcColor is not None:
-      if a in arcColor:
-        col = gumcols.proba2color(arcColor[a], cmapArc)
+    if arcWidth is not None and (n, j) in arcWidth:
+      if maxarcs != minarcs:
+        pw = 0.1 + 5 * (arcWidth[a] - minarcs) / (maxarcs - minarcs)
+      av = f"{n}&nbsp;&mdash;&nbsp;{j} : {arcWidth[a]}"
+
+    if arcColor is not None and a in arcColor:
+      col = gumcols.proba2color(arcColor[a], cmapArc)
 
     dotstr += f' "{mrf.variable(n).name()}"--"{mrf.variable(j).name()}" [penwidth="{pw}",tooltip="{av}",color="{col}"];'
   dotstr += '}'
 
   g = dot.graph_from_dot_data(dotstr)[0]
 
   # workaround for some badly parsed graph (pyparsing>=3.03)
@@ -420,18 +413,17 @@
 
     # defaults
     bgcol = gum.config["notebook", "default_node_bgcolor"]
     fgcol = gum.config["notebook", "default_node_fgcolor"]
     if len(targets) == 0 or name in targets or nid in targets:
       bgcol = gum.config["notebook", "figure_facecolor"]
 
-    if nodeColor is not None:
-      if name in nodeColor or nid in nodeColor:
-        bgcol = gumcols.proba2bgcolor(nodeColor[name], cmapNode)
-        fgcol = gumcols.proba2fgcolor(nodeColor[name], cmapNode)
+    if nodeColor is not None and (name in nodeColor or nid in nodeColor):
+      bgcol = gumcols.proba2bgcolor(nodeColor[name], cmapNode)
+      fgcol = gumcols.proba2fgcolor(nodeColor[name], cmapNode)
 
     # 'hard' colour for evidence (?)
     if name in evs or nid in evs:
       bgcol = gum.config["notebook", "evidence_bgcolor"]
       fgcol = gum.config["notebook", "evidence_fgcolor"]
 
     colorattribute = f'fillcolor="{bgcol}", fontcolor="{fgcol}", color="#000000"'
```

## pyAgrum/lib/notebook.py

```diff
@@ -533,21 +533,26 @@
     showDot(jt.toDot(), size)
 
 
 def getJunctionTree(bn, withNames=True, size=None):
   """
   get a HTML string for a junction tree (more specifically a join tree)
 
+  Parameters
+  ----------
     bn: "pyAgrum.BayesNet"
      the Bayesian network
     withNames: Boolean
      display the variable names or the node id in the clique
     size: str
      size (for graphviz) of the rendered graph
-  :return: the HTML representation of the graph
+  Returns
+  -------
+    str
+      the HTML representation of the graph
   """
   if size is None:
     size = gum.config["notebook", "junctiontree_graph_size"]
 
   jtg = gum.JunctionTreeGenerator()
   jt = jtg.junctionTree(bn)
 
@@ -1301,15 +1306,15 @@
         label = inst.variable(par).label(inst.val(par))
         if par == 1 or gum.config["notebook", "potential_parent_values"] == "nomerge":
           s += f"<th style='border:1px solid black;color:black;background-color:#BBBBBB'><center>{label}</center></th>"
         else:
           if sum([inst.val(i) for i in range(1, par)]) == 0:
             s += f"""<th style='border:1px solid black;color:black;background-color:#BBBBBB;' rowspan = '{offset[par]}'>
             <center>{label}</center></th>"""
-      for j in range(pot.variable(0).domainSize()):
+      for _ in range(pot.variable(0).domainSize()):
         s += _mkCell(pot.get(inst))
         inst.inc()
       s += "</tr>"
       html.append(s)
 
   html.append("</table>")
 
@@ -1448,15 +1453,15 @@
     captions = kwargs['captions']
   else:
     captions = None
 
   if 'valign' in kwargs and kwargs['valign'] in ['top', 'middle', 'bottom']:
     v_align = f"vertical-align:{kwargs['valign']};"
   else:
-    v_align = f"vertical-align:middle;"
+    v_align = "vertical-align:middle;"
 
   ncols = None
   if 'ncols' in kwargs:
     ncols = int(kwargs['ncols'])
     if ncols < 1:
       ncols = 1
 
@@ -1532,24 +1537,23 @@
     t += "<li><b>target(s)</b><br/>"
     if ie.nbrTargets() == ie.BN().size():
       t += " all"
     else:
       t += ", ".join([ie.BN().variable(n).name() for n in ie.targets()])
     t += "</li>"
 
-  if hasattr(ie, 'nbrJointTargets'):
-    if ie.nbrJointTargets() > 0:
-      t += "<li><b>Joint target(s)</b><br/>"
-      t += ", ".join(['['
-                      + (", ".join([ie.BN().variable(n).name()
-                                    for n in ns]
-                                   ))
-                      + ']' for ns in ie.jointTargets()]
-                     )
-      t += "</li>"
+  if hasattr(ie, 'nbrJointTargets') and ie.nbrJointTargets() > 0:
+    t += "<li><b>Joint target(s)</b><br/>"
+    t += ", ".join(['['
+                    + (", ".join([ie.BN().variable(n).name()
+                                  for n in ns]
+                                 ))
+                    + ']' for ns in ie.jointTargets()]
+                   )
+    t += "</li>"
   t += '</ul></div>'
   return getSideBySide(getBN(ie.BN()), t, captions=[inferenceCaption, "Evidence and targets"])
 
 
 def getJT(jt, size=None):
   """
   returns the representation of a junction tree as a HTML string
@@ -1636,16 +1640,14 @@
                             )
                    )
     graph.add_edge(dot.Edge('"' + sepnames(c1, c2) +
                             '"', '"' + cliqnames(c2) + '"'
                             )
                    )
 
-  if size is None:
-    size = gum.config["notebook", "default_graph_size"]
   graph.set_size(gum.config["notebook", "junctiontree_graph_size"])
 
   return graph.to_string()
 
 
 def getCliqueGraph(cg, size=None):
   """get a representation for clique graph. Special case for junction tree
```

## pyAgrum/lib/proba_histogram.py

```diff
@@ -18,14 +18,15 @@
 # IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS
 # ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE
 # OR PERFORMANCE OF THIS SOFTWARE!
 import math
 import numpy as np
 
 import matplotlib.pyplot as plt
+from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)
 
 import pyAgrum as gum
 
 
 def _stats(pot):
   mu = 0.0
   mu2 = 0.0
@@ -61,20 +62,19 @@
   var = p.variable(0)
   la = [var.label(int(i)) for i in np.arange(var.domainSize())]
   v = p.tolist()
   nzmin = None
   nzmax = None
   l = len(v) - 1
   for i in range(l + 1):
-    if v[i] != 0:
-      if nzmin is None:
-        if i > 0:
-          nzmin = i - 1
-        else:
-          nzmin = -1
+    if v[i] != 0 and nzmin is None:
+      if i > 0:
+        nzmin = i - 1
+      else:
+        nzmin = -1
     if v[l - i] != 0:
       if nzmax is None:
         if i > 0:
           nzmax = l - i + 1
         else:
           nzmax = -1
 
@@ -277,14 +277,40 @@
   # Even if utility, now we do show the mean/sigma of the distribution.
   ax.set_title(_getTitleHisto(p,True), color=txtcolor)
   ax.get_xaxis().grid(True)
   ax.margins(0)
 
   return fig
 
+def _getHistoForDiscretized(p, scale=1.0, txtcolor="Black"):
+  var = p.variable(0)
+  vx=var.ticks()
+  widths=[vx[i+1]-vx[i] for i in range(len(vx)-1)]
+  vals=[v/w for v,w in zip(p.tolist(),widths)]
+    
+  fig = plt.figure()
+  fig.set_figwidth(scale * max(15,len(vx))/8 )
+  fig.set_figheight(scale)
+
+  ax = fig.add_subplot(111)
+  ax.set_facecolor('white')
+  ax.set_xticks([vx[0],(vx[-1]+vx[0])/2,vx[-1]])
+  ax.xaxis.set_minor_locator(AutoMinorLocator())
+  ax.tick_params(which="minor",length=4)
+  ax.get_xaxis().grid(True,which="minor",zorder=0)
+
+  bars = ax.bar(vx[:-1], height=vals,width=widths,
+                align='edge',
+                color=gum.config['notebook', 'histogram_color'],edgecolor=gum.config['notebook', 'histogram_edge_color'],zorder=3)
+
+    # Even if utility, now we do show the mean/sigma of the distribution.
+  ax.set_title(_getTitleHisto(p,True), color=txtcolor)
+  ax.margins(0)
+
+  return fig
 
 def proba2histo(p, scale=1.0, util=None, txtcolor="Black"):
   """
   compute the representation of a histogram for a mono-dim Potential
 
   Parameters
   ----------
@@ -301,14 +327,18 @@
   -------
   matplotlib.Figure
     a matplotlib histogram for a Potential p.
   """
   if util is not None:
     return _getProbaH(p, scale, util=util, txtcolor=txtcolor)
 
+  if p.variable(0).varType()==gum.VarType_Discretized:
+    if gum.config['notebook','histogram_discretized_visualisation']=="histogram":
+      return _getHistoForDiscretized(p,scale,txtcolor)
+      
   if p.variable(0).domainSize() > int(gum.config['notebook', 'histogram_line_threshold']):
     return _getProbaLine(p, scale, txtcolor=txtcolor)
 
   if p.variable(0).domainSize() > int(gum.config['notebook', 'histogram_horizontal_threshold']):
     return _getProbaV(p, scale, txtcolor=txtcolor)
 
   return _getProbaH(p, scale, util=util, txtcolor=txtcolor)
```

## pyAgrum/lib/_colors.py

```diff
@@ -43,15 +43,15 @@
 
 def getBlackInTheme():
   """ return the color used for arc and text in graphs
   """
   return gum.config["notebook", "default_arc_color"]
 
 
-def hex2rgb(vstr):
+def hex2rgb(vstr:str)->List[int]:
   """
   from "#FFFFFF" to [255,255,255]
 
   Parameters
   ----------
   vstr: str
     the rbg string
```

## pyAgrum/skbn/bnclassifier.py

```diff
@@ -44,175 +44,185 @@
 from ._learningMethods import _fitStandard as BN_fitStandard
 from ._learningMethods import _fitNaiveBayes as BN_fitNaiveBayes
 from ._learningMethods import _fitTAN as BN_fitTAN
 from ._learningMethods import _fitChowLiu as BN_fitChowLiu
 
 
 class BNClassifier(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):
-  """ Represents a (scikit-learn compliant) classifier which uses a BN to classify. A BNClassifier is build using
+  """
+  Represents a (scikit-learn compliant) classifier which uses a BN to classify. A BNClassifier is build using
 
    - a Bayesian network,
    - a database and a learning algorithm and parameters
    - the use of BNDiscretizer to discretize with different algorithms some variables.
 
 
-      parameters:
-            learningMethod: str
-                A string designating which type of learning we want to use. Possible values are: Chow-Liu, NaiveBayes,
-                TAN, MIIC + (MDL ou NML), GHC, 3off2 + (MDL ou NML), Tabu.
-                GHC designates Greedy Hill Climbing.
-                MIIC designates Multivariate Information based Inductive Causation
-                TAN designates Tree-augmented NaiveBayes
-                Tabu designated Tabu list searching
-
-            prior: str
-                A string designating the type of a priorsmoothing we want to use. Possible values are Smoothing,
-                BDeu, Dirichlet and NoPrior .
-                Note: if using Dirichlet smoothing DirichletCsv cannot be set to none
-                By default (when prior is None) : a smoothing(0.01) is applied.
-
-            scoringType: str
-                A string designating the type of scoring we want to use. Since scoring is used while constructing the
-                network and not when learning its parameters, the scoring will be ignored if using a learning algorithm
-                with a fixed network structure such as Chow-Liu, TAN or NaiveBayes.
-                possible values are:  AIC, BIC, BD, BDeu, K2, Log2
-                AIC means Akaike information criterion
-                BIC means Bayesian Information criterion
-                BD means Bayesian-Dirichlet scoring
-                BDeu means Bayesian-Dirichlet equivalent uniform
-                Log2 means log2 likelihood ratio test
-
-            constraints: dict()
-                A dictionary designating the constraints that we want to put on the structure of the Bayesian network.
-                Ignored if using a learning algorithm where the structure is fixed such as TAN or NaiveBayes.
-                the keys of the dictionary should be the strings "PossibleEdges" , "MandatoryArcs" and  "ForbiddenArcs".
-                The format of the values should be a tuple of strings (tail,head) which designates the string arc from
-                tail to head. For example if we put the value ("x0"."y") in MandatoryArcs the network will surely have
-                an arc going from x0 to y.
-                Note: PossibleEdge allows between nodes x and y allows for either (x,y) or (y,x) (or none of them) to be added to the Bayesian network, while the others are not symmetric.
-
-            priorWeight: double
-                The weight used for a prior.
-
-            possibleSkeleton: pyAgrum.undigraph
-                An undirected graph that serves as a possible skeleton for the Bayesian network
-
-            DirichletCsv: str
-                the file name of the csv file we want to use for the dirichlet prior. Will be ignored if prior is not
-                set to Dirichlet.
-
-            discretizationStrategy: str
-                sets the default method of discretization for this discretizer. This method will be used if the user has
-                not specified another method for that specific variable using the setDiscretizationParameters method
-                possible values are: 'quantile', 'uniform', 'kmeans', 'NML', 'CAIM' and 'MDLP'
-
-            defaultNumberOfBins: str or int
-                sets the number of bins if the method used is quantile, kmeans, uniform. In this case this parameter can
-                also be set to the string 'elbowMethod' so that the best number of bins is found automatically.
-                If the method used is NML, this parameter sets the maximum number of bins up to which the NML
-                algorithm searches for the optimal number of bins. In this case this parameter must be an int
-                If any other discretization method is used, this parameter is ignored.
-
-            discretizationThreshold: int or float
-                When using default parameters a variable will be treated as continuous only if it has more unique values
-                than this number (if the number is an int greater than 1).
-                If the number is a float between 0 and 1, we will test if the proportion of unique values is bigger than
-                this number.
-                For instance, if you have entered 0.95, the variable will be treated as continuous only if more than 95%
-                of its values are unique.
-
-            usePR: bool
-                indicates if the threshold to choose is Prevision-Recall curve's threshold or ROC's threshold by
-                default.
-                ROC curves should be used when there are roughly equal numbers of observations for each class.
-                Precision-Recall curves should be used when there is a moderate to large class imbalance especially for
-                the target's class.
+  Parameters
+  ----------
+    learningMethod: str
+        A string designating which type of learning we want to use. Possible values are: Chow-Liu, NaiveBayes,
+        TAN, MIIC + (MDL ou NML), GHC, 3off2 + (MDL ou NML), Tabu.
+        GHC designates Greedy Hill Climbing.
+        MIIC designates Multivariate Information based Inductive Causation
+        TAN designates Tree-augmented NaiveBayes
+        Tabu designated Tabu list searching
+
+    prior: str
+        A string designating the type of a priorsmoothing we want to use. Possible values are Smoothing,
+        BDeu, Dirichlet and NoPrior .
+        Note: if using Dirichlet smoothing DirichletCsv cannot be set to none
+        By default (when prior is None) : a smoothing(0.01) is applied.
+
+    scoringType: str
+        A string designating the type of scoring we want to use. Since scoring is used while constructing the
+        network and not when learning its parameters, the scoring will be ignored if using a learning algorithm
+        with a fixed network structure such as Chow-Liu, TAN or NaiveBayes.
+        possible values are:  AIC, BIC, BD, BDeu, K2, Log2
+        AIC means Akaike information criterion
+        BIC means Bayesian Information criterion
+        BD means Bayesian-Dirichlet scoring
+        BDeu means Bayesian-Dirichlet equivalent uniform
+        Log2 means log2 likelihood ratio test
+
+    constraints: dict()
+        A dictionary designating the constraints that we want to put on the structure of the Bayesian network.
+        Ignored if using a learning algorithm where the structure is fixed such as TAN or NaiveBayes.
+        the keys of the dictionary should be the strings "PossibleEdges" , "MandatoryArcs" and  "ForbiddenArcs".
+        The format of the values should be a tuple of strings (tail,head) which designates the string arc from
+        tail to head. For example if we put the value ("x0"."y") in MandatoryArcs the network will surely have
+        an arc going from x0 to y.
+        Note: PossibleEdge allows between nodes x and y allows for either (x,y) or (y,x) (or none of them) to be added to the Bayesian network, while the others are not symmetric.
+
+    priorWeight: double
+        The weight used for a prior.
+
+    possibleSkeleton: pyAgrum.undigraph
+        An undirected graph that serves as a possible skeleton for the Bayesian network
+
+    DirichletCsv: str
+        the file name of the csv file we want to use for the dirichlet prior. Will be ignored if prior is not
+        set to Dirichlet.
+
+    discretizationStrategy: str
+        sets the default method of discretization for this discretizer. This method will be used if the user has
+        not specified another method for that specific variable using the setDiscretizationParameters method
+        possible values are: 'quantile', 'uniform', 'kmeans', 'NML', 'CAIM' and 'MDLP'
+
+    defaultNumberOfBins: str or int
+        sets the number of bins if the method used is quantile, kmeans, uniform. In this case this parameter can
+        also be set to the string 'elbowMethod' so that the best number of bins is found automatically.
+        If the method used is NML, this parameter sets the maximum number of bins up to which the NML
+        algorithm searches for the optimal number of bins. In this case this parameter must be an int
+        If any other discretization method is used, this parameter is ignored.
+
+    discretizationThreshold: int or float
+        When using default parameters a variable will be treated as continuous only if it has more unique values
+        than this number (if the number is an int greater than 1).
+        If the number is a float between 0 and 1, we will test if the proportion of unique values is bigger than
+        this number.
+        For instance, if you have entered 0.95, the variable will be treated as continuous only if more than 95%
+        of its values are unique.
+
+    usePR: bool
+        indicates if the threshold to choose is Prevision-Recall curve's threshold or ROC's threshold by
+        default.
+        ROC curves should be used when there are roughly equal numbers of observations for each class.
+        Precision-Recall curves should be used when there is a moderate to large class imbalance especially for
+        the target's class.
+
+    beta: float
+        if you choose the Precision-Recall curve's threshold, the F-beta score is maximized. By default, beta=1
+        to have the F1 score. A value inferior of 1 will give more weight to precision. A value superior of 1
+        will give more weight to recall. For example, beta = 0.5 makes precision twice as important as recall, 
+        while beta = 2 does the opposite.
 
-            significant_digit:
-                number of significant digits when computing probabilities
+    significant_digit:
+        number of significant digits when computing probabilities
     """
 
   @gum.deprecated_arg(newA="prior", oldA="apriori", version="1.1.2")
   def __init__(self, learningMethod="GHC", prior=None, scoringType="BIC", constraints=None, priorWeight=1,
                possibleSkeleton=None, DirichletCsv=None, discretizationStrategy="quantile", discretizationNbBins=5,
-               discretizationThreshold=25, usePR=False, significant_digit=10):
-    """ parameters:
-            learningMethod: str
-                A string designating which type of learning we want to use. Possible values are: Chow-Liu, NaiveBayes,
-                TAN, MIIC + (MDL ou NML), GHC, 3off2 + (MDL ou NML), Tabu.
-                GHC designates Greedy Hill Climbing.
-                MIIC designates Multivariate Information based Inductive Causation
-                TAN designates Tree-augmented NaiveBayes
-                Tabu designated Tabu list searching
-
-            prior: str
-                A string designating the type of prior we want to use. Possible values are Smoothing, BDeu ,
-                Dirichlet and NoPrior.
-                Note: if using Dirichlet smoothing DirichletCsv cannot be set to none
-
-            scoringType: str
-                A string designating the type of scoring we want to use. Since scoring is used while constructing the
-                network and not when learning its parameters, the scoring will be ignored if using a learning algorithm
-                with a fixed network structure such as Chow-Liu, TAN or NaiveBayes.
-                possible values are:  AIC, BIC, BD, BDeu, K2, Log2
-                AIC means Akaike information criterion
-                BIC means Bayesian Information criterion
-                BD means Bayesian-Dirichlet scoring
-                BDeu means Bayesian-Dirichlet equivalent uniform
-                Log2 means log2 likelihood ratio test
-
-            constraints: dict()
-                A dictionary designating the constraints that we want to put on the structure of the Bayesian network.
-                Ignored if using a learning algorithm where the structure is fixed such as TAN or NaiveBayes.
-                the keys of the dictionary should be the strings "PossibleEdges" , "MandatoryArcs" and  "ForbiddenArcs".
-                The format of the values should be a tuple of strings (tail,head) which designates the string arc from
-                tail to head. For example if we put the value ("x0"."y") in MandatoryArcs the network will surely have
-                an arc going from x0 to y.
-                Note: PossibleEdges allows for both (tail,head) and (head,tail) to be added to the Bayesian network,
-                while the others are not symmetric.
-
-            priorWeight: double
-                The weight used for a prior.
-
-            possibleSkeleton: pyagrum.undigraph
-                An undirected graph that serves as a possible skeleton for the Bayesian network
-
-            DirichletCsv: str
-                the file name of the csv file we want to use for the dirichlet prior. Will be ignored if prior is not
-                set to Dirichlet.
-
-            discretizationStrategy: str
-                sets the default method of discretization for this discretizer. This method will be used if the user has
-                not specified another method for that specific variable using the setDiscretizationParameters method
-                possible values are: 'quantile', 'uniform', 'kmeans', 'NML', 'CAIM' and 'MDLP'
-
-            defaultNumberOfBins: str or int
-                sets the number of bins if the method used is quantile, kmeans, uniform. In this case this parameter can
-                also be set to the string 'elbowMethod' so that the best number of bins is found automatically.
-                If the method used is NML, this parameter sets the the maximum number of bins up to which the NML
-                algorithm searches for the optimal number of bins. In this case this parameter must be an int
-                If any other discetization method is used, this parameter is ignored.
-
-            discretizationThreshold: int or float
-                When using default parameters a variable will be treated as continuous only if it has more unique values
-                than this number (if the number is an int greater than 1).
-                If the number is a float between 0 and 1, we will test if the proportion of unique values is bigger than
-                this number.
-                For instance, if you have entered 0.95, the variable will be treated as continouus only if more than 95%
-                of its values are unique.
-
-            usePR: bool
-                indicates if the threshold to choose is Prevision-Recall curve's threshold or ROC's threshold by
-                default.
-                ROC curves should be used when there are roughly equal numbers of observations for each class.
-                Precision-Recall curves should be used when there is a moderate to large class imbalance especially for
-                the target's class.
+               discretizationThreshold=25, usePR=False, beta=1, significant_digit=10):
+    """ 
+    Parameters
+    ----------
+      learningMethod: str
+          A string designating which type of learning we want to use. Possible values are: Chow-Liu, NaiveBayes,
+          TAN, MIIC + (MDL ou NML), GHC, 3off2 + (MDL ou NML), Tabu.
+          GHC designates Greedy Hill Climbing.
+          MIIC designates Multivariate Information based Inductive Causation
+          TAN designates Tree-augmented NaiveBayes
+          Tabu designated Tabu list searching
+
+      prior: str
+          A string designating the type of prior we want to use. Possible values are Smoothing, BDeu ,
+          Dirichlet and NoPrior.
+          Note: if using Dirichlet smoothing DirichletCsv cannot be set to none
+
+      scoringType: str
+          A string designating the type of scoring we want to use. Since scoring is used while constructing the
+          network and not when learning its parameters, the scoring will be ignored if using a learning algorithm
+          with a fixed network structure such as Chow-Liu, TAN or NaiveBayes.
+          possible values are:  AIC, BIC, BD, BDeu, K2, Log2
+          AIC means Akaike information criterion
+          BIC means Bayesian Information criterion
+          BD means Bayesian-Dirichlet scoring
+          BDeu means Bayesian-Dirichlet equivalent uniform
+          Log2 means log2 likelihood ratio test
+
+      constraints: dict()
+          A dictionary designating the constraints that we want to put on the structure of the Bayesian network.
+          Ignored if using a learning algorithm where the structure is fixed such as TAN or NaiveBayes.
+          the keys of the dictionary should be the strings "PossibleEdges" , "MandatoryArcs" and  "ForbiddenArcs".
+          The format of the values should be a tuple of strings (tail,head) which designates the string arc from
+          tail to head. For example if we put the value ("x0"."y") in MandatoryArcs the network will surely have
+          an arc going from x0 to y.
+          Note: PossibleEdges allows for both (tail,head) and (head,tail) to be added to the Bayesian network,
+          while the others are not symmetric.
+
+      priorWeight: double
+          The weight used for a prior.
+
+      possibleSkeleton: pyagrum.undigraph
+          An undirected graph that serves as a possible skeleton for the Bayesian network
+
+      DirichletCsv: str
+          the file name of the csv file we want to use for the dirichlet prior. Will be ignored if prior is not
+          set to Dirichlet.
+
+      discretizationStrategy: str
+          sets the default method of discretization for this discretizer. This method will be used if the user has
+          not specified another method for that specific variable using the setDiscretizationParameters method
+          possible values are: 'quantile', 'uniform', 'kmeans', 'NML', 'CAIM' and 'MDLP'
+
+      defaultNumberOfBins: str or int
+          sets the number of bins if the method used is quantile, kmeans, uniform. In this case this parameter can
+          also be set to the string 'elbowMethod' so that the best number of bins is found automatically.
+          If the method used is NML, this parameter sets the the maximum number of bins up to which the NML
+          algorithm searches for the optimal number of bins. In this case this parameter must be an int
+          If any other discetization method is used, this parameter is ignored.
+
+      discretizationThreshold: int or float
+          When using default parameters a variable will be treated as continuous only if it has more unique values
+          than this number (if the number is an int greater than 1).
+          If the number is a float between 0 and 1, we will test if the proportion of unique values is bigger than
+          this number.
+          For instance, if you have entered 0.95, the variable will be treated as continouus only if more than 95%
+          of its values are unique.
+
+      usePR: bool
+          indicates if the threshold to choose is Prevision-Recall curve's threshold or ROC's threshold by
+          default.
+          ROC curves should be used when there are roughly equal numbers of observations for each class.
+          Precision-Recall curves should be used when there is a moderate to large class imbalance especially for
+          the target's class.
 
-            significant_digit:
-                number of significant digits when computing probabilities
+      significant_digit:
+          number of significant digits when computing probabilities
     """
 
     # The method of learning used
     self.learningMethod = learningMethod
 
     # An object used to stock the learner object from pyAgrum
     self.learner = None
@@ -221,14 +231,15 @@
     self.bn = None
 
     # The threshold used for predicting the class. THe algorithm calculates the probability of a certain class, the
     # classifier designates it as that class only if the probability is higher than the threshold.
     # The ROC curve is used to calculate the optimal threshold
     self.threshold = 0.5
     self.usePR = usePR
+    self.beta = beta
 
     # the type of prior used
     self.prior = prior
 
     # the weight used for the a prior
     self.priorWeight = priorWeight
 
@@ -268,38 +279,37 @@
     # dict(str:int)
     # The keys of this dictionary are the names of the variables. The value associated to each name is
     # the index of the variable.
     self.variableNameIndexDictionary = None
 
   def fit(self, X=None, y=None, data=None, targetName=None, filename=None):
     """
-    parameters:
+    Fits the model to the training data provided. The two possible uses of this function are `fit(X,y)` and `fit(data=...,
+    targetName=...)`. Any other combination will raise a ValueError
+
+    Parameters
+    ----------
         X: {array-like, sparse matrix} of shape (n_samples, n_features)
             training data. Warning: Raises ValueError if either filename or targetname is not None. Raises ValueError
             if y is None.
         y: array-like of shape (n_samples)
             Target values. Warning: Raises ValueError if either filename or targetname is not None. Raises ValueError
             if X is None
         data: Union[str,pandas.DataFrame]
             the source of training data : csv filename or pandas.DataFrame. targetName is mandatory to find the class in this source.
         targetName: str
             specifies the name of the targetVariable in the csv file. Warning: Raises ValueError if either X or y is
             not None. Raises ValueError if filename is None.
         filename: str
             (deprecated, use data instead)
             specifies the csv file where the training data and target values are located. Warning: Raises ValueError
-            if either X or y is not None. Raises ValueError if targetName is None
-    returns:
-        void
-
-    Fits the model to the training data provided. The two possible uses of this function are `fit(X,y)` and `fit(data=...,
-    targetName=...)`. Any other combination will raise a ValueError
+            if either X or y is not None. Raises ValueError if targetName is None.
     """
     if filename is not None:
-      print("**pyAgrum** : 'filename' is deprecated since 1.1.1. Please use 'data' instead.")
+      warnings.warn("**pyAgrum** : 'filename' is deprecated since 1.1.1. Please use 'data' instead.")
       if data is None:
         data = filename
 
     if data is None:
       if targetName is not None:
         raise ValueError(
           "This function should be used either as fit(X,y) or fit(data=...,targetName=...). You have set "
@@ -343,15 +353,15 @@
     X, y = sklearn.utils.check_X_y(X, y, dtype=None, accept_sparse=True)
 
     d = X.shape[1]
 
     if variableNames is None:
       variableNames = ["x" + str(i) for i in range(d)]
 
-    self.variableNameIndexDictionary = dict()
+    self.variableNameIndexDictionary = {}
 
     for i in range(d):
       self.variableNameIndexDictionary[variableNames[i]] = i
 
     self.targetType = y.dtype
 
     possibleValuesY = numpy.unique(y)
@@ -421,22 +431,25 @@
 
     self.label = self.bn.variableFromName(self.target).labels()[1]
 
     self.MarkovBlanket = compileMarkovBlanket(self.bn, self.target)
 
     if self.isBinaryClassifier:
       self.threshold = CThreshold(
-        self.MarkovBlanket, self.target, csvfilename, self.usePR, self.significant_digit)
+        self.MarkovBlanket, self.target, csvfilename, self.usePR, self.beta, self.significant_digit)
 
     os.remove(csvfilename)
     os.remove(tmpfilename)
 
   def fromTrainedModel(self, bn, targetAttribute, targetModality="", copy=False, threshold=0.5, variableList=None):
     """
-    parameters:
+    Creates a BN classifier from an already trained pyAgrum Bayesian network
+
+    Parameters
+    ----------
         bn: pyagrum.BayesNet
             The Bayesian network we want to use for this classifier
         targetAttribute: str
             the attribute that will be the target in this classifier
         targetModality: str
             If this is a binary classifier we have to specify which modality we are looking at if the target
             attribute has more than 2 possible values
@@ -447,19 +460,14 @@
         threshold: double
             The classification threshold. If the probability that the target modality is true is larger than this
             threshold we predict that modality
         variableList: list(str)
             A list of strings. variableList[i] is the name of the variable that has the index i. We use this information
             when calling predict to know which column corresponds to which variable.
             If this list is set to none, then we use the order in which the variables were added to the network.
-
-    returns:
-        void
-
-    Creates a BN classifier from an already trained pyAgrum Bayesian network
     """
 
     self.fromModel = True
 
     # the set of the names of all the variables in the Bayesian network
     namesSet = set(bn.names())
 
@@ -514,58 +522,60 @@
       if self.bn.variableFromName(self.target).domainSize() == 2:
         self.isBinaryClassifier = True
         self.label = self.bn.variableFromName(self.target).labels()[
           1]  # we take the label 1 as targetModality
       else:
         self.isBinaryClassifier = False
 
-    def changeVariableName(self, oldName, newName):
-      """
-      parameters:
-          oldName: str
-              the old name of the variable
-          newName: str
-              the new name of the variable
-      returns:
-          void
-
-      changes the name of a variable inside the Bayesian network
-      """
-      if oldName == self.target:
-        self.bn.changeVariableName(oldName, newName)
-        self.target = newName
-        self.MarkovBlanket.changeVariableName(oldName, newName)
-        return
+  def changeVariableName(self, oldName, newName):
+    """
+    Changes the name of a variable inside the Bayesian network
 
-      if oldName not in self.variableNameIndexDictionary:
-        raise ValueError(
-          "The oldName you have specified is not a name of a variable in the Bayesian network")
-      index = self.variableNameIndexDictionary.pop(oldName)
+    Parameters
+    ----------
+        oldName: str
+            the old name of the variable
+        newName: str
+            the new name of the variable
+            
+    """
+    if oldName == self.target:
+      self.bn.changeVariableName(oldName, newName)
+      self.target = newName
+      self.MarkovBlanket.changeVariableName(oldName, newName)
+      return
+
+    if oldName not in self.variableNameIndexDictionary:
+      raise ValueError(
+        "The oldName you have specified is not a name of a variable in the Bayesian network")
+    index = self.variableNameIndexDictionary.pop(oldName)
 
-      self.variableNameIndexDictionary[newName] = index
+    self.variableNameIndexDictionary[newName] = index
 
-      self.bn.changeVariableName(oldName, newName)
+    self.bn.changeVariableName(oldName, newName)
 
-      if oldName in self.MarkovBlanket.names():
-        self.MarkovBlanket.changeVariableName(oldName, newName)
+    if oldName in self.MarkovBlanket.names():
+      self.MarkovBlanket.changeVariableName(oldName, newName)
 
   # ------------------method Markov Blanket and predict---------------------
 
   def predict(self, X, with_labels=True):
     """
-    parameters:
+    Predicts the most likely class for each row of input data, with bn's Markov Blanket
+
+    Parameters
+    ----------
         X: str,{array-like, sparse matrix} of shape (n_samples, n_features) or str
             test data, can be either dataFrame, matrix or name of a csv file
         with_labels: bool
             tells us whether the csv includes the labels themselves or their indexes.
-    returns:
-        y: array-like of shape (n_samples,)
+    Returns
+    -------
+        array-like of shape (n_samples,)
             Predicted classes
-
-    Predicts the most likely class for each row of input data, with bn's Markov Blanket
     """
     if type(X) == str:
       X, _ = self.XYfromCSV(X, target=self.target)
 
     if isinstance(X, pandas.DataFrame):  # type(X) == pandas.DataFrame:
       dictName = DFNames(X)
     else:
@@ -690,15 +700,15 @@
     dictName = self.variableNameIndexDictionary
 
     if isinstance(X, pandas.DataFrame):  # type(X) == pandas.DataFrame:
       dictName = DFNames(X)
       vals = X.to_numpy()
     elif type(X) == str:
       vals, _ = self.XYfromCSV(X, target=self.target)
-      dictName = DFNames(vals, vals)
+      dictName = DFNames(vals)
       vals = vals.to_numpy()
     else:
       vals = X
 
     if self.fromModel:
       vals = sklearn.utils.check_array(vals, dtype='str', ensure_2d=False)
     else:
@@ -764,15 +774,15 @@
 
     if not with_labels:
       variableList = X.columns.tolist()
       targetVariable = self.bn.variableFromName(target)
       for index in range(len(variableList)):
         variableList[index] = self.bn.variableFromName(variableList[index])
       for row in X:
-        for i in len(row):
+        for i in range(len(row)):
           row[i] = variableList[i].labels(row[i])
       if self.fromModel:
         if self.isBinaryClassifier:
           labelIndex = 0
           labelList = targetVariable.labels()
           while labelIndex < len(labelList):
             if labelList[labelIndex] == self.label:
@@ -813,15 +823,15 @@
     Returns
     -------
       pandas.Dataframe
     """
     if self.variableNameIndexDictionary is None:
       raise ValueError("First, you need to fit a model !")
     if filename is not None:
-      print("pyAgrum ** : filename is deprecated. Please use data instead.")
+      warnings.warn("pyAgrum ** : filename is deprecated. Please use data instead.")
       if data is None:
         data = filename
 
     targetName = self.target
     if data is None:
       if X is None or y is None:
         raise ValueError(
@@ -874,22 +884,24 @@
         ligne.append(bestTypedVal(var, var[str(val)]))
 
       ligne.append(bestTypedVal(varY, varY[str(y[n])]))
       df.loc[len(df)] = ligne
 
     return df
 
-  def showROC_PR(self, filename, save_fig=False, show_progress=False):
+  def showROC_PR(self, filename,*,  beta=1, save_fig=False, show_progress=False):
     """
     Use the `pyAgrum.lib.bn2roc` tools to create ROC and Precision-Recall curve
 
     Parameters
     ----------
     filename: str
         a csv filename
+    beta : float
+        the value of beta for the F-beta score
     save_fig : bool
         whether the graph should be saved
     show_progress : bool
         indicates if the resulting curve must be printed
     """
     import pyAgrum.lib.bn2roc as bn2roc
     bn2roc.showROC_PR(self.bn, filename, self.target,
```

## pyAgrum/skbn/discretizer.py

```diff
@@ -1,8 +1,7 @@
-# -*- coding: utf-8 -*-
 # (c) Copyright 2020-2023 by Pierre-Henri Wuillemin(@LIP6)  (pierre-henri.wuillemin@lip6.fr)
 
 # Permission to use, copy, modify, and distribute this
 # software and its documentation for any purpose and
 # without fee or royalty is hereby granted, provided
 # that the above copyright notice appear in all copies
 # and that both that copyright notice and this permission
@@ -15,176 +14,190 @@
 # SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, INDIRECT
 # OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER
 # RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER
 # IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS
 # ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE
 # OR PERFORMANCE OF THIS SOFTWARE!
 
-import numpy
+"""
+This module contains the BNDiscretizer class which is used to discretize the values of a database in order to learn
+a (discrete) Bayesian Network classifier
+"""
 import math
+
+import numpy
 import pandas
 import scipy
 import sklearn
 import sklearn.preprocessing as skp
 
 import pyAgrum as gum
 
-from ._utils import checkInt,checkFloat
+from ._utils import checkInt, checkFloat
+
 
 class BNDiscretizer():
   """
   Represents a tool to discretize some variables in a database in order to obtain  a way to learn a pyAgrum's (discrete)
   Bayesian networks.
 
-  parameters:
+  Parameters
+  ----------
       defaultDiscretizationMethod: str
             sets the default method of discretization for this discretizer. Possible values are: 'quantile', 'uniform',
             'kmeans', 'NML', 'CAIM' and 'MDLP'. This method will be used if the user has not specified another method
             for that specific variable using the setDiscretizationParameters method.
       defaultNumberOfBins: str or int
           sets the number of bins if the method used is quantile, kmeans, uniform. In this case this parameter can also
           be set to the string 'elbowMethod' so that the best number of bins is found automatically.
           If the method used is NML, this parameter sets the the maximum number of bins up to which the NML algorithm
           searches for the optimal number of bins. In this case this parameter must be an int
-          If any other discetization method is used, this parameter is ignored.
+          If any other discretization method is used, this parameter is ignored.
       discretizationThreshold: int or float
-          When using default parameters a variable will be treated as continous only if it has more unique values than
+          When using default parameters a variable will be treated as continuous only if it has more unique values than
           this number (if the number is an int greater than 1).
           If the number is a float between 0 and 1, we will test if the proportion of unique values is bigger than this
-          number. For example if you have entered 0.95, the variable will be treated as continous only if more than 95%
+          number. For example if you have entered 0.95, the variable will be treated as continuous only if more than 95%
           of its values are unique.
 
   """
 
   def __init__(self, defaultDiscretizationMethod='quantile', defaultNumberOfBins=10, discretizationThreshold=25):
     """
-    parameters:
+    Creates the BNDiscretizer
+
+    Parameters
+    ----------
         defaultDiscretizationMethod: str
             sets the default method of discretization for this discretizer. Possible values are: 'quantile', 'uniform',
             'kmeans', 'NML', 'CAIM' and 'MDLP'. This method will be used if the user has not specified another method
             for that specific variable using the setDiscretizationParameters method.
         defaultNumberOfBins: str or int
             sets the number of bins if the method used is quantile, kmeans, uniform. In this case this parameter can
             also be set to the string 'elbowMethod' so that the best number of bins is found automatically.
             If the method used is NML, this parameter sets the the maximum number of bins up to which the NML algorithm
             searches for the optimal number of bins. In this case this parameter must be an int
-            If any other discetization method is used, this parameter is ignored.
+            If any other discretization method is used, this parameter is ignored.
         discretizationThreshold: int or float
-            When using default parameters a variable will be treated as continous only if it has more unique values than
+            When using default parameters a variable will be treated as continuous only if it has more unique values than
             this number (if the number is an int greater than 1).
             If the number is a float between 0 and 1, we will test if the proportion of unique values is bigger than
-            this number. For example if you have entered 0.95, the variable will be treated as continous only if more
+            this number. For example if you have entered 0.95, the variable will be treated as continuous only if more
             than 95% of its values are unique.
-
-    Creates the BNDiscretizer
     """
-    self.discretizationParametersDictionary = dict()
-    self.numberOfContinous = 0
+    self.discretizationParametersDictionary = {}
+    self.numberOfContinuous = 0
     self.totalNumberOfBins = 0
     self.defaultMethod = None
-    self.defaultNbBins = None
+    self.defaultParamDiscretizationMethod = None
     self.setDiscretizationParameters(None, defaultDiscretizationMethod, defaultNumberOfBins)
 
     self.discretizationThreshold = discretizationThreshold
 
   def clear(self, clearDiscretizationParameters=False):
     """
-    parameters:
-        clearDiscretizationParamaters: bool
-            if True, this method also clears the parameters the user has set for each variable and resets them to the default.
-
-    returns:
-        void
-
-    Sets the number of continous variables and the total number of bins created by this discretizer to 0. If
+    Sets the number of continuous variables and the total number of bins created by this discretizer to 0. If
     clearDiscretizationParameters is True, also clears the the parameters for discretization the user has set for each
     variable.
-    """
-    self.numberOfContinous = 0
+    
+    Parameters
+    ----------
+      clearDiscretizationParameters: bool
+        if True, this method also clears the parameters the user has set for each variable and resets them to the default.
+   """
+    self.numberOfContinuous = 0
     self.totalNumberOfBins = 0
     if clearDiscretizationParameters:
-      self.discretizationParametersDictionary = dict()
+      self.discretizationParametersDictionary = {}
 
-  def setDiscretizationParameters(self, variableName=None, method=None, numberOfBins=None):
+  @gum.deprecated_arg(newA="paramDiscretizationMethod",oldA="numberOfBins",version="1.9.0")
+  def setDiscretizationParameters(self, variableName=None, method=None, paramDiscretizationMethod=None):
     """
-    parameters:
-        variableName: str
-            the name of the variable you want to set the discretization paramaters of. Set to None to set the new
-            default for this BNClassifier.
-        method: str
-            The method of discretization used for this variable. Type "NoDiscretization" if you do not want to discretize this
-            variable. Possible values are: 'NoDiscretization', 'quantile', 'uniform', 'kmeans', 'NML', 'CAIM' and 'MDLP'
-        numberOfBins:
-            sets the number of bins if the method used is quantile, kmeans, uniform. In this case this parameter can also
-            be set to the string 'elbowMethod' so that the best number of bins is found automatically.
-            if the method used is NML, this parameter sets the the maximum number of bins up to which the NML algorithm
-            searches for the optimal number of bins. In this case this parameter must be an int
-            If any other discetization method is used, this parameter is ignored.
+    Sets the discretization parameters for a variable. If variableName is None, sets the default parameters for this
 
-    returns:
-        void
+    parameters:
+      variableName: str
+          the name of the variable you want to set the discretization parameters of. Set to None to set the new
+          default for this BNClassifier.
+      method: str
+          The method of discretization used for this variable. Type "NoDiscretization" if you do not want to discretize this
+          variable. Possible values are: 'NoDiscretization', 'quantile', 'uniform', 'kmeans', 'NML', 'CAIM' and 'MDLP'
+      paramDiscretizationMethod:
+          sets the number of bins if the method used is quantile, kmeans, uniform. In this case this parameter can also
+          be set to the string 'elbowMethod' so that the best number of bins is found automatically.
+          if the method used is NML, this parameter sets the the maximum number of bins up to which the NML algorithm
+          searches for the optimal number of bins. In this case this parameter must be an int
+          If the method is NoDiscretization, this parameter can 
+          If any other discretization method is used, this parameter is ignored.
+  
     """
-    if variableName in self.discretizationParametersDictionary.keys():
-      oldNbBins = self.discretizationParametersDictionary[variableName]['k']
+    if variableName in self.discretizationParametersDictionary:
+      oldParamDiscretizationMethod = self.discretizationParametersDictionary[variableName]['param']
       oldMethod = self.discretizationParametersDictionary[variableName]['method']
     else:
-      oldNbBins = self.defaultNbBins
+      oldParamDiscretizationMethod = self.defaultParamDiscretizationMethod
       oldMethod = self.defaultMethod
 
     if method is None:
       method = oldMethod
 
-    if numberOfBins is None:
-      if method != 'NoDiscretization':
-        numberOfBins = oldNbBins
+    if paramDiscretizationMethod is None and method != 'NoDiscretization':
+      paramDiscretizationMethod = oldParamDiscretizationMethod
 
     if method not in {'kmeans', 'uniform', 'quantile', 'NML', 'MDLP', 'CAIM', 'NoDiscretization'}:
       raise ValueError(
-        "This discretization method is not recognized! Possible values are keans, uniform, quantile, NML, "
+        "This discretization method is not recognized! Possible values are kmeans, uniform, quantile, NML, "
         "CAIM and MDLP. You have entered " + str(
           method))
 
-    if numberOfBins == 'elbowMethod':
+    if paramDiscretizationMethod == 'elbowMethod':
       if method == "NML":
         raise ValueError(
-          "The elbow Method cannot be used as the number of bins for the algorithm NML. Please an integer value")
+          "The elbow Method cannot be used as the number of bins for the algorithm NML. Please select an integer value")
     elif method != 'NoDiscretization':
       try:
-        numberOfBins = int(numberOfBins)
+        paramDiscretizationMethod = int(paramDiscretizationMethod)
       except:
         raise ValueError(
-          "The possible values for numberOfBins are any integer or the string 'elbowMethod'. You have entered: " + str(
-            numberOfBins))
+          "The possible values for paramDiscretizationMethod are any integer or the string 'elbowMethod'. You have entered: " + str(
+            paramDiscretizationMethod))
+    else:
+      if paramDiscretizationMethod is not None and not isinstance(paramDiscretizationMethod,list):
+        raise ValueError(
+          "For a NotDiscretized variable, the parameter has to be None or a list of values (labels) but not '" + str(paramDiscretizationMethod))+"'."
+
     if variableName is None:
       self.defaultMethod = method
-      self.defaultNbBins = numberOfBins
+      self.defaultParamDiscretizationMethod = paramDiscretizationMethod
     else:
-      self.discretizationParametersDictionary[variableName] = dict()
-      self.discretizationParametersDictionary[variableName]['k'] = numberOfBins
+      self.discretizationParametersDictionary[variableName] = {}
       self.discretizationParametersDictionary[variableName]['method'] = method
+      self.discretizationParametersDictionary[variableName]['param'] = paramDiscretizationMethod
 
   def audit(self, X, y=None):
     """
-    parameters:
-        X: {array-like, sparse matrix} of shape (n_samples, n_features)
-            training data
-        y: array-like of shape (n_samples,)
-            Target values
-    returns:
-        auditDict: dict()
-
     Audits the passed values of X and y. Tells us which columns in X we think are already discrete and which need to
     be discretized, as well as the discretization algorithm that will be used to discretize them The parameters which
     are suggested will be used when creating the variables. To change this the user can manually set discretization
     parameters for each variable using the setDiscretizationParameters function.
-    """
+   
+    parameters:
+      X: {array-like, sparse matrix} of shape (n_samples, n_features)
+          training data
+      y: array-like of shape (n_samples,)
+          Target values
+    Returns
+    -------
+      Dict
+        for each variable, the proposition of audit
+     """
 
-    auditDict = dict()
+    auditDict = {}
 
-    if isinstance(X,pandas.DataFrame): #type(X) == pandas.DataFrame:
+    if isinstance(X, pandas.DataFrame):
       variableNames = X.columns.tolist()
     elif type(X) == pandas.core.series.Series:
       variableNames = [X.name]
     else:
       variableNames = None
 
     if y is not None:
@@ -193,86 +206,90 @@
       X = sklearn.utils.check_array(X, dtype=None)
 
     d = X.shape[1]
 
     if variableNames is None:
       variableNames = ["x" + str(i) for i in range(d)]
 
-    possibleValues = dict()  # pour la ligne on compte les valeurs possibles
+    possibleValues = {}  # counting the possible values gor this line
 
     for i in range(d):
       possibleValues[i] = numpy.unique(X[:, i])
     possibleValues[d] = numpy.unique(y)
 
     if len(possibleValues[d]) > 2:
       raise ValueError(
         "BNClassifier is a binary classifier! There are more than 2 possible values for y in the data provided")
     for i in range(d):
       variable = variableNames[i]
-      auditDict[variable] = dict()
+      auditDict[variable] = {}
       try:
         sklearn.utils.check_array(X[:, i], dtype='float', ensure_2d=False)
         isNumeric = True
       except ValueError:
         isNumeric = False
       if variable in self.discretizationParametersDictionary.keys():
         auditDict[variable] = self.discretizationParametersDictionary[variable]
         if self.discretizationParametersDictionary[variable]['method'] != "NoDiscretization" and not isNumeric:
           raise ValueError("The variable " + variable + " is not numeric and cannot be discretized!")
 
       else:
         if len(possibleValues[i]) > self.discretizationThreshold and isNumeric:
-          auditDict[variable]['k'] = self.defaultNbBins
           auditDict[variable]['method'] = self.defaultMethod
+          auditDict[variable]['nbBins'] = self.defaultParamDiscretizationMethod
         else:
           auditDict[variable]['method'] = 'NoDiscretization'
-          auditDict[variable]['k'] = len(possibleValues[i])
+          auditDict[variable]['values'] = possibleValues[i]
       if auditDict[variable]['method'] == "NoDiscretization":
         auditDict[variable]['type'] = 'Discrete'
       else:
         auditDict[variable]['type'] = 'Continuous'
 
     return auditDict
 
-  def discretizationElbowMethodRotation(self, discretizationStrategy, X):
+  @staticmethod
+  def discretizationElbowMethodRotation(discretizationStrategy, X):
     """
-    parameters:
-        discretizationStrategy: str
-            The method of discretization that will be used. Possible values are: 'quantile' , 'kmeans' and 'uniform'
-        X: one dimensional ndarray
-            Contains the data that should be discretized
-    returns:
-        binEdges: the edges of the bins the algorithm has chosen.
-
     Calculates the sum of squared errors as a function of the number of clusters using the discretization strategy
     that is passed as a parameter. Returns the bins that are optimal for minimizing the variation and the number of
     bins at the same time. Uses the elbow method to find this optimal point. To find the "elbow" we rotate the curve
     and look for its minimum.
+
+    Parameters
+    ----------
+        discretizationStrategy: str
+            The method of discretization that will be used. Possible values are: 'quantile' , 'kmeans' and 'uniform'
+        X: one dimensional ndarray
+            Contains the data that should be discretized
+    Returns
+    -------
+        List[float]
+          the edges of the bins the algorithm has chosen.
     """
 
     if discretizationStrategy not in {'kmeans', 'quantile', 'uniform'}:
       raise ValueError("cannot use elbow method with this type of discretization")
     variationArray = numpy.zeros(14)
     Xsorted = X[X.argsort(axis=None)]
     binEdgeMatrix = [[]] * 14
     for k in range(2, 16):
-      discretizer = skp.KBinsDiscretizer(k, strategy=discretizationStrategy)
+      discretizer = skp.KBinsDiscretizer(k, strategy=discretizationStrategy, subsample=None)
       discretizer.fit(Xsorted)
       binEdges = discretizer.bin_edges_[0]
       centresArray = (binEdges[1:] + binEdges[:-1]) / 2
       i = 0
       sumOfSquaredErrors = 0
       for x in Xsorted:
         if x > binEdges[i + 1]:
           i = i + 1
         sumOfSquaredErrors += (x - centresArray[i]) ** 2
       variationArray[k - 2] = sumOfSquaredErrors
       binEdgeMatrix[k - 2] = binEdges.to_list()
 
-    # we caclulate the slope of the line that connects the first and last point on our graph
+    # we calculate the slope of the line that connects the first and last point on our graph
     slope = (variationArray[13] - variationArray[0]) / 13
 
     # we calculate the slope of the line perpendicular to it
     otherSlope = -1 / slope
 
     # we change the coordinate system to the one with the two lines previously mentioned as its axes
     coordinateChangeMatrix = numpy.array([[1, slope], [1 / otherSlope, 1]])
@@ -292,43 +309,46 @@
     # when we have found the minimum, we apply the inverse linear transformation to recover the optimal value of k
     minimumVector = numpy.matmul(numpy.linalg.inv(coordinateChangeMatrix),
                                  transformedCoordinates[:, minkIndex].reshape(2, 1))
 
     # we return the list of bin edges found using said optimal number of k
     return binEdgeMatrix[int(round(minimumVector[0]))]
 
-  def discretizationMDLP(self, x, y, possibleValuesX, possibleValuesY):
+  def discretizationMDLP(self, X, y, possibleValuesX, possibleValuesY):
     """
-    parametres:
-        x: ndarray with shape (n,1) where n is the number of samples
+    Uses the MDLP algorithm described in Fayyad, 1995 to discretize the values of x.
+
+    Parameters
+    ----------
+        X: ndarray with shape (n,1) where n is the number of samples
             Column-vector that contains all the data that needs to be discretized
         y: ndarray with shape (n,1) where n is the number of samples
             Column-vector that contains the class for each sample. This vector will not be discretized, but the class-value of each sample is needed to properly apply the algorithm
         possibleValuesX: one dimensional ndarray
             Contains all the possible values that x can take sorted in increasing order. There shouldn't be any doubles inside
         possibleValuesY: one dimensional ndarray
             Contains the possible values of y. There should be two possible values since this is a binary classifier
-    returns:
-        binEdges: a list of the edges of the bins that are chosen by this algorithm
-
-    Uses the MDLP algorithm described in Fayyad, 1995 to discretize the values of x.
+    Returns
+    -------
+        List[float]
+         a list of the edges of the bins that are chosen by this algorithm
     """
-    xAndY = numpy.concatenate((x, y), axis=1)
+    xAndY = numpy.concatenate((X, y), axis=1)
     xAndY = xAndY[xAndY[:, 0].argsort()]
     B = (possibleValuesX[1:] + possibleValuesX[:-1]) / 2
-    [class0, class1] = possibleValuesY
+    [class0, _] = possibleValuesY
 
     binEdgesIndex = []
     nbElementsByIntervalClass0 = numpy.zeros(len(B) + 1)
     nbElementsByIntervalClass1 = numpy.zeros(len(B) + 1)
     currentIntervalIndex = 0
-    for x in xAndY:
-      if currentIntervalIndex < len(B) and x[0] > B[currentIntervalIndex]:
+    for X in xAndY:
+      if currentIntervalIndex < len(B) and X[0] > B[currentIntervalIndex]:
         currentIntervalIndex += 1
-      if x[1] == class0:
+      if X[1] == class0:
         nbElementsByIntervalClass0[currentIntervalIndex] += 1
       else:
         nbElementsByIntervalClass1[currentIntervalIndex] += 1
 
     Class0ByLargeInterval = [nbElementsByIntervalClass0.sum()]
     Class1ByLargeInterval = [nbElementsByIntervalClass1.sum()]
 
@@ -337,16 +357,16 @@
     probabilityClass0 = Class0ByLargeInterval[0] / totalCountByLargeInterval[0]
     probabilityClass1 = Class1ByLargeInterval[0] / totalCountByLargeInterval[0]
     shannonEntropyByLargeInterval = [
       (-1) * (probabilityClass0 * math.log2(probabilityClass0) + probabilityClass1 * math.log2(probabilityClass1))]
 
     continueDividingInterval = [True]
 
-    currentValues = dict()
-    minimalValues = dict()
+    currentValues = {}
+    minimalValues = {}
 
     while any(continueDividingInterval):
       minimalValues['classInformationEntropy'] = math.inf
       for param in {'boundaryIndex', 'leftSubintervalClass0', 'leftSubintervalClass1', 'leftSubintervalShannonEntropy',
                     'rightSubintervalClass0', 'rightSubintervalClass1', 'rightSubintervalShannonEntropy'}:
         (currentValues[param], minimalValues[param]) = (0, 0)
 
@@ -354,15 +374,15 @@
       while currentValues['boundaryIndex'] < len(B):
 
         while not continueDividingInterval[position]:
           position = position + 1
           currentValues['boundaryIndex'] = binEdgesIndex[position - 1] + 1
 
         if position < len(binEdgesIndex) and currentValues['boundaryIndex'] == binEdgesIndex[position]:
-          # this function decides whether to accept the cutpoint in this interval and updates the relevant lists if
+          # this function decides whether to accept the cut point in this interval and updates the relevant lists if
           # the value is accepted.
           self._divideIntervalMDLP(minimalValues, shannonEntropyByLargeInterval, Class0ByLargeInterval,
                                    Class1ByLargeInterval, continueDividingInterval, totalCountByLargeInterval, position,
                                    binEdgesIndex)
           position += 1
           while position < len(continueDividingInterval) and not continueDividingInterval[position]:
             position += 1
@@ -420,43 +440,47 @@
     binEdges = [xAndY[0][0]]
     for index in binEdgesIndex:
       binEdges.append(B[index])
     binEdges.append(xAndY[-1][0])
 
     return binEdges
 
-  def discretizationCAIM(self, x, y, possibleValuesX, possibleValuesY):
+  @staticmethod
+  def discretizationCAIM(X, y, possibleValuesX, possibleValuesY):
     """
-    parametres:
-        x: ndarray with shape (n,1) where n is the number of samples
+    Applies the CAIM algorithm to discretize the values of x
+
+    Parameters
+    ----------
+        X: ndarray with shape (n,1) where n is the number of samples
             Column-vector that contains all the data that needs to be discretized
         y: ndarray with shape (n,1) where n is the number of samples
             Column-vector that contains the class for each sample. This vector will not be discretized, but the class-value of each sample is needed to properly apply the algorithm
         possibleValuesX: one dimensional ndarray
             Contains all the possible values that x can take sorted in increasing order. There shouldn't be any doubles inside
         possibleValuesY: one dimensional ndarray
             Contains the possible values of y. There should be two possible values since this is a binary classifier
-    returns:
-        binEdges: a list of the edges of the bins that are chosen by this algorithm
-
-    Applies the CAIM algorithm to discretize the values of x
+    Returns
+    -------
+        list[float]
+          a list of the edges of the bins that are chosen by this algorithm
     """
-    xAndY = numpy.concatenate((x, y), axis=1)
+    xAndY = numpy.concatenate((X, y), axis=1)
     xAndY = xAndY[xAndY[:, 0].argsort()]
     B = (possibleValuesX[1:] + possibleValuesX[:-1]) / 2
     [class0, class1] = possibleValuesY
 
     binEdgesIndex = []
     nbElementsByIntervalClass0 = numpy.zeros(len(B) + 1)
     nbElementsByIntervalClass1 = numpy.zeros(len(B) + 1)
     currentIntervalIndex = 0
-    for x in xAndY:
-      if currentIntervalIndex < len(B) and x[0] > B[currentIntervalIndex]:
+    for X in xAndY:
+      if currentIntervalIndex < len(B) and X[0] > B[currentIntervalIndex]:
         currentIntervalIndex += 1
-      if x[1] == class0:
+      if X[1] == class0:
         nbElementsByIntervalClass0[currentIntervalIndex] += 1
       else:
         nbElementsByIntervalClass1[currentIntervalIndex] += 1
 
     Class0ByLargeInterval = [nbElementsByIntervalClass0.sum()]
     Class1ByLargeInterval = [nbElementsByIntervalClass1.sum()]
 
@@ -464,102 +488,110 @@
     globalCAIM = 0.0
     oldCaim = 0.0
     while True:
       caimMax = 0
       maxPosition = 0
       maxBoundaryIndex = 0
       position = 0
-      currentsumClass0 = 0
-      currentsumClass1 = 0
+      currentSumClass0 = 0
+      currentSumClass1 = 0
+      maxLeftIntervalClass0 = currentSumClass0
+      maxLeftIntervalClass1 = currentSumClass1
+      maxRightIntervalClass0 = maxLeftIntervalClass0
+      maxRightIntervalClass1 = maxLeftIntervalClass1
 
       for boundaryIndex in range(len(B)):
         if position < len(binEdgesIndex) and boundaryIndex == binEdgesIndex[position]:
 
           position += 1
           if Class0ByLargeInterval[position] > Class1ByLargeInterval[position]:
             oldCaim = globalCAIM * len(Class0ByLargeInterval) - math.pow(Class0ByLargeInterval[position], 2) / (
-                Class0ByLargeInterval[position] + Class1ByLargeInterval[position])
+               Class0ByLargeInterval[position] + Class1ByLargeInterval[position])
           else:
             oldCaim = globalCAIM * len(Class0ByLargeInterval) - math.pow(Class1ByLargeInterval[position], 2) / (
-                Class0ByLargeInterval[position] + Class1ByLargeInterval[position])
-          currentsumClass0 = 0
-          currentsumClass1 = 0
+               Class0ByLargeInterval[position] + Class1ByLargeInterval[position])
+          currentSumClass0 = 0
+          currentSumClass1 = 0
           continue
 
-        currentsumClass0 += nbElementsByIntervalClass0[boundaryIndex]
-        currentsumClass1 += nbElementsByIntervalClass1[boundaryIndex]
+        currentSumClass0 += nbElementsByIntervalClass0[boundaryIndex]
+        currentSumClass1 += nbElementsByIntervalClass1[boundaryIndex]
         caim = oldCaim
 
-        if currentsumClass0 > currentsumClass1:
-          caim = caim + math.pow(currentsumClass0, 2) / (currentsumClass0 + currentsumClass1)
+        if currentSumClass0 > currentSumClass1:
+          caim = caim + math.pow(currentSumClass0, 2) / (currentSumClass0 + currentSumClass1)
         else:
-          caim = caim + math.pow(currentsumClass1, 2) / (currentsumClass0 + currentsumClass1)
+          caim = caim + math.pow(currentSumClass1, 2) / (currentSumClass0 + currentSumClass1)
 
-        intervalclass0 = Class0ByLargeInterval[position] - currentsumClass0
-        intervalclass1 = Class1ByLargeInterval[position] - currentsumClass1
+        intervalClass0 = Class0ByLargeInterval[position] - currentSumClass0
+        intervalClass1 = Class1ByLargeInterval[position] - currentSumClass1
 
-        if intervalclass0 > intervalclass1:
-          caim = caim + math.pow(intervalclass0, 2) / (intervalclass0 + intervalclass1)
+        if intervalClass0 > intervalClass1:
+          caim = caim + math.pow(intervalClass0, 2) / (intervalClass0 + intervalClass1)
         else:
-          caim = caim + math.pow(intervalclass1, 2) / (intervalclass0 + intervalclass1)
+          caim = caim + math.pow(intervalClass1, 2) / (intervalClass0 + intervalClass1)
 
         caim = caim / (len(Class0ByLargeInterval) + 1)
 
         if caim > caimMax:
-          maxLeftIntervalClass0 = currentsumClass0
-          maxLeftIntervalClass1 = currentsumClass1
-          maxRightIntervalClass0 = intervalclass0
-          maxRightIntervalClass1 = intervalclass1
+          maxLeftIntervalClass0 = currentSumClass0
+          maxLeftIntervalClass1 = currentSumClass1
+          maxRightIntervalClass0 = intervalClass0
+          maxRightIntervalClass1 = intervalClass1
           caimMax = caim
           maxBoundaryIndex = boundaryIndex
           maxPosition = position
 
       if caimMax > globalCAIM:
         globalCAIM = caimMax
         binEdgesIndex.insert(maxPosition, maxBoundaryIndex)
         Class0ByLargeInterval.insert(maxPosition + 1, maxRightIntervalClass0)
         Class1ByLargeInterval.insert(maxPosition + 1, maxRightIntervalClass1)
         Class0ByLargeInterval[maxPosition] = maxLeftIntervalClass0
         Class1ByLargeInterval[maxPosition] = maxLeftIntervalClass1
         k = k + 1
         if Class0ByLargeInterval[0] > Class1ByLargeInterval[0]:
           oldCaim = globalCAIM * len(Class0ByLargeInterval) - math.pow(Class0ByLargeInterval[0], 2) / (
-              Class0ByLargeInterval[0] + Class1ByLargeInterval[0])
+             Class0ByLargeInterval[0] + Class1ByLargeInterval[0])
         else:
           oldCaim = globalCAIM * len(Class0ByLargeInterval) - math.pow(Class1ByLargeInterval[0], 2) / (
-              Class0ByLargeInterval[0] + Class1ByLargeInterval[0])
+             Class0ByLargeInterval[0] + Class1ByLargeInterval[0])
 
       else:
         break
 
     binEdges = [xAndY[0][0]]
     for index in binEdgesIndex:
       binEdges.append(B[index])
     binEdges.append(xAndY[-1][0])
 
     return binEdges
 
-  def discretizationNML(self, X, possibleValuesX, kMax=10, epsilon=None):
+  @staticmethod
+  def discretizationNML(X, possibleValuesX, kMax=10, epsilon=None):
     """
-    parameters:
+    Uses the discretization algorithm described in "MDL Histogram Density Estimator", Kontkaken and Myllymaki, 2007 to
+    discretize.
+
+    Parameters
+    ----------
         X: one dimensional ndarray
             array that that contains all the data that needs to be discretized
         possibleValuesX: one dimensional ndarray
             Contains all the possible values that x can take sorted in increasing order. There shouldn't be any doubles
             inside.
         kMax: int
             the maximum number of bins before the algorithm stops itself.
         epsilon: float or None
             the value of epsilon used in the algorithm. Should be as small as possible. If None is passed the value is
             automatically calculated.
-    returns:
-        binEdges: a list of the edges of the bins that are chosen by this algorithm
-
-    Uses the disceretization algorithm described in "MDL Histogram Density Estimator", Kontkaken and Myllymaki, 2007 to
-    discretize.
+    Returns
+    -------
+        List[float]
+          a list of the edges of the bins that are chosen by this algorithm
     """
     Xsorted = X[X.argsort(axis=None)]
     if epsilon is None:
       epsilon = numpy.amin(possibleValuesX[1:] - possibleValuesX[:-1]) / 2
     epsilon = epsilon / 2
     candidateCutPoints = numpy.zeros(2 * len(possibleValuesX))
     for i in range(len(possibleValuesX)):
@@ -578,40 +610,41 @@
       counter = counter + 1
     binCount[j] = counter
     n = binCount[-1]
     Rkminus1 = numpy.ones(E)
     Rk = numpy.zeros(E)
     for i in range(1, E):
       ne = int(binCount[i])
-      sum = 0
+      total_amount = 0
       for h1 in range(ne + 1):
         h2 = ne - h1
-        sum += math.pow(h1 / ne, h1) * math.pow(h2 / ne, h2) * scipy.special.comb(ne, h1)
-      Rk[i] = sum
+        total_amount += math.pow(h1 / ne, h1) * math.pow(h2 / ne, h2) * scipy.special.comb(ne, h1)
+      Rk[i] = total_amount
 
     k = 2
 
     Bkminus1 = numpy.zeros(E)
     for e in range(1, E):
       ne = binCount[e]
-      Bkminus1[e] = -ne * (math.log(2 * epsilon * ne) - math.log(n * ((candidateCutPoints[e] - candidateCutPoints[0]))))
+      Bkminus1[e] = -ne * (math.log(2 * epsilon * ne) - math.log(n * (candidateCutPoints[e] - candidateCutPoints[0])))
 
     Bk = numpy.zeros(E)
     cutpoints = [candidateCutPoints[0]]
     Bvalues = [Bkminus1[-1]]
+    minimumeprime = 0
     while k <= kMax:
 
       for e in range(k, E):
 
         minimum = math.inf
         minimumeprime = 0
         for eprime in range(k - 1, e):
           if binCount[e] > binCount[eprime]:
             temp = Bkminus1[eprime] - (binCount[e] - binCount[eprime]) * (
-                math.log(2 * epsilon * (binCount[e] - binCount[eprime])) - math.log(
+               math.log(2 * epsilon * (binCount[e] - binCount[eprime])) - math.log(
               n * (candidateCutPoints[e] - candidateCutPoints[eprime])))
           else:
             temp = Bkminus1[eprime]
           temp = temp + math.log(Rk[e] / Rkminus1[eprime])
           if minimum > temp:
             minimum = temp
             minimumeprime = eprime
@@ -634,94 +667,104 @@
     cutpoints = sorted(set(cutpoints[:minimumIndex + 1]))
     cutpoints.append(candidateCutPoints[-1])
 
     return cutpoints
 
   def createVariable(self, variableName, X, y=None, possibleValuesY=None):
     """
-    parameters:
+    Creates a variable for the column passed in as a parameter
+    
+    Parameters
+    ----------
         variableName:
             the name of the created variable
         X: ndarray shape(n,1)
             A column vector containing n samples of a feature. The column for which the variable will be created
         y: ndarray shape(n,1)
             A column vector containing the corresponding for each element in X.
-        possibleValuesX: onedimensional ndarray
-            An ndarray containing all the unique values of X
-        possibleValuesY: onedimensional ndarray
+        possibleValuesY: ndarray
             An ndarray containing all the unique values of y
-        returnModifiedX: bool
-            X could be modified by this function during
-    returns:
-        var: pyagrum.DiscreteVariable
+    Returns
+    -------
+        pyagrum.DiscreteVariable
             the created variable
-
-    Creates a variable for the column passed in as a parameter and places it in the Bayesian network
     """
 
     if y is not None:
       X, y = sklearn.utils.check_X_y(X, y, dtype=None, accept_sparse=True, ensure_2d=False)
     X = sklearn.utils.check_array(X, dtype=None, ensure_2d=False)
     try:
       Xtransformed = sklearn.utils.check_array(X, dtype='float', ensure_2d=False)
       isNumeric = True
     except ValueError:
       Xtransformed = X
       isNumeric = False
-    possibleValuesX = numpy.unique(X)
+
+    foundValuesX= set(numpy.unique(X))
     n = len(X)
 
-    if variableName not in self.discretizationParametersDictionary.keys():  # The user has not manually set the discretization parameters for this variable
-      if isNumeric and \
-          ((self.discretizationThreshold >= 1 and len(possibleValuesX) > self.discretizationThreshold)
-           or (self.discretizationThreshold < 1 and len(possibleValuesX) / len(X) > self.discretizationThreshold)):
-        self.discretizationParametersDictionary[variableName] = dict()
+    if variableName not in self.discretizationParametersDictionary:  # The user has not manually set the discretization parameters for this variable
+      if isNumeric and 1 <= self.discretizationThreshold < len(foundValuesX) or (
+         self.discretizationThreshold < 1 and len(foundValuesX) / len(X) > self.discretizationThreshold):
+        self.discretizationParametersDictionary[variableName] = {}
         self.discretizationParametersDictionary[variableName]['method'] = self.defaultMethod
-        self.discretizationParametersDictionary[variableName]['k'] = self.defaultNbBins
+        self.discretizationParametersDictionary[variableName]['param'] = self.defaultParamDiscretizationMethod
       else:
-        self.discretizationParametersDictionary[variableName] = dict()
+        self.discretizationParametersDictionary[variableName] = {}
         self.discretizationParametersDictionary[variableName]['method'] = "NoDiscretization"
       usingDefaultParameters = True
     else:
       usingDefaultParameters = False
       if self.discretizationParametersDictionary[variableName]['method'] != "NoDiscretization" and not isNumeric:
         raise ValueError("The variable " + variableName + " is not numeric and cannot be discretized!")
 
     if self.discretizationParametersDictionary[variableName]["method"] == "NoDiscretization":
-      is_int_var=True
-      min_v=max_v=None
+      is_int_var = True
+      min_v = max_v = None
+
+      possibleValuesX=None
+      if "param" in self.discretizationParametersDictionary[variableName]:
+        possibleValuesX= self.discretizationParametersDictionary[variableName]["param"]
+
+      if possibleValuesX is None:
+        possibleValuesX = foundValuesX
+      else:
+        # foundValuesX must be in possibleValuesX
+        if not foundValuesX.issubset(possibleValuesX):
+          raise ValueError(f"The values passed in possibleValues ({possibleValuesX}) do not match database values ({foundValuesX})")
+
       for value in possibleValuesX:
-        if not checkInt(value):
-          is_int_var=False
-          break
+        if checkInt(value):
+          v = int(value)
+          if min_v is None or min_v > v:
+            min_v = v
+          if max_v is None or max_v < v:
+            max_v = v
         else:
-          v=int(value)
-          if min_v is None or min_v>v:
-            min_v=v
-          if max_v is None or max_v<v:
-            max_v=v
+          is_int_var = False
+          break
 
       if is_int_var:
-        if len(possibleValuesX)==max_v-min_v+1: # no hole in the list of int
-          var =gum.RangeVariable(variableName, variableName, min_v,max_v)
+        if len(possibleValuesX) == max_v - min_v + 1:  # no hole in the list of int
+          var = gum.RangeVariable(variableName, variableName, min_v, max_v)
         else:
-          var=gum.IntegerVariable(variableName, variableName,[int(v) for v in possibleValuesX])
+          var = gum.IntegerVariable(variableName, variableName, [int(v) for v in possibleValuesX])
       else:
-        is_float_var=True
+        is_float_var = True
         for value in possibleValuesX:
           if not checkFloat(value):
-            is_float_var=False
+            is_float_var = False
             break
 
         if is_float_var:
           var = gum.NumericalDiscreteVariable(variableName, variableName, [float(v) for v in possibleValuesX])
         else:
-          var = gum.LabelizedVariable(variableName, variableName, [str(v) for v in possibleValuesX])
+          var = gum.LabelizedVariable(variableName, variableName, sorted([str(v) for v in possibleValuesX]))
     else:
-      self.numberOfContinous += 1
+      self.numberOfContinuous += 1
       if self.discretizationParametersDictionary[variableName]['method'] == "CAIM":
         if y is None:
           raise ValueError(
             "The CAIM discretization method requires a list of the associated classes for each data vector since it "
             "is a supervised discretization method. You should pass it as y.")
         if possibleValuesY is None:
           possibleValuesY = numpy.unique(y)
@@ -734,57 +777,59 @@
             "is a supervised discretization method. You should pass it as y.")
         if possibleValuesY is None:
           possibleValuesY = numpy.unique(y)
         binEdges = self.discretizationMDLP(Xtransformed.reshape(n, 1), y.reshape(n, 1), numpy.unique(Xtransformed),
                                            possibleValuesY)
       elif self.discretizationParametersDictionary[variableName]['method'] == "NML":
         binEdges = self.discretizationNML(Xtransformed.flatten(), numpy.unique(Xtransformed),
-                                          kMax=self.discretizationParametersDictionary[variableName]["k"])
+                                          kMax=self.discretizationParametersDictionary[variableName]["param"])
       else:
-        if self.discretizationParametersDictionary[variableName]['k'] == 'elbowMethod':
+        if self.discretizationParametersDictionary[variableName]['param'] == 'elbowMethod':
           binEdges = self.discretizationElbowMethodRotation(
             self.discretizationParametersDictionary[variableName]['method'], Xtransformed.flatten())
         else:
-          discre = skp.KBinsDiscretizer(self.discretizationParametersDictionary[variableName]['k'],
-                                        strategy=self.discretizationParametersDictionary[variableName]['method'])
+          discre = skp.KBinsDiscretizer(self.discretizationParametersDictionary[variableName]['param'],
+                                        strategy=self.discretizationParametersDictionary[variableName]['method'],
+                                        subsample=None)
           discre.fit(X.reshape(-1, 1))
           binEdges = discre.bin_edges_[0].tolist()
 
       if len(binEdges) == 2:
         raise ValueError("Due to an error the discretization method " + str(
           self.discretizationParametersDictionary[variableName]['method']) + " using " + str(
-          self.discretizationParametersDictionary[variableName]['k']) + " bins for the variable " + str(
+          self.discretizationParametersDictionary[variableName]['param']) + " bins for the variable " + str(
           variableName) + "gave only 1 bin. Try increasing the number of bins used by this variable using "
-                          "setDiscetizationParameters to avoid this error")
+                          "setDiscretizationParameters to avoid this error")
 
-      #we replace infinity as min and max by the new empirical flag.
-      #binEdges[0] = -math.inf
-      #binEdges[-1] = math.inf
+      # we replace infinity as min and max by the new empirical flag.
+      # binEdges[0] = -math.inf
+      # binEdges[-1] = math.inf
       self.totalNumberOfBins += len(binEdges) - 1
       var = gum.DiscretizedVariable(variableName, variableName, binEdges)
       var.setEmpirical(True)
 
     if usingDefaultParameters:
       self.discretizationParametersDictionary.pop(variableName)
 
     return var
 
-  def _divideIntervalMDLP(self, minimalValues, shannonEntropyByLargeInterval, Class0ByLargeInterval,
+  @staticmethod
+  def _divideIntervalMDLP(minimalValues, shannonEntropyByLargeInterval, Class0ByLargeInterval,
                           Class1ByLargeInterval, continueDividingInterval, totalCountByLargeInterval, position,
                           binEdgesIndex):
     shannonEntropy = shannonEntropyByLargeInterval[position]
 
     gain = shannonEntropy - minimalValues['classInformationEntropy']
 
     # all the 2's here should be replaced by the number of classes present in the interval. However we know that if
     # the number of classes in the interval is equal to 1, then the shannon entropy will be 0 so the product of the 2
     # will be 0.
     deltaS = math.log2(7) - (
-        2 * shannonEntropy - 2 * minimalValues['leftSubintervalShannonEntropy']
-        - 2 * minimalValues['rightSubintervalShannonEntropy'])
+       2 * shannonEntropy - 2 * minimalValues['leftSubintervalShannonEntropy']
+       - 2 * minimalValues['rightSubintervalShannonEntropy'])
 
     if gain > (math.log2(totalCountByLargeInterval[position] - 1) + deltaS) / totalCountByLargeInterval[
       position] or len(Class0ByLargeInterval) == 1:
       binEdgesIndex.insert(position, minimalValues['boundaryIndex'])
 
       Class0ByLargeInterval.insert(position + 1, minimalValues['rightSubintervalClass0'])
       Class1ByLargeInterval.insert(position + 1, minimalValues['rightSubintervalClass1'])
@@ -814,7 +859,21 @@
         continueDividingInterval[position] = False
 
       if position < len(binEdgesIndex) - 1 and binEdgesIndex[position] + 1 == binEdgesIndex[position + 1]:
         continueDividingInterval[position + 1] = False
       position += 1
     else:
       continueDividingInterval[position] = False
+
+  def discretizedBN(self, X, y=None, possibleValuesY=None):
+    """
+    return a BN discretized using the suggestion of the Discretized for date source X and for target y. This BN only contains the discretized variables. For instance, it can be used as a template for a BNLearner.
+
+    Example
+    -------
+    >>> discretizer=skbn.BNDiscretizer(defaultDiscretizationMethod='uniform',defaultParamDiscretizationMethod=7,discretizationThreshold=10)
+    >>> learner=gum.BNLearner(data,discretizer.discretizedBN(data))
+    """
+    template = gum.BayesNet()
+    for name in X:
+      template.add(self.createVariable(name, X[name], y, possibleValuesY))
+    return template
```

## pyAgrum/skbn/_learningMethods.py

```diff
@@ -1,8 +1,7 @@
-# -*- coding: utf-8 -*-
 # (c) Copyright 2020-2023 by Pierre-Henri Wuillemin(@LIP6)  (pierre-henri.wuillemin@lip6.fr)
 
 # Permission to use, copy, modify, and distribute this
 # software and its documentation for any purpose and
 # without fee or royalty is hereby granted, provided
 # that the above copyright notice appear in all copies
 # and that both that copyright notice and this permission
@@ -30,24 +29,22 @@
 
 from ._utils import _ImplementConstraints as implementConstraints
 from ._utils import _ImplementScore as implementScore
 
 
 def _fitStandard(X, y, learner, learningMethod, possibleSkeleton, scoringType, constraints):
   """
-  parameters:
+  The default fit function that uses MIIC, 3off2, Greedy Hill Climbing or Tabu list sorting depending on the learning method chosen.
+
+  Parameters
+  ---------
       X: {array-like, sparse matrix} of shape (n_samples, n_features)
       training data
       y: array-like of shape (n_samples)
       Target values
-
-  returns:
-      void
-
-  The default fit function that uses MIIC, 3off2, Greedy Hill Climbing or Tabu list sorting depending on the learning method chosen.
   """
 
   implementConstraints(constraints, learner)
 
   if learningMethod == "MIIC":
     learner.useMIIC()
   elif learningMethod == "MIICMDL":
@@ -77,24 +74,23 @@
   implementScore(scoringType, learner)
   bn = learner.learnBN()
   return bn
 
 
 def _fitTAN(X, y, bn, learner, variableList, target):
   """
-  parameters:
+  Uses Tree-Augmented NaiveBayes to learn the network structure and its parameters.
+
+  Parameters
+  ---------
       X: {array-like, sparse matrix} of shape (n_samples, n_features)
       training data
       y: array-like of shape (n_samples)
       Target values
 
-  returns:
-      void
-
-  Uses Tree-Augmented NaiveBayes to learn the network structure and its parameters.
   """
 
   # a list of all the variables in our Bayesian network sorted by their index
 
   # the number of columns in our data
   d = X.shape[1]
 
@@ -177,24 +173,23 @@
 
   bn = learner.learnParameters(bn.dag())
   return bn
 
 
 def _fitChowLiu(X, y, bn, learner, variableList, target):
   """
-  parameters:
+  Uses the Chow-Liu algorithm to learn the network structure and its parameters.
+
+  Parameters
+  ---------
       X: {array-like, sparse matrix} of shape (n_samples, n_features)
       training data
       y: array-like of shape (n_samples)
       Target values
-
-  returns:
-      void
-
-  Uses the Chow-Liu algorithm to learn the network structure and its parameters."""
+  """
 
   # since the chow liu algorithm doesn't differentiate between input and output variables, we construct a matrix that includes them both
   dimension = y.shape
   yColumn = numpy.reshape(y, (dimension[0], 1))
   xAndY = numpy.concatenate((yColumn, X), axis=1)
   d = xAndY.shape[1]
 
@@ -247,24 +242,24 @@
 
   bn = learner.learnParameters(bn.dag())
   return bn
 
 
 def _fitNaiveBayes(X, y, bn, learner, variableList, target, constraints):
   """
-  parameters:
+  Uses the Naive Bayes algorithm to learn the network parameters. The network structure isn't learned since it is fixed.
+
+  Parameters
+  ---------
       X: {array-like, sparse matrix} of shape (n_samples, n_features)
       training data
       y: array-like of shape (n_samples)
       Target values
 
-  returns:
-      void
 
-  Uses the Naive Bayes algortihm to learn the network parameters. The network structure isn't learned since it is fixed.
   """
 
   for variableName in variableList:
     bn.addArc(target, variableName)
   if constraints is not None:
     warnings.warn("The structure of Naive Bayes is fixed, so it is impossible to add any new constraints")
```

## pyAgrum/skbn/_MBCalcul.py

```diff
@@ -1,8 +1,7 @@
-# -*- coding: utf-8 -*-
 # (c) Copyright 2020-2023 by Pierre-Henri Wuillemin(@LIP6)  (pierre-henri.wuillemin@lip6.fr)
 
 # Permission to use, copy, modify, and distribute this
 # software and its documentation for any purpose and
 # without fee or royalty is hereby granted, provided
 # that the above copyright notice appear in all copies
 # and that both that copyright notice and this permission
@@ -21,24 +20,26 @@
 
 import pyAgrum as gum
 from ._utils import _listIdtoName as listIdtoName
 
 
 def compileMarkovBlanket(bn, target):
   """
-  parameters:
-      bn: gum.BayesNet
+  Create a Bayesian network with the children, their parents and the parents of the node target
+
+  Parameters
+  ----------
+      bn: pyAgrum.BayesNet
           Bayesian network to work on
       target: str or int
           Name or id of the target
-  returns:
-      MarkovBlanket: gum.BayesNet
+  Returns
+  -------
+      MarkovBlanket: pyAgrum.BayesNet
           Markov Blanket from bn
-
-  Create a Bayesian network with the children, their parents and the parents of the node target
   """
   mb = gum.BayesNet('MarkovBlanket')
 
   # add target to Markov Blanket
   mb.add(bn.variable(target))
 
   # list of target's children
@@ -103,26 +104,28 @@
   return mb
 
 
 def _calcul_proba_for_nary_class(row, local_inst, dictName, MarkovBlanket, target):
   """
   Calculate the posterior distribution of variable target (given its Markov blanket)
 
-  parameters:
+  Parameters
+  ----------
       row: numpyArray shape: (n features)
           test data
-      local_inst: Potential
-          Instanciation of the Markov Blanket EXCEPT the target
-      dictName: dict[str : int]
-          dictionnary of the name of a variable and his column in the data base
-      MarkovBlanket: gum.BayesNet
+      local_inst: pyAgrum.Potential
+          Instantiation of the Markov Blanket EXCEPT the target
+      dictName: Dict[str : int]
+          dictionary of the name of a variable and his column in the data base
+      MarkovBlanket: pyAgrum.BayesNet
           Markov Blanket to work on
       target: str
           Name of the target
-  returns:
+  Returns
+  -------
       proba:
           the probability distribution for target
   """
   # create Instantiation with Markov Blanket's variables
   for n in MarkovBlanket.names():
     if n == target:
       continue
@@ -136,56 +139,60 @@
   return p
 
 
 def _calcul_most_probable_for_nary_class(row, local_inst, dictName, MarkovBlanket, target):
   """
   Calculate the most probable class for variable target
 
-  parameters:
+  Parameters
+  ----------
       row: numpyArray shape: (n features)
           test data
-      local_inst: Potential
-          Instanciation of the Markov Blanket EXCEPT the target
-      dictName: dict[str : int]
-          dictionnary of the name of a variable and his column in the data base
-      MarkovBlanket: gum.BayesNet
+      local_inst: pyAgrum.Potential
+          Instantiation of the Markov Blanket EXCEPT the target
+      dictName: Dict[str : int]
+          dictionary of the name of a variable and his column in the data base
+      MarkovBlanket: pyAgrum.BayesNet
           Markov Blanket to work on
       target: str
           Name of the target
-  returns:
-      proba:
-          the probability distribution for target
+  Returns
+  -------
+      Tuple[int,proba]:
+          the value and the probability of the most probable class
   """
   p = _calcul_proba_for_nary_class(
       row, local_inst, dictName, MarkovBlanket, target)
   return p.argmax(), p.max()
 
 
 def _calcul_proba_for_binary_class(row, label1, labels, Inst, dictName, MarkovBlanket, target):
   """
-  parameters:
+  Calculate the probability of having label1 to the binary variable y
+
+  Parameters
+  ----------
       row: numpyArray shape: (n features)
           test data
       label1:
           the True value of y
       labels:
           the False values of y
       Inst: Potential
-          Instanciation of the Markov Blanket
+          Instantiation of the Markov Blanket
       dictName: dict[str : int]
-          dictionnary of the name of a variable and his column in the data base
-      MarkovBlanket: gum.BayesNet
+          dictionary of the name of a variable and his column in the data base
+      MarkovBlanket: pyAgrum.BayesNet
           Markov Blanket to work on
       target: str
           Name of the target
-  returns:
-      res: double
+  Returns
+  -------
+      double
           probability of getting label1 to the variable y
-
-  Calculate the probability of having label1 to the binary variable y
   """
 
   # create Instantiation with Markov Blanket's variables
   for n in MarkovBlanket.names():
     if n == target:
       continue
     Inst.chgVal(n, str(row[dictName.get(n)]))
```

## pyAgrum/skbn/_utils.py

```diff
@@ -23,66 +23,67 @@
 import csv
 import numpy
 import pyAgrum.lib.bn2roc as bn2roc
 import pandas as pd
 import tempfile
 
 
-def _CalculateThreshold(bn, targetName, csvfilename, usePR, significant_digits):
+def _CalculateThreshold(bn, targetName, csvfilename, usePR, beta, significant_digits):
   """
-  parameters:
+  The Bayesian network gives us the probability of the target knowing the values of the other variables. The value above which the probability needs to be for the input to be classified as that class is called the threshold.
+  This method calculates the optimal threshold using the roc or precision-recall curve.
+
+  Parameters
+  ----------
       bn: gum.BayesNet
           Bayesian network to work on
       targetName: str
           Name of the target
       csvfilename: str
           Name of the csv file
       usePR: bool
           indicates if the threshold to choose is Prevision-Recall curve's threhsold or ROC's threshold by default.
           ROC curves should be used when there are roughly equal numbers of observations for each class.
           Precision-Recall curves should be used when there is a moderate to large class imbalance especially for the target's class.
       significant_digit:
           number of significant digits when computing probabilities
-  returns:
-      threshold: float
+  Returns
+  -------
+      float
           optimal threshold for predictions
-
-  The Bayesian network gives us the probability of the target knowing the values of the other variables. The value above which the probability needs to be for the input to be classified as that class is called the threshold.
-  This method calculates the optimal threshold using the roc or precision-recall curve.
   """
   target = bn.variableFromName(targetName)
 
   if usePR:
     _, _, _, threshold = bn2roc.showROC_PR(bn, csvfilename, targetName, target.labels(
-    )[1], show_fig=False, show_ROC=False, show_PR=False, significant_digits=significant_digits, show_progress=False)
+    )[1],beta=beta, show_fig=False, show_ROC=False, show_PR=False, significant_digits=significant_digits, show_progress=False)
   else:
     _, threshold, _, _ = bn2roc.showROC_PR(bn, csvfilename, targetName, target.labels(
-    )[1], show_fig=False, show_ROC=False, show_PR=False, significant_digits=significant_digits, show_progress=False)
+    )[1], beta=beta, show_fig=False, show_ROC=False, show_PR=False, significant_digits=significant_digits, show_progress=False)
 
   return threshold
 
 
 def _ImplementScore(scoringType, learner):
   """
-  parameters:
+  Tells the Bayesian network which scoring type to use.
+
+  Parameters
+  ----------
       scoringType: str
-          A string designating the type of scoring we want to use. Since scoring is used while constructing the network and not when learning its parameters, the scoring will be ignored if using a learning algorithm
+          A string designating the type of scoring we want to use. Since scoring is used while constructing the network and not when learning its Parameters, the scoring will be ignored if using a learning algorithm
           with a fixed network structure such as Chow-Liu, TAN or NaiveBayes.
           possible values are:  AIC, BIC, BD, BDeu, K2, Log2
           AIC means Akaike information criterion
           BIC means Bayesian Information criterion
           BD means Bayesian-Dirichlet scoring
           BDeu means Bayesian-Dirichlet equivalent uniform
           Log2 means log2 likelihood ratio test
       learner:
           learner object from pyAgrum to apply the score
-  returns:
-      void
-
-  Tells the Bayesian network which scoring type to use.
   """
   if scoringType is None:
     return
   elif scoringType == 'AIC':
     learner.useScoreAIC()
   elif scoringType == 'BD':
     learner.useScoreBD()
@@ -97,35 +98,34 @@
   else:
     raise ValueError("Invalid scoringType! Possible values are : \
                           AIC , BD , BDeu, BIC , K2 and Log2")
 
 
 def _ImplementPrior(prior, learner, priorWeight, DirichletCsv):
   """
-  parameters:
+  Tells the Bayesian network which prior to use
+
+  Parameters
+  ----------
       prior: str
           A string designating the type of a priorsmoothing we want to use.
           Possible values are Smoothing, BDeu , Dirichlet and NoPrior.
           Note: if using Dirichlet smoothing DirichletCsv cannot be set to none
       learner:
           learner object from pyAgrum to apply the score
       priorWeight: double
           The weight used for a priorsmoothing.
       DirichletCsv: str
           the file name of the csv file we want to use for the dirichlet prior.
           Will be ignored if prior is not set to Dirichlet.
-   returns:
-      void
-
-  Tells the Bayesian network which prior to use
   """
-  if (prior == 'Smoothing'):
+  if prior == 'Smoothing':
     learner.useSmoothingPrior(priorWeight)
   elif prior == 'Dirichlet':
-    if DirichletCsv == None:
+    if DirichletCsv is None:
       raise ValueError("Must specify file for dirichlet prior as a parameter to the classifier if using a dirichlet prior. DirichletCsv cannot be set to none if prior is set to Dirichlet")
     learner.useDirichletPrior(DirichletCsv, priorWeight)
   elif prior == 'BDeu':
     learner.useBDeuPrior(priorWeight)
   elif prior == 'NoPrior':
     learner.useNoPrior()
   elif prior is None:  # default : (small) Laplace's adjustment
@@ -133,36 +133,34 @@
   else:
     raise ValueError(
         "Invalid prior! Possible values are : Smoothing , Dirichlet , BDeu and NoPrior")
 
 
 def _ImplementConstraints(constraints, learner):
   """
-  parameters:
+  Tells the Bayesian network which constraints should be put on the structure of the network.
+  More details on the nature of these constraints can be found in the documentation for the constructor of this class
+
+  Parameters
+  ----------
       constraints: dict()
           A dictionary designating the constraints that we want to put on the structure of the Bayesian network.
           Ignored if using a learning algorithm where the structure is fixed such as TAN or NaiveBayes.
           the keys of the dictionary should be the strings "PossibleEdges" , "MandatoryArcs" and  "ForbiddenArcs".
           The format of the values should be a tuple of strings (tail,head) which designates the string arc from tail to head.
           For example if we put the value ("x0"."y") in MandatoryArcs the network will surely have an arc going from x0 to y.
           Note: PossibleEdge between nodes x and y allows for either (x,y) or (y,x) (or none of them) to be added to the Bayesian network, while the others are not symmetric.
       learner:
           learner object from pyAgrum to apply the score
-  returns:
-      void
-
-  Tells the Bayesian network which constraints should be put on the structure of the network.
-  More details on the nature of these constraints can be found in the documentation for the constructor of this class
   """
   if constraints is None:  # default
     return
   if type(constraints) != dict:
     raise ValueError(
         "Invalid syntax for constraints. Constraints should be passed as a dictionary")
-    return
   for key in constraints:
     if key == 'MandatoryArcs':
       for (tail, head) in constraints[key]:
         learner.addMandatoryArc(tail, head)
     elif key == 'ForbiddenArcs':
       for (tail, head) in constraints[key]:
         learner.addForbiddenArc(tail, head)
@@ -172,70 +170,72 @@
     else:
       raise ValueError("Invalid syntax: the only keys in the constraints dictionary should be \
                              MandatoryArcs, PossibleEdges and ForbiddenArcs")
 
 
 def _DFNames(X):
   """
-  parameters:
+  Return a dictionary of variable's name and his index from a DataFrame
+
+  Parameters
+  ----------
       X: pandas.DataFrame
           DataFrame to read
 
-  returns:
-      res: dict[str : int]
-          Dictionnary of variable's name and his index
-
-  Return a dictionnay of variable's name and his index from a DataFrame
+  Returns
+  -------
+      Dict[str,int]
+          Dictionary of variable's name and his index
   """
 
   res = dict()
   i = 0
   for var in X.columns.tolist():
     res[var] = i
     i = i + 1
 
   return res
 
 
 def _listIdtoName(bn, liste):
   """
-  parameters:
+  Return a list of names of the variable which have their id in list.
+
+  Parameters
+  ----------
       bn: gum.BayesNet
           Bayesian network to work on
       liste: list[int]
           List of id
 
-  returns:
-      liste: list[str]
+  Returns
+  -------
+      List[str]
           List of names
-
-  Return a list of names of the variable which have their id in liste
   """
   return [bn.variable(i).name() for i in liste]
 
 
 def _createCSVfromNDArrays(X, y, target, variableNameIndexDictionary, csvfilename):
   """
-  parameters:
+  Creates a csv file from the matrices passed as Parameters.
+  csvfilename  is used by the fit function to learn the network structure and its Parameters
+
+  Parameters
+  ----------
       X: {array-like, sparse matrix} of shape (n_samples, n_features)
           training data
       y: array-like of shape (n_samples)
           Target values
       target: str
           Name of the target
       variableNameIndexDictionary: dict[str : int]
           dictionnary of the csvfilename of a variable and his column in the data base
       csvfilename: str
           csv's title
-
-  returns:
-      void
-
-  Creates a csv file from the matrices passed as parameters.
-  csvfilename  is used by the fit function to learn the network structure and its parameters
   """
 
   # verifies if the shape of
   X, y = sklearn.utils.check_X_y(X, y, dtype=None, accept_sparse=True)
   y = pd.DataFrame(y, columns=[target])
   variableList = [k for k, v in sorted(
       variableNameIndexDictionary.items(), key=(lambda item: item[1]), reverse=False)]
@@ -258,23 +258,24 @@
     if v[0] in ('-', '+'):
       return v[1:].isdigit()
     return v.isdigit()
   return False
 
 def checkFloat(v):
   """
-  Test is  a float or a str representing a float.
+  Test if v is a float or a str representing a float.
 
   Parameters
   ----------
-  v
+  v : Any
 
   Returns
   -------
-  True if v is a float
+  bool:
+    True if v is a float or a str representing a float
   """
   if isinstance(v,bool):
       return False
 
   try:
     float(v)
     return True
```

## Comparing `pyAgrum-1.8.3.dist-info/METADATA` & `pyAgrum-1.9.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 1.2
 Name: pyagrum
-Version: 1.8.3
+Version: 1.9.0
 Summary: Bayesian networks and other Probabilistic Graphical Models.
 Home-page: https://agrum.gitlab.io/
 Author: Pierre-Henri Wuillemin and Christophe Gonzales
 Author-email: info@agrum.org
 License: LGPLv3
 Keywords: probabilities probabilistic-graphical-models inference diagnosis
 Project-URL: Bug Tracker, https://gitlab.com/agrumery/aGrUM/-/issues
```

## Comparing `pyAgrum-1.8.3.dist-info/RECORD` & `pyAgrum-1.9.0.dist-info/RECORD`

 * *Files 25% similar despite different names*

```diff
@@ -1,44 +1,44 @@
 pyAgrum\config.py,sha256=vbCZPxXl8SpbnX8qjZrWeWVWlp-n_v84PQntCkyGlZ4,9039
-pyAgrum\defaults.ini,sha256=NWA9AxNiCatmWTnlX0uhpKgGGSrhzEwsmjTlwy1XRfM,3555
-pyAgrum\deprecated.py,sha256=lAvsxMutyPeaiFo18898j6eClBlzTGWJxbCcYisHeTU,9098
-pyAgrum\pyAgrum.py,sha256=9TZFRtp8u8MMX022ZcIu0dhuna6heXHwxRUCjcc_9hA,724210
-pyAgrum\_pyAgrum.cp39-win_amd64.pyd,sha256=MA0ilZ1u6tCXd4efDWNhpGQzNJ3rlO4_GoGK6FHtq7Y,7369216
-pyAgrum\__init__.py,sha256=n12_jpTLyCK582cyN4WVvK_5lVc2M1iQOsr4H0trHbw,27046
+pyAgrum\defaults.ini,sha256=KsdeGS1JCCMgevdOz8pX6Q4MNu14lG7NOb4ZDShYhCM,3663
+pyAgrum\deprecated.py,sha256=LkOG0o5DLp-ISTaQPXsyi6mcSazTrtTwqaRbR-6rv9s,8631
+pyAgrum\pyAgrum.py,sha256=Ti4m95w05DHhRnXAmFEHxNutKh1munxi7l-t1UN5xts,725587
+pyAgrum\_pyAgrum.cp39-win_amd64.pyd,sha256=uCDheP_nrDFUiHmkrgBTet3BRRTfYRi_IqxKbCYisZw,7369216
+pyAgrum\__init__.py,sha256=bck5DznlozYAtNHOxDGjm74h3g-hP4iMnn8doe1sQLY,27046
 pyAgrum\causal\notebook.py,sha256=dz0EGs4xMspBiZbPG8v92dzlu7jyrZnh4nHwKilr6-E,4185
 pyAgrum\causal\_CausalFormula.py,sha256=GmKP8SsP1OjROXBTVfz5KpbI06YTUcwJlZuJR6kMQeU,6933
 pyAgrum\causal\_causalImpact.py,sha256=Fny1_KSPqOWPKEaDVlMhikqwLRTPSFHZvTzD1qPvLjw,13335
 pyAgrum\causal\_CausalModel.py,sha256=NNQaVY_3cTODVwFB8wH8Sb592gXkSot02OezGjwnhUU,12429
 pyAgrum\causal\_doAST.py,sha256=Wdqa-P9emy15J8O2enX_jT9M5k1regkP8_FSPf3E5bc,19845
 pyAgrum\causal\_doCalculus.py,sha256=UrOiEPbohmVPCz-59vU62wFtmKGT1yBH87fWW1hAiJo,10387
 pyAgrum\causal\_doorCriteria.py,sha256=xiSM6Vg4fMNXUf4Y7f4my9rrXpJpskpIaMRFRjX0ReE,8731
 pyAgrum\causal\_dSeparation.py,sha256=CpNyyDXANcZxpP0N_4v_-H4_aO4H5bhrAoghBi-9kfY,14451
 pyAgrum\causal\_exceptions.py,sha256=Q5FJVzU_JzI2kPkqJiIISmaofBnti96M2qpgvo4DJ3Q,2032
 pyAgrum\causal\_types.py,sha256=cJwje297sBXcYiC9VBdS_rhqOtgKi-iRE3UowtKWKnE,1560
 pyAgrum\causal\__init__.py,sha256=yVIWnI9ZWJzLsfKV_u1bU8cS5YZET6xsRMldxvXmz3k,2310
-pyAgrum\lib\bn2graph.py,sha256=e0zaPvtj1UYazafuGZbAsItk--xIeHDsa_RuQdVtSlM,9230
-pyAgrum\lib\bn2roc.py,sha256=y5-Wt8t-mN-yLvxRvdGjAnAGMCetg1CJknWyKAUwlhM,15908
-pyAgrum\lib\bn2scores.py,sha256=2FPalZ9Ew661lXRq3oYRvigq5kE2ryIX_FmFKyfDswE,4839
-pyAgrum\lib\bn_vs_bn.py,sha256=Q8nh9u6_a-thwYclfJT9WlHRukxR4Uv03UFAaIztuqw,19114
+pyAgrum\lib\bn2graph.py,sha256=T5p9gNFUg_6FUsGwkXZhecwnmscPPxpG5e-hKsgaMxQ,9164
+pyAgrum\lib\bn2roc.py,sha256=LfWrT7krpNYIVYFXUChPSALNBtjiwlwVGKm63y7fUyI,25054
+pyAgrum\lib\bn2scores.py,sha256=hoQUFd9KggVnH94z7F6Z23NBdyLzAdWn2xda42zEY48,4839
+pyAgrum\lib\bn_vs_bn.py,sha256=oZ1QM5lg4zjN_TPkEpH96tUQ9hslwH8vlfsfFse2ruA,19143
 pyAgrum\lib\classifier.py,sha256=y_EPu3kVW0vo1zYCfotS9UmdByLXTL14ALvhTa0ztb4,1281
-pyAgrum\lib\cn2graph.py,sha256=xLwhRlZqM6p_Q_Mx2nmQlfcN4LbZUDLdfDq0jgIAvsA,9505
+pyAgrum\lib\cn2graph.py,sha256=IZidGJG52-hEM8xTlSDgG8KsV_S_YM7Zp1a8FRUGXt8,9437
 pyAgrum\lib\dynamicBN.py,sha256=yNDbz0gqKgVtJaNOrujgL1oKtM8tRD43ozWqX03GQK0,11095
-pyAgrum\lib\explain.py,sha256=I1wGUBWhgzlhG5ESpJboNVhxdFiVv739uGNdXpk9lKU,34130
-pyAgrum\lib\export.py,sha256=3ya0_jziZ_ERuXwPYe5q5QCeVOf9TiBHwIQy-O9yx-w,3474
-pyAgrum\lib\id2graph.py,sha256=qr0--958BF6ajc92Df8g32NI65p3k07zwmPvUyDFcTc,8640
-pyAgrum\lib\image.py,sha256=QNQaYm_vALcEuzwaH81e4dEhaGFhOLhCoHPcVg2dAuY,12628
+pyAgrum\lib\explain.py,sha256=tcOjygMHvbMxUZybhSd6_l5UuCNb5XX9UpHrQi8mkho,33987
+pyAgrum\lib\export.py,sha256=5JF9J25d63CVpxszRIWzCLyac0cn0SyJZ7t6r-X4XNI,3484
+pyAgrum\lib\id2graph.py,sha256=WAF6VPti_Xz2vy6BI_S3YVu7risMIs94V2cm2qRUzwY,8611
+pyAgrum\lib\image.py,sha256=36DRTLjW_lJ7_nxxkX5VfDXcG85Sk2gP3srNlBb1cWE,12628
 pyAgrum\lib\ipython.py,sha256=YqtEOSszzLtmtKB-_EujVyt7iaLDItlzYR92jCdgX6g,9370
-pyAgrum\lib\mn2graph.py,sha256=ql193HCmDGp9L6ZZRJOQzAIoqSMdqiaO7qStoDWHTUY,234
-pyAgrum\lib\mrf2graph.py,sha256=MkQj7DXXO0bDB46rxgk2fqgKXdVmd-JYuU5PnzZm_qw,17129
-pyAgrum\lib\notebook.py,sha256=Z1Sj04lInfBNarCy8XqTFf7hFJL1V81f_ImYNo4dZXc,54833
-pyAgrum\lib\proba_histogram.py,sha256=rkJIazNqDHXo7SnTKoEuc68NuvjN3AqPz0cutHLVeFc,12649
-pyAgrum\lib\_colors.py,sha256=Vjbk9zWGq0_7tNk1-tPWPrUDT_F-BNMuVYX4_iSwhzc,4969
+pyAgrum\lib\mn2graph.py,sha256=J9NYT8-BQdxC0ftBqR4xgQHHuM4mYKlehNjwk0tHt2k,235
+pyAgrum\lib\mrf2graph.py,sha256=7Qyz5GI0AMRR11hDr3ea3HcT6sx1S9JDcDpVyLS8fYc,17045
+pyAgrum\lib\notebook.py,sha256=8iFn3SfYM3zfYnPtSA1GZkkTBcBsfuzfbyoNhlYtRDQ,54788
+pyAgrum\lib\proba_histogram.py,sha256=u8rNKud-qOwuk3nwdq--413NfLKmnzOKaqxWRc30DOA,13848
+pyAgrum\lib\_colors.py,sha256=znQhuc6hvU4vb0zppxLPaNJ3gcu2gHh4HHspG6WK48U,4984
 pyAgrum\lib\__init__.py,sha256=fGtC5PW_k8FN_OBrH-PqJR609qDWdaTX0Nubv15OGSY,1108
-pyAgrum\skbn\bnclassifier.py,sha256=G0zR74S9Md1LNMgpwN89r_jNxCTR0982HuOi0xhDnC8,38297
-pyAgrum\skbn\discretizer.py,sha256=YcmBHN04uI1C09Kd4rnGiK6ibvjB1bKMUTr7uhOD2ZA,39166
-pyAgrum\skbn\_learningMethods.py,sha256=qJ-tfH5X8_wZIhKDJRhnrPjyh8AgauaAf7F-je7I5Lo,11326
-pyAgrum\skbn\_MBCalcul.py,sha256=8_bzFZ5m7yAGcW_QdJ0aeMWgaW-Cb3REUHjjVP2oJT8,6619
-pyAgrum\skbn\_utils.py,sha256=ey8N03IAV7wDTDljNR1JlMQ7Q5sb_9bofruUQvcTdD8,10287
+pyAgrum\skbn\bnclassifier.py,sha256=o0EXAeaxi8_zPKENW3tW2rhgDdke7uRhqLZW1u9qF6U,37942
+pyAgrum\skbn\discretizer.py,sha256=pJiG7xpf54-Ka7y7fKi7K_KIeS3eOHNAw7jsSIsstmc,41630
+pyAgrum\skbn\_learningMethods.py,sha256=ghg2TBRz96nKiChwnSCJSoRGA8P7iNSRL2MIuTpXzEw,11255
+pyAgrum\skbn\_MBCalcul.py,sha256=3dBTQq3PtZ8bnZV8sfNMN7digTlNPXBiFHG3sRLty7Q,6742
+pyAgrum\skbn\_utils.py,sha256=KS-_dNtLhcGFiattmvJ0b9Hv86FXFPrEvV8HTyJL6pw,10347
 pyAgrum\skbn\__init__.py,sha256=d9fAxGvp5Uxni3Mb9fqjG0C8j-oGgH1YoUeFKQkHsPg,1480
-pyAgrum-1.8.3.dist-info\METADATA,sha256=8od2xSiTZ6pFOfrbFq0FUUVOMI1DdIe7ibFcCq46VOc,2281
-pyAgrum-1.8.3.dist-info\WHEEL,sha256=6tS66PeQ4lIG2ui9evGqS1V2ylUn8-igSxwA24aBiCc,92
-pyAgrum-1.8.3.dist-info\RECORD,,
+pyAgrum-1.9.0.dist-info\METADATA,sha256=j6SSxgEm0WHhpF9VyX1RccxUNqUb2I4UQpMzOzj_Log,2281
+pyAgrum-1.9.0.dist-info\WHEEL,sha256=6tS66PeQ4lIG2ui9evGqS1V2ylUn8-igSxwA24aBiCc,92
+pyAgrum-1.9.0.dist-info\RECORD,,
```


# Comparing `tmp/space_stream-0.1.7.2-py3-none-any.whl.zip` & `tmp/space_stream-0.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,27 +1,32 @@
-Zip file size: 24410 bytes, number of entries: 25
--rw-r--r--  2.0 unx    12041 b- defN 23-Jun-28 08:38 spacestream/SpaceStreamPipeline.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-28 08:38 spacestream/__init__.py
--rw-r--r--  2.0 unx     5793 b- defN 23-Jun-28 08:38 spacestream/__main__.py
--rw-r--r--  2.0 unx     1117 b- defN 23-Jun-28 08:38 spacestream/codec/DepthCodec.py
--rw-r--r--  2.0 unx      714 b- defN 23-Jun-28 08:38 spacestream/codec/DepthCodecType.py
--rw-r--r--  2.0 unx      205 b- defN 23-Jun-28 08:38 spacestream/codec/InverseHueColorization.py
--rw-r--r--  2.0 unx     1967 b- defN 23-Jun-28 08:38 spacestream/codec/LinearCodec.py
--rw-r--r--  2.0 unx     1069 b- defN 23-Jun-28 08:38 spacestream/codec/RealSenseColorizer.py
--rw-r--r--  2.0 unx     4670 b- defN 23-Jun-28 08:38 spacestream/codec/UniformHueColorization.py
--rw-r--r--  2.0 unx       99 b- defN 23-Jun-28 08:38 spacestream/codec/__init__.py
--rw-r--r--  2.0 unx      782 b- defN 23-Jun-28 08:38 spacestream/fbs/FrameBufferSharingServer.py
--rw-r--r--  2.0 unx     1382 b- defN 23-Jun-28 08:38 spacestream/fbs/SpoutServer.py
--rw-r--r--  2.0 unx     3116 b- defN 23-Jun-28 08:38 spacestream/fbs/SyphonServer.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-28 08:38 spacestream/fbs/__init__.py
--rw-r--r--  2.0 unx      213 b- defN 23-Jun-28 08:38 spacestream/io/EnhancedJSONEncoder.py
--rw-r--r--  2.0 unx      531 b- defN 23-Jun-28 08:38 spacestream/io/StreamInformation.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-28 08:38 spacestream/io/__init__.py
--rw-r--r--  2.0 unx    12048 b- defN 23-Jun-28 08:38 spacestream/ui/MainWindow.py
--rw-r--r--  2.0 unx     6174 b- defN 23-Jun-28 08:38 spacestream/ui/PipelineView.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-28 08:38 spacestream/ui/__init__.py
--rw-r--r--  2.0 unx    12063 b- defN 23-Jun-28 08:38 space_stream-0.1.7.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-28 08:38 space_stream-0.1.7.2.dist-info/WHEEL
--rw-r--r--  2.0 unx       60 b- defN 23-Jun-28 08:38 space_stream-0.1.7.2.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 23-Jun-28 08:38 space_stream-0.1.7.2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2197 b- defN 23-Jun-28 08:38 space_stream-0.1.7.2.dist-info/RECORD
-25 files, 66345 bytes uncompressed, 20792 bytes compressed:  68.7%
+Zip file size: 26208 bytes, number of entries: 30
+-rw-r--r--  2.0 unx      890 b- defN 24-May-08 11:53 spacestream/SpaceStreamApp.py
+-rw-r--r--  2.0 unx     3220 b- defN 24-May-08 11:53 spacestream/SpaceStreamConfig.py
+-rw-r--r--  2.0 unx    14623 b- defN 24-May-08 11:53 spacestream/SpaceStreamGraph.py
+-rw-r--r--  2.0 unx     1295 b- defN 24-May-08 11:53 spacestream/WatchDog.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-08 11:53 spacestream/__init__.py
+-rw-r--r--  2.0 unx     5274 b- defN 24-May-08 11:53 spacestream/__main__.py
+-rw-r--r--  2.0 unx     1117 b- defN 24-May-08 11:53 spacestream/codec/DepthCodec.py
+-rw-r--r--  2.0 unx      714 b- defN 24-May-08 11:53 spacestream/codec/DepthCodecType.py
+-rw-r--r--  2.0 unx      205 b- defN 24-May-08 11:53 spacestream/codec/InverseHueColorization.py
+-rw-r--r--  2.0 unx     1967 b- defN 24-May-08 11:53 spacestream/codec/LinearCodec.py
+-rw-r--r--  2.0 unx     1069 b- defN 24-May-08 11:53 spacestream/codec/RealSenseColorizer.py
+-rw-r--r--  2.0 unx     4670 b- defN 24-May-08 11:53 spacestream/codec/UniformHueColorization.py
+-rw-r--r--  2.0 unx       99 b- defN 24-May-08 11:53 spacestream/codec/__init__.py
+-rw-r--r--  2.0 unx      782 b- defN 24-May-08 11:53 spacestream/fbs/FrameBufferSharingServer.py
+-rw-r--r--  2.0 unx     1382 b- defN 24-May-08 11:53 spacestream/fbs/SpoutServer.py
+-rw-r--r--  2.0 unx     3116 b- defN 24-May-08 11:53 spacestream/fbs/SyphonServer.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-08 11:53 spacestream/fbs/__init__.py
+-rw-r--r--  2.0 unx      213 b- defN 24-May-08 11:53 spacestream/io/EnhancedJSONEncoder.py
+-rw-r--r--  2.0 unx      531 b- defN 24-May-08 11:53 spacestream/io/StreamInformation.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-08 11:53 spacestream/io/__init__.py
+-rw-r--r--  2.0 unx     1894 b- defN 24-May-08 11:53 spacestream/nodes/ImageRectificationNode.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-08 11:53 spacestream/nodes/__init__.py
+-rw-r--r--  2.0 unx     9512 b- defN 24-May-08 11:53 spacestream/ui/MainWindow.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-08 11:53 spacestream/ui/__init__.py
+-rw-r--r--  2.0 unx     1075 b- defN 24-May-08 11:53 space_stream-0.2.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    12878 b- defN 24-May-08 11:53 space_stream-0.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-08 11:53 space_stream-0.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       60 b- defN 24-May-08 11:53 space_stream-0.2.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 24-May-08 11:53 space_stream-0.2.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2626 b- defN 24-May-08 11:53 space_stream-0.2.0.dist-info/RECORD
+30 files, 69316 bytes uncompressed, 21912 bytes compressed:  68.4%
```

## zipnote {}

```diff
@@ -1,8 +1,17 @@
-Filename: spacestream/SpaceStreamPipeline.py
+Filename: spacestream/SpaceStreamApp.py
+Comment: 
+
+Filename: spacestream/SpaceStreamConfig.py
+Comment: 
+
+Filename: spacestream/SpaceStreamGraph.py
+Comment: 
+
+Filename: spacestream/WatchDog.py
 Comment: 
 
 Filename: spacestream/__init__.py
 Comment: 
 
 Filename: spacestream/__main__.py
 Comment: 
@@ -45,32 +54,38 @@
 
 Filename: spacestream/io/StreamInformation.py
 Comment: 
 
 Filename: spacestream/io/__init__.py
 Comment: 
 
-Filename: spacestream/ui/MainWindow.py
+Filename: spacestream/nodes/ImageRectificationNode.py
+Comment: 
+
+Filename: spacestream/nodes/__init__.py
 Comment: 
 
-Filename: spacestream/ui/PipelineView.py
+Filename: spacestream/ui/MainWindow.py
 Comment: 
 
 Filename: spacestream/ui/__init__.py
 Comment: 
 
-Filename: space_stream-0.1.7.2.dist-info/METADATA
+Filename: space_stream-0.2.0.dist-info/LICENSE
+Comment: 
+
+Filename: space_stream-0.2.0.dist-info/METADATA
 Comment: 
 
-Filename: space_stream-0.1.7.2.dist-info/WHEEL
+Filename: space_stream-0.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: space_stream-0.1.7.2.dist-info/entry_points.txt
+Filename: space_stream-0.2.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: space_stream-0.1.7.2.dist-info/top_level.txt
+Filename: space_stream-0.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: space_stream-0.1.7.2.dist-info/RECORD
+Filename: space_stream-0.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## spacestream/__main__.py

```diff
@@ -1,87 +1,93 @@
 # fix conda load dll problem
+import faulthandler
 import os
+from pathlib import Path
+
+from duit.arguments.Arguments import DefaultArguments
+from visiongui.ui.UIContext import UIContext
+
+from spacestream.SpaceStreamApp import SpaceStreamApp
+from spacestream.SpaceStreamConfig import SpaceStreamConfig
+from spacestream.ui.MainWindow import MainWindow
 
 os.environ["CONDA_DLL_SEARCH_MODIFICATION_ENABLE"] = "1"
 
 import logging
 from functools import partial
 
 import configargparse
 import numba
 from visiongraph.input import add_input_step_choices
 
 from spacestream import codec
-from spacestream.codec.DepthCodecType import DepthCodecType
-from spacestream.fbs.FrameBufferSharingServer import FrameBufferSharingServer
 
 import visiongraph as vg
 import pyrealsense2 as rs
-import open3d as o3d
 
 segmentation_networks = {
     "mediapipe": partial(vg.MediaPipePoseEstimator.create, vg.PoseModelComplexity.Normal),
     "mediapipe-light": partial(vg.MediaPipePoseEstimator.create, vg.PoseModelComplexity.Light),
     "mediapipe-heavy": partial(vg.MediaPipePoseEstimator.create, vg.PoseModelComplexity.Heavy),
 
     # "maskrcnn": partial(vg.MaskRCNNEstimator.create, vg.MaskRCNNConfig.EfficientNet_608_FP32),
     # "maskrcnn-eff-480": partial(vg.MaskRCNNEstimator.create, vg.MaskRCNNConfig.EfficientNet_480_FP16),
     # "maskrcnn-eff-608": partial(vg.MaskRCNNEstimator.create, vg.MaskRCNNConfig.EfficientNet_608_FP16),
     # "maskrcnn-res50-768": partial(vg.MaskRCNNEstimator.create, vg.MaskRCNNConfig.ResNet50_1024x768_FP16),
     # "maskrcnn-res101-800": partial(vg.MaskRCNNEstimator.create, vg.MaskRCNNConfig.ResNet101_1344x800_FP16)
 }
 
 
-def parse_args():
+def parse_args(config: SpaceStreamConfig):
     parser = configargparse.ArgumentParser(prog="space-stream",
                                            description="RGB-D framebuffer sharing demo for visiongraph.")
     parser.add_argument("-c", "--config", required=False, is_config_file=True, help="Configuration file path.")
+    parser.add_argument("-s", "--settings", type=str, required=False, help="Settings file path (json).")
     vg.add_logging_parameter(parser)
 
+    DefaultArguments.add_arguments(parser, config)
+
     input_group = parser.add_argument_group("input provider")
     add_input_step_choices(input_group)
     input_group.add_argument("--midas", action="store_true", help="Use midas for depth capture.")
 
     masking_group = parser.add_argument_group("masking")
-    masking_group.add_argument("--mask", action="store_true", help="Apply mask by segmentation algorithm.")
     vg.add_step_choice_argument(masking_group, segmentation_networks, name="--segnet", default="mediapipe",
                                 help="Segmentation Network", add_params=False)
 
-    depth_group = parser.add_argument_group("depth codec")
-    vg.add_enum_choice_argument(depth_group, DepthCodecType, "--codec",
-                                help="Codec how the depth map will be encoded.", default=DepthCodecType.UniformHue)
-    depth_group.add_argument("--min-distance", type=float, default=0, help="Min distance to perceive by the camera.")
-    depth_group.add_argument("--max-distance", type=float, default=6, help="Max distance to perceive by the camera.")
-
     performance_group = parser.add_argument_group("performance")
     performance_group.add_argument("--parallel", action="store_true", help="Enable parallel for codec operations.")
     performance_group.add_argument("--num-threads", type=int, default=4, help="Number of threads for parallelization.")
     performance_group.add_argument("--no-fastmath", action="store_true", help="Disable fastmath for codec operations.")
 
-    output_group = parser.add_argument_group("output")
-    output_group.add_argument("--stream-name", type=str, default="RGBDStream", help="Spout / Syphon stream name.")
-
     debug_group = parser.add_argument_group("debug")
     debug_group.add_argument("--no-filter", action="store_true", help="Disable realsense image filter.")
     debug_group.add_argument("--no-preview", action="store_true", help="Disable preview to speed.")
-    debug_group.add_argument("--record", action="store_true", help="Record output into recordings folder.")
     debug_group.add_argument("--record-crf", type=int, default=23, help="Recording compression rate.")
     debug_group.add_argument("--view-pcd", action="store_true", help="Display PCB preview (deprecated, use --view-3d).")
     debug_group.add_argument("--view-3d", action="store_true", help="Display PCB preview.")
 
     args = parser.parse_args()
 
     if args.view_pcd:
         args.view_3d = True
     return args
 
 
 def main():
-    args = parse_args()
+    config = SpaceStreamConfig()
+
+    args = parse_args(config)
+    DefaultArguments.configure(args, config)
+
     vg.setup_logging(args.loglevel)
+    logging.info(f"Logging has ben set to {args.loglevel}")
+
+    if args.loglevel.lower() == "debug" or True:
+        faulthandler.enable()
 
     if args.parallel:
         num_threads = min(numba.config.NUMBA_NUM_THREADS, args.num_threads)
         numba.set_num_threads(num_threads)
         codec.ENABLE_PARALLEL = True
         logging.warning(f"Enable parallel with {num_threads} threads")
 
@@ -96,35 +102,34 @@
         args.depth = True
         args.color_scheme = vg.RealSenseColorScheme.WhiteToBlack
 
         if not args.no_filter:
             args.rs_filter = [rs.spatial_filter, rs.temporal_filter]
 
     if issubclass(args.input, vg.AzureKinectInput):
-        args.k4a_align = True
-
-    # create frame buffer sharing client
-    fbs_client = FrameBufferSharingServer.create(args.stream_name)
+        args.k4a_align_to_color = True
 
     show_ui = not args.no_preview
 
-    # run pipeline
-    from spacestream.SpaceStreamPipeline import SpaceStreamPipeline
-    pipeline = SpaceStreamPipeline(args.stream_name, args.input(), fbs_client, args.codec,
-                                   args.min_distance, args.max_distance,
-                                   args.record, args.mask, args.segnet(), args.midas,
-                                   multi_threaded=show_ui, handle_signals=not show_ui)
-    pipeline.configure(args)
+    # create app and graph
+    app = SpaceStreamApp(config, args.input(), args.segnet(), multi_threaded=show_ui)
+    app.graph.configure(args)
+
+    if args.settings is not None:
+        settings_path = Path(args.settings)
+        if settings_path.exists():
+            config.is_loading = True
+            app.load_config(settings_path)
+            config.is_loading = False
 
     if show_ui:
-        app = o3d.visualization.gui.Application.instance
-        app.initialize()
+        with UIContext():
+            window = MainWindow(app)
 
-        from spacestream.ui.MainWindow import MainWindow
-        win = MainWindow(pipeline, args)
-        app.run()
+            if args.settings is not None:
+                window.menu.settings_file = Path(args.settings)
     else:
-        pipeline.open()
+        app.graph.open()
 
 
 if __name__ == "__main__":
     main()
```

## spacestream/ui/MainWindow.py

```diff
@@ -1,183 +1,165 @@
 import logging
 import signal
 import traceback
-from typing import Optional, Sequence
+from typing import Sequence
 
 import cv2
 import numpy as np
 import open3d as o3d
+import pyrealsense2 as rs
+import visiongraph as vg
 from open3d.visualization import gui
-from duit.ui.open3d.Open3dPropertyRegistry import init_open3d_registry
-from duit.ui.open3d.Open3dPropertyPanel import Open3dPropertyPanel
+from visiongui.ui.VisiongraphUserInterface import VisiongraphUserInterface
 
-from spacestream.SpaceStreamPipeline import SpaceStreamPipeline
-from spacestream.codec.LinearCodec import LinearCodec
-from spacestream.ui.PipelineView import PipelineView
+from spacestream.SpaceStreamApp import SpaceStreamApp
+from spacestream.SpaceStreamConfig import SpaceStreamConfig
+from spacestream.WatchDog import HealthStatus, WatchDog
 
-import visiongraph as vg
-import pyrealsense2 as rs
 
+class MainWindow(VisiongraphUserInterface[SpaceStreamApp, SpaceStreamConfig]):
+    def __init__(self, app: SpaceStreamApp):
+        super().__init__(app, width=1000, height=800, handle_graph_state=True)
+
+        def on_stream_name_changed(new_stream_name: str):
+            self.window.title = f"SpaceStream - {new_stream_name}"
 
-class MainWindow:
-    def __init__(self, pipeline: SpaceStreamPipeline, args):
-        self.pipeline = pipeline
-        init_open3d_registry()
-
-        self.window: gui.Window = gui.Application.instance.create_window(f"Space Stream - {pipeline.stream_name}",
-                                                                         round(1000),
-                                                                         round(800))
-        self.window.set_on_layout(self._on_layout)
-        self.window.set_on_close(self._on_close)
-
-        self.em = self.window.theme.font_size
-        margin = 0.5 * self.em
-
-        # settings panel
-        self.settings_panel_width = 18 * self.em  # 15 ems wide
-        self.settings_panel = Open3dPropertyPanel(self.window)
-        self.settings_panel.data_context = pipeline
-        self.window.add_child(self.settings_panel)
+        self.config.stream_name.on_changed += on_stream_name_changed
+        self.config.stream_name.fire_latest()
 
         # used for colorized preview
         self.colorizer = rs.colorizer()
         self.colorizer.set_option(rs.option.histogram_equalization_enabled, 0)
         self.colorizer.set_option(rs.option.color_scheme, 9.0)
-        self.colorizer.set_option(rs.option.min_distance, self.pipeline.min_distance.value)
-        self.colorizer.set_option(rs.option.max_distance, self.pipeline.max_distance.value)
-
-        # info panel
-        self.settings_panel.add_child(gui.Label("View Parameter"))
+        self.colorizer.set_option(rs.option.min_distance, self.config.min_distance.value)
+        self.colorizer.set_option(rs.option.max_distance, self.config.max_distance.value)
 
-        self.display_depth_map = gui.Checkbox("Display Depth Map")
-        self.display_depth_map.checked = False
-        self.settings_panel.add_child(self.display_depth_map)
-
-        if args.view_3d:
-            self.display_16_bit = gui.Checkbox("Display 16bit")
-            self.display_16_bit.checked = True
-            self.settings_panel.add_child(self.display_16_bit)
-
-            self.render_3d_view = gui.Checkbox("Render 3D")
-            self.render_3d_view.checked = True
-            self.settings_panel.add_child(self.render_3d_view)
-
-            self.settings_panel.add_child(gui.Label("PCL Stride"))
-            self.pcl_stride = gui.Slider(gui.Slider.INT)
-            self.pcl_stride.set_limits(1, 10)
-            self.pcl_stride.int_value = 4
-            self.settings_panel.add_child(self.pcl_stride)
-
-            self.settings_panel.add_child(gui.Label("Point Size"))
-            self.point_size = gui.Slider(gui.Slider.INT)
-            self.point_size.set_limits(1, 10)
-            self.point_size.int_value = 2
-            self.settings_panel.add_child(self.point_size)
-
-            def on_point_size_changed(size):
-                if self.pipeline_view is None:
-                    return
-                self.pipeline_view.pcd_material.point_size = int(size * self.window.scaling)
-
-            self.point_size.set_on_value_changed(on_point_size_changed)
-
-        if isinstance(pipeline.input, vg.RealSenseInput) and pipeline.input.input_bag_file is not None:
+        if isinstance(self.graph.input, vg.RealSenseInput) and self.graph.input.input_bag_file is not None:
             self.settings_panel.add_child(gui.Label("RealSense"))
 
             self.play_bag = gui.Checkbox("Play")
             self.play_bag.checked = True
             self.settings_panel.add_child(self.play_bag)
 
             def on_play_bag_changed(value):
-                if not isinstance(pipeline.input, vg.RealSenseInput):
+                if not isinstance(self.graph.input, vg.RealSenseInput):
                     return
 
-                if pipeline.input.device is None:
+                if self.graph.device is None:
                     return
 
-                playback: rs.playback = pipeline.input.profile.get_device().as_playback()
+                playback: rs.playback = self.graph.input.profile.get_device().as_playback()
 
                 if value:
                     playback.resume()
                 else:
                     playback.pause()
 
             self.play_bag.set_on_checked(on_play_bag_changed)
 
         separation_height = int(round(0.5 * self.em))
 
         self.none_image = o3d.geometry.Image(np.zeros(shape=(1, 1, 3), dtype="uint8"))
 
-        # preview
-        self.rgb_widget = gui.ImageWidget(self.none_image)
-        self.window.add_child(self.rgb_widget)
-
         # hook to events
-        self.pipeline.on_frame_ready = self.on_frame_ready
-        self.pipeline.on_exception = self._on_pipeline_exception
-        self.pipeline.disable_preview.on_changed += self._disable_preview_changed
-        self.pipeline.disable_preview.fire_latest()
+        self.graph.on_frame_ready = self.on_frame_ready
+        self.graph.on_exception = self._on_pipeline_exception
 
-        signal.signal(signal.SIGINT, self._signal_handler)
+        self.config.disable_preview.on_changed += self._disable_preview_changed
+        self.config.disable_preview.fire_latest()
 
-        # pipeline
-        self.pipeline_view: Optional[PipelineView] = None
-
-        if args.view_3d:
-            self.pipeline_view = PipelineView(60, 640 * 480, self.window, on_window_close=self._on_close)
-            self.pipeline_view.window = self.window
-            self.window.add_child(self.pipeline_view.pcdview)
+        signal.signal(signal.SIGINT, self._signal_handler)
 
         self.restart_pipeline_button = gui.Button("Restart Pipeline")
         self.restart_pipeline_button.set_on_clicked(self._on_restart_clicked)
         self.settings_panel.add_child(self.restart_pipeline_button)
-        self.window.add_child(self.settings_panel)
 
-        # start pipeline
-        pipeline.fbs_client.setup()
-        pipeline.open()
+        # graph indicator
+        self.graph_indicator = gui.Horiz(margins=gui.Margins(8, 8, 8, 8))
+        self.graph_indicator.add_stretch()
+        self.indicator_label = gui.Label("Starting up...")
+        self.graph_indicator.add_child(self.indicator_label)
+        self.graph_indicator.add_stretch()
+
+        self.online_color = gui.Color(0.2, 0.6, 0.2, 1.0)
+        self.warning_color = gui.Color(0.6, 0.6, 0.2, 1.0)
+        self.offline_color = gui.Color(0.6, 0.2, 0.2, 1.0)
+
+        self.indicator_size = self.em * 2
+        self.window.add_child(self.graph_indicator)
+
+        self.watch_dog = WatchDog()
+        self.watch_dog.health.on_changed += self._on_health_update
+
+        self.window.set_on_tick_event(self._on_tick)
+
+    def _on_health_update(self, status: HealthStatus):
+        logging.warning(f"Health changed to: {status.name}")
+
+        def _update():
+            self.indicator_label.text = status.name
+
+            if status == HealthStatus.Online:
+                self.graph_indicator.background_color = self.online_color
+            elif status == HealthStatus.Warning:
+                self.graph_indicator.background_color = self.warning_color
+            elif status == HealthStatus.Offline:
+                self.graph_indicator.background_color = self.offline_color
+
+        self.invoke_on_gui(_update)
+
+    def _on_tick(self) -> bool:
+        self.watch_dog.update()
+        return True
+
+    def _on_layout(self, layout_context: gui.LayoutContext):
+        content_rect = self.window.content_rect
+
+        self.graph_indicator.frame = gui.Rect(content_rect.x, content_rect.y,
+                                              content_rect.width - self.settings_panel_width,
+                                              self.indicator_size)
+
+        self.image_view.frame = gui.Rect(content_rect.x, content_rect.y + self.indicator_size,
+                                         content_rect.width - self.settings_panel_width,
+                                         content_rect.height)
+
+        self.settings_panel.frame = gui.Rect(self.image_view.frame.get_right(),
+                                             content_rect.y, self.settings_panel_width,
+                                             content_rect.height)
 
     def _signal_handler(self, signal, frame):
         self.window.close()
 
     def _on_restart_clicked(self):
-        self.pipeline.close()
-        self.pipeline.fbs_client.release()
-
-        self.pipeline.fbs_client.setup()
-        self.pipeline.open()
+        self.graph.close()
+        self.graph.open()
 
     def _on_pipeline_exception(self, pipeline, ex):
         # display error message in console
         logging.warning("".join(traceback.TracebackException.from_exception(ex).format()))
 
-    def _on_layout(self, layout_context):
+    def _on_layout_unused(self, layout_context):
         content_rect = self.window.content_rect
         pcb_view_height = 0
 
         if self.pipeline_view is not None:
             pcb_view_height = content_rect.height // 2
 
             self.pipeline_view.pcdview.frame = gui.Rect(content_rect.x, content_rect.y,
                                                         content_rect.width - self.settings_panel_width,
                                                         pcb_view_height)
 
-        self.rgb_widget.frame = gui.Rect(content_rect.x, pcb_view_height,
+        self.image_view.frame = gui.Rect(content_rect.x, pcb_view_height,
                                          content_rect.width - self.settings_panel_width,
                                          content_rect.height - pcb_view_height)
 
-        self.settings_panel.frame = gui.Rect(self.rgb_widget.frame.get_right(),
+        self.settings_panel.frame = gui.Rect(self.image_view.frame.get_right(),
                                              content_rect.y, self.settings_panel_width,
                                              content_rect.height)
 
-    def _on_close(self):
-        self.pipeline.fbs_client.release()
-        self.pipeline.close()
-        gui.Application.instance.quit()
-
     @staticmethod
     def _create_preview_parameter(name: str, value: str) -> gui.Horiz:
         container = gui.Horiz()
 
         container.add_child(gui.Label(name))
         value_edit = gui.TextEdit()
         value_edit.text_value = value
@@ -187,105 +169,55 @@
             value_edit.text_value = value
 
         value_edit.set_on_text_changed(on_value_changed)
 
         return container
 
     def on_frame_ready(self, frame: np.ndarray):
+        self.watch_dog.reset()
+
+        if self.config.disable_preview.value:
+            return
+
         bgrd = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
         preview_image = bgrd
 
-        if self.display_depth_map.checked:
-            if isinstance(self.pipeline.input, vg.DepthBuffer):
-                if isinstance(self.pipeline.input, vg.RealSenseInput):
-                    self.colorizer.set_option(rs.option.min_distance, self.pipeline.min_distance.value)
-                    self.colorizer.set_option(rs.option.max_distance, self.pipeline.max_distance.value)
-                    colorized_frame = self.colorizer.colorize(self.pipeline.input.depth_frame)
+        if self.config.display_vertical_stack.value:
+            h, w = preview_image.shape[:2]
+            hw = w // 2
+            depth_roi = preview_image[0:h, 0:hw]
+            color_roi = preview_image[0:h, hw:w]
+            preview_image = np.vstack((color_roi, depth_roi))
+
+        if self.config.display_depth_map.value:
+            if isinstance(self.graph.input, vg.DepthBuffer):
+                if isinstance(self.graph.input, vg.RealSenseInput):
+                    self.colorizer.set_option(rs.option.min_distance, self.config.min_distance.value)
+                    self.colorizer.set_option(rs.option.max_distance, self.config.max_distance.value)
+                    colorized_frame = self.colorizer.colorize(self.graph.input.depth_frame)
                     preview_image = np.asanyarray(colorized_frame.get_data())
                 else:
-                    preview_image = self.pipeline.input.depth_map
+                    preview_image = self.graph.input.depth_map
 
-        if self.pipeline.record.value:
-            preview_image = bgrd.copy()
+        if self.config.record.value:
+            preview_image = preview_image.copy()
             h, w = preview_image.shape[:2]
             cv2.circle(preview_image, (w - 25, 25), 15, (255, 0, 0), -1)
 
         image = o3d.geometry.Image(preview_image)
 
-        h, tw = bgrd.shape[:2]
-        w = tw // 2
-
-        if self.pipeline_view is not None and self.render_3d_view.checked:
-            self.pipeline_view.max_pcd_vertices = h * w
-            self.create_3d_cloud(bgrd)
-
         def update():
             # send stream
-            self.pipeline.fbs_client.send(bgrd)
+            self.graph.fbs_client.send(bgrd)
 
             # update image
-            self.rgb_widget.update_image(image)
+            self.image_view.update_image(image)
 
         gui.Application.instance.post_to_main_thread(self.window, update)
 
-    def create_3d_cloud(self, frame):
-        if not isinstance(self.pipeline.input, vg.BaseDepthCamera):
-            return
-
-        h, tw = frame.shape[:2]
-        w = tw // 2
-
-        # settings
-        # read necessary data for visualisation
-        extrinsics = o3d.core.Tensor.eye(4, dtype=o3d.core.Dtype.Float32)
-
-        intrinsic_matrix = o3d.core.Tensor(
-            self.pipeline.input.camera_matrix,
-            dtype=o3d.core.Dtype.Float32)
-        depth_max = self.pipeline.max_distance.value  # m
-        pcd_stride = self.pcl_stride.int_value  # downsample point cloud, may increase frame rate
-        flag_normals = False
-        depth_scale = 1000
-
-        # split image / and visualise
-        color = np.copy(frame[0:h, w:w + w])
-        depth = np.copy(frame[0:h, 0:w])
-
-        # decode
-        min_value = round(self.pipeline.min_distance.value / self.pipeline.depth_units)
-        max_value = round(self.pipeline.max_distance.value / self.pipeline.depth_units)
-
-        if isinstance(self.pipeline.depth_codec, LinearCodec):
-            depth = self.pipeline.depth_codec.decode(depth, min_value, max_value,
-                                                     decode_8bit=not self.display_16_bit.checked)
-        else:
-            depth = self.pipeline.depth_codec.decode(depth, min_value, max_value)
-
-        depth = cv2.cvtColor(depth, cv2.COLOR_GRAY2RGB)
-
-        color_frame = o3d.t.geometry.Image(color)
-        depth_frame = o3d.t.geometry.Image(depth)
-
-        rgbd_image = o3d.t.geometry.RGBDImage(color_frame, depth_frame, True)
-        pcd = o3d.t.geometry.PointCloud.create_from_rgbd_image(rgbd_image, intrinsic_matrix, extrinsics,
-                                                               depth_scale, depth_max,
-                                                               pcd_stride, flag_normals)
-
-        frame_elements = {
-            'color': None,
-            'depth': None,
-            'pcd': pcd,
-            'status_message': ""
-        }
-
-        def update():
-            self.pipeline_view.update(frame_elements)
-
-        gui.Application.instance.post_to_main_thread(self.pipeline_view.window, update)
-
     def _disable_preview_changed(self, is_disabled: bool):
         if is_disabled:
             self.display_info("Preview Disabled")
 
     def display_info(self, text: str,
                      text_color: Sequence[int] = (255, 255, 255),
                      background_color: Sequence[int] = (0, 0, 0)):
@@ -304,10 +236,10 @@
 
         # add text centered on image
         cv2.putText(img, text, (text_x, text_y), font, 1, text_color, 2)
 
         image = o3d.geometry.Image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
 
         def update():
-            self.rgb_widget.update_image(image)
+            self.image_view.update_image(image)
 
         gui.Application.instance.post_to_main_thread(self.window, update)
```

## Comparing `spacestream/SpaceStreamPipeline.py` & `spacestream/SpaceStreamGraph.py`

 * *Files 23% similar despite different names*

```diff
@@ -6,122 +6,125 @@
 from pathlib import Path
 from typing import Callable, Optional, List
 
 import cv2
 import numpy as np
 import pyrealsense2 as rs
 import visiongraph as vg
-from duit.model.DataField import DataField
-import duit.ui as dui
+from duit.utils.name_reference import create_name_reference
 
+from spacestream.SpaceStreamConfig import SpaceStreamConfig
 from spacestream.codec.DepthCodec import DepthCodec
-from spacestream.codec.DepthCodecType import DepthCodecType
 from spacestream.codec.InverseHueColorization import InverseHueColorization
 from spacestream.codec.RealSenseColorizer import RealSenseColorizer
-from spacestream.fbs import FrameBufferSharingServer
+from spacestream.fbs.FrameBufferSharingServer import FrameBufferSharingServer
 from spacestream.io.EnhancedJSONEncoder import EnhancedJSONEncoder
 from spacestream.io.StreamInformation import StreamInformation, StreamSize, Vector2, RangeValue
+from spacestream.nodes.ImageRectificationNode import ImageRectificationNode
 
 
 def linear_interpolate(x):
     return x
 
 
 def ease_out_quad(x):
     # decode = sqrt(x)
     return x * (2 - x)
 
 
-class SpaceStreamPipeline(vg.BaseGraph):
+class SpaceStreamGraph(vg.VisionGraph):
 
-    def __init__(self, stream_name: str, input: vg.BaseInput, fbs_client: FrameBufferSharingServer,
-                 codec: DepthCodecType = DepthCodecType.UniformHue,
-                 min_distance: float = 0, max_distance: float = 6,
-                 record: bool = False, masking: bool = False,
-                 segnet: Optional[vg.InstanceSegmentationEstimator] = None, use_midas: bool = False,
-                 multi_threaded: bool = False, handle_signals: bool = True):
-        super().__init__(multi_threaded, False, handle_signals)
-
-        self.stream_name = stream_name
-        self.input = input
-        self.fbs_client = fbs_client
+    def __init__(self, config: SpaceStreamConfig,
+                 input_node: vg.BaseInput,
+                 segnet: Optional[vg.InstanceSegmentationEstimator] = None,
+                 multi_threaded: bool = False, daemon: bool = True, handle_signals: bool = True):
+        super().__init__(input_node,
+                         name=f"SpaceStream",
+                         multi_threaded=multi_threaded,
+                         daemon=daemon,
+                         handle_signals=handle_signals)
+
+        self.config = config
+
+        self.input = input_node
         self.fps_tracer = vg.FPSTracer()
+        self.fbs_client = FrameBufferSharingServer.create(config.stream_name.value)
+
+        self.rectifier: Optional[ImageRectificationNode] = None
+        if isinstance(self.input, vg.BaseCamera):
+            self.rectifier = ImageRectificationNode(self.input)
+            self.add_nodes(self.rectifier)
+
+        def on_stream_name_changed(new_stream_name: str):
+            if self.fbs_client is None:
+                return
+
+            logging.info("changing stream name...")
+            try:
+                self.fbs_client.release()
+            except Exception as ex:
+                logging.warning(f"Could not release fbs client: {ex}")
+            self.fbs_client = FrameBufferSharingServer.create(new_stream_name)
+            self.fbs_client.setup()
+            logging.info(f"stream name changed to {new_stream_name}")
+
+        self.config.stream_name.on_changed += on_stream_name_changed
 
         self.depth_units: float = 0.001
 
         self._intrinsic_update_requested = True
 
-        # options
-        self.pipeline_fps = DataField("-") | dui.Text("Pipeline FPS", readonly=True)
-        self.encoding_time = DataField("-") | dui.Text("Encoding Time", readonly=True)
-        self.disable_preview = DataField(False) | dui.Boolean("Disable Preview")
-        self.record = DataField(record) | dui.Boolean("Record")
-
-        self.serial_number = DataField("-")
-        self.intrinsics_res = DataField("-")
-        self.intrinsics_principle = DataField("-")
-        self.intrinsics_focal = DataField("-")
-        self.normalize_intrinsics = DataField(True)
-
         self.stream_information = StreamInformation()
 
         def _request_intrinsics_update(value: bool):
             self._intrinsic_update_requested = True
 
-        if isinstance(input, vg.BaseDepthCamera):
-            self.serial_number | dui.Text("Serial", readonly=True, copy_content=True)
-            self.intrinsics_res | dui.Text("Resolution", readonly=True, copy_content=True)
-            self.intrinsics_principle | dui.Text("Principle Point", readonly=True, copy_content=True)
-            self.intrinsics_focal | dui.Text("Focal Point", readonly=True, copy_content=True)
-            self.normalize_intrinsics | dui.Boolean("Normalize Intrinsics")
-            self.normalize_intrinsics.on_changed += _request_intrinsics_update
-
-        self.depth_codec: DepthCodec = codec.value()
-        self.codec = DataField(codec) | dui.Enum("Codec")
-        self.min_distance = DataField(float(min_distance)) | dui.Number("Min Distance")
-        self.max_distance = DataField(float(max_distance)) | dui.Number("Max Distance")
+        self.config.normalize_intrinsics.on_changed += _request_intrinsics_update
+        self.depth_codec: DepthCodec = self.config.codec.value.value()
 
         def codec_changed(c):
             self.depth_codec = c.value()
 
-        self.codec.on_changed += codec_changed
+        self.config.codec.on_changed += codec_changed
 
         self.recorder: Optional[vg.VidGearVideoRecorder] = None
         self.crf: int = 23
 
         self.show_preview = True
-
-        self.masking = masking
         self.segmentation_network: Optional[vg.InstanceSegmentationEstimator] = None
 
-        if self.masking:
+        if segnet is not None:
             self.segmentation_network = segnet
             if isinstance(self.segmentation_network, vg.MediaPipePoseEstimator):
                 self.segmentation_network.enable_segmentation = True
             self.add_nodes(self.segmentation_network)
 
-        self.use_midas = use_midas
+        # todo: enable midas again - currently it is disabled
+        self.use_midas = False
         self.midas_net: Optional[vg.MidasDepthEstimator] = None
 
         if self.use_midas:
             self.midas_net = vg.MidasDepthEstimator.create(vg.MidasConfig.MidasSmall)
             self.midas_net.prediction_bit_depth = 16
             self.add_nodes(self.midas_net)
 
         # events
         self.on_frame_ready: Optional[Callable[[np.ndarray], None]] = None
 
         # time
         self.encoding_watch = vg.ProfileWatch()
 
-        self.add_nodes(self.input)
+        self.add_nodes(self.fbs_client)
+
+        if isinstance(self.input, vg.BaseCamera):
+            self._setup_camera_settings(self.input)
 
     def _update_intrinsics(self, frame: np.ndarray) -> bool:
         h, w = frame.shape[:2]
-        self.intrinsics_res.value = f"{w} x {h}"
+        self.config.intrinsics_res.value = f"{w} x {h}"
 
         if isinstance(self.input, vg.BaseDepthCamera):
             try:
                 intrinsics = self.input.camera_matrix
             except Exception as ex:
                 print(f"Intrinsics could not be read: {ex}")
                 return False
@@ -131,86 +134,90 @@
 
             fx = intrinsics[0, 0]
             fy = intrinsics[1, 1]
 
             pp_str = f"{ppx:.2f} / {ppy:.2f}"
             f_str = f"{fx:.2f} / {fy:.2f}"
 
-            if self.normalize_intrinsics.value:
+            if self.config.normalize_intrinsics.value:
                 ppx /= w
                 ppy /= h
                 fx /= w
                 fy /= h
 
                 pp_str = f"{ppx:.4f} / {ppy:.4f}"
                 f_str = f"{fx:.4f} / {fy:.4f}"
 
-            self.intrinsics_principle.value = pp_str
-            self.intrinsics_focal.value = f_str
+            self.config.intrinsics_principle.value = pp_str
+            self.config.intrinsics_focal.value = f_str
 
             self.stream_information.serial = self.input.serial
             self.stream_information.resolution = StreamSize(w, h)
             self.stream_information.intrinsics.principle = Vector2(ppx, ppy)
             self.stream_information.intrinsics.focal = Vector2(fx, fy)
-            self.stream_information.distance = RangeValue(self.min_distance.value, self.max_distance.value)
+            self.stream_information.distance = RangeValue(self.config.min_distance.value,
+                                                          self.config.max_distance.value)
         else:
-            self.intrinsics_principle.value = "-"
-            self.intrinsics_focal.value = "-"
+            self.config.intrinsics_principle.value = "-"
+            self.config.intrinsics_focal.value = "-"
 
         return True
 
     def _init(self):
         super()._init()
 
         if threading.current_thread() is threading.main_thread():
             self.fbs_client.setup()
 
         if isinstance(self.input, vg.BaseDepthCamera):
-            self.serial_number.value = self.input.serial
+            self.config.serial_number.value = self.input.serial
             logging.info(f"Device Serial: {self.input.serial}")
 
         # set colorizer min and max settings
         if isinstance(self.input, vg.RealSenseInput):
             if not self.use_midas:
                 self.input.colorizer.set_option(rs.option.histogram_equalization_enabled, 0)
-                self.input.colorizer.set_option(rs.option.min_distance, self.min_distance.value)
-                self.input.colorizer.set_option(rs.option.max_distance, self.max_distance.value)
+                self.input.colorizer.set_option(rs.option.min_distance, self.config.min_distance.value)
+                self.input.colorizer.set_option(rs.option.max_distance, self.config.max_distance.value)
 
         if isinstance(self.input, vg.AzureKinectInput):
             from pyk4a import CalibrationType
 
             calibration = self.input.device.calibration
             mat = calibration.get_camera_matrix(CalibrationType.DEPTH)
             print(mat)
 
+        if isinstance(self.input, vg.BaseCamera):
+            self._apply_camera_settings(self.input)
+
     def _process(self):
         ts, frame = self.input.read()
 
         if frame is None:
             return
 
         # start recording
-        if self.record.value and self.recorder is None:
+        if self.config.record.value and self.recorder is None:
             time_str = datetime.now().strftime("%y-%m-%d-%H-%M-%S")
-            output_file_path = f"recordings/{self.stream_name}-{time_str}.mp4"
+            output_file_path = f"recordings/{self.config.stream_name.value}-{time_str}.mp4"
             self.recorder = vg.VidGearVideoRecorder(output_file_path, fps=self.input.fps)
             self.recorder.output_params.update({
                 "-crf": self.crf,
                 "-input_framerate": round(self.fps_tracer.smooth_fps)
             })
             self.recorder.open()
 
             # write recording parameters
             with open(Path(output_file_path).with_suffix(".json"), "w") as f:
                 json.dump(self.stream_information, f, cls=EnhancedJSONEncoder, indent=4)
-        elif not self.record.value and self.recorder is not None:
+        elif not self.config.record.value and self.recorder is not None:
             self.recorder.close()
             self.recorder = None
 
-        if self.masking:
+        if self.config.masking.value:
             segmentations: List[vg.InstanceSegmentationResult] = self.segmentation_network.process(frame)
             for segment in segmentations:
                 frame = self.mask_image(frame, segment.mask)
 
         if isinstance(self.input, vg.BaseDepthInput):
             if isinstance(self.input, vg.RealSenseInput):
                 self.depth_units = self.input.depth_frame.get_units()
@@ -222,48 +229,53 @@
 
             depth = depth_buffer.depth_buffer
 
             if self.use_midas:
                 depth = pow(2, 16) - depth
 
             # check pre-conditions (move them to the changing side)
-            if isinstance(self.depth_codec, InverseHueColorization) and self.min_distance.value <= 0.0:
+            if isinstance(self.depth_codec, InverseHueColorization) and self.config.min_distance.value <= 0.0:
                 logging.warning("Inverse Hue Colorization needs min-range to be higher than 0.0")
-                self.min_distance.value = 0.1
+                self.config.min_distance.value = 0.1
 
-            if self.min_distance.value < 0.0:
-                self.min_distance.value = 0.0
+            if self.config.min_distance.value < 0.0:
+                self.config.min_distance.value = 0.0
 
-            if self.max_distance.value == 0.0:
-                self.max_distance.value = 0.1
+            if self.config.max_distance.value == 0.0:
+                self.config.max_distance.value = 0.1
 
-            if self.min_distance.value >= self.max_distance.value:
-                self.min_distance.value = self.max_distance.value - 0.1
+            if self.config.min_distance.value >= self.config.max_distance.value:
+                self.config.min_distance.value = self.config.max_distance.value - 0.1
 
             # read depth map and create rgb-d image
-            min_value = round(self.min_distance.value / self.depth_units)
-            max_value = round(self.max_distance.value / self.depth_units)
+            min_value = round(self.config.min_distance.value / self.depth_units)
+            max_value = round(self.config.max_distance.value / self.depth_units)
 
-            self.encoding_watch.start()
             if isinstance(self.input, vg.RealSenseInput) and isinstance(self.depth_codec, RealSenseColorizer):
-                depth_map = self.depth_codec.encode(self.input.depth_frame, min_value, max_value)
-            else:
-                depth_map = self.depth_codec.encode(depth, min_value, max_value)
+                depth = self.input.depth_frame
+
+            # rectify image if necessary
+            if self.config.depth_rectification.value and self.rectifier is not None:
+                depth = self.rectifier.process(depth)
+                frame = self.rectifier.process(frame)
+
+            self.encoding_watch.start()
+            depth_map = self.depth_codec.encode(depth, min_value, max_value)
             self.encoding_watch.stop()
 
             # fix realsense image if it has been aligned to remove lines
             if isinstance(self.input, vg.RealSenseInput):
                 depth_map = cv2.medianBlur(depth_map, 3)
 
             # resize to match rgb image if necessary
             if depth_map.shape != frame.shape:
                 h, w = frame.shape[:2]
                 depth_map = cv2.resize(depth_map, (w, h), interpolation=cv2.INTER_AREA)
 
-            if self.masking:
+            if self.config.masking.value:
                 for segment in segmentations:
                     depth_map = self.mask_image(depth_map, segment.mask)
 
             rgbd = np.hstack((depth_map, frame))
         else:
             # just send rgb image for testing
             rgbd = frame
@@ -273,36 +285,102 @@
             self._intrinsic_update_requested = not success
 
         if threading.current_thread() is threading.main_thread():
             # send rgb-d over spout
             bgrd = cv2.cvtColor(rgbd, cv2.COLOR_RGB2BGR)
             self.fbs_client.send(bgrd)
 
-        if self.record and self.recorder is not None:
+        if self.config.record.value and self.recorder is not None:
             self.recorder.add_image(rgbd)
 
-        if not self.disable_preview.value and self.on_frame_ready is not None:
+        if not self.config.disable_preview.value and self.on_frame_ready is not None:
             self.on_frame_ready(rgbd)
         else:
+            if self.on_frame_ready is not None:
+                self.on_frame_ready(rgbd)
+
             bgrd = cv2.cvtColor(rgbd, cv2.COLOR_RGB2BGR)
             self.fbs_client.send(bgrd)
 
         self.fps_tracer.update()
-        self.pipeline_fps.value = f"{self.fps_tracer.fps:.2f}"
+        self.config.pipeline_fps.value = f"{self.fps_tracer.fps:.2f}"
 
-        self.encoding_time.value = f"{self.encoding_watch.average():.2f} ms"
+        self.config.encoding_time.value = f"{self.encoding_watch.average():.2f} ms"
 
     def _release(self):
         if threading.current_thread() is threading.main_thread():
             self.fbs_client.release()
 
         super()._release()
-        if self.record.value and self.recorder is not None:
+        if self.config.record.value and self.recorder is not None:
             self.recorder.close()
 
+    def _setup_camera_settings(self, cam: vg.BaseCamera):
+        cam_ref = create_name_reference(cam)
+
+        def _on_auto_exposure_change(on: bool):
+            if self.config.is_loading:
+                return
+
+            try:
+                cam.enable_auto_exposure = on
+
+                if not on:
+                    cam.exposure = int(self.config.cam_exposure.value * 1000)
+                    cam.gain = int(self.config.cam_iso.value)
+            except AttributeError:
+                pass
+
+        def _on_auto_white_balance_change(on: bool):
+            if self.config.is_loading:
+                return
+
+            try:
+                cam.enable_auto_white_balance = on
+
+                if not on:
+                    cam.white_balance = int(self.config.cam_white_balance.value)
+            except AttributeError:
+                pass
+
+        def _on_exposure_change(value: int):
+            if self.config.is_loading:
+                return
+
+            self.config.cam_auto_exposure.value = False
+
+            try:
+                cam.exposure = int(value * 1000)
+            except Exception as ex:
+                logging.warning(f"Could not set exposure ({value}): {ex}")
+
+        def _on_white_balance_change(value: int):
+            if self.config.is_loading:
+                return
+
+            self.config.cam_auto_white_balance.value = False
+
+            try:
+                cam.white_balance = int(value)
+            except Exception as ex:
+                logging.warning(f"Could not set white-balance ({value}): {ex}")
+
+        self.config.cam_auto_exposure.on_changed += _on_auto_exposure_change
+        self.config.cam_exposure.on_changed += _on_exposure_change
+
+        self.config.cam_auto_white_balance.on_changed += _on_auto_white_balance_change
+        self.config.cam_white_balance.on_changed += _on_white_balance_change
+
+        self.config.cam_iso.bind_to_attribute(cam, cam_ref.gain, lambda x: int(x))
+
+    def _apply_camera_settings(self, cam: vg.BaseCamera):
+        self.config.cam_iso.fire()
+        self.config.cam_auto_exposure.fire()
+        self.config.cam_auto_white_balance.fire()
+
     @staticmethod
     def mask_image(image: np.ndarray, mask: np.ndarray) -> np.ndarray:
         masked = cv2.bitwise_and(image, image, mask=mask)
         return masked
 
     @staticmethod
     def add_params(parser: argparse.ArgumentParser):
```

## Comparing `space_stream-0.1.7.2.dist-info/METADATA` & `space_stream-0.2.0.dist-info/METADATA`

 * *Files 11% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 Metadata-Version: 2.1
 Name: space-stream
-Version: 0.1.7.2
+Version: 0.2.0
 Summary: Send RGB-D images over spout / syphon with visiongraph.
 Home-page: https://github.com/cansik/space-stream
 Author: Florian Bruggisser
 Author-email: github@broox.ch
 License: MIT License
 Platform: UNKNOWN
 Description-Content-Type: text/markdown
+License-File: LICENSE
 Requires-Dist: configargparse
 Requires-Dist: pyopengl
-Requires-Dist: visiongraph[azure,media,mediapipe,numba,onnx,realsense] (==0.1.47.4)
-Requires-Dist: protobuf (<=3.20.1)
-Requires-Dist: numpy (<=1.22.4)
-Requires-Dist: open3d (==0.17.0)
-Requires-Dist: duit[all] (==0.1.7.2)
+Requires-Dist: visiongraph[azure,media,mediapipe,numba,onnx,realsense] ~=0.1.52.0
+Requires-Dist: protobuf ~=3.20.0
+Requires-Dist: numpy ~=1.24.4
+Requires-Dist: visiongraph-ui ~=0.2.1
+Requires-Dist: open3d ~=0.17.0
+Requires-Dist: duit[all] ~=0.1.9.0
 Requires-Dist: syphonpy ; platform_system == "Darwin"
 Requires-Dist: glfw ; platform_system == "Darwin"
-Requires-Dist: SpoutGL (>=0.0.4) ; platform_system == "Windows"
+Requires-Dist: SpoutGL ~=0.0.4 ; platform_system == "Windows"
 
 # Space Stream [![PyPI](https://img.shields.io/pypi/v/space-stream)](https://pypi.org/project/space-stream/)
 Send RGB-D images over spout / syphon with visiongraph.
 
 ![Example Map](images/space-stream-ui.jpg)
 *Source: Intel RealSense [Sample Data](https://github.com/IntelRealSense/librealsense/blob/master/doc/sample-data.md)*
 
@@ -29,27 +31,39 @@
 It is recommended to use `Python 3.8`, `Python 3.9` or `Python 3.10` and should run on any OS. First create a new [virtualenv](https://docs.python.org/3/library/venv.html) and activate it. 
 After that install all dependencies:
 
 ```bash
 pip install space-stream
 ```
 
+#### ZED Camera
+To be able to use a ZED Camera, please follow the tutorial on the [ZED Python API](https://www.stereolabs.com/docs/app-development/python/install/) website.
+
+1. Install the [ZED SDK](https://www.stereolabs.com/developers/release/) (together with CUDA)
+2. Run the command `python "C:\Program Files (x86)\ZED SDK\get_python_api.py"` inside the virtual python environment
+
 ### Usage
 Simply run the `spacestream` module with the following command to run a capturing pipeline (RealSense based). After that you can open a [spout receiver](https://github.com/leadedge/Spout2/releases) / syphon receiver and check the result there.
 
 ```
 space-stream --input realsense
 ```
 
 To use the Azure Kinect use the `azure` input type:
 
 ```
 space-stream --input azure
 ```
 
+### Build
+
+```bash
+python setup.py distribute
+```
+
 ### Development
 To develop with this project, clone the git repository and install the dependencies from the requirements:
 
 ```bash
 pip install -r requirements.txt
 ```
 
@@ -75,27 +89,31 @@
 
 #### Distance Range
 To define the min and max distance to encode, use the `--min-distance` and `--max-distance` parameter.
 
 #### Help
 
 ```
-usage: space-stream [-h] [-c CONFIG]
+usage: space-stream [-h] [-c CONFIG] [-s SETTINGS]
                     [--loglevel {critical,error,warning,info,debug}]
-                    [--input video-capture,image,realsense,azure,camgear]
+                    [--record RECORD]
+                    [--codec Linear, UniformHue, InverseHue, RSColorizer]
+                    [--min-distance MIN_DISTANCE]
+                    [--max-distance MAX_DISTANCE] [--stream-name STREAM_NAME]
+                    [--input video-capture,image,realsense,azure,camgear,zed]
                     [--input-size width height] [--input-fps INPUT_FPS]
                     [--input-rotate 90,-90,180] [--input-flip h,v]
                     [--input-mask INPUT_MASK] [--input-crop x y width height]
                     [--raw-input] [--channel CHANNEL]
                     [--input-skip INPUT_SKIP]
                     [--input-backend any,vfw,v4l,v4l2,firewire,fireware,ieee1394,dc1394,cmu1394,qt,unicap,dshow,pvapi,openni,openni_asus,android,xiapi,avfoundation,giganetix,msmf,winrt,intelperc,openni2,openni2_asus,gphoto2,gstreamer,ffmpeg,images,aravis,opencv_mjpeg,intel_mfx,xine]
                     [-src SOURCE] [--input-path INPUT_PATH]
-                    [--input-delay INPUT_DELAY] [--depth] [--depth-as-input]
-                    [-ir] [--exposure EXPOSURE] [--gain GAIN]
-                    [--white-balance WHITE_BALANCE] [--rs-serial RS_SERIAL]
+                    [--input-delay INPUT_DELAY] [--exposure EXPOSURE]
+                    [--gain GAIN] [--white-balance WHITE_BALANCE] [--depth]
+                    [--depth-as-input] [-ir] [--rs-serial RS_SERIAL]
                     [--rs-json RS_JSON] [--rs-play-bag RS_PLAY_BAG]
                     [--rs-record-bag RS_RECORD_BAG] [--rs-disable-emitter]
                     [--rs-bag-offline]
                     [--rs-auto-exposure-limit RS_AUTO_EXPOSURE_LIMIT]
                     [--rs-auto-gain-limit RS_AUTO_GAIN_LIMIT]
                     [--rs-filter decimation,spatial,temporal,hole-filling [decimation,spatial,temporal,hole-filling ...]]
                     [--rs-color-scheme Jet,Classic,WhiteToBlack,BlackToWhite,Bio,Cold,Warm,Quantized,Pattern]
@@ -107,34 +125,41 @@
                     [--k4a-passive-ir]
                     [--k4a-color-resolution OFF,RES_720P,RES_1080P,RES_1440P,RES_1536P,RES_2160P,RES_3072P]
                     [--k4a-color-format COLOR_MJPG,COLOR_NV12,COLOR_YUY2,COLOR_BGRA32,DEPTH16,IR16,CUSTOM8,CUSTOM16,CUSTOM]
                     [--k4a-wired-sync-mode STANDALONE,MASTER,SUBORDINATE]
                     [--k4a-subordinate-delay-off-master-usec K4A_SUBORDINATE_DELAY_OFF_MASTER_USEC]
                     [--midas] [--mask]
                     [--segnet mediapipe,mediapipe-light,mediapipe-heavy]
-                    [--codec Linear,UniformHue,InverseHue,RSColorizer]
-                    [--min-distance MIN_DISTANCE]
-                    [--max-distance MAX_DISTANCE] [--parallel]
-                    [--num-threads NUM_THREADS] [--no-fastmath]
-                    [--stream-name STREAM_NAME] [--no-filter] [--no-preview]
-                    [--record] [--record-crf RECORD_CRF] [--view-pcd]
-                    [--view-3d]
+                    [--parallel] [--num-threads NUM_THREADS] [--no-fastmath]
+                    [--no-filter] [--no-preview] [--record-crf RECORD_CRF]
+                    [--view-pcd] [--view-3d]
 
 RGB-D framebuffer sharing demo for visiongraph.
 
 options:
   -h, --help            show this help message and exit
   -c CONFIG, --config CONFIG
                         Configuration file path.
+  -s SETTINGS, --settings SETTINGS
+                        Settings file path (json).
   --loglevel {critical,error,warning,info,debug}
                         Provide logging level. Example --loglevel debug,
                         default=warning
+  --record RECORD       Record output into recordings folder.
+  --codec Linear, UniformHue, InverseHue, RSColorizer
+                        Codec how the depth map will be encoded.
+  --min-distance MIN_DISTANCE
+                        Min distance to perceive by the camera.
+  --max-distance MAX_DISTANCE
+                        Max distance to perceive by the camera.
+  --stream-name STREAM_NAME
+                        Spout / Syphon stream name.
 
 input provider:
-  --input video-capture,image,realsense,azure,camgear
+  --input video-capture,image,realsense,azure,camgear,zed
                         Image input provider, default: video-capture.
   --input-size width height
                         Requested input media size.
   --input-fps INPUT_FPS
                         Requested input media framerate.
   --input-rotate 90,-90,180
                         Rotate input media.
@@ -153,23 +178,23 @@
                         VideoCapture API backends identifier., default: any.
   -src SOURCE, --source SOURCE
                         Generic input source for all inputs.
   --input-path INPUT_PATH
                         Path to the input image.
   --input-delay INPUT_DELAY
                         Input delay time (s).
-  --depth               Enable RealSense depth stream.
-  --depth-as-input      Use colored depth stream as input stream.
-  -ir, --infrared       Use infrared as input stream.
   --exposure EXPOSURE   Exposure value (usec) for depth camera input (disables
                         auto-exposure).
   --gain GAIN           Gain value for depth input (disables auto-exposure).
   --white-balance WHITE_BALANCE
                         White-Balance value for depth input (disables auto-
                         white-balance).
+  --depth               Enable RealSense depth stream.
+  --depth-as-input      Use colored depth stream as input stream.
+  -ir, --infrared       Use infrared as input stream.
   --rs-serial RS_SERIAL
                         RealSense serial number to choose specific device.
   --rs-json RS_JSON     RealSense json configuration to apply.
   --rs-play-bag RS_PLAY_BAG
                         Path to a pre-recorded bag file for playback.
   --rs-record-bag RS_RECORD_BAG
                         Path to a bag file to store the current recording.
@@ -211,40 +236,31 @@
   --midas               Use midas for depth capture.
 
 masking:
   --mask                Apply mask by segmentation algorithm.
   --segnet mediapipe,mediapipe-light,mediapipe-heavy
                         Segmentation Network, default: mediapipe.
 
-depth codec:
-  --codec Linear,UniformHue,InverseHue,RSColorizer
-                        Codec how the depth map will be encoded., default:
-                        UniformHue.
-  --min-distance MIN_DISTANCE
-                        Min distance to perceive by the camera.
-  --max-distance MAX_DISTANCE
-                        Max distance to perceive by the camera.
-
 performance:
   --parallel            Enable parallel for codec operations.
   --num-threads NUM_THREADS
                         Number of threads for parallelization.
   --no-fastmath         Disable fastmath for codec operations.
 
-output:
-  --stream-name STREAM_NAME
-                        Spout / Syphon stream name.
-
 debug:
   --no-filter           Disable realsense image filter.
   --no-preview          Disable preview to speed.
-  --record              Record output into recordings folder.
   --record-crf RECORD_CRF
                         Recording compression rate.
   --view-pcd            Display PCB preview (deprecated, use --view-3d).
   --view-3d             Display PCB preview.
+
+Args that start with '--' can also be set in a config file (specified via -c).
+Config file syntax allows: key=value, flag=true, stuff=[a,b,c] (for details,
+see syntax at https://goo.gl/R74nmi). In general, command-line values override
+config file values which override defaults.
 ```
 
 ### About
 Copyright (c) 2023 Florian Bruggisser
```

## Comparing `space_stream-0.1.7.2.dist-info/RECORD` & `space_stream-0.2.0.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,25 +1,30 @@
-spacestream/SpaceStreamPipeline.py,sha256=Eoo-03AhXGHvFkm6TsUsnhkULouah7h1p-tVf0hfkH4,12041
+spacestream/SpaceStreamApp.py,sha256=t2zktYUAwOlvlkiP6CYmNpNW0n63xa0SJMMizd8La-U,890
+spacestream/SpaceStreamConfig.py,sha256=o9rVKozAxVkPhgXm2mrG41MBAmEfZf1KEN3WhD_o8ig,3220
+spacestream/SpaceStreamGraph.py,sha256=Bk-5iuNfyERIjB0_pSMYHup6RF9eoR4UF_PbmWmrpxA,14623
+spacestream/WatchDog.py,sha256=qnYqvLiPeJ_6ocqbDJMz9HuFoQyz2uFzF-MgTpPJ2_A,1295
 spacestream/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-spacestream/__main__.py,sha256=6mG06ziVc6TBhwtFVMUZgypTIYLobDf-f4CMlE4Bd_g,5793
+spacestream/__main__.py,sha256=0zDWRCb3QYITpExhVhd-k8Lj61jBgKxfmomQxCupF0k,5274
 spacestream/codec/DepthCodec.py,sha256=Ho5gpU6g9HwNIO6woc1GgY1PnVl8qNjqRsbzQDm_oSE,1117
 spacestream/codec/DepthCodecType.py,sha256=vwgIN3ycMWSgh9hYO-NFU437R6z86S7NKv8MOU7Z7Kk,714
 spacestream/codec/InverseHueColorization.py,sha256=PF9K40rWA5FpBEn75Xz3NNVdAfs0Gd1ECSdj6tKWA_E,205
 spacestream/codec/LinearCodec.py,sha256=yhs8ABAzxhKSI81Pb7GU5iLrV_6xgvFlBOgLPfVgWgw,1967
 spacestream/codec/RealSenseColorizer.py,sha256=d6x34JXNAULZNkQXcPFYlvQPfLAKvYGBry_4R-zNec4,1069
 spacestream/codec/UniformHueColorization.py,sha256=P3klAuO8pEzZp7xBBCBuXYsevcwx1uyjyU5fCZJtFNs,4670
 spacestream/codec/__init__.py,sha256=RKoiiCfjHH_429tMrX8956hpcpsYNV_GZWZNB7vNUWo,99
 spacestream/fbs/FrameBufferSharingServer.py,sha256=5qhQ-k9A192WXUADxEhkBUEiwJjI4mJGr54p7DxLeIQ,782
 spacestream/fbs/SpoutServer.py,sha256=GmRHY6rXzLcPcOHXMnGJTIv7f7jeQu4sG2tYVL9LJ60,1382
 spacestream/fbs/SyphonServer.py,sha256=cLwZAw6G6JWlLrEqOaVOSKS_rVIXq9aayaMtTzj9tG8,3116
 spacestream/fbs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 spacestream/io/EnhancedJSONEncoder.py,sha256=_nyYum5ZfL4bVADV3MGWgg16KwrcBHyLgO8sjp5vRJI,213
 spacestream/io/StreamInformation.py,sha256=n9uRRVwxd-13mVDziuE6L4icJG1BQNvljuDB-6Nc1fM,531
 spacestream/io/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-spacestream/ui/MainWindow.py,sha256=N92s5_LQhWc6BlGt4wkvztoRaWIuH-tZfapSxxwr4Us,12048
-spacestream/ui/PipelineView.py,sha256=KgExzYLzqjliK__n3gnfw_iYx2PHzbouy9tycuE9TYk,6174
+spacestream/nodes/ImageRectificationNode.py,sha256=exrZzWoAsucQiuXeZKbyRstf1sd3O_wtz3XgPw11-sM,1894
+spacestream/nodes/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+spacestream/ui/MainWindow.py,sha256=gavz4fYpL3frITXDKVpMInECiEAAAS53iuo7u13QhoE,9512
 spacestream/ui/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-space_stream-0.1.7.2.dist-info/METADATA,sha256=PUdOJ6v7FL5XbQXt3EE5GTLjb6M04ssAUWyO3Qo8TE0,12063
-space_stream-0.1.7.2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-space_stream-0.1.7.2.dist-info/entry_points.txt,sha256=uN7dDoiKHwLY96Y2GqmgRqj3Nkn6lmcj1V3QsrgSDVc,60
-space_stream-0.1.7.2.dist-info/top_level.txt,sha256=58PDKqMtgX_60uGduCJAm82jEraaJZ0CGhMoFFftDlo,12
-space_stream-0.1.7.2.dist-info/RECORD,,
+space_stream-0.2.0.dist-info/LICENSE,sha256=KpHMG-i0KjhuOO-CAYnMMI1aQLBi54LarTr5FKT1uwQ,1075
+space_stream-0.2.0.dist-info/METADATA,sha256=Cr5i2EW8zTmVFODUUIR-83Ot9jUzoBM69hfx8ARuZR8,12878
+space_stream-0.2.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+space_stream-0.2.0.dist-info/entry_points.txt,sha256=uN7dDoiKHwLY96Y2GqmgRqj3Nkn6lmcj1V3QsrgSDVc,60
+space_stream-0.2.0.dist-info/top_level.txt,sha256=58PDKqMtgX_60uGduCJAm82jEraaJZ0CGhMoFFftDlo,12
+space_stream-0.2.0.dist-info/RECORD,,
```


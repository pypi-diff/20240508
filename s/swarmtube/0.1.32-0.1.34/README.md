# Comparing `tmp/swarmtube-0.1.32-py2.py3-none-any.whl.zip` & `tmp/swarmtube-0.1.34-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,9 +1,9 @@
-Zip file size: 47165 bytes, number of entries: 46
--rw-rw-r--  2.0 unx     1684 b- defN 24-May-07 05:47 swarmtube/__init__.py
+Zip file size: 47691 bytes, number of entries: 46
+-rw-rw-r--  2.0 unx     1684 b- defN 24-May-08 13:27 swarmtube/__init__.py
 -rw-rw-r--  2.0 unx      665 b- defN 24-May-04 08:01 swarmtube/definitions.py
 -rw-rw-r--  2.0 unx     6691 b- defN 24-May-02 16:20 swarmtube/swarmtube.py
 -rw-rw-r--  2.0 unx     1780 b- defN 24-May-02 16:20 swarmtube/cli/__init__.py
 -rw-rw-r--  2.0 unx     3834 b- defN 24-May-02 16:20 swarmtube/cli/config.py
 -rw-rw-r--  2.0 unx     2279 b- defN 24-May-02 16:20 swarmtube/cli/inventory.py
 -rw-rw-r--  2.0 unx     1577 b- defN 24-May-02 16:20 swarmtube/cli/main.py
 -rw-rw-r--  2.0 unx     1800 b- defN 24-May-02 16:20 swarmtube/cli/publish.py
@@ -19,30 +19,30 @@
 -rw-rw-r--  2.0 unx     7449 b- defN 24-May-02 16:20 swarmtube/helpers/general.py
 -rw-rw-r--  2.0 unx     4264 b- defN 24-May-02 16:20 swarmtube/helpers/loaders.py
 -rw-rw-r--  2.0 unx      725 b- defN 24-May-02 16:20 swarmtube/helpers/models.py
 -rw-rw-r--  2.0 unx     3388 b- defN 24-May-02 16:20 swarmtube/helpers/surreal.py
 -rw-rw-r--  2.0 unx      134 b- defN 24-May-02 16:20 swarmtube/logic/__init__.py
 -rw-rw-r--  2.0 unx     4448 b- defN 24-May-02 16:20 swarmtube/logic/crawler.py
 -rw-rw-r--  2.0 unx      611 b- defN 24-May-02 16:20 swarmtube/logic/inventory.py
--rw-rw-r--  2.0 unx    20338 b- defN 24-May-06 13:30 swarmtube/logic/swarmtube.py
+-rw-rw-r--  2.0 unx    22620 b- defN 24-May-08 13:30 swarmtube/logic/swarmtube.py
 -rw-rw-r--  2.0 unx       24 b- defN 24-May-02 16:20 swarmtube/mappers/__init__.py
 -rw-rw-r--  2.0 unx     1025 b- defN 24-May-02 16:20 swarmtube/mappers/inventory.py
 -rw-rw-r--  2.0 unx       24 b- defN 24-May-02 16:20 swarmtube/models/__init__.py
 -rw-rw-r--  2.0 unx     3568 b- defN 24-May-02 16:20 swarmtube/models/base.py
 -rw-rw-r--  2.0 unx     1887 b- defN 24-May-02 16:20 swarmtube/models/config.py
 -rw-rw-r--  2.0 unx       34 b- defN 24-May-02 16:20 swarmtube/models/enums.py
 -rw-rw-r--  2.0 unx     1886 b- defN 24-May-02 16:20 swarmtube/models/inventory.py
 -rw-rw-r--  2.0 unx     1675 b- defN 24-May-02 16:20 swarmtube/models/script.py
 -rw-rw-r--  2.0 unx     1907 b- defN 24-May-02 16:20 swarmtube/models/swarmtube.py
 -rw-rw-r--  2.0 unx     1649 b- defN 24-May-02 16:20 swarmtube/models/task.py
 -rw-rw-r--  2.0 unx      555 b- defN 24-May-02 16:20 swarmtube/ports/__init__.py
 -rw-rw-r--  2.0 unx     1389 b- defN 24-May-02 16:20 swarmtube/ports/preferences.py
 -rw-rw-r--  2.0 unx      182 b- defN 24-May-02 16:20 swarmtube/storage/__init__.py
 -rw-rw-r--  2.0 unx     1330 b- defN 24-May-02 16:20 swarmtube/storage/surreal.py
--rw-rw-r--  2.0 unx      168 b- defN 24-May-07 05:48 swarmtube-0.1.32.dist-info/AUTHORS.rst
--rw-rw-r--  2.0 unx     1075 b- defN 24-May-07 05:48 swarmtube-0.1.32.dist-info/LICENSE
--rw-rw-r--  2.0 unx     2201 b- defN 24-May-07 05:48 swarmtube-0.1.32.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 24-May-07 05:48 swarmtube-0.1.32.dist-info/WHEEL
--rw-rw-r--  2.0 unx       49 b- defN 24-May-07 05:48 swarmtube-0.1.32.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       10 b- defN 24-May-07 05:48 swarmtube-0.1.32.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3814 b- defN 24-May-07 05:48 swarmtube-0.1.32.dist-info/RECORD
-46 files, 126923 bytes uncompressed, 41119 bytes compressed:  67.6%
+-rw-rw-r--  2.0 unx      168 b- defN 24-May-08 13:32 swarmtube-0.1.34.dist-info/AUTHORS.rst
+-rw-rw-r--  2.0 unx     1075 b- defN 24-May-08 13:32 swarmtube-0.1.34.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     2201 b- defN 24-May-08 13:32 swarmtube-0.1.34.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 24-May-08 13:32 swarmtube-0.1.34.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       49 b- defN 24-May-08 13:32 swarmtube-0.1.34.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       10 b- defN 24-May-08 13:32 swarmtube-0.1.34.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3814 b- defN 24-May-08 13:32 swarmtube-0.1.34.dist-info/RECORD
+46 files, 129205 bytes uncompressed, 41645 bytes compressed:  67.8%
```

## zipnote {}

```diff
@@ -111,29 +111,29 @@
 
 Filename: swarmtube/storage/__init__.py
 Comment: 
 
 Filename: swarmtube/storage/surreal.py
 Comment: 
 
-Filename: swarmtube-0.1.32.dist-info/AUTHORS.rst
+Filename: swarmtube-0.1.34.dist-info/AUTHORS.rst
 Comment: 
 
-Filename: swarmtube-0.1.32.dist-info/LICENSE
+Filename: swarmtube-0.1.34.dist-info/LICENSE
 Comment: 
 
-Filename: swarmtube-0.1.32.dist-info/METADATA
+Filename: swarmtube-0.1.34.dist-info/METADATA
 Comment: 
 
-Filename: swarmtube-0.1.32.dist-info/WHEEL
+Filename: swarmtube-0.1.34.dist-info/WHEEL
 Comment: 
 
-Filename: swarmtube-0.1.32.dist-info/entry_points.txt
+Filename: swarmtube-0.1.34.dist-info/entry_points.txt
 Comment: 
 
-Filename: swarmtube-0.1.32.dist-info/top_level.txt
+Filename: swarmtube-0.1.34.dist-info/top_level.txt
 Comment: 
 
-Filename: swarmtube-0.1.32.dist-info/RECORD
+Filename: swarmtube-0.1.34.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## swarmtube/__init__.py

```diff
@@ -29,8 +29,8 @@
 11. [ ] define test cases (tests/*.py) and get main
 12. [ ] implement automatic test cases
 
 """
 
 __author__ = """Asterio Gonzalez"""
 __email__ = "asterio.gonzalez@gmail.com"
-__version__ = "0.1.32"
+__version__ = "0.1.34"
```

## swarmtube/logic/swarmtube.py

```diff
@@ -9,15 +9,15 @@
 import functools
 from typing import Callable, List, Dict, Any
 from pydantic import BaseModel
 
 from agptools.progress import Progress
 
 from syncmodels.storage import (
-    Storage, 
+    Storage,
     TUBE_SYNC,
     TUBE_META,
     TUBE_WAVE,
     SurrealistStorage,
 )
 
 
@@ -142,16 +142,16 @@
     async def _start_live(self):
         log.info("[%s] _start_live", self.uid)
 
     async def _stop_live(self):
         log.info("[%s] _stop_live", self.uid)
 
     async def _idle(self):
-        log.info("[%s] alive", self.uid)
-        await asyncio.sleep(2)
+        log.debug("[%s] alive", self.uid)
+        await asyncio.sleep(5)
 
 
 class Tube(iAgent):
     """Represents the concept of a stream of events that
     can be located by a UID or searching metadata
     """
 
@@ -265,34 +265,54 @@
             MONOTONIC_KEY: time.time(),  # TODO: let the storage set this value?
             #'uid': uid,
             "payload": self.counter,
         }
         self.storage.put(self.uid, edge)
 
 
+class SwarmTubeException(Exception):
+    "base for all SwarmTube Exceptions"
+
+
+class SkipWave(SwarmTubeException):
+    """the item can't be processed but we
+    need to advance the Wave to the next one
+    """
+
+
+class RetryWave(SwarmTubeException):
+    """the item can't be processed but we
+    need to retry later on, so the Wave
+    doesn't jump to the next one
+    """
+
+
 class Particle(Tube):
     "TBD"
     MAX_EVENTS = 1024
-    buffer = Dict[UID, List[Event]] | None
+    _live = Dict[UID, List[Event]] | None
     _historical = Dict[UID, List[Event]] | None
 
+    RETRY_DELAY = 15
+
     def __init__(self, uid: UID, sources: List[UID], broker: Broker, storage: Storage):
         super().__init__(uid, sources=sources, broker=broker, storage=storage)
         # self.uid = uid
         # self.sources = sources
         # self.broker = broker
         # self.storage = storage
-        self.wave0 = {}
-        self.buffer = {}
+        self._wave = {}
+        self._live = {}
         self._historical = {}
+        self._live_activity = asyncio.Queue()
         self._need_resync = False
         self._milk = []
         self.metrics = Progress()
         for uid in self.sources:
-            self.buffer[uid] = []
+            self._live[uid] = []
             self._historical[uid] = []
 
     async def main(self):
         "TBD"
         self._need_resync = True
         # self.metrics.start()
         while self._need_resync:
@@ -301,15 +321,33 @@
             await self._start_historical()
 
         log.info("=" * 70)
         log.info("[%s] >> Idle", self.uid)
         log.info("=" * 70)
 
         while self.state < ST_STOPPED:
-            await asyncio.sleep(2)
+            try:
+                event = await asyncio.wait_for(self._live_activity.get(), timeout=2.0)
+                _uid, _wave, edge = self.pop_edge(self._live)
+                if edge:
+                    await self.dispatch(edge)
+                    # self._wave[_uid] = _wave
+                else:
+                    assert not all(
+                        self._live.values()
+                    ), "some input must be missing here!"
+                    break
+            except asyncio.TimeoutError:
+                pass  #  No live data has been received
+            except Exception as why:
+                log.error(why)
+                log.error("".join(traceback.format_exception(*sys.exc_info())))
+
+            # self._live_activity.clear()
+
             self.metrics.update(n=0)
 
         await self._stop_live()
 
     async def _start_live(self):
         "TBD"
         log.info("[%s] ++ Requesting LIVE STREAMING", self.uid)
@@ -322,47 +360,49 @@
         for uid in self.sources:
             await self.broker.unsubscribe(uid, self.live)
 
     async def _start_historical(self):
         "TBD"
         self.state = ST_HISTORICAL
         # TODO: review last_waves()
-        self.wave0 = self.storage.last_waves(self.sources, self.uid)
-        #self.wave0 = {uid: _wave for uid in self.sources}
+        self._wave = await self.storage.last_waves(self.sources, self.uid)
+        assert isinstance(self._wave, Dict), "missing await?"
+
+        # self._wave = {uid: _wave for uid in self.sources}
         buffer = self._historical
         self._milk = list(self.sources)
         log.info("-" * 70)
         log.info("[%s] -- Switching to HISTORICAL", self.uid)
         log.info("-" * 70)
         while self.state < ST_LIVE:
             # load some historical data
             n = 0
             while self._milk:
                 uid = self._milk.pop()
-                data = self.storage.since(
-                    uid, self.wave0[uid], max_results=self.MAX_EVENTS
+                data = await self.storage.since(
+                    uid, self._wave[uid], max_results=self.MAX_EVENTS
                 )
                 if data:
                     buffer[uid].extend(data)
                     n += len(data)
-                    self.wave0[uid] = data[-1][MONOTONIC_KEY]
+                    self._wave[uid] = data[-1][MONOTONIC_KEY]
                     break
 
             if n == 0:  # no more historical data
                 # move live data to historical and try to continue until
                 # we get a state with no more historical data and no more live data
                 self.state = ST_SWITCHING
                 # time.sleep(0.9)
-                assert id(buffer) != id(self.buffer)
-                for uid, _buff in self.buffer.items():
+                assert id(buffer) != id(self._live)
+                for uid, _buff in self._live.items():
                     _hist = self._historical[uid]
                     while True:
                         try:
                             candidate = _buff.pop(0)
-                            if candidate[MONOTONIC_KEY] > self.wave0[uid]:
+                            if candidate[MONOTONIC_KEY] > self._wave[uid]:
                                 _hist.append(candidate)
                                 n += 1
                             else:
                                 # this live event has been captured by historical polling
                                 # so is already processed
                                 # print(f"*** already processed: --> {candidate}")
                                 foo = 1
@@ -375,15 +415,15 @@
                     self.metrics.update(n=0, force=True)
                     self.state = ST_LIVE
 
             # try to process buffer
             while self.state < ST_LIVE:
                 _uid, _wave, edge = self.pop_edge(buffer)
                 if edge:
-                    self.dispatch(edge)
+                    await self.dispatch(edge)
                     if not buffer[uid]:  # request more data
                         self._milk.append(_uid)
                 elif self.state == ST_SWITCHING:
                     break
                 else:
                     break  # continue loading more historical
             # check if we have an overflow while processing historical data
@@ -391,46 +431,50 @@
                 log.info(
                     "[%s] *** ---> Request Stopping Streaming due OVERFLOW", self.uid
                 )
                 await self._stop_live()
 
     def live(self, uid: UID, event: Dict):
         "TBD"
-        if len(self.buffer[uid]) >= self.MAX_EVENTS:
+        if len(self._live[uid]) >= self.MAX_EVENTS:
             self._need_resync = True
             return
-        self.buffer[uid].append(event)
+        self._live[uid].append(event)
+        self._live_activity.put_nowait(event)
+
         if self.state == ST_LIVE:
+            pass
             # try process events until the edge is incomplete
-            while True:
-                _uid, _wave, edge = self.pop_edge(self.buffer)
-                if edge:
-                    self.dispatch(edge)
-                    self.wave0[_uid] = _wave
-                else:
-                    assert not all(
-                        self.buffer.values()
-                    ), "some input must be missing here!"
-                    break
-        elif len(self.buffer[uid]) >= self.MAX_EVENTS:
+            # while self.state < ST_STOPPED:
+            # _uid, _wave, edge = self.pop_edge(self._live)
+            # if edge:
+            # self.dispatch(edge)
+            ## self._wave[_uid] = _wave
+            # else:
+            # assert not all(
+            # self._live.values()
+            # ), "some input must be missing here!"
+            # break
+
+        elif len(self._live[uid]) >= self.MAX_EVENTS:
             # request stop streaming and a new re-sync process
             self._need_resync = True
 
     def pop_edge(self, buffer):
         "analyze buffer and return edge if all data is available for processing next step"
         # TODO: implement a policy delegation criteria to know when edge is ready to be processed
         waves = {}
         for uid, data in buffer.items():
             if data:
                 waves[uid] = data[0][MONOTONIC_KEY]
 
         # have we all needed data?
         if len(waves) == len(self.sources):
             _uid, _wave, edge = self._pop_policy(buffer, waves)
-            self.wave0[_uid] = _wave
+            self._wave[_uid] = _wave
             return _uid, _wave, edge
         return None, None, None
 
     def _pop_policy(self, buffer, waves):
         """Default policy is to make the minimal step for computation.
         - get the minimal wave
         - drop the input related with the minimal wave
@@ -444,61 +488,73 @@
                 data.pop(0)
                 _uid = uid
             edge[uid] = item
 
         edge[MONOTONIC_KEY] = _wave
         return _uid, _wave, edge
 
-    def dispatch(self, edge):
+    async def dispatch(self, edge):
         "TBD"
         # build the data to be processed
         # split metadata (".*__" fields by now) and each
         # input stream
         # TODO: review the criteria for keywords filtering
         ikeys = set([k for k in edge if k.endswith("__")])
         ekeys = ikeys.symmetric_difference(edge)
         assert ekeys, "no payload in the edge?"
-        
+
         # set found metadata
         data = {k: edge[k] for k in ikeys}
-
-        log.info(f"[%s] -> dispatch: %s", self.uid, edge)
-
-        try:
-            # do the computation
-            payload = self._compute(edge, ekeys)
-            if payload is None:
-                log.info(f"[%s] <- dispatch: SKIP due NO DATA", self.uid)
-            else:
-                data.update(payload)
-        
-                # store results
-                # and shift sync / waves info
-                self.storage.put(self.uid, data, mode=Storage.MODE_TUBE)
-                N = sum([len(_) for _ in self.buffer.values()])
-                self.metrics.update(buffer=N)
-        
-                log.info(f"[%s] <- dispatch: %s", self.uid, data)
-    
-            wave = data.get(MONOTONIC_KEY) # Must exist!
-            if wave:
-                self.storage.update_sync_wave(
-                    self.sources,
-                    self.uid,
-                    wave,
+        while self.state < ST_STOPPED:
+            log.info(f"[%s] -> dispatch: %s", self.uid, edge)
+            try:
+                # do the computation
+                payload = await self._compute(edge, ekeys)
+                if payload is None:
+                    log.info(f"[%s] <- dispatch: SKIP due NO DATA", self.uid)
+                else:
+                    data.update(payload)
+                    # store results
+                    # and shift sync / waves info
+                    self.storage.put(self.uid, data, mode=Storage.MODE_TUBE)
+                    N = sum([len(_) for _ in self._live.values()])
+                    self.metrics.update(buffer=N)
+                    log.info(f"[%s] <- dispatch: %s", self.uid, data)
+
+                wave = data.get(MONOTONIC_KEY)  # Must exist!
+                if wave:
+                    await self.storage.update_sync_wave(
+                        self.sources,
+                        self.uid,
+                        wave,
+                    )
+                else:
+                    log.error(f"data %s has no %s key", data, MONOTONIC_KEY)
+                break  #  let continue with next wave
+            except SkipWave as why:
+                # some error is produced, but we want to jump to the next wave
+                log.info("Skip wave, reason: %s", why)
+                break  #  let continue with next wave
+            except RetryWave as why:
+                delay = self.RETRY_DELAY
+                for msg in why.args:
+                    log.info("Retry wave, reason: %s", msg)
+                    if isinstance(msg, dict):
+                        delay = msg.get('delay', self.RETRY_DELAY)
+                log.warning(
+                    "%s._compute() has failed but is needed a retry (%s seg)",
+                    str(self),
+                    delay,
                 )
-            else:
-                log.error(f"data %s has no %s key", data, MONOTONIC_KEY)
-        except Exception as why:
-            log.error(why)
-            log.error("".join(traceback.format_exception(*sys.exc_info())))
-        
-        
+                await asyncio.sleep(delay)
+            except Exception as why:
+                log.error(why)
+                log.error("".join(traceback.format_exception(*sys.exc_info())))
 
-    def _compute(self, edge, ekeys):
+    async def _compute(self, edge, ekeys):
         """
         Return None if we don't want to store info
         """
         raise NotImplementedError()
 
 
 # ---------------------------------------------------------
@@ -615,22 +671,28 @@
         query = f"""
         SELECT * FROM {TUBE_SYNC} WHERE
         source = $source AND
         target = $target
         """
         waves = {}
         for source in sources:
-            res = self.connection.query(query, variables={'source': source, 'target': uid,})
-        
+            res = self.connection.query(
+                query,
+                variables={
+                    "source": source,
+                    "target": uid,
+                },
+            )
+
             result = res.result
             if result:
                 waves[source] = result[0][MONOTONIC_KEY]
             else:
                 waves[source] = 0
-            
+
         return waves
 
     def put(self, uid: UID, edge):
         "put data"
         # insert data into database
         self.connection.create(uid, edge)
 
@@ -640,28 +702,27 @@
         return res.result
 
 
 # ---------------------------------------------------------
 # Example of a Particle Implementation
 # ---------------------------------------------------------
 class PlusOne(Particle):
-    def _compute(self, edge, ekeys):
+    async def _compute(self, edge, ekeys):
         s = 0
         for k in ekeys:
             s += edge[k]["payload"]
 
         s /= len(ekeys)
         s += random.random()
         return s
 
 
 class TempDiff(Particle):
-    def _compute(self, edge, ekeys):
+    async def _compute(self, edge, ekeys):
         X = [edge[k]["payload"] for k in ekeys]
         y = X[0] - X[-1]
         return y
 
 
 # ---------------------------------------------------------
 class TubeSync(Particle):
     """Do nothing special, but syncronize data"""
-
```

## Comparing `swarmtube-0.1.32.dist-info/LICENSE` & `swarmtube-0.1.34.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `swarmtube-0.1.32.dist-info/METADATA` & `swarmtube-0.1.34.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: swarmtube
-Version: 0.1.32
+Version: 0.1.34
 Summary: Swarm Workflow based on Tubes
 Home-page: https://github.com/asterio.gonzalez/swarmtube
 Author: Asterio Gonzalez
 Author-email: asterio.gonzalez@gmail.com
 License: MIT license
 Keywords: swarmtube
 Classifier: Development Status :: 2 - Pre-Alpha
```

## Comparing `swarmtube-0.1.32.dist-info/RECORD` & `swarmtube-0.1.34.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-swarmtube/__init__.py,sha256=_y-9y6ZZQcebXVAR8cMVu0xQi10PMPlkNgBrzBlOOpM,1684
+swarmtube/__init__.py,sha256=_2pSxk0RuVgToyCuKWPtzfLNvDKBomMLgJRhjs8Gmdk,1684
 swarmtube/definitions.py,sha256=aU08kgwbLPmUwZKdMYAAWNSNl5ZBIk70pkkx0VjKzlI,665
 swarmtube/swarmtube.py,sha256=uktSYYLbcB6wLd3SH6n_7MsZULi4xXl3tljJ1sCj7Js,6691
 swarmtube/cli/__init__.py,sha256=vn0EccAVZb4LjRugK__sLjr4OHBrPV8tK-ScyYSoQpc,1780
 swarmtube/cli/config.py,sha256=3oW1q326XFGlLABSZLqT-NSu3j27rhkAyXqcypBUO7Q,3834
 swarmtube/cli/inventory.py,sha256=Vnjc4uKeOspYyX5-GOvFx04F96lbzLTtla0ba33j2zY,2279
 swarmtube/cli/main.py,sha256=D7OxrEy-fy7N85CwdWOH81We37MTk2PgBrtuU_ClR28,1577
 swarmtube/cli/publish.py,sha256=JhZWxYagMQrIRgFtlaTE9XJ8saXFkhMqnFIPFAeIxwo,1800
@@ -18,29 +18,29 @@
 swarmtube/helpers/general.py,sha256=9JodQzN-muq9x1fcIUAblsAjM8jG8faGAeKmwgpF3xI,7449
 swarmtube/helpers/loaders.py,sha256=g_aBdqFWnOVh5CzPA_cluI9hKbav-5u8prkUXTJ-4zQ,4264
 swarmtube/helpers/models.py,sha256=p6BexQJtL5hui1OiOrddgvdyyl4i3t_ngiO1tXvjGMA,725
 swarmtube/helpers/surreal.py,sha256=x7OuU4igSD9IteRahY7-GMjIxic0stpFJNgijd8rZK4,3388
 swarmtube/logic/__init__.py,sha256=kUGgcg-toK_leB9i-HhPZ7BPClBI7yecUaVPhnJ0pzI,134
 swarmtube/logic/crawler.py,sha256=vFvaWpED_vlU_ViFQIG2A9IiaK8b_HafHiwJeq5H_og,4448
 swarmtube/logic/inventory.py,sha256=qRgbQGjEO6vv2MWTSl8h3y6VBR2IxuY4bhIw1GQ2Hzg,611
-swarmtube/logic/swarmtube.py,sha256=h0lUS5JPN7dXiNGL9cMJagb6MSvOJZiNoTIMg8vZd_Y,20338
+swarmtube/logic/swarmtube.py,sha256=UXfWsohF7pAeaB22M8hhJl4BqnoPqILcN5AQvzlt-X8,22620
 swarmtube/mappers/__init__.py,sha256=R6ShzAMfqiKh_FSrZK7T-giu44Ioo3ePkZpsoGPdo-Y,24
 swarmtube/mappers/inventory.py,sha256=LVlPPbrUZwwBdQP4OWk1occWl8-7ccGFGe8OnCrQ1WM,1025
 swarmtube/models/__init__.py,sha256=1H3-lETubfvf57NaX29AUdoJ1JG_r9yxHSGbljiEtaM,24
 swarmtube/models/base.py,sha256=F49yBmd1J4oSzLKU1EFFm7XdFi_rjs03B99dOcu_Eeg,3568
 swarmtube/models/config.py,sha256=cMhlQzCgo0N9qBMRQEPbBj1UxAC_OZc_PNLeNSoGV2M,1887
 swarmtube/models/enums.py,sha256=QxW9YtWOVokO3gXjELfE52paN_ZUNnXhs1HBtTpvZFE,34
 swarmtube/models/inventory.py,sha256=-18lewwVXVrRlq7CfALqKiNjVdwSoKCdRX-KtN2_uD0,1886
 swarmtube/models/script.py,sha256=pwCxOht-OXeysa8c0D9M6OyUdSDe7UaHiCVxSdOVU0M,1675
 swarmtube/models/swarmtube.py,sha256=dACX0MkqchNveuuoxGyYzKvIL6zTKKBYeSmxp_41OKc,1907
 swarmtube/models/task.py,sha256=ygyl1Fd8MI_x8S4KSFo50o3_YaE5n7F9kx4390T-ItU,1649
 swarmtube/ports/__init__.py,sha256=DUg8Upo_F0Y8LxfTqh1Q8IYSChmGSp1ZlZqwI-NXdag,555
 swarmtube/ports/preferences.py,sha256=RrSG1IrHu0_eiFVr9-LhFckPHDj7RcEVQTTDhJKO_uk,1389
 swarmtube/storage/__init__.py,sha256=CSNhFbyDpDg_fxHHw9x2GcOdx8qXZTw50_cKxKgtJ4g,182
 swarmtube/storage/surreal.py,sha256=QTEikrYghbv8nyKjc02aJRBcvBlppCNKmxjI4Qned-8,1330
-swarmtube-0.1.32.dist-info/AUTHORS.rst,sha256=3ZPoqg8Aav8DSYKd0fwcwn4_5HwSiMLart0E5Un00-U,168
-swarmtube-0.1.32.dist-info/LICENSE,sha256=JqzQ9S4hVjQBOM8SiUG9-qhokt1oKfE2qJMRNa3t410,1075
-swarmtube-0.1.32.dist-info/METADATA,sha256=uyOoZXnhjjR7b7ESPx15ps3Ei_DwQBCc1752AWctrLc,2201
-swarmtube-0.1.32.dist-info/WHEEL,sha256=DZajD4pwLWue70CAfc7YaxT1wLUciNBvN_TTcvXpltE,110
-swarmtube-0.1.32.dist-info/entry_points.txt,sha256=X-CwHgWDulWyiPxEbHmHwg9mE7FxpaCOHctheYiJgeM,49
-swarmtube-0.1.32.dist-info/top_level.txt,sha256=jJMGZgOJVEYy1TfxYBXsNV5EzL59YaT1xSDO5Fe4Pr8,10
-swarmtube-0.1.32.dist-info/RECORD,,
+swarmtube-0.1.34.dist-info/AUTHORS.rst,sha256=3ZPoqg8Aav8DSYKd0fwcwn4_5HwSiMLart0E5Un00-U,168
+swarmtube-0.1.34.dist-info/LICENSE,sha256=JqzQ9S4hVjQBOM8SiUG9-qhokt1oKfE2qJMRNa3t410,1075
+swarmtube-0.1.34.dist-info/METADATA,sha256=STkMSZ6-b5EoWRvyjCrWvFb2FiT8HRWnjJcbIDctyzc,2201
+swarmtube-0.1.34.dist-info/WHEEL,sha256=DZajD4pwLWue70CAfc7YaxT1wLUciNBvN_TTcvXpltE,110
+swarmtube-0.1.34.dist-info/entry_points.txt,sha256=X-CwHgWDulWyiPxEbHmHwg9mE7FxpaCOHctheYiJgeM,49
+swarmtube-0.1.34.dist-info/top_level.txt,sha256=jJMGZgOJVEYy1TfxYBXsNV5EzL59YaT1xSDO5Fe4Pr8,10
+swarmtube-0.1.34.dist-info/RECORD,,
```


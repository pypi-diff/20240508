# Comparing `tmp/nvidia_modelopt-0.11.1-cp39-cp39-win_amd64.whl.zip` & `tmp/nvidia_modelopt-0.11.2-cp310-cp310-manylinux_2_28_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,130 +1,156 @@
-Zip file size: 472606 bytes, number of entries: 128
--rw-rw-rw-  2.0 fat      694 b- defN 24-May-06 08:47 modelopt/__init__.py
--rw-rw-rw-  2.0 fat      604 b- defN 24-May-06 08:47 modelopt/deploy/__init__.py
--rw-rw-rw-  2.0 fat     2390 b- defN 24-May-06 08:47 modelopt/deploy/llm/__init__.py
--rw-rw-rw-  2.0 fat     5213 b- defN 24-May-06 08:47 modelopt/deploy/llm/generate.py
--rw-rw-rw-  2.0 fat    12337 b- defN 24-May-06 08:47 modelopt/deploy/llm/model_config_trt.py
--rw-rw-rw-  2.0 fat     6892 b- defN 24-May-06 08:47 modelopt/deploy/llm/nemo_utils.py
--rw-rw-rw-  2.0 fat      791 b- defN 24-May-06 08:47 modelopt/onnx/__init__.py
--rw-rw-rw-  2.0 fat     8003 b- defN 24-May-06 08:47 modelopt/onnx/op_types.py
--rw-rw-rw-  2.0 fat    19828 b- defN 24-May-06 08:47 modelopt/onnx/utils.py
--rw-rw-rw-  2.0 fat      619 b- defN 24-May-06 08:47 modelopt/onnx/quantization/__init__.py
--rw-rw-rw-  2.0 fat     4605 b- defN 24-May-06 08:47 modelopt/onnx/quantization/__main__.py
--rw-rw-rw-  2.0 fat     4605 b- defN 24-May-06 08:47 modelopt/onnx/quantization/calib_utils.py
--rw-rw-rw-  2.0 fat    18754 b- defN 24-May-06 08:47 modelopt/onnx/quantization/graph_utils.py
--rw-rw-rw-  2.0 fat     4132 b- defN 24-May-06 08:47 modelopt/onnx/quantization/gs_patching.py
--rw-rw-rw-  2.0 fat    18389 b- defN 24-May-07 07:17 modelopt/onnx/quantization/int4.py
--rw-rw-rw-  2.0 fat     4306 b- defN 24-May-06 08:47 modelopt/onnx/quantization/operators.py
--rw-rw-rw-  2.0 fat     8217 b- defN 24-May-06 08:47 modelopt/onnx/quantization/ort_patching.py
--rw-rw-rw-  2.0 fat     1053 b- defN 24-May-06 08:47 modelopt/onnx/quantization/ort_utils.py
--rw-rw-rw-  2.0 fat    14046 b- defN 24-May-06 08:47 modelopt/onnx/quantization/partitioning.py
--rw-rw-rw-  2.0 fat     7440 b- defN 24-May-06 08:47 modelopt/onnx/quantization/qdq_utils.py
--rw-rw-rw-  2.0 fat     2475 b- defN 24-May-06 08:47 modelopt/onnx/quantization/quant_utils.py
--rw-rw-rw-  2.0 fat    21116 b- defN 24-May-06 08:47 modelopt/onnx/quantization/quantize.py
--rw-rw-rw-  2.0 fat      805 b- defN 24-May-07 07:17 modelopt/torch/__init__.py
--rw-rw-rw-  2.0 fat      876 b- defN 24-May-06 08:47 modelopt/torch/_deploy/__init__.py
--rw-rw-rw-  2.0 fat     4824 b- defN 24-May-06 08:47 modelopt/torch/_deploy/compilation.py
--rw-rw-rw-  2.0 fat     5522 b- defN 24-May-06 08:47 modelopt/torch/_deploy/device_model.py
--rw-rw-rw-  2.0 fat     7588 b- defN 24-May-06 08:47 modelopt/torch/_deploy/profiling.py
--rw-rw-rw-  2.0 fat      957 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/__init__.py
--rw-rw-rw-  2.0 fat     2327 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/common.py
--rw-rw-rw-  2.0 fat     7266 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/ort_client.py
--rw-rw-rw-  2.0 fat     2224 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/registry.py
--rw-rw-rw-  2.0 fat     5865 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/runtime_client.py
--rw-rw-rw-  2.0 fat     7730 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/trt_client.py
--rw-rw-rw-  2.0 fat     2594 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/constants.py
--rw-rw-rw-  2.0 fat   119296 b- defN 24-May-07 07:19 modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     1763 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py
--rw-rw-rw-  2.0 fat     6972 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py
--rw-rw-rw-  2.0 fat     5691 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py
--rw-rw-rw-  2.0 fat     6524 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py
--rw-rw-rw-  2.0 fat      633 b- defN 24-May-06 08:47 modelopt/torch/_deploy/utils/__init__.py
--rw-rw-rw-  2.0 fat     4730 b- defN 24-May-06 08:47 modelopt/torch/_deploy/utils/onnx_optimizer.py
--rw-rw-rw-  2.0 fat    20946 b- defN 24-May-06 08:47 modelopt/torch/_deploy/utils/torch_onnx.py
--rw-rw-rw-  2.0 fat      848 b- defN 24-May-06 08:47 modelopt/torch/export/__init__.py
--rw-rw-rw-  2.0 fat    12668 b- defN 24-May-06 08:47 modelopt/torch/export/distribute.py
--rw-rw-rw-  2.0 fat    52089 b- defN 24-May-07 07:17 modelopt/torch/export/layer_utils.py
--rw-rw-rw-  2.0 fat    13780 b- defN 24-May-06 08:47 modelopt/torch/export/model_config.py
--rw-rw-rw-  2.0 fat    17778 b- defN 24-May-06 08:47 modelopt/torch/export/model_config_export.py
--rw-rw-rw-  2.0 fat    18360 b- defN 24-May-06 08:47 modelopt/torch/export/model_config_utils.py
--rw-rw-rw-  2.0 fat    29334 b- defN 24-May-06 08:47 modelopt/torch/export/postprocess.py
--rw-rw-rw-  2.0 fat     3182 b- defN 24-May-06 08:47 modelopt/torch/export/scaling_factor_utils.py
--rw-rw-rw-  2.0 fat    11289 b- defN 24-May-06 08:47 modelopt/torch/export/tensorrt_llm_utils.py
--rw-rw-rw-  2.0 fat     4088 b- defN 24-May-06 08:47 modelopt/torch/export/transformer_engine.py
--rw-rw-rw-  2.0 fat     1360 b- defN 24-May-06 08:47 modelopt/torch/opt/__init__.py
--rw-rw-rw-  2.0 fat     3137 b- defN 24-May-06 08:47 modelopt/torch/opt/_hooks.py
--rw-rw-rw-  2.0 fat    15227 b- defN 24-May-06 08:47 modelopt/torch/opt/config.py
--rw-rw-rw-  2.0 fat    24050 b- defN 24-May-06 08:47 modelopt/torch/opt/conversion.py
--rw-rw-rw-  2.0 fat    57369 b- defN 24-May-06 08:47 modelopt/torch/opt/dynamic.py
--rw-rw-rw-  2.0 fat     9461 b- defN 24-May-06 08:47 modelopt/torch/opt/hparam.py
--rw-rw-rw-  2.0 fat    12723 b- defN 24-May-06 08:47 modelopt/torch/opt/mode.py
--rw-rw-rw-  2.0 fat     9570 b- defN 24-May-06 08:47 modelopt/torch/opt/searcher.py
--rw-rw-rw-  2.0 fat     1955 b- defN 24-May-06 08:47 modelopt/torch/opt/utils.py
--rw-rw-rw-  2.0 fat      846 b- defN 24-May-06 08:47 modelopt/torch/quantization/__init__.py
--rw-rw-rw-  2.0 fat    13912 b- defN 24-May-07 07:17 modelopt/torch/quantization/config.py
--rw-rw-rw-  2.0 fat    10771 b- defN 24-May-06 08:47 modelopt/torch/quantization/conversion.py
--rw-rw-rw-  2.0 fat     1109 b- defN 24-May-07 07:17 modelopt/torch/quantization/extensions.py
--rw-rw-rw-  2.0 fat     4062 b- defN 24-May-06 08:47 modelopt/torch/quantization/mode.py
--rw-rw-rw-  2.0 fat   322560 b- defN 24-May-07 07:19 modelopt/torch/quantization/model_calib.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     6302 b- defN 24-May-06 08:47 modelopt/torch/quantization/model_quant.py
--rw-rw-rw-  2.0 fat     1393 b- defN 24-May-06 08:47 modelopt/torch/quantization/optim.py
--rw-rw-rw-  2.0 fat     1725 b- defN 24-May-06 08:47 modelopt/torch/quantization/quant_modules.py
--rw-rw-rw-  2.0 fat    29725 b- defN 24-May-07 07:17 modelopt/torch/quantization/tensor_quant.py
--rw-rw-rw-  2.0 fat     5703 b- defN 24-May-06 08:47 modelopt/torch/quantization/utils.py
--rw-rw-rw-  2.0 fat      816 b- defN 24-May-06 08:47 modelopt/torch/quantization/calib/__init__.py
--rw-rw-rw-  2.0 fat     1832 b- defN 24-May-06 08:47 modelopt/torch/quantization/calib/calibrator.py
--rw-rw-rw-  2.0 fat    17445 b- defN 24-May-06 08:47 modelopt/torch/quantization/calib/histogram.py
--rw-rw-rw-  2.0 fat     3436 b- defN 24-May-06 08:47 modelopt/torch/quantization/calib/max.py
--rw-rw-rw-  2.0 fat      945 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/__init__.py
--rw-rw-rw-  2.0 fat     2822 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/functional.py
--rw-rw-rw-  2.0 fat      601 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/__init__.py
--rw-rw-rw-  2.0 fat     8338 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/_utils.py
--rw-rw-rw-  2.0 fat     2317 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/clip.py
--rw-rw-rw-  2.0 fat     1389 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_activations.py
--rw-rw-rw-  2.0 fat     1568 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_batchnorm.py
--rw-rw-rw-  2.0 fat     3986 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_conv.py
--rw-rw-rw-  2.0 fat     1556 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_instancenorm.py
--rw-rw-rw-  2.0 fat     2762 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_linear.py
--rw-rw-rw-  2.0 fat     7849 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_module.py
--rw-rw-rw-  2.0 fat     3556 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_pooling.py
--rw-rw-rw-  2.0 fat    34362 b- defN 24-May-07 07:17 modelopt/torch/quantization/nn/modules/tensor_quantizer.py
--rw-rw-rw-  2.0 fat     1892 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/__init__.py
--rw-rw-rw-  2.0 fat     1484 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/apex.py
--rw-rw-rw-  2.0 fat     4226 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/custom.py
--rw-rw-rw-  2.0 fat     3453 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/diffusers.py
--rw-rw-rw-  2.0 fat     2489 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/huggingface.py
--rw-rw-rw-  2.0 fat     4804 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/megatron.py
--rw-rw-rw-  2.0 fat     2472 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/nemo.py
--rw-rw-rw-  2.0 fat     3061 b- defN 24-May-06 08:47 modelopt/torch/quantization/src/tensor_quant.cpp
--rw-rw-rw-  2.0 fat     1320 b- defN 24-May-06 08:47 modelopt/torch/quantization/src/tensor_quant_fp8.cpp
--rw-rw-rw-  2.0 fat     5557 b- defN 24-May-06 08:47 modelopt/torch/quantization/src/tensor_quant_gpu.cu
--rw-rw-rw-  2.0 fat     2082 b- defN 24-May-06 08:47 modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu
--rw-rw-rw-  2.0 fat      671 b- defN 24-May-06 08:47 modelopt/torch/sparsity/__init__.py
--rw-rw-rw-  2.0 fat     1641 b- defN 24-May-06 08:47 modelopt/torch/sparsity/config.py
--rw-rw-rw-  2.0 fat     5352 b- defN 24-May-06 08:47 modelopt/torch/sparsity/magnitude.py
--rw-rw-rw-  2.0 fat     6847 b- defN 24-May-06 08:47 modelopt/torch/sparsity/mode.py
--rw-rw-rw-  2.0 fat     3588 b- defN 24-May-06 08:47 modelopt/torch/sparsity/module.py
--rw-rw-rw-  2.0 fat     3199 b- defN 24-May-06 08:47 modelopt/torch/sparsity/searcher.py
--rw-rw-rw-  2.0 fat    10791 b- defN 24-May-06 08:47 modelopt/torch/sparsity/sparsegpt.py
--rw-rw-rw-  2.0 fat     5740 b- defN 24-May-06 08:47 modelopt/torch/sparsity/sparsification.py
--rw-rw-rw-  2.0 fat      990 b- defN 24-May-06 08:47 modelopt/torch/sparsity/plugins/__init__.py
--rw-rw-rw-  2.0 fat     2762 b- defN 24-May-06 08:47 modelopt/torch/sparsity/plugins/megatron.py
--rw-rw-rw-  2.0 fat      804 b- defN 24-May-06 08:47 modelopt/torch/utils/__init__.py
--rw-rw-rw-  2.0 fat     5582 b- defN 24-May-06 08:47 modelopt/torch/utils/_pytree.py
--rw-rw-rw-  2.0 fat     2419 b- defN 24-May-06 08:47 modelopt/torch/utils/cpp_extension.py
--rw-rw-rw-  2.0 fat     7448 b- defN 24-May-06 08:47 modelopt/torch/utils/dataset_utils.py
--rw-rw-rw-  2.0 fat     8262 b- defN 24-May-06 08:47 modelopt/torch/utils/distributed.py
--rw-rw-rw-  2.0 fat     3894 b- defN 24-May-06 08:47 modelopt/torch/utils/graph.py
--rw-rw-rw-  2.0 fat     1951 b- defN 24-May-06 08:47 modelopt/torch/utils/list.py
--rw-rw-rw-  2.0 fat     2991 b- defN 24-May-06 08:47 modelopt/torch/utils/logging.py
--rw-rw-rw-  2.0 fat    22299 b- defN 24-May-06 08:47 modelopt/torch/utils/network.py
--rw-rw-rw-  2.0 fat     2816 b- defN 24-May-06 08:47 modelopt/torch/utils/perf.py
--rw-rw-rw-  2.0 fat     5706 b- defN 24-May-06 08:47 modelopt/torch/utils/random.py
--rw-rw-rw-  2.0 fat     1999 b- defN 24-May-06 08:47 modelopt/torch/utils/tensor.py
--rw-rw-rw-  2.0 fat      540 b- defN 24-May-07 07:19 nvidia_modelopt-0.11.1.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     2589 b- defN 24-May-07 07:19 nvidia_modelopt-0.11.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-May-07 07:19 nvidia_modelopt-0.11.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       72 b- defN 24-May-07 07:18 nvidia_modelopt-0.11.1.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat        9 b- defN 24-May-07 07:18 nvidia_modelopt-0.11.1.dist-info/top_level.txt
--rw-rw-r--  2.0 fat    12435 b- defN 24-May-07 07:19 nvidia_modelopt-0.11.1.dist-info/RECORD
-128 files, 1329935 bytes uncompressed, 452482 bytes compressed:  66.0%
+Zip file size: 1650963 bytes, number of entries: 154
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 nvidia_modelopt.libs/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 nvidia_modelopt-0.11.2.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/
+-rw-rw-rw-  2.0 unx      531 b- defN 24-May-08 07:19 nvidia_modelopt-0.11.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx      114 b- defN 24-May-08 07:19 nvidia_modelopt-0.11.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx     2251 b- defN 24-May-08 07:19 nvidia_modelopt-0.11.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       72 b- defN 24-May-08 07:19 nvidia_modelopt-0.11.2.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx    12799 b- defN 24-May-08 07:19 nvidia_modelopt-0.11.2.dist-info/RECORD
+-rw-r--r--  2.0 unx        9 b- defN 24-May-08 07:19 nvidia_modelopt-0.11.2.dist-info/top_level.txt
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/onnx/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/deploy/
+-rw-r--r--  2.0 unx      680 b- defN 24-May-08 07:19 modelopt/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/quantization/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/_deploy/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/export/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/opt/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/sparsity/
+-rw-r--r--  2.0 unx      790 b- defN 24-May-08 07:19 modelopt/torch/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/quantization/plugins/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/quantization/nn/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/quantization/calib/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/quantization/src/
+-rw-r--r--  2.0 unx     1680 b- defN 24-May-08 07:19 modelopt/torch/quantization/quant_modules.py
+-rw-r--r--  2.0 unx     6153 b- defN 24-May-08 07:19 modelopt/torch/quantization/model_quant.py
+-rw-r--r--  2.0 unx    10900 b- defN 24-May-08 07:19 modelopt/torch/quantization/config.py
+-rw-r--r--  2.0 unx     3945 b- defN 24-May-08 07:19 modelopt/torch/quantization/mode.py
+-rw-r--r--  2.0 unx     1079 b- defN 24-May-08 07:19 modelopt/torch/quantization/extensions.py
+-rw-r--r--  2.0 unx      826 b- defN 24-May-08 07:19 modelopt/torch/quantization/__init__.py
+-rwxr-xr-x  2.0 unx  3609088 b- defN 24-May-08 07:19 modelopt/torch/quantization/model_calib.cpython-310-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx    10516 b- defN 24-May-08 07:19 modelopt/torch/quantization/conversion.py
+-rw-r--r--  2.0 unx    28994 b- defN 24-May-08 07:19 modelopt/torch/quantization/tensor_quant.py
+-rw-r--r--  2.0 unx     5536 b- defN 24-May-08 07:19 modelopt/torch/quantization/utils.py
+-rw-r--r--  2.0 unx     1360 b- defN 24-May-08 07:19 modelopt/torch/quantization/optim.py
+-rw-r--r--  2.0 unx     4705 b- defN 24-May-08 07:19 modelopt/torch/quantization/plugins/megatron.py
+-rw-r--r--  2.0 unx     2433 b- defN 24-May-08 07:19 modelopt/torch/quantization/plugins/huggingface.py
+-rw-r--r--  2.0 unx     4124 b- defN 24-May-08 07:19 modelopt/torch/quantization/plugins/custom.py
+-rw-r--r--  2.0 unx     1903 b- defN 24-May-08 07:19 modelopt/torch/quantization/plugins/__init__.py
+-rw-r--r--  2.0 unx     2411 b- defN 24-May-08 07:19 modelopt/torch/quantization/plugins/nemo.py
+-rw-r--r--  2.0 unx     3367 b- defN 24-May-08 07:19 modelopt/torch/quantization/plugins/diffusers.py
+-rw-r--r--  2.0 unx     1444 b- defN 24-May-08 07:19 modelopt/torch/quantization/plugins/apex.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/quantization/nn/modules/
+-rw-r--r--  2.0 unx      924 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/__init__.py
+-rw-r--r--  2.0 unx     2756 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/functional.py
+-rw-r--r--  2.0 unx     3444 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/quant_pooling.py
+-rw-r--r--  2.0 unx     7672 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/quant_module.py
+-rw-r--r--  2.0 unx     2259 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/clip.py
+-rw-r--r--  2.0 unx     1350 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/quant_activations.py
+-rw-r--r--  2.0 unx      590 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/__init__.py
+-rw-r--r--  2.0 unx     8131 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/_utils.py
+-rw-r--r--  2.0 unx    33520 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/tensor_quantizer.py
+-rw-r--r--  2.0 unx     1527 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/quant_batchnorm.py
+-rw-r--r--  2.0 unx     1516 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/quant_instancenorm.py
+-rw-r--r--  2.0 unx     2689 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/quant_linear.py
+-rw-r--r--  2.0 unx     3868 b- defN 24-May-08 07:19 modelopt/torch/quantization/nn/modules/quant_conv.py
+-rw-r--r--  2.0 unx     1774 b- defN 24-May-08 07:19 modelopt/torch/quantization/calib/calibrator.py
+-rw-r--r--  2.0 unx      797 b- defN 24-May-08 07:19 modelopt/torch/quantization/calib/__init__.py
+-rw-r--r--  2.0 unx    17018 b- defN 24-May-08 07:19 modelopt/torch/quantization/calib/histogram.py
+-rw-r--r--  2.0 unx     3338 b- defN 24-May-08 07:19 modelopt/torch/quantization/calib/max.py
+-rw-rw-rw-  2.0 unx     3004 b- defN 24-May-08 07:19 modelopt/torch/quantization/src/tensor_quant.cpp
+-rw-rw-rw-  2.0 unx     5427 b- defN 24-May-08 07:19 modelopt/torch/quantization/src/tensor_quant_gpu.cu
+-rw-rw-rw-  2.0 unx     2037 b- defN 24-May-08 07:19 modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu
+-rw-rw-rw-  2.0 unx     1284 b- defN 24-May-08 07:19 modelopt/torch/quantization/src/tensor_quant_fp8.cpp
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/_deploy/_runtime/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/_deploy/utils/
+-rw-r--r--  2.0 unx     7399 b- defN 24-May-08 07:19 modelopt/torch/_deploy/profiling.py
+-rw-r--r--  2.0 unx     5394 b- defN 24-May-08 07:19 modelopt/torch/_deploy/device_model.py
+-rw-r--r--  2.0 unx      858 b- defN 24-May-08 07:19 modelopt/torch/_deploy/__init__.py
+-rw-r--r--  2.0 unx     4707 b- defN 24-May-08 07:19 modelopt/torch/_deploy/compilation.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/_deploy/_runtime/tensorrt/
+-rw-r--r--  2.0 unx     2164 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/registry.py
+-rw-r--r--  2.0 unx      932 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/__init__.py
+-rw-r--r--  2.0 unx     7090 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/ort_client.py
+-rw-r--r--  2.0 unx     2259 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/common.py
+-rw-r--r--  2.0 unx     7560 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/trt_client.py
+-rw-r--r--  2.0 unx     5726 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/runtime_client.py
+-rw-r--r--  2.0 unx     5539 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py
+-rw-r--r--  2.0 unx     1718 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py
+-rw-r--r--  2.0 unx     6796 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py
+-rw-r--r--  2.0 unx     6328 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py
+-rwxr-xr-x  2.0 unx  1135008 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cpython-310-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx     2503 b- defN 24-May-08 07:19 modelopt/torch/_deploy/_runtime/tensorrt/constants.py
+-rw-r--r--  2.0 unx     4618 b- defN 24-May-08 07:19 modelopt/torch/_deploy/utils/onnx_optimizer.py
+-rw-r--r--  2.0 unx      620 b- defN 24-May-08 07:19 modelopt/torch/_deploy/utils/__init__.py
+-rw-r--r--  2.0 unx    20466 b- defN 24-May-08 07:19 modelopt/torch/_deploy/utils/torch_onnx.py
+-rw-r--r--  2.0 unx    17911 b- defN 24-May-08 07:19 modelopt/torch/export/model_config_utils.py
+-rw-r--r--  2.0 unx    50776 b- defN 24-May-08 07:19 modelopt/torch/export/layer_utils.py
+-rw-r--r--  2.0 unx    10999 b- defN 24-May-08 07:19 modelopt/torch/export/tensorrt_llm_utils.py
+-rw-r--r--  2.0 unx    28642 b- defN 24-May-08 07:19 modelopt/torch/export/postprocess.py
+-rw-r--r--  2.0 unx      831 b- defN 24-May-08 07:19 modelopt/torch/export/__init__.py
+-rw-r--r--  2.0 unx    12338 b- defN 24-May-08 07:19 modelopt/torch/export/distribute.py
+-rw-r--r--  2.0 unx     3998 b- defN 24-May-08 07:19 modelopt/torch/export/transformer_engine.py
+-rw-r--r--  2.0 unx    13332 b- defN 24-May-08 07:19 modelopt/torch/export/model_config.py
+-rw-r--r--  2.0 unx    17383 b- defN 24-May-08 07:19 modelopt/torch/export/model_config_export.py
+-rw-r--r--  2.0 unx     3094 b- defN 24-May-08 07:19 modelopt/torch/export/scaling_factor_utils.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/opt/plugins/
+-rw-r--r--  2.0 unx    14834 b- defN 24-May-08 07:19 modelopt/torch/opt/config.py
+-rw-r--r--  2.0 unx    12405 b- defN 24-May-08 07:19 modelopt/torch/opt/mode.py
+-rw-r--r--  2.0 unx     1335 b- defN 24-May-08 07:19 modelopt/torch/opt/__init__.py
+-rw-r--r--  2.0 unx     9316 b- defN 24-May-08 07:19 modelopt/torch/opt/searcher.py
+-rw-r--r--  2.0 unx    23486 b- defN 24-May-08 07:19 modelopt/torch/opt/conversion.py
+-rw-r--r--  2.0 unx     3074 b- defN 24-May-08 07:19 modelopt/torch/opt/_hooks.py
+-rw-r--r--  2.0 unx    56103 b- defN 24-May-08 07:19 modelopt/torch/opt/dynamic.py
+-rw-r--r--  2.0 unx     1902 b- defN 24-May-08 07:19 modelopt/torch/opt/utils.py
+-rw-r--r--  2.0 unx     9234 b- defN 24-May-08 07:19 modelopt/torch/opt/hparam.py
+-rw-r--r--  2.0 unx      812 b- defN 24-May-08 07:19 modelopt/torch/opt/plugins/__init__.py
+-rw-r--r--  2.0 unx    13411 b- defN 24-May-08 07:19 modelopt/torch/opt/plugins/mcore_dist_checkpointing.py
+-rw-r--r--  2.0 unx     7244 b- defN 24-May-08 07:19 modelopt/torch/utils/dataset_utils.py
+-rw-r--r--  2.0 unx     7993 b- defN 24-May-08 07:19 modelopt/torch/utils/distributed.py
+-rw-r--r--  2.0 unx     2900 b- defN 24-May-08 07:19 modelopt/torch/utils/logging.py
+-rw-r--r--  2.0 unx     3775 b- defN 24-May-08 07:19 modelopt/torch/utils/graph.py
+-rw-r--r--  2.0 unx      784 b- defN 24-May-08 07:19 modelopt/torch/utils/__init__.py
+-rw-r--r--  2.0 unx    21757 b- defN 24-May-08 07:19 modelopt/torch/utils/network.py
+-rw-r--r--  2.0 unx     2733 b- defN 24-May-08 07:19 modelopt/torch/utils/perf.py
+-rw-r--r--  2.0 unx     5454 b- defN 24-May-08 07:19 modelopt/torch/utils/_pytree.py
+-rw-r--r--  2.0 unx     2358 b- defN 24-May-08 07:19 modelopt/torch/utils/cpp_extension.py
+-rw-r--r--  2.0 unx     1898 b- defN 24-May-08 07:19 modelopt/torch/utils/list.py
+-rw-r--r--  2.0 unx     1949 b- defN 24-May-08 07:19 modelopt/torch/utils/tensor.py
+-rw-r--r--  2.0 unx     5552 b- defN 24-May-08 07:19 modelopt/torch/utils/random.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/torch/sparsity/plugins/
+-rw-r--r--  2.0 unx     5208 b- defN 24-May-08 07:19 modelopt/torch/sparsity/magnitude.py
+-rw-r--r--  2.0 unx     1594 b- defN 24-May-08 07:19 modelopt/torch/sparsity/config.py
+-rw-r--r--  2.0 unx     6646 b- defN 24-May-08 07:19 modelopt/torch/sparsity/mode.py
+-rw-r--r--  2.0 unx      657 b- defN 24-May-08 07:19 modelopt/torch/sparsity/__init__.py
+-rw-r--r--  2.0 unx     3116 b- defN 24-May-08 07:19 modelopt/torch/sparsity/searcher.py
+-rw-r--r--  2.0 unx     5623 b- defN 24-May-08 07:19 modelopt/torch/sparsity/sparsification.py
+-rw-r--r--  2.0 unx    10523 b- defN 24-May-08 07:19 modelopt/torch/sparsity/sparsegpt.py
+-rw-r--r--  2.0 unx     3504 b- defN 24-May-08 07:19 modelopt/torch/sparsity/module.py
+-rw-r--r--  2.0 unx     2688 b- defN 24-May-08 07:19 modelopt/torch/sparsity/plugins/megatron.py
+-rw-r--r--  2.0 unx      961 b- defN 24-May-08 07:19 modelopt/torch/sparsity/plugins/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/onnx/quantization/
+-rw-r--r--  2.0 unx      775 b- defN 24-May-08 07:19 modelopt/onnx/__init__.py
+-rw-r--r--  2.0 unx     7712 b- defN 24-May-08 07:19 modelopt/onnx/op_types.py
+-rw-r--r--  2.0 unx    19251 b- defN 24-May-08 07:19 modelopt/onnx/utils.py
+-rw-r--r--  2.0 unx     7986 b- defN 24-May-08 07:19 modelopt/onnx/quantization/ort_patching.py
+-rw-r--r--  2.0 unx    21538 b- defN 24-May-08 07:19 modelopt/onnx/quantization/quantize.py
+-rw-r--r--  2.0 unx     4025 b- defN 24-May-08 07:19 modelopt/onnx/quantization/gs_patching.py
+-rw-r--r--  2.0 unx      608 b- defN 24-May-08 07:19 modelopt/onnx/quantization/__init__.py
+-rw-r--r--  2.0 unx     4467 b- defN 24-May-08 07:19 modelopt/onnx/quantization/__main__.py
+-rw-r--r--  2.0 unx     4505 b- defN 24-May-08 07:19 modelopt/onnx/quantization/calib_utils.py
+-rw-r--r--  2.0 unx     2418 b- defN 24-May-08 07:19 modelopt/onnx/quantization/quant_utils.py
+-rw-r--r--  2.0 unx     4215 b- defN 24-May-08 07:19 modelopt/onnx/quantization/operators.py
+-rw-r--r--  2.0 unx     1054 b- defN 24-May-08 07:19 modelopt/onnx/quantization/ort_utils.py
+-rw-r--r--  2.0 unx    13673 b- defN 24-May-08 07:19 modelopt/onnx/quantization/partitioning.py
+-rw-r--r--  2.0 unx    17915 b- defN 24-May-08 07:19 modelopt/onnx/quantization/int4.py
+-rw-r--r--  2.0 unx    18295 b- defN 24-May-08 07:19 modelopt/onnx/quantization/graph_utils.py
+-rw-r--r--  2.0 unx     7198 b- defN 24-May-08 07:19 modelopt/onnx/quantization/qdq_utils.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-08 07:19 modelopt/deploy/llm/
+-rw-r--r--  2.0 unx      593 b- defN 24-May-08 07:19 modelopt/deploy/__init__.py
+-rw-r--r--  2.0 unx     5083 b- defN 24-May-08 07:19 modelopt/deploy/llm/generate.py
+-rw-r--r--  2.0 unx     2320 b- defN 24-May-08 07:19 modelopt/deploy/llm/__init__.py
+-rw-r--r--  2.0 unx    12040 b- defN 24-May-08 07:19 modelopt/deploy/llm/model_config_trt.py
+-rw-r--r--  2.0 unx     6722 b- defN 24-May-08 07:19 modelopt/deploy/llm/nemo_utils.py
+154 files, 5622846 bytes uncompressed, 1627377 bytes compressed:  71.1%
```

## zipnote {}

```diff
@@ -1,385 +1,463 @@
+Filename: nvidia_modelopt.libs/
+Comment: 
+
+Filename: nvidia_modelopt-0.11.2.dist-info/
+Comment: 
+
+Filename: modelopt/
+Comment: 
+
+Filename: nvidia_modelopt-0.11.2.dist-info/LICENSE
+Comment: 
+
+Filename: nvidia_modelopt-0.11.2.dist-info/WHEEL
+Comment: 
+
+Filename: nvidia_modelopt-0.11.2.dist-info/METADATA
+Comment: 
+
+Filename: nvidia_modelopt-0.11.2.dist-info/entry_points.txt
+Comment: 
+
+Filename: nvidia_modelopt-0.11.2.dist-info/RECORD
+Comment: 
+
+Filename: nvidia_modelopt-0.11.2.dist-info/top_level.txt
+Comment: 
+
+Filename: modelopt/torch/
+Comment: 
+
+Filename: modelopt/onnx/
+Comment: 
+
+Filename: modelopt/deploy/
+Comment: 
+
 Filename: modelopt/__init__.py
 Comment: 
 
-Filename: modelopt/deploy/__init__.py
+Filename: modelopt/torch/quantization/
 Comment: 
 
-Filename: modelopt/deploy/llm/__init__.py
+Filename: modelopt/torch/_deploy/
 Comment: 
 
-Filename: modelopt/deploy/llm/generate.py
+Filename: modelopt/torch/export/
 Comment: 
 
-Filename: modelopt/deploy/llm/model_config_trt.py
+Filename: modelopt/torch/opt/
 Comment: 
 
-Filename: modelopt/deploy/llm/nemo_utils.py
+Filename: modelopt/torch/utils/
 Comment: 
 
-Filename: modelopt/onnx/__init__.py
+Filename: modelopt/torch/sparsity/
 Comment: 
 
-Filename: modelopt/onnx/op_types.py
+Filename: modelopt/torch/__init__.py
 Comment: 
 
-Filename: modelopt/onnx/utils.py
+Filename: modelopt/torch/quantization/plugins/
 Comment: 
 
-Filename: modelopt/onnx/quantization/__init__.py
+Filename: modelopt/torch/quantization/nn/
 Comment: 
 
-Filename: modelopt/onnx/quantization/__main__.py
+Filename: modelopt/torch/quantization/calib/
 Comment: 
 
-Filename: modelopt/onnx/quantization/calib_utils.py
+Filename: modelopt/torch/quantization/src/
 Comment: 
 
-Filename: modelopt/onnx/quantization/graph_utils.py
+Filename: modelopt/torch/quantization/quant_modules.py
 Comment: 
 
-Filename: modelopt/onnx/quantization/gs_patching.py
+Filename: modelopt/torch/quantization/model_quant.py
 Comment: 
 
-Filename: modelopt/onnx/quantization/int4.py
+Filename: modelopt/torch/quantization/config.py
 Comment: 
 
-Filename: modelopt/onnx/quantization/operators.py
+Filename: modelopt/torch/quantization/mode.py
 Comment: 
 
-Filename: modelopt/onnx/quantization/ort_patching.py
+Filename: modelopt/torch/quantization/extensions.py
 Comment: 
 
-Filename: modelopt/onnx/quantization/ort_utils.py
+Filename: modelopt/torch/quantization/__init__.py
 Comment: 
 
-Filename: modelopt/onnx/quantization/partitioning.py
+Filename: modelopt/torch/quantization/model_calib.cpython-310-x86_64-linux-gnu.so
 Comment: 
 
-Filename: modelopt/onnx/quantization/qdq_utils.py
+Filename: modelopt/torch/quantization/conversion.py
 Comment: 
 
-Filename: modelopt/onnx/quantization/quant_utils.py
+Filename: modelopt/torch/quantization/tensor_quant.py
 Comment: 
 
-Filename: modelopt/onnx/quantization/quantize.py
+Filename: modelopt/torch/quantization/utils.py
 Comment: 
 
-Filename: modelopt/torch/__init__.py
+Filename: modelopt/torch/quantization/optim.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/__init__.py
+Filename: modelopt/torch/quantization/plugins/megatron.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/compilation.py
+Filename: modelopt/torch/quantization/plugins/huggingface.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/device_model.py
+Filename: modelopt/torch/quantization/plugins/custom.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/profiling.py
+Filename: modelopt/torch/quantization/plugins/__init__.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/__init__.py
+Filename: modelopt/torch/quantization/plugins/nemo.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/common.py
+Filename: modelopt/torch/quantization/plugins/diffusers.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/ort_client.py
+Filename: modelopt/torch/quantization/plugins/apex.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/registry.py
+Filename: modelopt/torch/quantization/nn/modules/
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/runtime_client.py
+Filename: modelopt/torch/quantization/nn/__init__.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/trt_client.py
+Filename: modelopt/torch/quantization/nn/functional.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/tensorrt/constants.py
+Filename: modelopt/torch/quantization/nn/modules/quant_pooling.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cp39-win_amd64.pyd
+Filename: modelopt/torch/quantization/nn/modules/quant_module.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py
+Filename: modelopt/torch/quantization/nn/modules/clip.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py
+Filename: modelopt/torch/quantization/nn/modules/quant_activations.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py
+Filename: modelopt/torch/quantization/nn/modules/__init__.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py
+Filename: modelopt/torch/quantization/nn/modules/_utils.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/utils/__init__.py
+Filename: modelopt/torch/quantization/nn/modules/tensor_quantizer.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/utils/onnx_optimizer.py
+Filename: modelopt/torch/quantization/nn/modules/quant_batchnorm.py
 Comment: 
 
-Filename: modelopt/torch/_deploy/utils/torch_onnx.py
+Filename: modelopt/torch/quantization/nn/modules/quant_instancenorm.py
 Comment: 
 
-Filename: modelopt/torch/export/__init__.py
+Filename: modelopt/torch/quantization/nn/modules/quant_linear.py
 Comment: 
 
-Filename: modelopt/torch/export/distribute.py
+Filename: modelopt/torch/quantization/nn/modules/quant_conv.py
 Comment: 
 
-Filename: modelopt/torch/export/layer_utils.py
+Filename: modelopt/torch/quantization/calib/calibrator.py
 Comment: 
 
-Filename: modelopt/torch/export/model_config.py
+Filename: modelopt/torch/quantization/calib/__init__.py
 Comment: 
 
-Filename: modelopt/torch/export/model_config_export.py
+Filename: modelopt/torch/quantization/calib/histogram.py
 Comment: 
 
-Filename: modelopt/torch/export/model_config_utils.py
+Filename: modelopt/torch/quantization/calib/max.py
 Comment: 
 
-Filename: modelopt/torch/export/postprocess.py
+Filename: modelopt/torch/quantization/src/tensor_quant.cpp
 Comment: 
 
-Filename: modelopt/torch/export/scaling_factor_utils.py
+Filename: modelopt/torch/quantization/src/tensor_quant_gpu.cu
 Comment: 
 
-Filename: modelopt/torch/export/tensorrt_llm_utils.py
+Filename: modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu
 Comment: 
 
-Filename: modelopt/torch/export/transformer_engine.py
+Filename: modelopt/torch/quantization/src/tensor_quant_fp8.cpp
 Comment: 
 
-Filename: modelopt/torch/opt/__init__.py
+Filename: modelopt/torch/_deploy/_runtime/
 Comment: 
 
-Filename: modelopt/torch/opt/_hooks.py
+Filename: modelopt/torch/_deploy/utils/
 Comment: 
 
-Filename: modelopt/torch/opt/config.py
+Filename: modelopt/torch/_deploy/profiling.py
 Comment: 
 
-Filename: modelopt/torch/opt/conversion.py
+Filename: modelopt/torch/_deploy/device_model.py
 Comment: 
 
-Filename: modelopt/torch/opt/dynamic.py
+Filename: modelopt/torch/_deploy/__init__.py
 Comment: 
 
-Filename: modelopt/torch/opt/hparam.py
+Filename: modelopt/torch/_deploy/compilation.py
 Comment: 
 
-Filename: modelopt/torch/opt/mode.py
+Filename: modelopt/torch/_deploy/_runtime/tensorrt/
 Comment: 
 
-Filename: modelopt/torch/opt/searcher.py
+Filename: modelopt/torch/_deploy/_runtime/registry.py
 Comment: 
 
-Filename: modelopt/torch/opt/utils.py
+Filename: modelopt/torch/_deploy/_runtime/__init__.py
 Comment: 
 
-Filename: modelopt/torch/quantization/__init__.py
+Filename: modelopt/torch/_deploy/_runtime/ort_client.py
 Comment: 
 
-Filename: modelopt/torch/quantization/config.py
+Filename: modelopt/torch/_deploy/_runtime/common.py
 Comment: 
 
-Filename: modelopt/torch/quantization/conversion.py
+Filename: modelopt/torch/_deploy/_runtime/trt_client.py
 Comment: 
 
-Filename: modelopt/torch/quantization/extensions.py
+Filename: modelopt/torch/_deploy/_runtime/runtime_client.py
 Comment: 
 
-Filename: modelopt/torch/quantization/mode.py
+Filename: modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py
 Comment: 
 
-Filename: modelopt/torch/quantization/model_calib.cp39-win_amd64.pyd
+Filename: modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py
 Comment: 
 
-Filename: modelopt/torch/quantization/model_quant.py
+Filename: modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py
 Comment: 
 
-Filename: modelopt/torch/quantization/optim.py
+Filename: modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py
 Comment: 
 
-Filename: modelopt/torch/quantization/quant_modules.py
+Filename: modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cpython-310-x86_64-linux-gnu.so
 Comment: 
 
-Filename: modelopt/torch/quantization/tensor_quant.py
+Filename: modelopt/torch/_deploy/_runtime/tensorrt/constants.py
 Comment: 
 
-Filename: modelopt/torch/quantization/utils.py
+Filename: modelopt/torch/_deploy/utils/onnx_optimizer.py
 Comment: 
 
-Filename: modelopt/torch/quantization/calib/__init__.py
+Filename: modelopt/torch/_deploy/utils/__init__.py
 Comment: 
 
-Filename: modelopt/torch/quantization/calib/calibrator.py
+Filename: modelopt/torch/_deploy/utils/torch_onnx.py
 Comment: 
 
-Filename: modelopt/torch/quantization/calib/histogram.py
+Filename: modelopt/torch/export/model_config_utils.py
 Comment: 
 
-Filename: modelopt/torch/quantization/calib/max.py
+Filename: modelopt/torch/export/layer_utils.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/__init__.py
+Filename: modelopt/torch/export/tensorrt_llm_utils.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/functional.py
+Filename: modelopt/torch/export/postprocess.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/__init__.py
+Filename: modelopt/torch/export/__init__.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/_utils.py
+Filename: modelopt/torch/export/distribute.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/clip.py
+Filename: modelopt/torch/export/transformer_engine.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/quant_activations.py
+Filename: modelopt/torch/export/model_config.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/quant_batchnorm.py
+Filename: modelopt/torch/export/model_config_export.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/quant_conv.py
+Filename: modelopt/torch/export/scaling_factor_utils.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/quant_instancenorm.py
+Filename: modelopt/torch/opt/plugins/
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/quant_linear.py
+Filename: modelopt/torch/opt/config.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/quant_module.py
+Filename: modelopt/torch/opt/mode.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/quant_pooling.py
+Filename: modelopt/torch/opt/__init__.py
 Comment: 
 
-Filename: modelopt/torch/quantization/nn/modules/tensor_quantizer.py
+Filename: modelopt/torch/opt/searcher.py
 Comment: 
 
-Filename: modelopt/torch/quantization/plugins/__init__.py
+Filename: modelopt/torch/opt/conversion.py
 Comment: 
 
-Filename: modelopt/torch/quantization/plugins/apex.py
+Filename: modelopt/torch/opt/_hooks.py
 Comment: 
 
-Filename: modelopt/torch/quantization/plugins/custom.py
+Filename: modelopt/torch/opt/dynamic.py
 Comment: 
 
-Filename: modelopt/torch/quantization/plugins/diffusers.py
+Filename: modelopt/torch/opt/utils.py
 Comment: 
 
-Filename: modelopt/torch/quantization/plugins/huggingface.py
+Filename: modelopt/torch/opt/hparam.py
 Comment: 
 
-Filename: modelopt/torch/quantization/plugins/megatron.py
+Filename: modelopt/torch/opt/plugins/__init__.py
 Comment: 
 
-Filename: modelopt/torch/quantization/plugins/nemo.py
+Filename: modelopt/torch/opt/plugins/mcore_dist_checkpointing.py
 Comment: 
 
-Filename: modelopt/torch/quantization/src/tensor_quant.cpp
+Filename: modelopt/torch/utils/dataset_utils.py
 Comment: 
 
-Filename: modelopt/torch/quantization/src/tensor_quant_fp8.cpp
+Filename: modelopt/torch/utils/distributed.py
 Comment: 
 
-Filename: modelopt/torch/quantization/src/tensor_quant_gpu.cu
+Filename: modelopt/torch/utils/logging.py
 Comment: 
 
-Filename: modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu
+Filename: modelopt/torch/utils/graph.py
 Comment: 
 
-Filename: modelopt/torch/sparsity/__init__.py
+Filename: modelopt/torch/utils/__init__.py
 Comment: 
 
-Filename: modelopt/torch/sparsity/config.py
+Filename: modelopt/torch/utils/network.py
+Comment: 
+
+Filename: modelopt/torch/utils/perf.py
+Comment: 
+
+Filename: modelopt/torch/utils/_pytree.py
+Comment: 
+
+Filename: modelopt/torch/utils/cpp_extension.py
+Comment: 
+
+Filename: modelopt/torch/utils/list.py
+Comment: 
+
+Filename: modelopt/torch/utils/tensor.py
+Comment: 
+
+Filename: modelopt/torch/utils/random.py
+Comment: 
+
+Filename: modelopt/torch/sparsity/plugins/
 Comment: 
 
 Filename: modelopt/torch/sparsity/magnitude.py
 Comment: 
 
+Filename: modelopt/torch/sparsity/config.py
+Comment: 
+
 Filename: modelopt/torch/sparsity/mode.py
 Comment: 
 
-Filename: modelopt/torch/sparsity/module.py
+Filename: modelopt/torch/sparsity/__init__.py
 Comment: 
 
 Filename: modelopt/torch/sparsity/searcher.py
 Comment: 
 
+Filename: modelopt/torch/sparsity/sparsification.py
+Comment: 
+
 Filename: modelopt/torch/sparsity/sparsegpt.py
 Comment: 
 
-Filename: modelopt/torch/sparsity/sparsification.py
+Filename: modelopt/torch/sparsity/module.py
+Comment: 
+
+Filename: modelopt/torch/sparsity/plugins/megatron.py
 Comment: 
 
 Filename: modelopt/torch/sparsity/plugins/__init__.py
 Comment: 
 
-Filename: modelopt/torch/sparsity/plugins/megatron.py
+Filename: modelopt/onnx/quantization/
 Comment: 
 
-Filename: modelopt/torch/utils/__init__.py
+Filename: modelopt/onnx/__init__.py
 Comment: 
 
-Filename: modelopt/torch/utils/_pytree.py
+Filename: modelopt/onnx/op_types.py
 Comment: 
 
-Filename: modelopt/torch/utils/cpp_extension.py
+Filename: modelopt/onnx/utils.py
 Comment: 
 
-Filename: modelopt/torch/utils/dataset_utils.py
+Filename: modelopt/onnx/quantization/ort_patching.py
 Comment: 
 
-Filename: modelopt/torch/utils/distributed.py
+Filename: modelopt/onnx/quantization/quantize.py
 Comment: 
 
-Filename: modelopt/torch/utils/graph.py
+Filename: modelopt/onnx/quantization/gs_patching.py
 Comment: 
 
-Filename: modelopt/torch/utils/list.py
+Filename: modelopt/onnx/quantization/__init__.py
 Comment: 
 
-Filename: modelopt/torch/utils/logging.py
+Filename: modelopt/onnx/quantization/__main__.py
 Comment: 
 
-Filename: modelopt/torch/utils/network.py
+Filename: modelopt/onnx/quantization/calib_utils.py
 Comment: 
 
-Filename: modelopt/torch/utils/perf.py
+Filename: modelopt/onnx/quantization/quant_utils.py
 Comment: 
 
-Filename: modelopt/torch/utils/random.py
+Filename: modelopt/onnx/quantization/operators.py
 Comment: 
 
-Filename: modelopt/torch/utils/tensor.py
+Filename: modelopt/onnx/quantization/ort_utils.py
 Comment: 
 
-Filename: nvidia_modelopt-0.11.1.dist-info/LICENSE
+Filename: modelopt/onnx/quantization/partitioning.py
 Comment: 
 
-Filename: nvidia_modelopt-0.11.1.dist-info/METADATA
+Filename: modelopt/onnx/quantization/int4.py
 Comment: 
 
-Filename: nvidia_modelopt-0.11.1.dist-info/WHEEL
+Filename: modelopt/onnx/quantization/graph_utils.py
 Comment: 
 
-Filename: nvidia_modelopt-0.11.1.dist-info/entry_points.txt
+Filename: modelopt/onnx/quantization/qdq_utils.py
 Comment: 
 
-Filename: nvidia_modelopt-0.11.1.dist-info/top_level.txt
+Filename: modelopt/deploy/llm/
 Comment: 
 
-Filename: nvidia_modelopt-0.11.1.dist-info/RECORD
+Filename: modelopt/deploy/__init__.py
+Comment: 
+
+Filename: modelopt/deploy/llm/generate.py
+Comment: 
+
+Filename: modelopt/deploy/llm/__init__.py
+Comment: 
+
+Filename: modelopt/deploy/llm/model_config_trt.py
+Comment: 
+
+Filename: modelopt/deploy/llm/nemo_utils.py
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=deflate
+Zip archive data, at least v2.0 to extract, compression method=store
```

## modelopt/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Nvidia TensorRT Model Optimizer (modelopt)."""
-from importlib.metadata import version
-
-__version__ = version("nvidia-modelopt")
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Nvidia TensorRT Model Optimizer (modelopt)."""
+from importlib.metadata import version
+
+__version__ = version("nvidia-modelopt")
```

## modelopt/deploy/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Model Optimizer's deployment package."""
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Model Optimizer's deployment package."""
```

## modelopt/deploy/llm/__init__.py

 * *Ordering differences only*

```diff
@@ -1,70 +1,70 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""LLM deployment package with tensorrt_llm.
-
-Model Optimizer supports automatic conversion of Model Optimizer exported LLM to TensorRT-LLM
-engines for accelerated inferencing.
-
-Convert to TensorRT-LLM:
-
-Model Optimizer offers a single API to build the exported model from the quantization stage on top
-of the TensorRT-LLM build API.
-
-.. code-block:: python
-
-    from modelopt.deploy.llm import build_tensorrt_llm
-
-    build_tensorrt_llm(
-        pretrained_config=pretrained_config_json_path,
-        engine_dir=engine_dir,
-        max_input_len=max_input_len,
-        max_output_len=max_output_len,
-        max_batch_size=max_batch_size,
-        max_beam_width=max_num_beams,
-        num_build_workers=num_build_workers,
-    )
-
-Batched Inference with TensorRT-LLM:
-
-Model Optimizer offers an easy-to-use python API to run batched offline inferences to test the TensorRT-LLM
-engine(s) built.
-
-For example:
-
-.. code-block:: python
-
-    from modelopt.deploy.llm import generate, load
-
-    # The host_context loading (called once).
-    host_context = load(tokenizer=tokenizer, engine_dir=engine_dir, num_beams=num_beams)
-    # generate could be called multiple times as long as the host_context is present.
-    outputs = generate(input_texts, max_output_len, host_context)
-    print(outputs)
-
-"""
-
-from mpi4py import MPI
-
-# Pre load MPI libs to avoid tensorrt_llm importing failures.
-print(f"Loaded mpi lib {MPI.__file__} successfully")
-
-# Pre import tensorrt_llm
-try:
-    import tensorrt_llm
-except Exception as e:
-    print(
-        "tensorrt_llm package is not installed. Please build or install tensorrt_llm package"
-        " properly before calling the llm deployment API."
-    )
-    raise (e)
-
-from .model_config_trt import *  # noqa
-from .generate import *  # noqa
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""LLM deployment package with tensorrt_llm.
+
+Model Optimizer supports automatic conversion of Model Optimizer exported LLM to TensorRT-LLM
+engines for accelerated inferencing.
+
+Convert to TensorRT-LLM:
+
+Model Optimizer offers a single API to build the exported model from the quantization stage on top
+of the TensorRT-LLM build API.
+
+.. code-block:: python
+
+    from modelopt.deploy.llm import build_tensorrt_llm
+
+    build_tensorrt_llm(
+        pretrained_config=pretrained_config_json_path,
+        engine_dir=engine_dir,
+        max_input_len=max_input_len,
+        max_output_len=max_output_len,
+        max_batch_size=max_batch_size,
+        max_beam_width=max_num_beams,
+        num_build_workers=num_build_workers,
+    )
+
+Batched Inference with TensorRT-LLM:
+
+Model Optimizer offers an easy-to-use python API to run batched offline inferences to test the TensorRT-LLM
+engine(s) built.
+
+For example:
+
+.. code-block:: python
+
+    from modelopt.deploy.llm import generate, load
+
+    # The host_context loading (called once).
+    host_context = load(tokenizer=tokenizer, engine_dir=engine_dir, num_beams=num_beams)
+    # generate could be called multiple times as long as the host_context is present.
+    outputs = generate(input_texts, max_output_len, host_context)
+    print(outputs)
+
+"""
+
+from mpi4py import MPI
+
+# Pre load MPI libs to avoid tensorrt_llm importing failures.
+print(f"Loaded mpi lib {MPI.__file__} successfully")
+
+# Pre import tensorrt_llm
+try:
+    import tensorrt_llm
+except Exception as e:
+    print(
+        "tensorrt_llm package is not installed. Please build or install tensorrt_llm package"
+        " properly before calling the llm deployment API."
+    )
+    raise (e)
+
+from .model_config_trt import *  # noqa
+from .generate import *  # noqa
```

## modelopt/deploy/llm/generate.py

 * *Ordering differences only*

```diff
@@ -1,130 +1,130 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""A wrapper over the TensorRT-LLM high level API runner."""
-
-
-import json
-from pathlib import Path
-from typing import Dict, Iterable, List, Union
-
-from tensorrt_llm.bindings import KvCacheConfig as TRT_KvCacheConfig
-from tensorrt_llm.hlapi.llm import LLM as TRT_LLM
-from tensorrt_llm.hlapi.llm import ModelConfig as TRT_ModelConfig
-from tensorrt_llm.hlapi.tokenizer import TokenizerBase, TransformersTokenizer
-
-
-class LLM(TRT_LLM):
-    """A wrapper over the ``tensorrt_llm.hlapi.llm.LLM`` for LLM profiling and validation."""
-
-    def __init__(
-        self,
-        engine_dir: Union[str, Path],
-        tokenizer: TokenizerBase,
-        kv_cache_config: Dict[str, Union[int, float]] = {},
-    ):
-        """Initializes the LLM runner class.
-
-        Args:
-            engine_dir: the directory path of the TensorRT-LLM engine.
-            tokenizer: the tokenizer. For example, a tokenizer from the Huggingface model.
-            kv_cache_config: the kv cache config as a dict. Please refer to
-                https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-best-practices.md
-        """
-        trt_llm_config = TRT_ModelConfig(model_dir=engine_dir)
-
-        with open(Path(engine_dir) / "config.json", "r") as engine_config_file:
-            engine_config = json.load(engine_config_file)
-            build_config = engine_config["build_config"]
-            max_tokens_in_paged_kv_cache = (
-                build_config["max_input_len"]
-                + build_config["max_output_len"] * build_config["max_beam_width"]
-            ) * build_config["max_batch_size"]
-
-        trt_kv_cache_config = TRT_KvCacheConfig()
-
-        if (
-            "kv_cache_free_gpu_mem_fraction" in kv_cache_config
-            and "max_tokens_in_paged_kv_cache" not in kv_cache_config
-        ):
-            trt_kv_cache_config.free_gpu_memory_fraction = kv_cache_config[
-                "free_gpu_memory_fraction"
-            ]
-        else:
-            trt_kv_cache_config.max_tokens = kv_cache_config.get(
-                "max_tokens_in_paged_kv_cache", max_tokens_in_paged_kv_cache
-            )
-
-        super().__init__(
-            trt_llm_config,
-            tokenizer=TransformersTokenizer(tokenizer),
-            kv_cache_config=trt_kv_cache_config,
-        )
-
-    @property
-    def max_input_len(self):
-        """Get the max input length from the LLM instance."""
-        return self.config.max_input_len
-
-    @property
-    def max_beam_width(self):
-        """Get the max beam width from the LLM instance."""
-        return self.config.max_beam_width
-
-    def generate_text(
-        self,
-        prompts: Union[Iterable[str], Iterable[List[int]]],
-        max_new_tokens: int,
-        temperature: float = 1.0,
-        keep_input_prompt: bool = True,
-    ) -> Union[List[str], List[List[str]]]:
-        """Generates the text based on the input prompts.
-
-        Args:
-            prompts: The input prompts. Could be a list of strings or token lists.
-            max_new_tokens: The max output token length.
-            temperature: The sampling temperature
-            keep_input_prompt: Set to include input prommpts in the outputs.
-
-        Returns:
-            a list of output text strings if max_beam_width is 1 or a 2D list with shape [batch, beam].
-        """
-        beam_width = self.max_beam_width
-        sampling_config = self.get_default_sampling_config()
-        sampling_config.max_new_tokens = max_new_tokens
-        sampling_config.beam_width = beam_width
-        sampling_config.temperature = [temperature]
-
-        prompt_ids = [
-            self.tokenizer.encode(prompt) if isinstance(prompt, str) else prompt
-            for prompt in prompts
-        ]
-        outputs = self.generate(prompt_ids, sampling_config=sampling_config)
-
-        output_texts = []
-        for prompt_id, output in zip(prompt_ids, outputs):
-            if isinstance(output.token_ids[0], list):
-                # beam search
-                output_token_ids = output.token_ids
-            else:
-                output_token_ids = [output.token_ids]
-
-            for output_token_id in output_token_ids:
-                output_texts.append(
-                    self.tokenizer.decode(
-                        output_token_id if keep_input_prompt else output_token_id[len(prompt_id) :]
-                    )
-                )
-
-        return (
-            output_texts
-            if beam_width == 1
-            else [output_texts[i : i + beam_width] for i in range(0, len(output_texts), beam_width)]
-        )
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""A wrapper over the TensorRT-LLM high level API runner."""
+
+
+import json
+from pathlib import Path
+from typing import Dict, Iterable, List, Union
+
+from tensorrt_llm.bindings import KvCacheConfig as TRT_KvCacheConfig
+from tensorrt_llm.hlapi.llm import LLM as TRT_LLM
+from tensorrt_llm.hlapi.llm import ModelConfig as TRT_ModelConfig
+from tensorrt_llm.hlapi.tokenizer import TokenizerBase, TransformersTokenizer
+
+
+class LLM(TRT_LLM):
+    """A wrapper over the ``tensorrt_llm.hlapi.llm.LLM`` for LLM profiling and validation."""
+
+    def __init__(
+        self,
+        engine_dir: Union[str, Path],
+        tokenizer: TokenizerBase,
+        kv_cache_config: Dict[str, Union[int, float]] = {},
+    ):
+        """Initializes the LLM runner class.
+
+        Args:
+            engine_dir: the directory path of the TensorRT-LLM engine.
+            tokenizer: the tokenizer. For example, a tokenizer from the Huggingface model.
+            kv_cache_config: the kv cache config as a dict. Please refer to
+                https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-best-practices.md
+        """
+        trt_llm_config = TRT_ModelConfig(model_dir=engine_dir)
+
+        with open(Path(engine_dir) / "config.json", "r") as engine_config_file:
+            engine_config = json.load(engine_config_file)
+            build_config = engine_config["build_config"]
+            max_tokens_in_paged_kv_cache = (
+                build_config["max_input_len"]
+                + build_config["max_output_len"] * build_config["max_beam_width"]
+            ) * build_config["max_batch_size"]
+
+        trt_kv_cache_config = TRT_KvCacheConfig()
+
+        if (
+            "kv_cache_free_gpu_mem_fraction" in kv_cache_config
+            and "max_tokens_in_paged_kv_cache" not in kv_cache_config
+        ):
+            trt_kv_cache_config.free_gpu_memory_fraction = kv_cache_config[
+                "free_gpu_memory_fraction"
+            ]
+        else:
+            trt_kv_cache_config.max_tokens = kv_cache_config.get(
+                "max_tokens_in_paged_kv_cache", max_tokens_in_paged_kv_cache
+            )
+
+        super().__init__(
+            trt_llm_config,
+            tokenizer=TransformersTokenizer(tokenizer),
+            kv_cache_config=trt_kv_cache_config,
+        )
+
+    @property
+    def max_input_len(self):
+        """Get the max input length from the LLM instance."""
+        return self.config.max_input_len
+
+    @property
+    def max_beam_width(self):
+        """Get the max beam width from the LLM instance."""
+        return self.config.max_beam_width
+
+    def generate_text(
+        self,
+        prompts: Union[Iterable[str], Iterable[List[int]]],
+        max_new_tokens: int,
+        temperature: float = 1.0,
+        keep_input_prompt: bool = True,
+    ) -> Union[List[str], List[List[str]]]:
+        """Generates the text based on the input prompts.
+
+        Args:
+            prompts: The input prompts. Could be a list of strings or token lists.
+            max_new_tokens: The max output token length.
+            temperature: The sampling temperature
+            keep_input_prompt: Set to include input prommpts in the outputs.
+
+        Returns:
+            a list of output text strings if max_beam_width is 1 or a 2D list with shape [batch, beam].
+        """
+        beam_width = self.max_beam_width
+        sampling_config = self.get_default_sampling_config()
+        sampling_config.max_new_tokens = max_new_tokens
+        sampling_config.beam_width = beam_width
+        sampling_config.temperature = [temperature]
+
+        prompt_ids = [
+            self.tokenizer.encode(prompt) if isinstance(prompt, str) else prompt
+            for prompt in prompts
+        ]
+        outputs = self.generate(prompt_ids, sampling_config=sampling_config)
+
+        output_texts = []
+        for prompt_id, output in zip(prompt_ids, outputs):
+            if isinstance(output.token_ids[0], list):
+                # beam search
+                output_token_ids = output.token_ids
+            else:
+                output_token_ids = [output.token_ids]
+
+            for output_token_id in output_token_ids:
+                output_texts.append(
+                    self.tokenizer.decode(
+                        output_token_id if keep_input_prompt else output_token_id[len(prompt_id) :]
+                    )
+                )
+
+        return (
+            output_texts
+            if beam_width == 1
+            else [output_texts[i : i + beam_width] for i in range(0, len(output_texts), beam_width)]
+        )
```

## modelopt/deploy/llm/model_config_trt.py

 * *Ordering differences only*

```diff
@@ -1,297 +1,297 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""The API convert the TensorRT-LLM checkpoint to the engines."""
-
-import traceback
-from concurrent.futures import ProcessPoolExecutor, as_completed
-from multiprocessing import get_context
-from pathlib import Path
-from typing import Any, Dict, Optional, Union
-
-import torch
-from tensorrt_llm._common import check_max_num_tokens
-from tensorrt_llm.builder import BuildConfig
-from tensorrt_llm.commands.build import build_and_save
-from tensorrt_llm.models import PretrainedConfig
-from tensorrt_llm.models.modeling_utils import WEIGHT_LOADER_MODELS
-from tensorrt_llm.plugin import PluginConfig
-
-
-# TODO: re-enable the refit_engine_dir flag.
-def build_tensorrt_llm(
-    pretrained_config: Union[str, Path],
-    engine_dir: Union[str, Path],
-    max_input_len: int = 200,
-    max_output_len: int = 200,
-    max_batch_size: int = 1,
-    max_beam_width: int = 1,
-    max_num_tokens: Optional[int] = None,
-    num_build_workers: int = 1,
-    enable_sparsity: bool = False,
-    max_prompt_embedding_table_size: int = 0,
-):
-    """The API to convert the TensorRT-LLM checkpoint to engines.
-
-    Args:
-        pretrained_config: The pretrained_config (file path) exported by
-            ``modelopt.torch.export.export_tensorrt_llm_checkpoint``.
-        engine_dir: The target output directory to save the built tensorrt_llm engines.
-        max_input_len: The max input sequence length.
-        max_output_len: The max output sequence length.
-        max_batch_size: The max batch size.
-        max_beam_width: The max beam search width.
-        max_num_tokens: The max number of tokens that can be processed at the same time.
-            For the context phase, the max_num_tokens counts the full sequence length.
-            For the generation phase, the max_num_tokens counts only the ones under generation
-            as the input sequence has been processed as cached.
-            max_num_tokens should fall between [max_batch_size * max_beam_width, max_batch_size * max_input_len].
-            when inflight batching is enabled.
-            Higher max_num_tokens means more GPU memory will be used for resource allocation.
-            If not specified the max_num_tokens will be set to the max bound.
-            Details: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md
-        num_build_workers: The number of workers to use for the building process.
-            If build time is a concern, you can increase this worker count to num of GPUs.
-            At a lost of higer CPU memory usage footprint.
-            If CPU memory is limited, num_build_workers should be set to 1 to conserve memory.
-        enable_sparsity: The switch to enable sparsity for TRT compiler.
-            With this flag, the TRT compiler will search tactics of sparse kernels for each node of which
-            weight tensors are sparsified. This increases engine building time significantly.
-        max_prompt_embedding_table_size: Length of the prepended/concatenated embeddings (either multimodal
-            feature embeddings or prompt tuning embeddings) to the LLM input embeddings.
-    """
-    engine_dir = Path(engine_dir)
-    engine_dir.mkdir(parents=True, exist_ok=True)
-
-    pretrained_config_json = Path(pretrained_config)
-    assert pretrained_config_json.exists()
-    ckpt_dir = pretrained_config_json.parent
-
-    config = PretrainedConfig.from_json_file(pretrained_config_json)
-
-    build_config = _get_build_config(
-        config,
-        max_input_len=max_input_len,
-        max_output_len=max_output_len,
-        max_batch_size=max_batch_size,
-        max_beam_width=max_beam_width,
-        max_num_tokens=max_num_tokens,
-        weight_sparsity=enable_sparsity,
-        max_prompt_embedding_table_size=max_prompt_embedding_table_size,
-    )
-
-    if num_build_workers == 1:
-        for rank in range(config.mapping.world_size):
-            passed = _build_tensorrt_llm_rank(
-                rank,
-                rank % num_build_workers,
-                config,
-                build_config,
-                engine_dir,
-                ckpt_dir=ckpt_dir,
-            )
-            assert passed, "Engine building failed, please check error log."
-    else:
-        with ProcessPoolExecutor(
-            mp_context=get_context("spawn"), max_workers=num_build_workers
-        ) as p:
-            futures = [
-                p.submit(
-                    _build_tensorrt_llm_rank,
-                    rank,
-                    rank % num_build_workers,
-                    config,
-                    build_config,
-                    engine_dir,
-                    ckpt_dir=ckpt_dir,
-                )
-                for rank in range(config.mapping.world_size)
-            ]
-            exceptions = []
-            for future in as_completed(futures):
-                try:
-                    future.result()
-                except Exception as e:
-                    traceback.print_exc()
-                    exceptions.append(e)
-            assert len(exceptions) == 0, "Engine building failed, please check error log."
-
-
-def build_tensorrt_llm_rank(
-    pretrained_config: Dict[str, Any],
-    weights: Dict[str, torch.Tensor],
-    rank: int,
-    engine_dir: Union[str, Path],
-    max_input_len: int = 200,
-    max_output_len: int = 200,
-    max_batch_size: int = 1,
-    max_beam_width: int = 1,
-    max_num_tokens: Optional[int] = None,
-    enable_sparsity: bool = False,
-    max_prompt_embedding_table_size: int = 0,
-):
-    """The API to convert the TensorRT-LLM checkpoint to the engine for a single rank.
-
-    Args:
-        pretrained_config: The pretrained_config (dict) exported by
-            ``modelopt.torch.export.torch_to_tensorrt_llm_checkpoint``.
-        weights: a dict of model weights and scaling factors.
-            If not provided, the weights will be loaded from the directory of the pretrained_config.
-        rank: the GPU rank of the engine to build.
-        engine_dir: The target output directory to save the built tensorrt_llm engines.
-        max_input_len: The max input sequence length.
-        max_output_len: The max output sequence length.
-        max_batch_size: The max batch size.
-        max_beam_width: The max beam search width.
-        max_num_tokens: The max number of tokens that can be processed at the same time.
-            For the context phase, the max_num_tokens counts the full sequence length.
-            For the generation phase, the max_num_tokens counts only the ones under generation
-            as the input sequence has been processed as cached.
-            max_num_tokens should fall between [max_batch_size * max_beam_width, max_batch_size * max_input_len].
-            when inflight batching is enabled.
-            Higher max_num_tokens means more GPU memory will be used for resource allocation.
-            If not specified the max_num_tokens will be set to the max bound.
-            Details: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md
-        enable_sparsity: The switch to enable sparsity for TRT compiler.
-            With this flag, the TRT compiler will search tactics of sparse kernels for each node of which
-            weight tensors are sparsified. This increases engine building time significantly.
-        max_prompt_embedding_table_size: Length of the prepended/concatenated embeddings (either multimodal
-            feature embeddings or prompt tuning embeddings) to the LLM input embeddings.
-    """
-    engine_dir = Path(engine_dir)
-    engine_dir.mkdir(parents=True, exist_ok=True)
-
-    config = PretrainedConfig.from_dict(pretrained_config)
-    assert weights, "Please specify the weights dict"
-
-    build_config = _get_build_config(
-        config,
-        max_input_len=max_input_len,
-        max_output_len=max_output_len,
-        max_batch_size=max_batch_size,
-        max_beam_width=max_beam_width,
-        max_num_tokens=max_num_tokens,
-        weight_sparsity=enable_sparsity,
-        max_prompt_embedding_table_size=max_prompt_embedding_table_size,
-    )
-
-    passed = _build_tensorrt_llm_rank(
-        rank,
-        rank % torch.cuda.device_count(),
-        config,
-        build_config,
-        engine_dir,
-        weights=weights,
-    )
-    assert passed, "Engine building failed, please check error log."
-
-
-def _get_build_config(
-    pretrained_config: PretrainedConfig,
-    max_input_len: int,
-    max_output_len: int,
-    max_batch_size: int,
-    max_beam_width: int,
-    max_num_tokens: Optional[int],
-    weight_sparsity: Optional[bool] = False,
-    max_prompt_embedding_table_size: int = 0,
-):
-    # logits_dtype is the same as the model dtype.
-    pretrained_config.logits_dtype = pretrained_config.dtype
-
-    quant_algo = pretrained_config.quantization.quant_algo
-    is_awq = quant_algo is not None and "AWQ" in quant_algo
-
-    use_qdq = (
-        pretrained_config.quantization.quant_algo
-        in [
-            "FP8",
-            "W8A8_SQ_PER_CHANNEL",
-        ],
-    )
-
-    plugin_config = PluginConfig.from_dict(
-        {
-            # Plugins
-            "gpt_attention_plugin": pretrained_config.dtype,
-            "gemm_plugin": pretrained_config.dtype if not use_qdq else None,
-            "nccl_plugin": pretrained_config.dtype,
-            "weight_only_groupwise_quant_matmul_plugin": (
-                pretrained_config.dtype if is_awq else None
-            ),
-        }
-    )
-
-    max_num_tokens, opt_num_tokens = check_max_num_tokens(
-        max_num_tokens=max_num_tokens,
-        opt_num_tokens=None,  # equal to max_batch_size*max_beam_width by default
-        max_batch_size=max_batch_size,
-        max_input_len=max_input_len,
-        max_beam_width=max_beam_width,
-        remove_input_padding=plugin_config.remove_input_padding,
-        enable_context_fmha=plugin_config.context_fmha,
-        tokens_per_block=plugin_config.tokens_per_block,
-    )
-
-    build_config = BuildConfig.from_dict(
-        {
-            "max_input_len": max_input_len,
-            "max_output_len": max_output_len,
-            "max_batch_size": max_batch_size,
-            "max_beam_width": max_beam_width,
-            "max_num_tokens": max_num_tokens,
-            "opt_num_tokens": opt_num_tokens,
-            "max_prompt_embedding_table_size": max_prompt_embedding_table_size,
-            "gather_context_logits": False,
-            "gather_generation_logits": False,
-            "strongly_typed": use_qdq,
-            "builder_opt": 4,
-            "weight_sparsity": weight_sparsity,
-            "profiling_verbosity": "layer_names_only",
-            "enable_debug_output": False,
-        },
-        plugin_config=plugin_config,
-    )
-
-    build_config.use_fused_mlp = pretrained_config.quantization.quant_algo in [
-        "FP8",
-        None,
-    ] and pretrained_config.hidden_act in ["silu", "swiglu", "fast-swiglu"]
-
-    return build_config
-
-
-def _build_tensorrt_llm_rank(
-    rank: int,
-    gpu_id: int,
-    pretrained_config: PretrainedConfig,
-    build_config: BuildConfig,
-    engine_dir: Union[str, Path],
-    ckpt_dir: Optional[Path] = None,
-    log_level="warning",
-):
-    kwargs = {}
-    if pretrained_config.architecture in WEIGHT_LOADER_MODELS:
-        kwargs["tp_size"] = pretrained_config.mapping.tp_size
-        kwargs["pp_size"] = pretrained_config.mapping.pp_size
-
-    success = build_and_save(
-        rank,
-        gpu_id,
-        ckpt_dir,
-        build_config,
-        engine_dir,
-        log_level,
-        pretrained_config,
-        model_cls=None,
-        **kwargs,
-    )
-
-    return success
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""The API convert the TensorRT-LLM checkpoint to the engines."""
+
+import traceback
+from concurrent.futures import ProcessPoolExecutor, as_completed
+from multiprocessing import get_context
+from pathlib import Path
+from typing import Any, Dict, Optional, Union
+
+import torch
+from tensorrt_llm._common import check_max_num_tokens
+from tensorrt_llm.builder import BuildConfig
+from tensorrt_llm.commands.build import build_and_save
+from tensorrt_llm.models import PretrainedConfig
+from tensorrt_llm.models.modeling_utils import WEIGHT_LOADER_MODELS
+from tensorrt_llm.plugin import PluginConfig
+
+
+# TODO: re-enable the refit_engine_dir flag.
+def build_tensorrt_llm(
+    pretrained_config: Union[str, Path],
+    engine_dir: Union[str, Path],
+    max_input_len: int = 200,
+    max_output_len: int = 200,
+    max_batch_size: int = 1,
+    max_beam_width: int = 1,
+    max_num_tokens: Optional[int] = None,
+    num_build_workers: int = 1,
+    enable_sparsity: bool = False,
+    max_prompt_embedding_table_size: int = 0,
+):
+    """The API to convert the TensorRT-LLM checkpoint to engines.
+
+    Args:
+        pretrained_config: The pretrained_config (file path) exported by
+            ``modelopt.torch.export.export_tensorrt_llm_checkpoint``.
+        engine_dir: The target output directory to save the built tensorrt_llm engines.
+        max_input_len: The max input sequence length.
+        max_output_len: The max output sequence length.
+        max_batch_size: The max batch size.
+        max_beam_width: The max beam search width.
+        max_num_tokens: The max number of tokens that can be processed at the same time.
+            For the context phase, the max_num_tokens counts the full sequence length.
+            For the generation phase, the max_num_tokens counts only the ones under generation
+            as the input sequence has been processed as cached.
+            max_num_tokens should fall between [max_batch_size * max_beam_width, max_batch_size * max_input_len].
+            when inflight batching is enabled.
+            Higher max_num_tokens means more GPU memory will be used for resource allocation.
+            If not specified the max_num_tokens will be set to the max bound.
+            Details: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md
+        num_build_workers: The number of workers to use for the building process.
+            If build time is a concern, you can increase this worker count to num of GPUs.
+            At a lost of higer CPU memory usage footprint.
+            If CPU memory is limited, num_build_workers should be set to 1 to conserve memory.
+        enable_sparsity: The switch to enable sparsity for TRT compiler.
+            With this flag, the TRT compiler will search tactics of sparse kernels for each node of which
+            weight tensors are sparsified. This increases engine building time significantly.
+        max_prompt_embedding_table_size: Length of the prepended/concatenated embeddings (either multimodal
+            feature embeddings or prompt tuning embeddings) to the LLM input embeddings.
+    """
+    engine_dir = Path(engine_dir)
+    engine_dir.mkdir(parents=True, exist_ok=True)
+
+    pretrained_config_json = Path(pretrained_config)
+    assert pretrained_config_json.exists()
+    ckpt_dir = pretrained_config_json.parent
+
+    config = PretrainedConfig.from_json_file(pretrained_config_json)
+
+    build_config = _get_build_config(
+        config,
+        max_input_len=max_input_len,
+        max_output_len=max_output_len,
+        max_batch_size=max_batch_size,
+        max_beam_width=max_beam_width,
+        max_num_tokens=max_num_tokens,
+        weight_sparsity=enable_sparsity,
+        max_prompt_embedding_table_size=max_prompt_embedding_table_size,
+    )
+
+    if num_build_workers == 1:
+        for rank in range(config.mapping.world_size):
+            passed = _build_tensorrt_llm_rank(
+                rank,
+                rank % num_build_workers,
+                config,
+                build_config,
+                engine_dir,
+                ckpt_dir=ckpt_dir,
+            )
+            assert passed, "Engine building failed, please check error log."
+    else:
+        with ProcessPoolExecutor(
+            mp_context=get_context("spawn"), max_workers=num_build_workers
+        ) as p:
+            futures = [
+                p.submit(
+                    _build_tensorrt_llm_rank,
+                    rank,
+                    rank % num_build_workers,
+                    config,
+                    build_config,
+                    engine_dir,
+                    ckpt_dir=ckpt_dir,
+                )
+                for rank in range(config.mapping.world_size)
+            ]
+            exceptions = []
+            for future in as_completed(futures):
+                try:
+                    future.result()
+                except Exception as e:
+                    traceback.print_exc()
+                    exceptions.append(e)
+            assert len(exceptions) == 0, "Engine building failed, please check error log."
+
+
+def build_tensorrt_llm_rank(
+    pretrained_config: Dict[str, Any],
+    weights: Dict[str, torch.Tensor],
+    rank: int,
+    engine_dir: Union[str, Path],
+    max_input_len: int = 200,
+    max_output_len: int = 200,
+    max_batch_size: int = 1,
+    max_beam_width: int = 1,
+    max_num_tokens: Optional[int] = None,
+    enable_sparsity: bool = False,
+    max_prompt_embedding_table_size: int = 0,
+):
+    """The API to convert the TensorRT-LLM checkpoint to the engine for a single rank.
+
+    Args:
+        pretrained_config: The pretrained_config (dict) exported by
+            ``modelopt.torch.export.torch_to_tensorrt_llm_checkpoint``.
+        weights: a dict of model weights and scaling factors.
+            If not provided, the weights will be loaded from the directory of the pretrained_config.
+        rank: the GPU rank of the engine to build.
+        engine_dir: The target output directory to save the built tensorrt_llm engines.
+        max_input_len: The max input sequence length.
+        max_output_len: The max output sequence length.
+        max_batch_size: The max batch size.
+        max_beam_width: The max beam search width.
+        max_num_tokens: The max number of tokens that can be processed at the same time.
+            For the context phase, the max_num_tokens counts the full sequence length.
+            For the generation phase, the max_num_tokens counts only the ones under generation
+            as the input sequence has been processed as cached.
+            max_num_tokens should fall between [max_batch_size * max_beam_width, max_batch_size * max_input_len].
+            when inflight batching is enabled.
+            Higher max_num_tokens means more GPU memory will be used for resource allocation.
+            If not specified the max_num_tokens will be set to the max bound.
+            Details: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md
+        enable_sparsity: The switch to enable sparsity for TRT compiler.
+            With this flag, the TRT compiler will search tactics of sparse kernels for each node of which
+            weight tensors are sparsified. This increases engine building time significantly.
+        max_prompt_embedding_table_size: Length of the prepended/concatenated embeddings (either multimodal
+            feature embeddings or prompt tuning embeddings) to the LLM input embeddings.
+    """
+    engine_dir = Path(engine_dir)
+    engine_dir.mkdir(parents=True, exist_ok=True)
+
+    config = PretrainedConfig.from_dict(pretrained_config)
+    assert weights, "Please specify the weights dict"
+
+    build_config = _get_build_config(
+        config,
+        max_input_len=max_input_len,
+        max_output_len=max_output_len,
+        max_batch_size=max_batch_size,
+        max_beam_width=max_beam_width,
+        max_num_tokens=max_num_tokens,
+        weight_sparsity=enable_sparsity,
+        max_prompt_embedding_table_size=max_prompt_embedding_table_size,
+    )
+
+    passed = _build_tensorrt_llm_rank(
+        rank,
+        rank % torch.cuda.device_count(),
+        config,
+        build_config,
+        engine_dir,
+        weights=weights,
+    )
+    assert passed, "Engine building failed, please check error log."
+
+
+def _get_build_config(
+    pretrained_config: PretrainedConfig,
+    max_input_len: int,
+    max_output_len: int,
+    max_batch_size: int,
+    max_beam_width: int,
+    max_num_tokens: Optional[int],
+    weight_sparsity: Optional[bool] = False,
+    max_prompt_embedding_table_size: int = 0,
+):
+    # logits_dtype is the same as the model dtype.
+    pretrained_config.logits_dtype = pretrained_config.dtype
+
+    quant_algo = pretrained_config.quantization.quant_algo
+    is_awq = quant_algo is not None and "AWQ" in quant_algo
+
+    use_qdq = (
+        pretrained_config.quantization.quant_algo
+        in [
+            "FP8",
+            "W8A8_SQ_PER_CHANNEL",
+        ],
+    )
+
+    plugin_config = PluginConfig.from_dict(
+        {
+            # Plugins
+            "gpt_attention_plugin": pretrained_config.dtype,
+            "gemm_plugin": pretrained_config.dtype if not use_qdq else None,
+            "nccl_plugin": pretrained_config.dtype,
+            "weight_only_groupwise_quant_matmul_plugin": (
+                pretrained_config.dtype if is_awq else None
+            ),
+        }
+    )
+
+    max_num_tokens, opt_num_tokens = check_max_num_tokens(
+        max_num_tokens=max_num_tokens,
+        opt_num_tokens=None,  # equal to max_batch_size*max_beam_width by default
+        max_batch_size=max_batch_size,
+        max_input_len=max_input_len,
+        max_beam_width=max_beam_width,
+        remove_input_padding=plugin_config.remove_input_padding,
+        enable_context_fmha=plugin_config.context_fmha,
+        tokens_per_block=plugin_config.tokens_per_block,
+    )
+
+    build_config = BuildConfig.from_dict(
+        {
+            "max_input_len": max_input_len,
+            "max_output_len": max_output_len,
+            "max_batch_size": max_batch_size,
+            "max_beam_width": max_beam_width,
+            "max_num_tokens": max_num_tokens,
+            "opt_num_tokens": opt_num_tokens,
+            "max_prompt_embedding_table_size": max_prompt_embedding_table_size,
+            "gather_context_logits": False,
+            "gather_generation_logits": False,
+            "strongly_typed": use_qdq,
+            "builder_opt": 4,
+            "weight_sparsity": weight_sparsity,
+            "profiling_verbosity": "layer_names_only",
+            "enable_debug_output": False,
+        },
+        plugin_config=plugin_config,
+    )
+
+    build_config.use_fused_mlp = pretrained_config.quantization.quant_algo in [
+        "FP8",
+        None,
+    ] and pretrained_config.hidden_act in ["silu", "swiglu", "fast-swiglu"]
+
+    return build_config
+
+
+def _build_tensorrt_llm_rank(
+    rank: int,
+    gpu_id: int,
+    pretrained_config: PretrainedConfig,
+    build_config: BuildConfig,
+    engine_dir: Union[str, Path],
+    ckpt_dir: Optional[Path] = None,
+    log_level="warning",
+):
+    kwargs = {}
+    if pretrained_config.architecture in WEIGHT_LOADER_MODELS:
+        kwargs["tp_size"] = pretrained_config.mapping.tp_size
+        kwargs["pp_size"] = pretrained_config.mapping.pp_size
+
+    success = build_and_save(
+        rank,
+        gpu_id,
+        ckpt_dir,
+        build_config,
+        engine_dir,
+        log_level,
+        pretrained_config,
+        model_cls=None,
+        **kwargs,
+    )
+
+    return success
```

## modelopt/deploy/llm/nemo_utils.py

 * *Ordering differences only*

```diff
@@ -1,170 +1,170 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""The utils to support Nemo models."""
-import warnings
-from pathlib import Path
-from typing import Dict
-
-import numpy as np
-import torch
-from transformers import GPT2Tokenizer, PreTrainedTokenizer, T5Tokenizer
-
-try:
-    from nemo.collections.common.tokenizers.sentencepiece_tokenizer import SentencePieceTokenizer
-except Exception:
-    warnings.warn("Cannot import nemo package, falling back to PreTrainedTokenizer!")
-    SentencePieceTokenizer = PreTrainedTokenizer
-
-
-class CustomSentencePieceTokenizer(SentencePieceTokenizer):
-    """Custom tokenizer based on Nemo SentencePieceTokenizer.
-
-    This extension of SentencePieceTokenizer is to make API consistent with HuggingFace tokenizers
-    in order to run evaluation tools in examples/tensorrt_llm/scripts/nemo_example.sh script.
-    """
-
-    def __init__(self, *args, **kwargs):
-        """Constructor method with extra check for non-legacy SentencePieceTokenizer variant."""
-        super().__init__(*args, **kwargs)
-        assert not self.legacy, "Only non-legacy tokenizer is supported"
-
-    @property
-    def pad_token(self):
-        """pad_token."""
-        return self.pad_id
-
-    @property
-    def eos_token(self):
-        """eos_token."""
-        return self.eos_id
-
-    @property
-    def pad_token_id(self):
-        """pad_token_id."""
-        return self.pad_id
-
-    @property
-    def eos_token_id(self):
-        """eos_token_id."""
-        return self.eos_id
-
-    def encode(self, text, return_tensors=None, max_length=None, **kwargs):
-        """Method introduced for HF tokenizers API consistency for evaluation scripts.
-
-        Note: kwargs other than return_tensors and max_length are ignored.
-        """
-        output = self.tokenizer.encode_as_ids(text)
-        if max_length is not None:
-            if isinstance(text, str):
-                output = output[:max_length]
-            if isinstance(text, list):
-                output = [x[:max_length] for x in output]
-        if return_tensors == "pt":
-            # Only plain text input is supported since for list of strings some padding needs to be introduced
-            assert isinstance(
-                text, str
-            ), "Returning 'pt' tensors is only supported for simple text input"
-            output = torch.LongTensor(output).reshape((1, -1))
-        return output
-
-    def batch_encode_plus(self, texts, **kwargs):
-        """Method introduced for HF tokenizers API consistency for evaluation scripts.
-
-        Note: kwargs are ignored.
-        """
-        assert isinstance(texts, list), f"Expected list of texts got {texts}"
-        return {"input_ids": self.tokenizer.encode_as_ids(texts)}
-
-    def decode(self, ids, **kwargs):
-        """MMethod introduced for HF tokenizers API consistency for evaluation scripts.
-
-        Note: kwargs are ignored.
-        """
-        if isinstance(ids, np.ndarray) or torch.is_tensor(ids):
-            ids = ids.tolist()
-        return self.tokenizer.decode(ids)
-
-    def batch_decode(self, ids, **kwargs):
-        """Method introduced for HF tokenizers API consistency for evaluation scripts."""
-        return self.decode(ids, **kwargs)
-
-
-def _build_tokenizer(tokenizer_config: Dict):
-    if tokenizer_config["library"] == "sentencepiece":
-        # Model Optimizer modification.
-        # Turn off legacy model by default: See https://github.com/huggingface/transformers/pull/24622
-        tokenizer = T5Tokenizer(tokenizer_config["model"], extra_ids=0, legacy=False)
-    elif "GPT2" in tokenizer_config["type"]:
-        tokenizer = GPT2Tokenizer(tokenizer_config["vocab_file"], tokenizer_config["merge_file"])
-    else:
-        raise ValueError(f'Tokenizer type {tokenizer_config["library"]} not handled')
-
-    if tokenizer.bos_token_id is None:
-        tokenizer.add_special_tokens({"bos_token": "<s>"})
-    if tokenizer.eos_token_id is None:
-        tokenizer.add_special_tokens({"eos_token": "</s>"})
-
-    return tokenizer
-
-
-def get_tokenzier(tokenizer_dir_or_path: Path) -> PreTrainedTokenizer:
-    """Loads the tokenizer from the decoded NEMO weights dir."""
-    # TODO: Remove this function sometime in the future
-    warnings.warn(
-        (
-            "Function get_tokenzier is deprecated and may be removed soon. "
-            "Please consider using get_nemo_tokenizer instead."
-        ),
-        DeprecationWarning,
-    )
-    model_path = (
-        tokenizer_dir_or_path / "tokenizer.model"
-        if tokenizer_dir_or_path.is_dir()
-        else tokenizer_dir_or_path
-    )
-    tokenizer_config = {"library": "sentencepiece", "model": str(model_path)}
-    return _build_tokenizer(tokenizer_config)
-
-
-def get_nemo_tokenizer(tokenizer_cfg_path: str):
-    """Build tokenizer from Nemo tokenizer config.
-
-    Refer to the logic of get_nmt_tokenizer function on how to instantiate tokenizers in Nemo, see
-    https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/modules/common/tokenizer_utils.py.
-    """
-    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer
-    from omegaconf import OmegaConf
-
-    print(f"Initializing tokenizer from tokenizer config {tokenizer_cfg_path}")
-    tokenizer_cfg = OmegaConf.load(tokenizer_cfg_path)
-
-    library = tokenizer_cfg.library
-    legacy = tokenizer_cfg.get("sentencepiece_legacy", library == "sentencepiece")
-    special_tokens_dict = tokenizer_cfg.get("special_tokens", {})
-
-    if library == "huggingface":
-        print(f"Getting HuggingFace AutoTokenizer with pretrained_model_name: {tokenizer_cfg.type}")
-        tokenizer = AutoTokenizer(
-            pretrained_model_name=tokenizer_cfg.type,
-            vocab_file=tokenizer_cfg.vocab_file,
-            merges_file=tokenizer_cfg.merge_file,
-            use_fast=tokenizer_cfg.get("use_fast", False),
-            **special_tokens_dict,
-        )
-    elif library == "sentencepiece":
-        print(f"Getting SentencePiece with model: {tokenizer_cfg.model}")
-        tokenizer = CustomSentencePieceTokenizer(model_path=tokenizer_cfg.model, legacy=legacy)
-    else:
-        raise NotImplementedError(
-            "Currently we only support 'huggingface' and 'sentencepiece' tokenizer libraries."
-        )
-
-    return tokenizer
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""The utils to support Nemo models."""
+import warnings
+from pathlib import Path
+from typing import Dict
+
+import numpy as np
+import torch
+from transformers import GPT2Tokenizer, PreTrainedTokenizer, T5Tokenizer
+
+try:
+    from nemo.collections.common.tokenizers.sentencepiece_tokenizer import SentencePieceTokenizer
+except Exception:
+    warnings.warn("Cannot import nemo package, falling back to PreTrainedTokenizer!")
+    SentencePieceTokenizer = PreTrainedTokenizer
+
+
+class CustomSentencePieceTokenizer(SentencePieceTokenizer):
+    """Custom tokenizer based on Nemo SentencePieceTokenizer.
+
+    This extension of SentencePieceTokenizer is to make API consistent with HuggingFace tokenizers
+    in order to run evaluation tools in examples/tensorrt_llm/scripts/nemo_example.sh script.
+    """
+
+    def __init__(self, *args, **kwargs):
+        """Constructor method with extra check for non-legacy SentencePieceTokenizer variant."""
+        super().__init__(*args, **kwargs)
+        assert not self.legacy, "Only non-legacy tokenizer is supported"
+
+    @property
+    def pad_token(self):
+        """pad_token."""
+        return self.pad_id
+
+    @property
+    def eos_token(self):
+        """eos_token."""
+        return self.eos_id
+
+    @property
+    def pad_token_id(self):
+        """pad_token_id."""
+        return self.pad_id
+
+    @property
+    def eos_token_id(self):
+        """eos_token_id."""
+        return self.eos_id
+
+    def encode(self, text, return_tensors=None, max_length=None, **kwargs):
+        """Method introduced for HF tokenizers API consistency for evaluation scripts.
+
+        Note: kwargs other than return_tensors and max_length are ignored.
+        """
+        output = self.tokenizer.encode_as_ids(text)
+        if max_length is not None:
+            if isinstance(text, str):
+                output = output[:max_length]
+            if isinstance(text, list):
+                output = [x[:max_length] for x in output]
+        if return_tensors == "pt":
+            # Only plain text input is supported since for list of strings some padding needs to be introduced
+            assert isinstance(
+                text, str
+            ), "Returning 'pt' tensors is only supported for simple text input"
+            output = torch.LongTensor(output).reshape((1, -1))
+        return output
+
+    def batch_encode_plus(self, texts, **kwargs):
+        """Method introduced for HF tokenizers API consistency for evaluation scripts.
+
+        Note: kwargs are ignored.
+        """
+        assert isinstance(texts, list), f"Expected list of texts got {texts}"
+        return {"input_ids": self.tokenizer.encode_as_ids(texts)}
+
+    def decode(self, ids, **kwargs):
+        """MMethod introduced for HF tokenizers API consistency for evaluation scripts.
+
+        Note: kwargs are ignored.
+        """
+        if isinstance(ids, np.ndarray) or torch.is_tensor(ids):
+            ids = ids.tolist()
+        return self.tokenizer.decode(ids)
+
+    def batch_decode(self, ids, **kwargs):
+        """Method introduced for HF tokenizers API consistency for evaluation scripts."""
+        return self.decode(ids, **kwargs)
+
+
+def _build_tokenizer(tokenizer_config: Dict):
+    if tokenizer_config["library"] == "sentencepiece":
+        # Model Optimizer modification.
+        # Turn off legacy model by default: See https://github.com/huggingface/transformers/pull/24622
+        tokenizer = T5Tokenizer(tokenizer_config["model"], extra_ids=0, legacy=False)
+    elif "GPT2" in tokenizer_config["type"]:
+        tokenizer = GPT2Tokenizer(tokenizer_config["vocab_file"], tokenizer_config["merge_file"])
+    else:
+        raise ValueError(f'Tokenizer type {tokenizer_config["library"]} not handled')
+
+    if tokenizer.bos_token_id is None:
+        tokenizer.add_special_tokens({"bos_token": "<s>"})
+    if tokenizer.eos_token_id is None:
+        tokenizer.add_special_tokens({"eos_token": "</s>"})
+
+    return tokenizer
+
+
+def get_tokenzier(tokenizer_dir_or_path: Path) -> PreTrainedTokenizer:
+    """Loads the tokenizer from the decoded NEMO weights dir."""
+    # TODO: Remove this function sometime in the future
+    warnings.warn(
+        (
+            "Function get_tokenzier is deprecated and may be removed soon. "
+            "Please consider using get_nemo_tokenizer instead."
+        ),
+        DeprecationWarning,
+    )
+    model_path = (
+        tokenizer_dir_or_path / "tokenizer.model"
+        if tokenizer_dir_or_path.is_dir()
+        else tokenizer_dir_or_path
+    )
+    tokenizer_config = {"library": "sentencepiece", "model": str(model_path)}
+    return _build_tokenizer(tokenizer_config)
+
+
+def get_nemo_tokenizer(tokenizer_cfg_path: str):
+    """Build tokenizer from Nemo tokenizer config.
+
+    Refer to the logic of get_nmt_tokenizer function on how to instantiate tokenizers in Nemo, see
+    https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/modules/common/tokenizer_utils.py.
+    """
+    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer
+    from omegaconf import OmegaConf
+
+    print(f"Initializing tokenizer from tokenizer config {tokenizer_cfg_path}")
+    tokenizer_cfg = OmegaConf.load(tokenizer_cfg_path)
+
+    library = tokenizer_cfg.library
+    legacy = tokenizer_cfg.get("sentencepiece_legacy", library == "sentencepiece")
+    special_tokens_dict = tokenizer_cfg.get("special_tokens", {})
+
+    if library == "huggingface":
+        print(f"Getting HuggingFace AutoTokenizer with pretrained_model_name: {tokenizer_cfg.type}")
+        tokenizer = AutoTokenizer(
+            pretrained_model_name=tokenizer_cfg.type,
+            vocab_file=tokenizer_cfg.vocab_file,
+            merges_file=tokenizer_cfg.merge_file,
+            use_fast=tokenizer_cfg.get("use_fast", False),
+            **special_tokens_dict,
+        )
+    elif library == "sentencepiece":
+        print(f"Getting SentencePiece with model: {tokenizer_cfg.model}")
+        tokenizer = CustomSentencePieceTokenizer(model_path=tokenizer_cfg.model, legacy=legacy)
+    else:
+        raise NotImplementedError(
+            "Currently we only support 'huggingface' and 'sentencepiece' tokenizer libraries."
+        )
+
+    return tokenizer
```

## modelopt/onnx/__init__.py

 * *Ordering differences only*

```diff
@@ -1,16 +1,16 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Model optimization subpackage for onnx."""
-
-try:
-    from . import quantization  # noqa: F401
-except (ModuleNotFoundError, ImportError) as e:
-    raise ImportError("Please install optional ``[onnx]`` dependencies.") from e
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Model optimization subpackage for onnx."""
+
+try:
+    from . import quantization  # noqa: F401
+except (ModuleNotFoundError, ImportError) as e:
+    raise ImportError("Please install optional ``[onnx]`` dependencies.") from e
```

## modelopt/onnx/op_types.py

 * *Ordering differences only*

```diff
@@ -1,291 +1,291 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions to categorize onnx ops."""
-from typing import List
-
-from onnxruntime.quantization.registry import QDQRegistry, QLinearOpsRegistry
-
-
-def is_unary_op(op_type: str):
-    """Returns whether the given op is a unary operator or not."""
-    return op_type in [
-        "Neg",
-        "Sqrt",
-        "Abs",
-        "Log",
-        "Exp",
-        "Not",
-        "Cast",
-        "Floor",
-        "Ceil",
-        "Round",
-        "Erf",
-        "Gelu",
-        "Sin",
-        "Cos",
-        "Atan",
-        "Sign",
-        "IsNaN",
-        "IsInf",
-        "Log",
-        "LeakyRelu",
-        # "Relu",
-        "Elu",
-        "Tanh",
-        "Sigmoid",
-        # "BatchNormalization",
-        "Softmax",
-        "Softplus",
-        "InstanceNormalization",
-        "CumSum",
-    ]
-
-
-def is_binary_op(op_type: str):
-    """Returns whether the given op is a binary operator or not."""
-    return op_type in [
-        "Add",
-        "Sub",
-        "Mul",
-        "Pow",
-        "Div",
-        "Min",
-        "Max",
-        "Greater",
-        "GreaterOrEqual",
-        "Less",
-        "LessOrEqual",
-        "Equal",
-        "BitwiseOr",
-        "BitwiseAnd",
-        "BitwiseXor",
-        "BitShift",
-    ]
-
-
-def is_fusible_reduction_op(op_type: str):
-    """Returns whether the given op type is of reduction category and fusible by Myelin."""
-    return op_type in [
-        "ReduceMax",
-        "ReduceMin",
-        "ReduceMean",
-        "ReduceProd",
-        "ReduceSum",
-        "TopK",  # Transformed to BottomK based on `largest` param
-    ]
-
-
-def is_copy_op(op_type: str):
-    """Returns whether the given op is a copy operator or not."""
-    return op_type in [
-        "Flatten",
-        "Transpose",
-        "Concat",
-        "Split",
-        "Squeeze",
-        "Expand",
-        "ReverseSequence",
-        "Reshape",
-        "Tile",
-        "Gather",
-        "Slice",
-        "GatherElements",
-        "GatherND",
-        "ScatterElements",
-        "ScatterND",
-        "OneHot",
-    ]
-
-
-def is_linear_op(op_type: str):
-    """Returns whether the given op type is of Linear category or not."""
-    return op_type in ["Conv", "Gemm", "MatMul"]
-
-
-def is_pointwise_or_elementwise_op(op_type: str):
-    """Returns whether the given op type is of Pointwise or Elementwise category or not.
-
-    This considers only the fusible types.
-    """
-    return is_unary_op(op_type) or is_binary_op(op_type)
-
-
-def is_pooling_or_window_op(op_type: str):
-    """Returns whether the given op type is of Pooling/Window category or not."""
-    return op_type in [
-        "AveragePool",
-        "GlobalAveragePool",
-        "MaxPool",
-        "GlobalMaxPool",
-        "GlobalLpPool",
-        "LpPool",
-        "MaxPoolGridSample",
-        "HammingWindow",
-        "BlackmanWindow",
-        "HannWindow",
-    ]
-
-
-def is_normalization_op(op_type: str):
-    """Returns whether the given op type is of Normalization category or not."""
-    return op_type in [
-        "BatchNormalization",
-        "InstanceNormalization",
-        "LRN",
-        "LpNormalization",
-        "GroupNormalization",
-        "LayerNormalization",
-    ]
-
-
-def is_conversion_op(op_type: str):
-    """Returns whether the given op type is of Conversion category or not."""
-    return op_type in ["Cast", "QuantizeLinear", "DequantizeLinear"]
-
-
-def is_non_reshape_copy_op(op_type: str):
-    """Returns whether the given op is a non-reshape copy op or not."""
-    return is_copy_op(op_type) and (op_type != "Reshape")
-
-
-def is_irregular_mem_access_op(op_type: str):
-    """Returns whether the given op type is of Irreggular mem access category or not."""
-    return op_type in [
-        "Gather",
-        "GatherElements",
-        "GatherND",
-        "MaxRoiPool",
-        "RoiAlign",
-        "ScatterND",
-        "ScatterElements",
-        "NonMaxSuppression",
-    ]
-
-
-def is_generator_op(op_type: str):
-    """Returns whether the given op type is of Generator category or not."""
-    return op_type in [
-        "Const",
-        "ConstOfShape",
-        "EyeLike",
-        "OneHot",
-        "Multinomial",
-        "RandomNormal",
-        "RandomUniform",
-        "Bernoulli",
-    ]
-
-
-def is_modifier_op(op_type: str):
-    """Returns whether the given op type is of Modifier category or not."""
-    return op_type in [
-        "Identity",
-        "Trilu",
-        "Expand",
-        "Pad",
-        "Dropout",
-        "TileDropout",
-        "Col2Im",
-        "MaxUnpool",
-    ]
-
-
-def is_sequence_op(op_type: str):
-    """Returns whether the given op type is of Sequence category or not."""
-    return op_type in [
-        "SequenceAt",
-        "SequenceConstruct",
-        "SequenceEmpty",
-        "SequenceErase",
-        "SequenceInsert",
-        "SequenceLength",
-    ]
-
-
-def is_selection_op(op_type: str):
-    """Returns whether the given op type is of Selection category or not."""
-    return op_type in ["Where", "Compress"]
-
-
-def is_control_flow_op(op_type: str):
-    """Returns whether the given op type is of Control Flow category or not."""
-    return op_type in ["If", "Loop"]
-
-
-def is_multiclass_op(op_type: str):
-    """Returns whether the given op type is of Multiclass category or not."""
-    return op_type in ["Einsum"]
-
-
-def is_recurrent_op(op_type: str):
-    """Returns whether the given op type is of Recurrent category or not."""
-    return op_type in ["LSTM", "RNN", "GRU"]
-
-
-def is_shape_op(op_type: str):
-    """Returns whether the given op type is of Shape category or not."""
-    return op_type in ["Shape", "Size"]
-
-
-def is_default_quantizable_op_by_ort(op_type: str):
-    """Returns if ort quantizes the op type by default.
-
-    Note. Subject to change with different ORT versions.
-    Note. Users can use nodes_to_quantize and/or op_types_to_quantize arguments to quantize
-    non-default operations.
-    Reference: https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/registry.py
-    """
-    return op_type in [
-        "Conv",
-        "Gemm",
-        "ArgMax",
-        "Relu",
-        "Split",
-        "MaxPool",
-        "InstanceNormalization",
-        "Softmax",
-        "Where",
-        "Squeeze",
-        "GlobalAveragePool",
-        "Pad",
-        "Resize",
-        "ConvTranspose",
-        "Gather",
-        "Sigmoid",
-        "EmbedLayerNormalization",
-        "Reshape",
-        "Unsqueeze",
-        "Transpose",
-        "MatMul",
-        "Concat",
-        "Mul",
-        "Clip",
-        "Add",
-        "LeakyRelu",
-        "AveragePool",
-    ]
-
-
-def get_quantizable_op_types(op_types_to_quantize: List[str]) -> List[str]:
-    """Returns a set of quantizable op types.
-
-    Note. This function should be called after quantize._configure_ort() is called once.
-    This returns quantizable op types either from the user supplied parameter
-    or from modelopt.onnx's default quantizable ops setting.
-    """
-    op_types_to_quantize = op_types_to_quantize or []
-
-    if not op_types_to_quantize or len(op_types_to_quantize) == 0:
-        q_linear_ops = list(QLinearOpsRegistry.keys())
-        qdq_ops = list(QDQRegistry.keys())
-        op_types_to_quantize = list(set(q_linear_ops + qdq_ops))
-
-    return op_types_to_quantize
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions to categorize onnx ops."""
+from typing import List
+
+from onnxruntime.quantization.registry import QDQRegistry, QLinearOpsRegistry
+
+
+def is_unary_op(op_type: str):
+    """Returns whether the given op is a unary operator or not."""
+    return op_type in [
+        "Neg",
+        "Sqrt",
+        "Abs",
+        "Log",
+        "Exp",
+        "Not",
+        "Cast",
+        "Floor",
+        "Ceil",
+        "Round",
+        "Erf",
+        "Gelu",
+        "Sin",
+        "Cos",
+        "Atan",
+        "Sign",
+        "IsNaN",
+        "IsInf",
+        "Log",
+        "LeakyRelu",
+        # "Relu",
+        "Elu",
+        "Tanh",
+        "Sigmoid",
+        # "BatchNormalization",
+        "Softmax",
+        "Softplus",
+        "InstanceNormalization",
+        "CumSum",
+    ]
+
+
+def is_binary_op(op_type: str):
+    """Returns whether the given op is a binary operator or not."""
+    return op_type in [
+        "Add",
+        "Sub",
+        "Mul",
+        "Pow",
+        "Div",
+        "Min",
+        "Max",
+        "Greater",
+        "GreaterOrEqual",
+        "Less",
+        "LessOrEqual",
+        "Equal",
+        "BitwiseOr",
+        "BitwiseAnd",
+        "BitwiseXor",
+        "BitShift",
+    ]
+
+
+def is_fusible_reduction_op(op_type: str):
+    """Returns whether the given op type is of reduction category and fusible by Myelin."""
+    return op_type in [
+        "ReduceMax",
+        "ReduceMin",
+        "ReduceMean",
+        "ReduceProd",
+        "ReduceSum",
+        "TopK",  # Transformed to BottomK based on `largest` param
+    ]
+
+
+def is_copy_op(op_type: str):
+    """Returns whether the given op is a copy operator or not."""
+    return op_type in [
+        "Flatten",
+        "Transpose",
+        "Concat",
+        "Split",
+        "Squeeze",
+        "Expand",
+        "ReverseSequence",
+        "Reshape",
+        "Tile",
+        "Gather",
+        "Slice",
+        "GatherElements",
+        "GatherND",
+        "ScatterElements",
+        "ScatterND",
+        "OneHot",
+    ]
+
+
+def is_linear_op(op_type: str):
+    """Returns whether the given op type is of Linear category or not."""
+    return op_type in ["Conv", "Gemm", "MatMul"]
+
+
+def is_pointwise_or_elementwise_op(op_type: str):
+    """Returns whether the given op type is of Pointwise or Elementwise category or not.
+
+    This considers only the fusible types.
+    """
+    return is_unary_op(op_type) or is_binary_op(op_type)
+
+
+def is_pooling_or_window_op(op_type: str):
+    """Returns whether the given op type is of Pooling/Window category or not."""
+    return op_type in [
+        "AveragePool",
+        "GlobalAveragePool",
+        "MaxPool",
+        "GlobalMaxPool",
+        "GlobalLpPool",
+        "LpPool",
+        "MaxPoolGridSample",
+        "HammingWindow",
+        "BlackmanWindow",
+        "HannWindow",
+    ]
+
+
+def is_normalization_op(op_type: str):
+    """Returns whether the given op type is of Normalization category or not."""
+    return op_type in [
+        "BatchNormalization",
+        "InstanceNormalization",
+        "LRN",
+        "LpNormalization",
+        "GroupNormalization",
+        "LayerNormalization",
+    ]
+
+
+def is_conversion_op(op_type: str):
+    """Returns whether the given op type is of Conversion category or not."""
+    return op_type in ["Cast", "QuantizeLinear", "DequantizeLinear"]
+
+
+def is_non_reshape_copy_op(op_type: str):
+    """Returns whether the given op is a non-reshape copy op or not."""
+    return is_copy_op(op_type) and (op_type != "Reshape")
+
+
+def is_irregular_mem_access_op(op_type: str):
+    """Returns whether the given op type is of Irreggular mem access category or not."""
+    return op_type in [
+        "Gather",
+        "GatherElements",
+        "GatherND",
+        "MaxRoiPool",
+        "RoiAlign",
+        "ScatterND",
+        "ScatterElements",
+        "NonMaxSuppression",
+    ]
+
+
+def is_generator_op(op_type: str):
+    """Returns whether the given op type is of Generator category or not."""
+    return op_type in [
+        "Const",
+        "ConstOfShape",
+        "EyeLike",
+        "OneHot",
+        "Multinomial",
+        "RandomNormal",
+        "RandomUniform",
+        "Bernoulli",
+    ]
+
+
+def is_modifier_op(op_type: str):
+    """Returns whether the given op type is of Modifier category or not."""
+    return op_type in [
+        "Identity",
+        "Trilu",
+        "Expand",
+        "Pad",
+        "Dropout",
+        "TileDropout",
+        "Col2Im",
+        "MaxUnpool",
+    ]
+
+
+def is_sequence_op(op_type: str):
+    """Returns whether the given op type is of Sequence category or not."""
+    return op_type in [
+        "SequenceAt",
+        "SequenceConstruct",
+        "SequenceEmpty",
+        "SequenceErase",
+        "SequenceInsert",
+        "SequenceLength",
+    ]
+
+
+def is_selection_op(op_type: str):
+    """Returns whether the given op type is of Selection category or not."""
+    return op_type in ["Where", "Compress"]
+
+
+def is_control_flow_op(op_type: str):
+    """Returns whether the given op type is of Control Flow category or not."""
+    return op_type in ["If", "Loop"]
+
+
+def is_multiclass_op(op_type: str):
+    """Returns whether the given op type is of Multiclass category or not."""
+    return op_type in ["Einsum"]
+
+
+def is_recurrent_op(op_type: str):
+    """Returns whether the given op type is of Recurrent category or not."""
+    return op_type in ["LSTM", "RNN", "GRU"]
+
+
+def is_shape_op(op_type: str):
+    """Returns whether the given op type is of Shape category or not."""
+    return op_type in ["Shape", "Size"]
+
+
+def is_default_quantizable_op_by_ort(op_type: str):
+    """Returns if ort quantizes the op type by default.
+
+    Note. Subject to change with different ORT versions.
+    Note. Users can use nodes_to_quantize and/or op_types_to_quantize arguments to quantize
+    non-default operations.
+    Reference: https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/registry.py
+    """
+    return op_type in [
+        "Conv",
+        "Gemm",
+        "ArgMax",
+        "Relu",
+        "Split",
+        "MaxPool",
+        "InstanceNormalization",
+        "Softmax",
+        "Where",
+        "Squeeze",
+        "GlobalAveragePool",
+        "Pad",
+        "Resize",
+        "ConvTranspose",
+        "Gather",
+        "Sigmoid",
+        "EmbedLayerNormalization",
+        "Reshape",
+        "Unsqueeze",
+        "Transpose",
+        "MatMul",
+        "Concat",
+        "Mul",
+        "Clip",
+        "Add",
+        "LeakyRelu",
+        "AveragePool",
+    ]
+
+
+def get_quantizable_op_types(op_types_to_quantize: List[str]) -> List[str]:
+    """Returns a set of quantizable op types.
+
+    Note. This function should be called after quantize._configure_ort() is called once.
+    This returns quantizable op types either from the user supplied parameter
+    or from modelopt.onnx's default quantizable ops setting.
+    """
+    op_types_to_quantize = op_types_to_quantize or []
+
+    if not op_types_to_quantize or len(op_types_to_quantize) == 0:
+        q_linear_ops = list(QLinearOpsRegistry.keys())
+        qdq_ops = list(QDQRegistry.keys())
+        op_types_to_quantize = list(set(q_linear_ops + qdq_ops))
+
+    return op_types_to_quantize
```

## modelopt/onnx/utils.py

 * *Ordering differences only*

```diff
@@ -1,577 +1,577 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions related to onnx."""
-import io
-import os
-from collections import defaultdict
-from typing import Any, Dict, List, Optional, Tuple, Union
-
-import numpy as np
-import onnx
-import onnx.onnx_cpp2py_export.checker as C  # noqa: N812
-from onnx import numpy_helper
-from onnx_graphsurgeon import Node, Variable
-
-
-def get_input_names_from_bytes(model_bytes: bytes, external_inputs_only: bool = True) -> List[str]:
-    """This function returns the inputs names of the given onnx model in bytes.
-
-    Args:
-        model_bytes: Onnx model in bytes.
-
-    Returns:
-        List of input names of the model.
-    """
-    model = onnx.load_from_string(model_bytes)
-    return get_input_names(model, external_inputs_only)
-
-
-def get_all_input_names(model: onnx.onnx_ml_pb2.ModelProto) -> List[str]:
-    """This function returns the inputs names of the given onnx model."""
-    return [graph_input.name for graph_input in model.graph.input]
-
-
-def _get_initializer_names(model: onnx.onnx_ml_pb2.ModelProto) -> List[str]:
-    return [initializer.name for initializer in model.graph.initializer]
-
-
-def get_input_names(
-    model: onnx.onnx_ml_pb2.ModelProto, external_inputs_only: bool = True
-) -> List[str]:
-    """This function returns the external inputs names of the given onnx model.
-
-    Note: external_input_names = input_names - initializer_names
-
-    Args:
-        model: Loaded in-memory onnx ModelProto.
-
-    Returns:
-        List of external input names of the model.
-    """
-    input_names = get_all_input_names(model)
-    if not external_inputs_only:
-        return input_names
-
-    initializer_names = _get_initializer_names(model)
-    external_input_names = list(np.setdiff1d(input_names, initializer_names))
-    return external_input_names
-
-
-def get_output_names_from_bytes(model_bytes: bytes) -> List[str]:
-    """This function returns the output names of the given onnx model in bytes.
-
-    Args:
-        model_bytes: Onnx model in bytes.
-
-    Returns:
-        List of output names of the model.
-    """
-    model = onnx.load_from_string(model_bytes)
-    return get_output_names(model)
-
-
-def get_output_names(model: onnx.onnx_ml_pb2.ModelProto) -> List[str]:
-    """This function returns the output names of the given onnx model.
-
-    Args:
-        model: Loaded in-memory onnx ModelProto.
-
-    Returns:
-        List of output names of the model.
-    """
-    return [output.name for output in model.graph.output]
-
-
-def get_node_names_from_bytes(model_bytes: bytes) -> List[str]:
-    """This function returns all node names from the given onnx model in bytes.
-
-    Args:
-        model: onnx model in bytes.
-
-    Returns:
-        List of node names of the model.
-    """
-    model = onnx.load_from_string(model_bytes)
-    return get_node_names(model)
-
-
-def get_node_names(model: onnx.onnx_ml_pb2.ModelProto) -> List[str]:
-    """This function returns all node names from the given onnx model.
-
-    Args:
-        model: Loaded in-memory onnx ModelProto.
-
-    Returns:
-        List of node names of the model.
-    """
-    return [node.name for node in model.graph.node]
-
-
-def _get_tensor_shape(tensor: onnx.onnx_ml_pb2.ValueInfoProto) -> List[int]:
-    """This function returns the shape of the input onnx tensor.
-
-    Onnx tensors are of type ValueInfoProto and their dimensions are stored in a
-    RepeatedCompositeContainer. Each of these dimensions is of type onnx.onnx_ml_pb2.Dimension.
-    In a loop we access each of the Dimension object and create a shape list to return.
-    Dynamic dimensions (i.e. with "dim_param" field) are replaced with 1.
-
-    Args:
-        tensor: Onnx tensor object for which the shape needs to be computed.
-
-    Returns:
-        Shape of the input tensor.
-    """
-    if not hasattr(tensor.type, "tensor_type"):
-        raise NotImplementedError("Only tensor type inputs are supported.")
-
-    dimensions = tensor.type.tensor_type.shape.dim
-    shape = []
-    for dim in dimensions:
-        if dim.HasField("dim_param"):
-            shape.append(1)
-        if dim.HasField("dim_value"):
-            if dim.dim_value == -1:
-                shape.append(1)
-            else:
-                shape.append(dim.dim_value)
-
-    return shape
-
-
-def _get_all_shapes(container: Any) -> Dict[str, List[int]]:
-    """This method returns the shape of tensors within a RepeatedCompositeContainer.
-
-    Args:
-        container: Model graph input/output container.
-
-    Returns:
-        Dictionary of tensor names and shape of the tensors within the container.
-    """
-    results = {}
-    for tensor in container:
-        results[tensor.name] = _get_tensor_shape(tensor)
-    return results
-
-
-def _get_selected_shapes(container: Any, inputs_to_include: List[str]) -> Dict[str, List[int]]:
-    """This method returns the shape tensors within a RepeatedCompositeContainer.
-
-    It only computes the shape of the tensors with name containing in `inputs_to_include` list.
-
-    Args:
-        container: Model graph input/output container.
-
-    Returns:
-        Dictionary of tensor names in inputs_to_include and their shapes.
-    """
-    results = {}
-    for tensor in container:
-        if tensor.name in inputs_to_include:
-            results[tensor.name] = _get_tensor_shape(tensor)
-    return results
-
-
-def get_input_shapes_from_bytes(model_bytes: bytes) -> Dict[str, List[int]]:
-    """This function returns the input shapes of the given onnx model in bytes.
-
-    Args:
-        model_bytes: Onnx model in bytes.
-
-    Returns:
-        Dictionary of inputs names and shapes.
-    """
-    model = onnx.load_from_string(model_bytes)
-    return get_input_shapes(model)
-
-
-def get_input_shapes(
-    model: onnx.onnx_ml_pb2.ModelProto, external_inputs_only: bool = True
-) -> Dict[str, List[int]]:
-    """This function returns the inputs shapes for the given onnx model."""
-    if external_inputs_only:
-        return _get_selected_shapes(model.graph.input, get_input_names(model))
-    return _get_all_shapes(model.graph.input)
-
-
-def get_output_shapes(model: onnx.onnx_ml_pb2.ModelProto) -> Dict[str, List[int]]:
-    """This function returns the output shapes for the given onnx model."""
-    return _get_all_shapes(model.graph.output)
-
-
-def _get_tensor_type(tensor: onnx.onnx_ml_pb2.ValueInfoProto) -> int:
-    if not hasattr(tensor.type, "tensor_type"):
-        raise NotImplementedError("Only tensor type inputs are supported.")
-    type_ = tensor.type.tensor_type.elem_type
-    return type_
-
-
-def _get_container_types(
-    container, inputs_to_include: Union[None, List[str]] = None
-) -> Dict[str, int]:
-    results = {}
-    for tensor in container:
-        if inputs_to_include is not None:
-            if tensor.name not in inputs_to_include:
-                continue
-        t_type = _get_tensor_type(tensor)
-        results[tensor.name] = t_type
-    return results
-
-
-def _get_input_types(
-    model: onnx.onnx_ml_pb2.ModelProto, external_inputs_only: bool = True
-) -> Dict[str, int]:
-    inputs_to_include = get_input_names(model, external_inputs_only)
-    return _get_container_types(model.graph.input, inputs_to_include)
-
-
-def _get_output_types(model: onnx.onnx_ml_pb2.ModelProto) -> Dict[str, int]:
-    results = _get_container_types(model.graph.output)
-    return results
-
-
-def _convert_types_to_np(types: Union[Dict[str, int], List[int], int]) -> Any:
-    if isinstance(types, dict):
-        types_np = {}
-        for name in types.keys():
-            types_np[name] = onnx.helper.tensor_dtype_to_np_dtype(types[name])
-        return types_np
-    elif isinstance(types, list):
-        return [onnx.helper.tensor_dtype_to_np_dtype(type_) for type_ in types]
-    else:
-        return onnx.helper.tensor_dtype_to_np_dtype(types)
-
-
-def gen_random_inputs(model: onnx.onnx_ml_pb2.ModelProto) -> Dict[str, np.ndarray]:
-    """This function generates random inputs for an onnx model.
-
-    Args:
-        model: Loaded in-memory onnx ModelProto.
-
-    Returns:
-        Dictionary of numpy tensors.
-    """
-    input_dict = {}
-    types = _get_input_types(model)
-    types_np = _convert_types_to_np(types)
-
-    for graph_input in model.graph.input:
-        # Generate tensors for external inputs only
-        if graph_input.name not in types_np:
-            continue
-
-        shape_arr = _get_tensor_shape(graph_input)
-
-        input_dict[graph_input.name] = np.random.uniform(size=shape_arr).astype(
-            types_np[graph_input.name]
-        )
-
-    return input_dict
-
-
-def remove_weights_data(onnx_bytes: bytes) -> bytes:
-    """Removes raw weight data from the onnx model."""
-    model = onnx.load_from_string(onnx_bytes)
-    inits = model.graph.initializer
-
-    for idx, init in enumerate(inits):
-        # Only remove arrays with dimension larger than 1
-        if len(init.dims) > 1:
-            dtype = onnx.helper.tensor_dtype_to_np_dtype(init.data_type)
-            if dtype in ["float16", "float32", "float64"]:
-                # Setting up some metadata to randomize the weights later
-                np_tensor = np.frombuffer(init.raw_data, dtype=dtype)
-                meta = model.metadata_props.add()
-                meta.key = init.name + "_avg"
-                meta.value = str(np.average(np_tensor))
-
-                meta = model.metadata_props.add()
-                meta.key = init.name + "_var"
-                meta.value = str(np.var(np_tensor))
-
-                # Note that, onnx.checker will fail due to data cleaning
-                # We should not check the model till weights are reassigned
-                model.graph.initializer[idx].raw_data = b""
-
-    buffer = io.BytesIO()
-    onnx.save_model(model, buffer)
-    buffer.seek(0, 0)
-
-    return buffer.read()
-
-
-def randomize_weights(onnx_path: str) -> None:
-    """Assigns random values to the onnx model weights."""
-    with open(onnx_path, "rb") as f:
-        onnx_bytes = f.read()
-        onnx_bytes = randomize_weights_onnx_bytes(onnx_bytes)
-
-    with open(onnx_path, "wb") as f:
-        # Write the modified onnx model to the same path
-        f.write(onnx_bytes)
-
-
-def randomize_weights_onnx_bytes(onnx_bytes: bytes, seed: int = 0) -> bytes:
-    """Assigns random values to the onnx model weights."""
-    model = onnx.load_from_string(onnx_bytes)
-    inits = model.graph.initializer
-    np.random.seed(seed)
-    weight_metadata = {item.key: item.value for item in model.metadata_props}
-
-    for idx, init in enumerate(inits):
-        # Randomize only the arrays with dimension larger than 1
-        if len(init.dims) > 1:
-            dtype = onnx.helper.tensor_dtype_to_np_dtype(init.data_type)
-            if dtype in ["float16", "float32", "float64"]:
-                avg = weight_metadata.get(init.name + "_avg", None)
-                var = weight_metadata.get(init.name + "_var", None)
-                if avg and var:
-                    numpy_array = np.random.normal(float(avg), float(var), size=init.dims).astype(
-                        dtype
-                    )
-                    tensor = numpy_helper.from_array(numpy_array, init.name)
-                    model.graph.initializer[idx].CopyFrom(tensor)
-
-    buffer = io.BytesIO()
-    onnx.save_model(model, buffer)
-    buffer.seek(0, 0)
-
-    return buffer.read()
-
-
-def validate_onnx(onnx_bytes: bytes) -> bool:
-    """Returns True if the onnx_bytes is valid, else False."""
-    if not onnx_bytes:
-        return False
-
-    try:
-        onnx_model = onnx.load_from_string(onnx_bytes)
-        return onnx_model is not None
-    except Exception:
-        return False
-
-
-def validate_batch_size(onnx_bytes: bytes, batch_size: int) -> bool:
-    """Returns True if all the model inputs has batch dimension equal to batch_size."""
-    input_shapes = list(get_input_shapes_from_bytes(onnx_bytes).values())
-    for shape in input_shapes:
-        if shape[0] != batch_size:
-            return False
-
-    return True
-
-
-def get_batch_size(model: onnx.onnx_ml_pb2.ModelProto) -> int:
-    """Returns the batch size of the given onnx model.
-
-    Assertion will fail if batch size is not same over all the inputs.
-    """
-    input_shapes = list(get_input_shapes(model).values())
-    batch_size = input_shapes[0][0]
-    for shape in input_shapes:
-        if batch_size != shape[0]:
-            # The model does not have the batch dimension
-            return 1
-
-    return batch_size
-
-
-def get_batch_size_from_bytes(onnx_bytes: bytes) -> int:
-    """Returns the batch size of the given onnx model.
-
-    Assertion will fail if batch size is not same over all the inputs.
-    """
-    model = onnx.load_from_string(onnx_bytes)
-    return get_batch_size(model)
-
-
-def save_onnx_bytes_to_dir(onnx_bytes: bytes, onnx_dir: str, onnx_name: str) -> None:
-    """Saves the onnx bytes to a directory with specified file name."""
-    os.makedirs(onnx_dir, exist_ok=True)
-    file_path = os.path.join(onnx_dir, onnx_name + ".onnx")
-
-    try:
-        with open(file_path, "wb") as f:
-            f.write(onnx_bytes)
-        print(f"Onnx model saved as {file_path}")
-    except Exception as e:
-        print(f"Onnx model exporting as {file_path} failed, error {str(e)}")
-
-
-def name_onnx_nodes(graph: onnx.onnx_ml_pb2.GraphProto) -> bool:
-    """Assigns name to the onnx nodes if not present and return the modified status."""
-    is_modified = False
-    node_names = set([node.name for node in graph.node])
-    start_id = len(node_names)
-    for node in graph.node:
-        if not node.name:
-            new_name = f"{node.op_type}_{start_id}"
-            while new_name in node_names:
-                start_id += 1
-                new_name = f"{node.op_type}_{start_id}"
-
-            node.name = new_name
-            node_names.add(new_name)
-            is_modified = True
-
-    return is_modified
-
-
-def duplicate_shared_linear_weights(
-    graph: onnx.onnx_ml_pb2.GraphProto,
-) -> bool:
-    """Duplicate weights of linear operators if they are shared."""
-
-    def _get_tensor_consumer_nodes() -> Dict[str, List[onnx.onnx_ml_pb2.NodeProto]]:
-        tensor_consumers = defaultdict(list)
-        for node in graph.node:
-            for input_name in node.input:
-                tensor_consumers[input_name].append(node)
-
-        return tensor_consumers
-
-    tensor_consumers = _get_tensor_consumer_nodes()
-    graph_nodes = {node.name: (idx, node) for idx, node in enumerate(graph.node)}
-    is_modified = False
-
-    for tensor in graph.initializer:
-        consumers = tensor_consumers[tensor.name]
-        if not consumers:
-            continue
-
-        first_consumer_node = graph_nodes[consumers[0].name][1]
-        assert first_consumer_node
-
-        # Weight duplication is necessary for weighted layers
-        if first_consumer_node.op_type not in ["Conv", "MatMul", "Gemm"]:
-            continue
-
-        dtype = onnx.helper.tensor_dtype_to_np_dtype(tensor.data_type)
-        if dtype not in ["bfloat16", "float16", "float32", "float64"]:
-            continue
-
-        # If the initializer has more than one consumer, duplicate it with renaming for different consumers
-        for idx in range(1, len(consumers)):
-            is_modified = True
-            graph.initializer.append(tensor)
-            graph.initializer[-1].name = tensor.name + "_" + str(idx)
-            consumer_node = graph_nodes[consumers[idx].name][1]
-            assert consumer_node
-            inp_idx = [i for i, x in enumerate(consumer_node.input) if x == tensor.name][0]
-            consumer_node.input[inp_idx] = graph.initializer[-1].name
-
-    return is_modified
-
-
-def is_valid_onnx_model(file_path):
-    """Checks if the given file is a valid ONNX model."""
-    if not os.path.exists(file_path):
-        print(f"No file found at {file_path}")
-        return False
-
-    try:
-        # Load the ONNX model
-        model = onnx.load(file_path)
-
-        # Check the model
-        onnx.checker.check_model(model)
-        print(f"ONNX model at {file_path} is valid.")
-        return True
-    except C.ValidationError as e:
-        print(f"The file is not a valid ONNX model. {e}")
-    except Exception as e:
-        print(f"An error occurred: {e}")
-
-    return False
-
-
-def find_lowest_common_ancestor(node1: Node, node2: Node) -> Tuple[Optional[str], int, int]:
-    """Function to find the lowest common ancestor of two nodes.
-
-    Args:
-        node1: First node name.
-        node2: Second node name.
-
-    Returns:
-        LCA node.
-        Distance from first node.
-        Distance from second node.
-    """
-
-    def _find_ancestors(node: Node):
-        ancestors = {node.name: 0}
-        stack = [(node, 0)]
-        while stack:
-            cur_node, distance = stack.pop()
-            for parent_node in get_parent_nodes(cur_node):
-                if parent_node.name not in ancestors:
-                    ancestors[parent_node.name] = distance + 1
-                    stack.append((parent_node, distance + 1))
-
-        return ancestors
-
-    ancestors1 = _find_ancestors(node1)
-    ancestors2 = _find_ancestors(node2)
-
-    # Find the lowest common ancestor
-    common_ancestors = set(ancestors1.keys()).intersection(ancestors2.keys())
-    if common_ancestors:
-        lowest_common_ancestor = common_ancestors.pop()
-        distance1 = ancestors1[lowest_common_ancestor]
-        distance2 = ancestors2[lowest_common_ancestor]
-        return lowest_common_ancestor, distance1, distance2
-    else:
-        return None, -1, -1  # No common ancestor found
-
-
-def get_parent_nodes(node: Node) -> List[Node]:
-    """Returns list of input producer nodes for the given node."""
-    parents = []
-    for tensor in node.inputs:
-        # If the tensor is not a constant or graph input and has a producer,
-        # the producer is a parent of node `node`
-        if len(tensor.inputs) == 1:
-            parents.append(tensor.inputs[0])
-
-    return parents
-
-
-def get_child_nodes(node: Node) -> List[Node]:
-    """Returns list of output consumer nodes for the given node."""
-    children = []
-    for tensor in node.outputs:
-        for consumer in tensor.outputs:  # Traverse all consumer of the tensor
-            children.append(consumer)
-
-    return children
-
-
-def get_variable_inputs(node: Node) -> List[Variable]:
-    """Returns the variable inputs of the given Node."""
-    var_inputs = []
-    for tensor in node.inputs:
-        if isinstance(tensor, Variable):
-            if not tensor.inputs or (tensor.inputs and tensor.inputs[0].op != "Constant"):
-                var_inputs.append(tensor)
-    return var_inputs
-
-
-def save_onnx(
-    onnx_model: onnx.onnx_ml_pb2.ModelProto, onnx_path: str, save_as_external_data: bool = False
-):
-    """Save an ONNX model to given path."""
-    onnx.save_model(
-        onnx_model,
-        onnx_path,
-        save_as_external_data=save_as_external_data,
-        location=os.path.basename(onnx_path) + "_data",
-        size_threshold=0,
-    )
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions related to onnx."""
+import io
+import os
+from collections import defaultdict
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+import numpy as np
+import onnx
+import onnx.onnx_cpp2py_export.checker as C  # noqa: N812
+from onnx import numpy_helper
+from onnx_graphsurgeon import Node, Variable
+
+
+def get_input_names_from_bytes(model_bytes: bytes, external_inputs_only: bool = True) -> List[str]:
+    """This function returns the inputs names of the given onnx model in bytes.
+
+    Args:
+        model_bytes: Onnx model in bytes.
+
+    Returns:
+        List of input names of the model.
+    """
+    model = onnx.load_from_string(model_bytes)
+    return get_input_names(model, external_inputs_only)
+
+
+def get_all_input_names(model: onnx.onnx_ml_pb2.ModelProto) -> List[str]:
+    """This function returns the inputs names of the given onnx model."""
+    return [graph_input.name for graph_input in model.graph.input]
+
+
+def _get_initializer_names(model: onnx.onnx_ml_pb2.ModelProto) -> List[str]:
+    return [initializer.name for initializer in model.graph.initializer]
+
+
+def get_input_names(
+    model: onnx.onnx_ml_pb2.ModelProto, external_inputs_only: bool = True
+) -> List[str]:
+    """This function returns the external inputs names of the given onnx model.
+
+    Note: external_input_names = input_names - initializer_names
+
+    Args:
+        model: Loaded in-memory onnx ModelProto.
+
+    Returns:
+        List of external input names of the model.
+    """
+    input_names = get_all_input_names(model)
+    if not external_inputs_only:
+        return input_names
+
+    initializer_names = _get_initializer_names(model)
+    external_input_names = list(np.setdiff1d(input_names, initializer_names))
+    return external_input_names
+
+
+def get_output_names_from_bytes(model_bytes: bytes) -> List[str]:
+    """This function returns the output names of the given onnx model in bytes.
+
+    Args:
+        model_bytes: Onnx model in bytes.
+
+    Returns:
+        List of output names of the model.
+    """
+    model = onnx.load_from_string(model_bytes)
+    return get_output_names(model)
+
+
+def get_output_names(model: onnx.onnx_ml_pb2.ModelProto) -> List[str]:
+    """This function returns the output names of the given onnx model.
+
+    Args:
+        model: Loaded in-memory onnx ModelProto.
+
+    Returns:
+        List of output names of the model.
+    """
+    return [output.name for output in model.graph.output]
+
+
+def get_node_names_from_bytes(model_bytes: bytes) -> List[str]:
+    """This function returns all node names from the given onnx model in bytes.
+
+    Args:
+        model: onnx model in bytes.
+
+    Returns:
+        List of node names of the model.
+    """
+    model = onnx.load_from_string(model_bytes)
+    return get_node_names(model)
+
+
+def get_node_names(model: onnx.onnx_ml_pb2.ModelProto) -> List[str]:
+    """This function returns all node names from the given onnx model.
+
+    Args:
+        model: Loaded in-memory onnx ModelProto.
+
+    Returns:
+        List of node names of the model.
+    """
+    return [node.name for node in model.graph.node]
+
+
+def _get_tensor_shape(tensor: onnx.onnx_ml_pb2.ValueInfoProto) -> List[int]:
+    """This function returns the shape of the input onnx tensor.
+
+    Onnx tensors are of type ValueInfoProto and their dimensions are stored in a
+    RepeatedCompositeContainer. Each of these dimensions is of type onnx.onnx_ml_pb2.Dimension.
+    In a loop we access each of the Dimension object and create a shape list to return.
+    Dynamic dimensions (i.e. with "dim_param" field) are replaced with 1.
+
+    Args:
+        tensor: Onnx tensor object for which the shape needs to be computed.
+
+    Returns:
+        Shape of the input tensor.
+    """
+    if not hasattr(tensor.type, "tensor_type"):
+        raise NotImplementedError("Only tensor type inputs are supported.")
+
+    dimensions = tensor.type.tensor_type.shape.dim
+    shape = []
+    for dim in dimensions:
+        if dim.HasField("dim_param"):
+            shape.append(1)
+        if dim.HasField("dim_value"):
+            if dim.dim_value == -1:
+                shape.append(1)
+            else:
+                shape.append(dim.dim_value)
+
+    return shape
+
+
+def _get_all_shapes(container: Any) -> Dict[str, List[int]]:
+    """This method returns the shape of tensors within a RepeatedCompositeContainer.
+
+    Args:
+        container: Model graph input/output container.
+
+    Returns:
+        Dictionary of tensor names and shape of the tensors within the container.
+    """
+    results = {}
+    for tensor in container:
+        results[tensor.name] = _get_tensor_shape(tensor)
+    return results
+
+
+def _get_selected_shapes(container: Any, inputs_to_include: List[str]) -> Dict[str, List[int]]:
+    """This method returns the shape tensors within a RepeatedCompositeContainer.
+
+    It only computes the shape of the tensors with name containing in `inputs_to_include` list.
+
+    Args:
+        container: Model graph input/output container.
+
+    Returns:
+        Dictionary of tensor names in inputs_to_include and their shapes.
+    """
+    results = {}
+    for tensor in container:
+        if tensor.name in inputs_to_include:
+            results[tensor.name] = _get_tensor_shape(tensor)
+    return results
+
+
+def get_input_shapes_from_bytes(model_bytes: bytes) -> Dict[str, List[int]]:
+    """This function returns the input shapes of the given onnx model in bytes.
+
+    Args:
+        model_bytes: Onnx model in bytes.
+
+    Returns:
+        Dictionary of inputs names and shapes.
+    """
+    model = onnx.load_from_string(model_bytes)
+    return get_input_shapes(model)
+
+
+def get_input_shapes(
+    model: onnx.onnx_ml_pb2.ModelProto, external_inputs_only: bool = True
+) -> Dict[str, List[int]]:
+    """This function returns the inputs shapes for the given onnx model."""
+    if external_inputs_only:
+        return _get_selected_shapes(model.graph.input, get_input_names(model))
+    return _get_all_shapes(model.graph.input)
+
+
+def get_output_shapes(model: onnx.onnx_ml_pb2.ModelProto) -> Dict[str, List[int]]:
+    """This function returns the output shapes for the given onnx model."""
+    return _get_all_shapes(model.graph.output)
+
+
+def _get_tensor_type(tensor: onnx.onnx_ml_pb2.ValueInfoProto) -> int:
+    if not hasattr(tensor.type, "tensor_type"):
+        raise NotImplementedError("Only tensor type inputs are supported.")
+    type_ = tensor.type.tensor_type.elem_type
+    return type_
+
+
+def _get_container_types(
+    container, inputs_to_include: Union[None, List[str]] = None
+) -> Dict[str, int]:
+    results = {}
+    for tensor in container:
+        if inputs_to_include is not None:
+            if tensor.name not in inputs_to_include:
+                continue
+        t_type = _get_tensor_type(tensor)
+        results[tensor.name] = t_type
+    return results
+
+
+def _get_input_types(
+    model: onnx.onnx_ml_pb2.ModelProto, external_inputs_only: bool = True
+) -> Dict[str, int]:
+    inputs_to_include = get_input_names(model, external_inputs_only)
+    return _get_container_types(model.graph.input, inputs_to_include)
+
+
+def _get_output_types(model: onnx.onnx_ml_pb2.ModelProto) -> Dict[str, int]:
+    results = _get_container_types(model.graph.output)
+    return results
+
+
+def _convert_types_to_np(types: Union[Dict[str, int], List[int], int]) -> Any:
+    if isinstance(types, dict):
+        types_np = {}
+        for name in types.keys():
+            types_np[name] = onnx.helper.tensor_dtype_to_np_dtype(types[name])
+        return types_np
+    elif isinstance(types, list):
+        return [onnx.helper.tensor_dtype_to_np_dtype(type_) for type_ in types]
+    else:
+        return onnx.helper.tensor_dtype_to_np_dtype(types)
+
+
+def gen_random_inputs(model: onnx.onnx_ml_pb2.ModelProto) -> Dict[str, np.ndarray]:
+    """This function generates random inputs for an onnx model.
+
+    Args:
+        model: Loaded in-memory onnx ModelProto.
+
+    Returns:
+        Dictionary of numpy tensors.
+    """
+    input_dict = {}
+    types = _get_input_types(model)
+    types_np = _convert_types_to_np(types)
+
+    for graph_input in model.graph.input:
+        # Generate tensors for external inputs only
+        if graph_input.name not in types_np:
+            continue
+
+        shape_arr = _get_tensor_shape(graph_input)
+
+        input_dict[graph_input.name] = np.random.uniform(size=shape_arr).astype(
+            types_np[graph_input.name]
+        )
+
+    return input_dict
+
+
+def remove_weights_data(onnx_bytes: bytes) -> bytes:
+    """Removes raw weight data from the onnx model."""
+    model = onnx.load_from_string(onnx_bytes)
+    inits = model.graph.initializer
+
+    for idx, init in enumerate(inits):
+        # Only remove arrays with dimension larger than 1
+        if len(init.dims) > 1:
+            dtype = onnx.helper.tensor_dtype_to_np_dtype(init.data_type)
+            if dtype in ["float16", "float32", "float64"]:
+                # Setting up some metadata to randomize the weights later
+                np_tensor = np.frombuffer(init.raw_data, dtype=dtype)
+                meta = model.metadata_props.add()
+                meta.key = init.name + "_avg"
+                meta.value = str(np.average(np_tensor))
+
+                meta = model.metadata_props.add()
+                meta.key = init.name + "_var"
+                meta.value = str(np.var(np_tensor))
+
+                # Note that, onnx.checker will fail due to data cleaning
+                # We should not check the model till weights are reassigned
+                model.graph.initializer[idx].raw_data = b""
+
+    buffer = io.BytesIO()
+    onnx.save_model(model, buffer)
+    buffer.seek(0, 0)
+
+    return buffer.read()
+
+
+def randomize_weights(onnx_path: str) -> None:
+    """Assigns random values to the onnx model weights."""
+    with open(onnx_path, "rb") as f:
+        onnx_bytes = f.read()
+        onnx_bytes = randomize_weights_onnx_bytes(onnx_bytes)
+
+    with open(onnx_path, "wb") as f:
+        # Write the modified onnx model to the same path
+        f.write(onnx_bytes)
+
+
+def randomize_weights_onnx_bytes(onnx_bytes: bytes, seed: int = 0) -> bytes:
+    """Assigns random values to the onnx model weights."""
+    model = onnx.load_from_string(onnx_bytes)
+    inits = model.graph.initializer
+    np.random.seed(seed)
+    weight_metadata = {item.key: item.value for item in model.metadata_props}
+
+    for idx, init in enumerate(inits):
+        # Randomize only the arrays with dimension larger than 1
+        if len(init.dims) > 1:
+            dtype = onnx.helper.tensor_dtype_to_np_dtype(init.data_type)
+            if dtype in ["float16", "float32", "float64"]:
+                avg = weight_metadata.get(init.name + "_avg", None)
+                var = weight_metadata.get(init.name + "_var", None)
+                if avg and var:
+                    numpy_array = np.random.normal(float(avg), float(var), size=init.dims).astype(
+                        dtype
+                    )
+                    tensor = numpy_helper.from_array(numpy_array, init.name)
+                    model.graph.initializer[idx].CopyFrom(tensor)
+
+    buffer = io.BytesIO()
+    onnx.save_model(model, buffer)
+    buffer.seek(0, 0)
+
+    return buffer.read()
+
+
+def validate_onnx(onnx_bytes: bytes) -> bool:
+    """Returns True if the onnx_bytes is valid, else False."""
+    if not onnx_bytes:
+        return False
+
+    try:
+        onnx_model = onnx.load_from_string(onnx_bytes)
+        return onnx_model is not None
+    except Exception:
+        return False
+
+
+def validate_batch_size(onnx_bytes: bytes, batch_size: int) -> bool:
+    """Returns True if all the model inputs has batch dimension equal to batch_size."""
+    input_shapes = list(get_input_shapes_from_bytes(onnx_bytes).values())
+    for shape in input_shapes:
+        if shape[0] != batch_size:
+            return False
+
+    return True
+
+
+def get_batch_size(model: onnx.onnx_ml_pb2.ModelProto) -> int:
+    """Returns the batch size of the given onnx model.
+
+    Assertion will fail if batch size is not same over all the inputs.
+    """
+    input_shapes = list(get_input_shapes(model).values())
+    batch_size = input_shapes[0][0]
+    for shape in input_shapes:
+        if batch_size != shape[0]:
+            # The model does not have the batch dimension
+            return 1
+
+    return batch_size
+
+
+def get_batch_size_from_bytes(onnx_bytes: bytes) -> int:
+    """Returns the batch size of the given onnx model.
+
+    Assertion will fail if batch size is not same over all the inputs.
+    """
+    model = onnx.load_from_string(onnx_bytes)
+    return get_batch_size(model)
+
+
+def save_onnx_bytes_to_dir(onnx_bytes: bytes, onnx_dir: str, onnx_name: str) -> None:
+    """Saves the onnx bytes to a directory with specified file name."""
+    os.makedirs(onnx_dir, exist_ok=True)
+    file_path = os.path.join(onnx_dir, onnx_name + ".onnx")
+
+    try:
+        with open(file_path, "wb") as f:
+            f.write(onnx_bytes)
+        print(f"Onnx model saved as {file_path}")
+    except Exception as e:
+        print(f"Onnx model exporting as {file_path} failed, error {str(e)}")
+
+
+def name_onnx_nodes(graph: onnx.onnx_ml_pb2.GraphProto) -> bool:
+    """Assigns name to the onnx nodes if not present and return the modified status."""
+    is_modified = False
+    node_names = set([node.name for node in graph.node])
+    start_id = len(node_names)
+    for node in graph.node:
+        if not node.name:
+            new_name = f"{node.op_type}_{start_id}"
+            while new_name in node_names:
+                start_id += 1
+                new_name = f"{node.op_type}_{start_id}"
+
+            node.name = new_name
+            node_names.add(new_name)
+            is_modified = True
+
+    return is_modified
+
+
+def duplicate_shared_linear_weights(
+    graph: onnx.onnx_ml_pb2.GraphProto,
+) -> bool:
+    """Duplicate weights of linear operators if they are shared."""
+
+    def _get_tensor_consumer_nodes() -> Dict[str, List[onnx.onnx_ml_pb2.NodeProto]]:
+        tensor_consumers = defaultdict(list)
+        for node in graph.node:
+            for input_name in node.input:
+                tensor_consumers[input_name].append(node)
+
+        return tensor_consumers
+
+    tensor_consumers = _get_tensor_consumer_nodes()
+    graph_nodes = {node.name: (idx, node) for idx, node in enumerate(graph.node)}
+    is_modified = False
+
+    for tensor in graph.initializer:
+        consumers = tensor_consumers[tensor.name]
+        if not consumers:
+            continue
+
+        first_consumer_node = graph_nodes[consumers[0].name][1]
+        assert first_consumer_node
+
+        # Weight duplication is necessary for weighted layers
+        if first_consumer_node.op_type not in ["Conv", "MatMul", "Gemm"]:
+            continue
+
+        dtype = onnx.helper.tensor_dtype_to_np_dtype(tensor.data_type)
+        if dtype not in ["bfloat16", "float16", "float32", "float64"]:
+            continue
+
+        # If the initializer has more than one consumer, duplicate it with renaming for different consumers
+        for idx in range(1, len(consumers)):
+            is_modified = True
+            graph.initializer.append(tensor)
+            graph.initializer[-1].name = tensor.name + "_" + str(idx)
+            consumer_node = graph_nodes[consumers[idx].name][1]
+            assert consumer_node
+            inp_idx = [i for i, x in enumerate(consumer_node.input) if x == tensor.name][0]
+            consumer_node.input[inp_idx] = graph.initializer[-1].name
+
+    return is_modified
+
+
+def is_valid_onnx_model(file_path):
+    """Checks if the given file is a valid ONNX model."""
+    if not os.path.exists(file_path):
+        print(f"No file found at {file_path}")
+        return False
+
+    try:
+        # Load the ONNX model
+        model = onnx.load(file_path)
+
+        # Check the model
+        onnx.checker.check_model(model)
+        print(f"ONNX model at {file_path} is valid.")
+        return True
+    except C.ValidationError as e:
+        print(f"The file is not a valid ONNX model. {e}")
+    except Exception as e:
+        print(f"An error occurred: {e}")
+
+    return False
+
+
+def find_lowest_common_ancestor(node1: Node, node2: Node) -> Tuple[Optional[str], int, int]:
+    """Function to find the lowest common ancestor of two nodes.
+
+    Args:
+        node1: First node name.
+        node2: Second node name.
+
+    Returns:
+        LCA node.
+        Distance from first node.
+        Distance from second node.
+    """
+
+    def _find_ancestors(node: Node):
+        ancestors = {node.name: 0}
+        stack = [(node, 0)]
+        while stack:
+            cur_node, distance = stack.pop()
+            for parent_node in get_parent_nodes(cur_node):
+                if parent_node.name not in ancestors:
+                    ancestors[parent_node.name] = distance + 1
+                    stack.append((parent_node, distance + 1))
+
+        return ancestors
+
+    ancestors1 = _find_ancestors(node1)
+    ancestors2 = _find_ancestors(node2)
+
+    # Find the lowest common ancestor
+    common_ancestors = set(ancestors1.keys()).intersection(ancestors2.keys())
+    if common_ancestors:
+        lowest_common_ancestor = common_ancestors.pop()
+        distance1 = ancestors1[lowest_common_ancestor]
+        distance2 = ancestors2[lowest_common_ancestor]
+        return lowest_common_ancestor, distance1, distance2
+    else:
+        return None, -1, -1  # No common ancestor found
+
+
+def get_parent_nodes(node: Node) -> List[Node]:
+    """Returns list of input producer nodes for the given node."""
+    parents = []
+    for tensor in node.inputs:
+        # If the tensor is not a constant or graph input and has a producer,
+        # the producer is a parent of node `node`
+        if len(tensor.inputs) == 1:
+            parents.append(tensor.inputs[0])
+
+    return parents
+
+
+def get_child_nodes(node: Node) -> List[Node]:
+    """Returns list of output consumer nodes for the given node."""
+    children = []
+    for tensor in node.outputs:
+        for consumer in tensor.outputs:  # Traverse all consumer of the tensor
+            children.append(consumer)
+
+    return children
+
+
+def get_variable_inputs(node: Node) -> List[Variable]:
+    """Returns the variable inputs of the given Node."""
+    var_inputs = []
+    for tensor in node.inputs:
+        if isinstance(tensor, Variable):
+            if not tensor.inputs or (tensor.inputs and tensor.inputs[0].op != "Constant"):
+                var_inputs.append(tensor)
+    return var_inputs
+
+
+def save_onnx(
+    onnx_model: onnx.onnx_ml_pb2.ModelProto, onnx_path: str, save_as_external_data: bool = False
+):
+    """Save an ONNX model to given path."""
+    onnx.save_model(
+        onnx_model,
+        onnx_path,
+        save_as_external_data=save_as_external_data,
+        location=os.path.basename(onnx_path) + "_data",
+        size_threshold=0,
+    )
```

## modelopt/onnx/quantization/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Model optimization subpackage for onnx quantization."""
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Model optimization subpackage for onnx quantization."""
```

## modelopt/onnx/quantization/__main__.py

 * *Ordering differences only*

```diff
@@ -1,138 +1,138 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-"""Command-line entrypoint for ONNX PTQ."""
-
-import argparse
-
-import numpy as np
-import onnx
-
-from ..utils import gen_random_inputs
-from .quantize import quantize
-
-__all__ = ["main"]
-
-
-def parse_args():
-    argparser = argparse.ArgumentParser("python -m modelopt.onnx.quantization")
-    argparser.add_argument(
-        "--onnx_path", required=True, type=str, help="Input onnx model without Q/DQ nodes."
-    )
-    argparser.add_argument(
-        "--output_path",
-        type=str,
-        default=None,
-        help=(
-            "Output filename to save the converted ONNX model. If None, save it in the same dir as"
-            " the original ONNX model with an appropriate suffix."
-        ),
-    )
-    argparser.add_argument(
-        "--calibration_data_path",
-        "-c",
-        type=str,
-        default=None,
-        help="Calibration data in npz/npy format. If None, use random data for calibration.",
-    )
-    argparser.add_argument(
-        "--calibration_method",
-        type=str,
-        default="entropy",
-        help="Calibration method. Options={entropy (default), minmax}.",
-    )
-    argparser.add_argument(
-        "--op_types_to_quantize",
-        "-t",
-        type=str,
-        default=[],
-        nargs="+",
-        help="A space-separated list of types of nodes to quantize.",
-    )
-    argparser.add_argument(
-        "--op_types_to_exclude",
-        "-e",
-        type=str,
-        default=[],
-        nargs="+",
-        help="A space-separated list of types of nodes to exclude from quantization.",
-    )
-    argparser.add_argument(
-        "--nodes_to_quantize",
-        "-q",
-        type=str,
-        default=[],
-        nargs="+",
-        help="A space-separated list of node names to quantize.",
-    )
-    argparser.add_argument(
-        "--nodes_to_exclude",
-        "-x",
-        type=str,
-        default=[],
-        nargs="+",
-        help="A space-separated list of node names to exclude from quantization.",
-    )
-    argparser.add_argument(
-        "--keep_intermediate_files",
-        action="store_true",
-        help=(
-            "If True, keep the files generated during the ONNX models' conversion/calibration."
-            "Otherwise, only the converted ONNX file is kept for the user."
-        ),
-    )
-    argparser.add_argument(
-        "--use_external_data_format",
-        action="store_true",
-        help="If True, <MODEL_NAME>.onnx_data will be used to load weights and constants.",
-    )
-    argparser.add_argument(
-        "--verbose",
-        action="store_true",
-        help="If verbose, print all the debug info.",
-    )
-    argparser.add_argument(
-        "--quantize_mode",
-        type=str,
-        default="int8",
-        help=(
-            "Quantization Mode. One of ['int8', 'int4_rtn', 'int4_rtn_dq', 'int4_rtn_trt',"
-            " 'int4_rtn_trt_dq', 'int4_awq_clip', 'int4_awq_clip_trt', 'fp8']"
-        ),
-    )
-    return argparser.parse_args()
-
-
-def main():
-    """Command-line entrypoint for ONNX PTQ."""
-    args = parse_args()
-    if args.calibration_data_path:
-        calibration_data = np.load(args.calibration_data_path, allow_pickle=True)
-    else:
-        print("WARNING: No calibration data provided. Using random data for calibration.")
-        calibration_data = gen_random_inputs(onnx.load(args.onnx_path))
-
-    quantize(
-        args.onnx_path,
-        calibration_data=calibration_data,
-        calibration_method=args.calibration_method,
-        op_types_to_quantize=args.op_types_to_quantize,
-        op_types_to_exclude=args.op_types_to_exclude,
-        nodes_to_quantize=args.nodes_to_quantize,
-        nodes_to_exclude=args.nodes_to_exclude,
-        use_external_data_format=args.use_external_data_format,
-        keep_intermediate_files=args.keep_intermediate_files,
-        output_path=args.output_path,
-        verbose=args.verbose,
-        quantize_mode=args.quantize_mode,
-    )
-
-
-if __name__ == "__main__":
-    main()
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+"""Command-line entrypoint for ONNX PTQ."""
+
+import argparse
+
+import numpy as np
+import onnx
+
+from ..utils import gen_random_inputs
+from .quantize import quantize
+
+__all__ = ["main"]
+
+
+def parse_args():
+    argparser = argparse.ArgumentParser("python -m modelopt.onnx.quantization")
+    argparser.add_argument(
+        "--onnx_path", required=True, type=str, help="Input onnx model without Q/DQ nodes."
+    )
+    argparser.add_argument(
+        "--output_path",
+        type=str,
+        default=None,
+        help=(
+            "Output filename to save the converted ONNX model. If None, save it in the same dir as"
+            " the original ONNX model with an appropriate suffix."
+        ),
+    )
+    argparser.add_argument(
+        "--calibration_data_path",
+        "-c",
+        type=str,
+        default=None,
+        help="Calibration data in npz/npy format. If None, use random data for calibration.",
+    )
+    argparser.add_argument(
+        "--calibration_method",
+        type=str,
+        default="entropy",
+        help="Calibration method. Options={entropy (default), minmax}.",
+    )
+    argparser.add_argument(
+        "--op_types_to_quantize",
+        "-t",
+        type=str,
+        default=[],
+        nargs="+",
+        help="A space-separated list of types of nodes to quantize.",
+    )
+    argparser.add_argument(
+        "--op_types_to_exclude",
+        "-e",
+        type=str,
+        default=[],
+        nargs="+",
+        help="A space-separated list of types of nodes to exclude from quantization.",
+    )
+    argparser.add_argument(
+        "--nodes_to_quantize",
+        "-q",
+        type=str,
+        default=[],
+        nargs="+",
+        help="A space-separated list of node names to quantize.",
+    )
+    argparser.add_argument(
+        "--nodes_to_exclude",
+        "-x",
+        type=str,
+        default=[],
+        nargs="+",
+        help="A space-separated list of node names to exclude from quantization.",
+    )
+    argparser.add_argument(
+        "--keep_intermediate_files",
+        action="store_true",
+        help=(
+            "If True, keep the files generated during the ONNX models' conversion/calibration."
+            "Otherwise, only the converted ONNX file is kept for the user."
+        ),
+    )
+    argparser.add_argument(
+        "--use_external_data_format",
+        action="store_true",
+        help="If True, <MODEL_NAME>.onnx_data will be used to load weights and constants.",
+    )
+    argparser.add_argument(
+        "--verbose",
+        action="store_true",
+        help="If verbose, print all the debug info.",
+    )
+    argparser.add_argument(
+        "--quantize_mode",
+        type=str,
+        default="int8",
+        help=(
+            "Quantization Mode. One of ['int8', 'int4_rtn', 'int4_rtn_dq', 'int4_rtn_trt',"
+            " 'int4_rtn_trt_dq', 'int4_awq_clip', 'int4_awq_clip_trt', 'fp8']"
+        ),
+    )
+    return argparser.parse_args()
+
+
+def main():
+    """Command-line entrypoint for ONNX PTQ."""
+    args = parse_args()
+    if args.calibration_data_path:
+        calibration_data = np.load(args.calibration_data_path, allow_pickle=True)
+    else:
+        print("WARNING: No calibration data provided. Using random data for calibration.")
+        calibration_data = gen_random_inputs(onnx.load(args.onnx_path))
+
+    quantize(
+        args.onnx_path,
+        calibration_data=calibration_data,
+        calibration_method=args.calibration_method,
+        op_types_to_quantize=args.op_types_to_quantize,
+        op_types_to_exclude=args.op_types_to_exclude,
+        nodes_to_quantize=args.nodes_to_quantize,
+        nodes_to_exclude=args.nodes_to_exclude,
+        use_external_data_format=args.use_external_data_format,
+        keep_intermediate_files=args.keep_intermediate_files,
+        output_path=args.output_path,
+        verbose=args.verbose,
+        quantize_mode=args.quantize_mode,
+    )
+
+
+if __name__ == "__main__":
+    main()
```

## modelopt/onnx/quantization/calib_utils.py

 * *Ordering differences only*

```diff
@@ -1,100 +1,100 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Provides basic calibration utils."""
-
-from typing import Dict, List, Union
-
-import numpy as np
-import onnx
-from onnxruntime.quantization import CalibrationDataReader
-
-from modelopt.onnx.utils import gen_random_inputs, get_input_names, get_input_shapes
-
-CalibrationDataType = Union[np.ndarray, Dict[str, np.ndarray]]
-
-
-class CalibrationDataProvider(CalibrationDataReader):
-    """Calibration data provider class."""
-
-    def __init__(self, onnx_path: str, calibration_data: CalibrationDataType):
-        """Intializes the data provider class with the calibration data iterator.
-
-        Args:
-            onnx_path: Path to the ONNX model.
-            calibration_data: Numpy data to calibrate the model.
-                Ex. If a model has input shapes like {"sample": (2, 4, 64, 64), "timestep": (1,),
-                "encoder_hidden_states": (2, 16, 768)}, the calibration data should have dictionary
-                of tensors with shapes like {"sample": (1024, 4, 64, 64), "timestep": (512,),
-                "encoder_hidden_states": (1024, 16, 768)} to calibrate with 512 samples.
-        """
-        onnx_model = onnx.load(onnx_path)
-        input_names = get_input_names(onnx_model)
-        input_shapes = get_input_shapes(onnx_model)
-
-        # Validate calibration data against expected inputs by the model
-        if isinstance(calibration_data, np.ndarray):
-            assert len(input_names) == 1, "Calibration data has only one tensor."
-            calibration_data = {input_names[0]: calibration_data}
-        elif isinstance(calibration_data, dict):
-            assert len(input_names) == len(
-                calibration_data
-            ), "Model input count and calibration data doesn't match."
-            for input_name in input_names:
-                assert input_name in calibration_data
-        else:
-            raise ValueError(
-                f"calibration data must be numpy array or dict, got {type(calibration_data)}"
-            )
-
-        # Check against expected input shapes except batch dimension
-        for input_name in input_names:
-            assert (
-                input_shapes[input_name][1:] == list(calibration_data[input_name].shape)[1:]
-            ), f"Tensor shape does't match for {input_name}."
-
-        # Validate calibration data size across different inputs
-        n_itr = int(
-            calibration_data[input_names[0]].shape[0] / max(1, input_shapes[input_names[0]][0])
-        )
-        for input_name in calibration_data.keys():
-            assert n_itr <= int(
-                calibration_data[input_name].shape[0] / max(1, input_shapes[input_name][0])
-            )
-            assert calibration_data[input_name].shape[0] >= max(1, input_shapes[input_name][0])
-
-        # Create list of model inputs with appropriate batch size
-        # So that we can create an input iterator
-        calibration_data_list = [{}] * n_itr
-        for input_name in input_names:
-            for idx, calib_data in enumerate(
-                np.array_split(calibration_data[input_name], n_itr, axis=0)
-            ):
-                calibration_data_list[idx][input_name] = calib_data
-
-        self.calibration_data_reader = iter(calibration_data_list)
-
-    def get_next(self):
-        """Returns the next available calibration input from the reader."""
-        return next(self.calibration_data_reader, None)
-
-
-class RandomDataProvider(CalibrationDataReader):
-    """Calibration data reader class with random data provider."""
-
-    def __init__(self, onnx_path: str):
-        """Initializes the data reader class with random calibration data."""
-        onnx_model = onnx.load(onnx_path)
-        calibration_data_list: List[Dict[str, np.ndarray]] = [gen_random_inputs(onnx_model)]
-        self.calibration_data_reader = iter(calibration_data_list)
-
-    def get_next(self):
-        """Returns the next available calibration input from the reader."""
-        return next(self.calibration_data_reader, None)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Provides basic calibration utils."""
+
+from typing import Dict, List, Union
+
+import numpy as np
+import onnx
+from onnxruntime.quantization import CalibrationDataReader
+
+from modelopt.onnx.utils import gen_random_inputs, get_input_names, get_input_shapes
+
+CalibrationDataType = Union[np.ndarray, Dict[str, np.ndarray]]
+
+
+class CalibrationDataProvider(CalibrationDataReader):
+    """Calibration data provider class."""
+
+    def __init__(self, onnx_path: str, calibration_data: CalibrationDataType):
+        """Intializes the data provider class with the calibration data iterator.
+
+        Args:
+            onnx_path: Path to the ONNX model.
+            calibration_data: Numpy data to calibrate the model.
+                Ex. If a model has input shapes like {"sample": (2, 4, 64, 64), "timestep": (1,),
+                "encoder_hidden_states": (2, 16, 768)}, the calibration data should have dictionary
+                of tensors with shapes like {"sample": (1024, 4, 64, 64), "timestep": (512,),
+                "encoder_hidden_states": (1024, 16, 768)} to calibrate with 512 samples.
+        """
+        onnx_model = onnx.load(onnx_path)
+        input_names = get_input_names(onnx_model)
+        input_shapes = get_input_shapes(onnx_model)
+
+        # Validate calibration data against expected inputs by the model
+        if isinstance(calibration_data, np.ndarray):
+            assert len(input_names) == 1, "Calibration data has only one tensor."
+            calibration_data = {input_names[0]: calibration_data}
+        elif isinstance(calibration_data, dict):
+            assert len(input_names) == len(
+                calibration_data
+            ), "Model input count and calibration data doesn't match."
+            for input_name in input_names:
+                assert input_name in calibration_data
+        else:
+            raise ValueError(
+                f"calibration data must be numpy array or dict, got {type(calibration_data)}"
+            )
+
+        # Check against expected input shapes except batch dimension
+        for input_name in input_names:
+            assert (
+                input_shapes[input_name][1:] == list(calibration_data[input_name].shape)[1:]
+            ), f"Tensor shape does't match for {input_name}."
+
+        # Validate calibration data size across different inputs
+        n_itr = int(
+            calibration_data[input_names[0]].shape[0] / max(1, input_shapes[input_names[0]][0])
+        )
+        for input_name in calibration_data.keys():
+            assert n_itr <= int(
+                calibration_data[input_name].shape[0] / max(1, input_shapes[input_name][0])
+            )
+            assert calibration_data[input_name].shape[0] >= max(1, input_shapes[input_name][0])
+
+        # Create list of model inputs with appropriate batch size
+        # So that we can create an input iterator
+        calibration_data_list = [{}] * n_itr
+        for input_name in input_names:
+            for idx, calib_data in enumerate(
+                np.array_split(calibration_data[input_name], n_itr, axis=0)
+            ):
+                calibration_data_list[idx][input_name] = calib_data
+
+        self.calibration_data_reader = iter(calibration_data_list)
+
+    def get_next(self):
+        """Returns the next available calibration input from the reader."""
+        return next(self.calibration_data_reader, None)
+
+
+class RandomDataProvider(CalibrationDataReader):
+    """Calibration data reader class with random data provider."""
+
+    def __init__(self, onnx_path: str):
+        """Initializes the data reader class with random calibration data."""
+        onnx_model = onnx.load(onnx_path)
+        calibration_data_list: List[Dict[str, np.ndarray]] = [gen_random_inputs(onnx_model)]
+        self.calibration_data_reader = iter(calibration_data_list)
+
+    def get_next(self):
+        """Returns the next available calibration input from the reader."""
+        return next(self.calibration_data_reader, None)
```

## modelopt/onnx/quantization/graph_utils.py

 * *Ordering differences only*

```diff
@@ -1,459 +1,459 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Provides ONNX graph related utils for QDQ placement."""
-import logging
-from typing import Dict, List, Optional, Tuple
-
-from onnx_graphsurgeon.ir.graph import Graph
-from onnx_graphsurgeon.ir.node import Node
-from onnx_graphsurgeon.ir.tensor import Constant, Tensor
-
-from modelopt.onnx.op_types import is_copy_op, is_linear_op
-from modelopt.onnx.utils import (
-    find_lowest_common_ancestor,
-    get_child_nodes,
-    get_parent_nodes,
-)
-
-
-def is_const_input(tensor: Tensor) -> bool:
-    """Returns whether the given tensor is an initializer or produced by const-foldable nodes."""
-    if isinstance(tensor, Constant):
-        return True
-
-    # Tensor is a graph input variable
-    if len(tensor.inputs) == 0:
-        return False
-
-    producer_node = tensor.inputs[0]  # Generally tensors has single producer
-    if producer_node.op in ["Constant", "Identity"]:
-        return True
-
-    # Second axes input to Squeeze/Unsqueeze is a constant, we need to check the first input
-    if producer_node.op in ["Squeeze", "Unsqueeze"] and is_const_input(producer_node.inputs[0]):
-        return True
-
-    # Const -> Clip -> Exp -> Mul pattern matching for swin_v2
-    if producer_node.op == "Exp":
-        clip_node = producer_node.i()
-        if clip_node.op == "Clip" and has_const_input(clip_node):
-            return True
-
-    return False
-
-
-def has_const_input(node: Node) -> bool:
-    """Returns whether the given node has any constant input."""
-    for tensor in node.inputs:
-        if is_const_input(tensor):
-            return True
-
-    return False
-
-
-def has_path_type(
-    node: Node,
-    graph: Graph,
-    path_type: List[str],
-    is_forward: bool,
-    wild_card_types: List[str] = [],
-    path_nodes: List[Node] = [],
-) -> bool:
-    """Checks if the given node is start/end of a given forward/backward path type.
-
-    Note, Path can be forward or backward wrt a node depending on the next level nodes.
-    Additionally, this method can work with optional nodes and collect the traversed path.
-
-    Args:
-        node: Start node of the path.
-        graph: ONNX model graph.
-        path_type: Path types to match from the given node.
-        is_forward: Whether to match forward or backward path.
-        wild_card_types: Wild card types, these type of nodes are skipped and not matched with the path_type.
-        path_nodes: Accumulated nodes in the matched path.
-
-    Returns:
-        Bool, whether the given node is start/end of the given forward/backward path type.
-    """
-    optional_path_types = ["BiasAdd", "ConstMul"]
-    if not path_type:
-        # All types matched
-        return True
-
-    # Current node type and special type conversion for optional BiasAdd and ConstMul
-    # Note, matching path with Add/Mul type nodes with const input will fail
-    node_type = node.op
-    if node_type == "Add" and has_const_input(node):
-        node_type = "BiasAdd"
-    elif node_type == "Mul" and has_const_input(node):
-        node_type = "ConstMul"
-
-    # Check if current non-wild node type does not match the expected path type
-    # And if path type is not optional (ex. BiasAdd)
-    is_match = (node_type == path_type[0]) or (node.op == path_type[0])
-    is_wild_match = node_type in wild_card_types
-    if not is_match and not is_wild_match and (path_type[0] not in optional_path_types):
-        return False
-
-    # Add current node name in the path
-    if is_match:
-        path_nodes.append(node)
-
-    # If current node type matches the expected path type or path type is optional (ex. BiasAdd), we have a type match
-    # Update the remaining path types to match
-    next_path_type = path_type[:]
-
-    # Non-repeatable optional types should be consumed
-    if is_match or (path_type[0] in ["BiasAdd", "ConstMul"]):
-        next_path_type = path_type[1:]
-
-    # If current node is not wild card and didn't match, go ahead and match with the
-    # remaining path types starting with the current node
-    if not is_match and not is_wild_match:
-        assert path_type[0] in optional_path_types
-        return has_path_type(
-            node,
-            graph,
-            next_path_type,
-            is_forward,
-            wild_card_types,
-            path_nodes,
-        )
-
-    if is_forward:
-        next_level_nodes = get_child_nodes(node)
-    else:
-        next_level_nodes = get_parent_nodes(node)
-
-    # Check if any child (forward path) or parent (backward path) can match the remaining path types
-    for next_node in next_level_nodes:
-        sub_path = []
-        if has_path_type(next_node, graph, next_path_type, is_forward, wild_card_types, sub_path):
-            path_nodes.extend(sub_path)
-            return True
-
-    # Path type matches if there is no remaining types to match
-    return not next_path_type
-
-
-def get_fusible_backbone(node: Node, graph: Graph) -> Optional[Node]:
-    """Returns the linear backbone node for a given node if it matches the pattern.
-
-    TensorRT fuses convolution with BN, Relu etc. when in some specific pattern.
-    This rule tries to match some of those patterns.
-    Note. BiasAdd and ConstMul are optional in path types.
-
-    Args:
-        node: Start node of the pattern.
-        graph: ONNX model graph.
-
-    Returns:
-        Backbone node of the given node, None if not found.
-    """
-
-    def _get_backbone(root: Node):
-        if root.op == "Conv":
-            return root
-
-        for tensor in root.inputs:
-            if not isinstance(tensor, Constant):
-                parent_node = tensor.inputs[0]
-                bb = _get_backbone(parent_node)
-                if bb:
-                    return bb
-
-    fusible_linear_path_types = [
-        # ["Sigmoid", "Conv"],  # With following Mul
-        # ["Resize", "Relu", "Conv"],   # Note. this causes regression in MTL_v1
-        ["BiasAdd", "ConstMul", "Conv"],
-        ["Relu", "BiasAdd", "ConstMul", "Conv"],
-        ["BatchNormalization", "BiasAdd", "Conv"],
-        ["Relu", "BatchNormalization", "BiasAdd", "Conv"],
-    ]
-    for idx, path_type in enumerate(fusible_linear_path_types):
-        if has_path_type(node, graph, path_type, is_forward=False, wild_card_types=[]):
-            return _get_backbone(node)
-
-    return None
-
-
-def filter_quantizable_kgen_heads(
-    cask_fusible_partitions: List[List[Node]],
-    kgen_partitions: List[List[Node]],
-    quantizable_op_types: List[str],
-) -> Tuple[List[Node], List[Tuple[Node, Node, str]]]:
-    """Returns the list of kgen head names if it follows a CASK partition."""
-    cask_partition_nodes = set()
-    for partition in cask_fusible_partitions:
-        cask_partition_nodes.update([node.name for node in partition])
-
-    cask_partition_heads = [partition[0] for partition in cask_fusible_partitions]
-
-    def _is_following_cask_partition(node: Node):
-        # Checking if cask fusible partition can be reached backward
-        # ignoring the copy ops
-        if node.name in cask_partition_nodes:
-            return True
-
-        if not is_copy_op(node.op):
-            return False
-
-        for parent in get_parent_nodes(node):
-            if _is_following_cask_partition(parent):
-                return True
-
-        return False
-
-    def _has_other_quantizable_consumer(
-        tensor: Tensor, quantizable_kgen_heads: List[Node], head_name: str
-    ):
-        # Note. this is kinda approximate analysis,
-        # all quantizable kgen heads may haven't got discovered yet
-        quantizable_ops = [node.name for node in cask_partition_heads + quantizable_kgen_heads]
-
-        # Look for other quantizable consumer than the current kgen head
-        if head_name in quantizable_ops:
-            quantizable_ops.remove(head_name)
-
-        for consumer in tensor.outputs:
-            if consumer.name in quantizable_ops:
-                return True
-
-        return False
-
-    quantizable_kgen_heads = []
-    no_quantize_inputs = []  # list of tuple [(src_node_name, dst_node_name, input_name), ...]
-    output_quantization_candidates = [
-        "AveragePool",
-        "BatchNormalization",
-        "GlobalAveragePool",
-        "MaxPool",
-    ]
-
-    for partition in kgen_partitions:
-        head_node = partition[0]
-        # Check if partition head is of default quantizable type
-        if head_node.op not in quantizable_op_types:
-            continue
-
-        # If the node has cost input, do not quantize
-        if has_const_input(head_node):
-            continue
-
-        head_parents = get_parent_nodes(head_node)
-        no_quantize_inputs_of_head = []
-        has_quantizable_input = False
-
-        # Check each of the parent (input producer for partition head)
-        # or predecessor nodes and see if output quantization is needed for them
-        # and decide which input of kgen head needs quantization
-        for parent in head_parents:
-            # If the head is consuming output of any quantizable op, then it is quantizable
-            if _is_following_cask_partition(parent) or parent.op in output_quantization_candidates:
-                quantizable_kgen_heads.append(partition[0])
-                has_quantizable_input = True
-            # If the input from the current parent has no other quantizable consumer, do not quantize that input
-            elif not _has_other_quantizable_consumer(
-                parent.outputs[0], quantizable_kgen_heads, head_node.name
-            ):
-                no_quantize_inputs_of_head.append((parent, partition[0], parent.outputs[0].name))
-
-        # If at least one input of Add is quantizable, collect if there is any non-quantizable inputs
-        if head_node.op == "Add" and has_quantizable_input:
-            no_quantize_inputs.extend(no_quantize_inputs_of_head)
-
-    return quantizable_kgen_heads, no_quantize_inputs
-
-
-def classify_partition_nodes(
-    partitions: List[List[Node]],
-) -> Tuple[List[Node], List[Node], List[Tuple[Node, Node, str]]]:
-    """We should partially quantize the partition nodes with inputs outside of the partition.
-
-    Args:
-        partitions: Partitions created by modelopt ptq algo.
-
-    Returns:
-        List of non-quantizable nodes.
-        List of quantizable nodes.
-        List of partially-quantizable inputs with non-quantizable input info as (src, dst, input_name)
-    """
-    non_quantizable_partition_nodes = []  # list of Node [node1, ...]
-    quantizable_partition_nodes = []  # list of Node [node1, ...]
-    no_quantize_inputs = []  # list of tuple [(src_node, dst_node, input_name), ...]
-
-    for partition in partitions:
-        partition_root_type = partition[0].op
-        assert is_linear_op(partition_root_type)
-
-        # Collect tensor names produced by partition nodes
-        partition_node_outputs = []
-        for node in partition:
-            for node_output in node.outputs:
-                partition_node_outputs.append(node_output.name)
-
-        for node in partition:
-            has_external_inputs = False
-            internal_inputs = []  # Keeps (producer, consumer, tensor)
-            for tensor in node.inputs:
-                if is_const_input(tensor):
-                    continue
-
-                # If a KGEN op has external non-constant input, it is considered partially quantizable
-                if tensor.name not in partition_node_outputs:
-                    # partition heads will be fully quantizable and added
-                    has_external_inputs = True
-                else:
-                    producer_node = tensor.inputs[0]
-                    # format: source, target, input
-                    # Note. it might happen that this node was not quantized
-                    # We just ignore it from no_quantize_inputs list in post-processing
-                    internal_inputs.append((producer_node, node, tensor.name))
-
-            if not has_external_inputs:
-                non_quantizable_partition_nodes.append(node)
-            elif has_external_inputs and internal_inputs:
-                no_quantize_inputs.extend(internal_inputs)
-            else:
-                # partition head is quantizable
-                quantizable_partition_nodes.append(node)
-
-    return non_quantizable_partition_nodes, quantizable_partition_nodes, no_quantize_inputs
-
-
-def build_non_residual_input_map(graph: Graph) -> Dict[str, str]:
-    """Builds a map of non-residual Add input name to the Add node name from the given graph.
-
-    This assumes that the Add layer only has 2 inputs.
-
-    We will refer to a subgraph which has a Convolution node with a single output that is summed (element-wise)
-    with another non-constant input-tensor as a residual-add subgraph, because it occurs in modern
-    convnets that use residual connections.
-
-    Args:
-        graph: Onnx model graph.
-
-    Returns:
-        Dictionary of Add node names vs their non-residual input name.
-    """
-    non_residual_inputs = {}
-    for node in graph.nodes:
-        if node.op in ["Add"]:
-            # Add nodes with constant or graph input does not have non-residual input
-            # Here, A = node.inputs[0], B = node.inputs[1] and A.inputs means producer nodes of A
-            # TODO: make this check a util?
-            if (
-                has_const_input(node)
-                or len(node.inputs[0].inputs) == 0
-                or len(node.inputs[1].inputs) == 0
-            ):
-                non_residual_inputs[node.name] = None
-                continue
-
-            input1_producer = node.i(0, 0)
-            input2_producer = node.i(1, 0)
-
-            backbone1 = get_fusible_backbone(input1_producer, graph)
-            backbone2 = get_fusible_backbone(input2_producer, graph)
-
-            # Generally if both the inputs have a backbone then both backbones are of the same type
-            if backbone1 and backbone2:
-                if backbone1 == backbone2:
-                    non_residual_inputs[node.name] = None
-                    continue
-
-                assert backbone1.op == backbone2.op, (
-                    f"{backbone1.name} and {backbone2.name} are different types of backbone for"
-                    f" {node.name}!"
-                )
-                # Input in the longest path to LCA is the non-residual input
-                _, d1, d2 = find_lowest_common_ancestor(input1_producer, input2_producer)
-                if d1 > d2:
-                    non_residual_inputs[node.name] = node.inputs[0].name
-                else:
-                    non_residual_inputs[node.name] = node.inputs[1].name
-            elif backbone1:
-                non_residual_inputs[node.name] = node.inputs[0].name
-            elif backbone2:
-                non_residual_inputs[node.name] = node.inputs[1].name
-            else:
-                # Not a residual Add node
-                non_residual_inputs[node.name] = None
-
-    return non_residual_inputs
-
-
-def remove_partial_input_qdq(
-    graph: Graph,
-    no_quantize_inputs: List[Tuple[Node, Node, str]],
-) -> None:
-    """Modifies the onnx model by removing QDQ nodes from the marked inputs, ex. non-residual inputs etc.
-
-    Args:
-        graph: Onnx model graph.
-        no_quantize_inputs: List non-quantizable input info as (src, dst, input_name)
-    """
-    logging.info("Deleting QDQ nodes from marked inputs to make certain operations fusible ...")
-    graph_nodes = {node.name: node for node in graph.nodes}
-    for source, target, non_qdq_input_name in no_quantize_inputs:
-        # Note. no_quantize_inputs objects are from non-quantized input graph
-        # we are deleting some QDQ from the new quantized output graph
-        source_node = graph_nodes[source.name]
-        try:
-            dq_node = source_node.o().o()
-        except Exception:
-            # Reached end of the graph
-            continue
-        if dq_node.op == "DequantizeLinear":
-            dq_node = dq_node.outputs[0]  # source_node->Q->DQ->target_node
-            while len(dq_node.outputs):
-                # Find the input index in the target connecting with source_node
-                target_input_idx_arr = [
-                    idx
-                    for idx, inp in enumerate(dq_node.outputs[0].inputs)
-                    if inp.name == dq_node.name
-                ]
-                target_input_idx = target_input_idx_arr[0] if target_input_idx_arr else 0
-
-                # Connect the output of source_node with the outputs of DQ until DQ is not connected to any other
-                #   layers. Note that when a connection is removed, this is also deleted from dq_node.outputs, thus
-                #   why we keep iterating over the same idx=0 in dq_node.outputs[0].
-                dq_node.outputs[0].inputs[target_input_idx] = source_node.outputs[0]
-
-    graph.cleanup()
-
-
-def print_stat(graph: Graph, verbose: bool) -> None:
-    """Collect and print stats of the quantized model."""
-    count = 0
-    quantized_node_types = set()
-    quantized_nodes = []
-    output_names = [output_node.name for output_node in graph.outputs]
-    for node in graph.nodes:
-        for tensor in node.inputs:
-            if len(tensor.inputs) == 0:
-                continue
-
-            producer_node = tensor.inputs[0]
-            if producer_node.op == "DequantizeLinear":
-                quantized_node_types.add(node.op)
-                quantized_nodes.append(node.name)
-                count += 1
-                break
-            else:
-                # Sometimes "_DequantizeLinear_Output" is not suffix of the "DequantizeLinear" typed node,
-                # if that node is also in final model output. Ex. CLIP-ViT-L-14-opset16.onnx
-                assert tensor.name in output_names or producer_node.op != "DequantizeLinear"
-
-    if verbose:
-        logging.info(f"Quantized nodes: {quantized_nodes}")
-    logging.info(f"Total number of quantized nodes: {count}")
-    logging.info(f"Quantized node types: {quantized_node_types}")
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Provides ONNX graph related utils for QDQ placement."""
+import logging
+from typing import Dict, List, Optional, Tuple
+
+from onnx_graphsurgeon.ir.graph import Graph
+from onnx_graphsurgeon.ir.node import Node
+from onnx_graphsurgeon.ir.tensor import Constant, Tensor
+
+from modelopt.onnx.op_types import is_copy_op, is_linear_op
+from modelopt.onnx.utils import (
+    find_lowest_common_ancestor,
+    get_child_nodes,
+    get_parent_nodes,
+)
+
+
+def is_const_input(tensor: Tensor) -> bool:
+    """Returns whether the given tensor is an initializer or produced by const-foldable nodes."""
+    if isinstance(tensor, Constant):
+        return True
+
+    # Tensor is a graph input variable
+    if len(tensor.inputs) == 0:
+        return False
+
+    producer_node = tensor.inputs[0]  # Generally tensors has single producer
+    if producer_node.op in ["Constant", "Identity"]:
+        return True
+
+    # Second axes input to Squeeze/Unsqueeze is a constant, we need to check the first input
+    if producer_node.op in ["Squeeze", "Unsqueeze"] and is_const_input(producer_node.inputs[0]):
+        return True
+
+    # Const -> Clip -> Exp -> Mul pattern matching for swin_v2
+    if producer_node.op == "Exp":
+        clip_node = producer_node.i()
+        if clip_node.op == "Clip" and has_const_input(clip_node):
+            return True
+
+    return False
+
+
+def has_const_input(node: Node) -> bool:
+    """Returns whether the given node has any constant input."""
+    for tensor in node.inputs:
+        if is_const_input(tensor):
+            return True
+
+    return False
+
+
+def has_path_type(
+    node: Node,
+    graph: Graph,
+    path_type: List[str],
+    is_forward: bool,
+    wild_card_types: List[str] = [],
+    path_nodes: List[Node] = [],
+) -> bool:
+    """Checks if the given node is start/end of a given forward/backward path type.
+
+    Note, Path can be forward or backward wrt a node depending on the next level nodes.
+    Additionally, this method can work with optional nodes and collect the traversed path.
+
+    Args:
+        node: Start node of the path.
+        graph: ONNX model graph.
+        path_type: Path types to match from the given node.
+        is_forward: Whether to match forward or backward path.
+        wild_card_types: Wild card types, these type of nodes are skipped and not matched with the path_type.
+        path_nodes: Accumulated nodes in the matched path.
+
+    Returns:
+        Bool, whether the given node is start/end of the given forward/backward path type.
+    """
+    optional_path_types = ["BiasAdd", "ConstMul"]
+    if not path_type:
+        # All types matched
+        return True
+
+    # Current node type and special type conversion for optional BiasAdd and ConstMul
+    # Note, matching path with Add/Mul type nodes with const input will fail
+    node_type = node.op
+    if node_type == "Add" and has_const_input(node):
+        node_type = "BiasAdd"
+    elif node_type == "Mul" and has_const_input(node):
+        node_type = "ConstMul"
+
+    # Check if current non-wild node type does not match the expected path type
+    # And if path type is not optional (ex. BiasAdd)
+    is_match = (node_type == path_type[0]) or (node.op == path_type[0])
+    is_wild_match = node_type in wild_card_types
+    if not is_match and not is_wild_match and (path_type[0] not in optional_path_types):
+        return False
+
+    # Add current node name in the path
+    if is_match:
+        path_nodes.append(node)
+
+    # If current node type matches the expected path type or path type is optional (ex. BiasAdd), we have a type match
+    # Update the remaining path types to match
+    next_path_type = path_type[:]
+
+    # Non-repeatable optional types should be consumed
+    if is_match or (path_type[0] in ["BiasAdd", "ConstMul"]):
+        next_path_type = path_type[1:]
+
+    # If current node is not wild card and didn't match, go ahead and match with the
+    # remaining path types starting with the current node
+    if not is_match and not is_wild_match:
+        assert path_type[0] in optional_path_types
+        return has_path_type(
+            node,
+            graph,
+            next_path_type,
+            is_forward,
+            wild_card_types,
+            path_nodes,
+        )
+
+    if is_forward:
+        next_level_nodes = get_child_nodes(node)
+    else:
+        next_level_nodes = get_parent_nodes(node)
+
+    # Check if any child (forward path) or parent (backward path) can match the remaining path types
+    for next_node in next_level_nodes:
+        sub_path = []
+        if has_path_type(next_node, graph, next_path_type, is_forward, wild_card_types, sub_path):
+            path_nodes.extend(sub_path)
+            return True
+
+    # Path type matches if there is no remaining types to match
+    return not next_path_type
+
+
+def get_fusible_backbone(node: Node, graph: Graph) -> Optional[Node]:
+    """Returns the linear backbone node for a given node if it matches the pattern.
+
+    TensorRT fuses convolution with BN, Relu etc. when in some specific pattern.
+    This rule tries to match some of those patterns.
+    Note. BiasAdd and ConstMul are optional in path types.
+
+    Args:
+        node: Start node of the pattern.
+        graph: ONNX model graph.
+
+    Returns:
+        Backbone node of the given node, None if not found.
+    """
+
+    def _get_backbone(root: Node):
+        if root.op == "Conv":
+            return root
+
+        for tensor in root.inputs:
+            if not isinstance(tensor, Constant):
+                parent_node = tensor.inputs[0]
+                bb = _get_backbone(parent_node)
+                if bb:
+                    return bb
+
+    fusible_linear_path_types = [
+        # ["Sigmoid", "Conv"],  # With following Mul
+        # ["Resize", "Relu", "Conv"],   # Note. this causes regression in MTL_v1
+        ["BiasAdd", "ConstMul", "Conv"],
+        ["Relu", "BiasAdd", "ConstMul", "Conv"],
+        ["BatchNormalization", "BiasAdd", "Conv"],
+        ["Relu", "BatchNormalization", "BiasAdd", "Conv"],
+    ]
+    for idx, path_type in enumerate(fusible_linear_path_types):
+        if has_path_type(node, graph, path_type, is_forward=False, wild_card_types=[]):
+            return _get_backbone(node)
+
+    return None
+
+
+def filter_quantizable_kgen_heads(
+    cask_fusible_partitions: List[List[Node]],
+    kgen_partitions: List[List[Node]],
+    quantizable_op_types: List[str],
+) -> Tuple[List[Node], List[Tuple[Node, Node, str]]]:
+    """Returns the list of kgen head names if it follows a CASK partition."""
+    cask_partition_nodes = set()
+    for partition in cask_fusible_partitions:
+        cask_partition_nodes.update([node.name for node in partition])
+
+    cask_partition_heads = [partition[0] for partition in cask_fusible_partitions]
+
+    def _is_following_cask_partition(node: Node):
+        # Checking if cask fusible partition can be reached backward
+        # ignoring the copy ops
+        if node.name in cask_partition_nodes:
+            return True
+
+        if not is_copy_op(node.op):
+            return False
+
+        for parent in get_parent_nodes(node):
+            if _is_following_cask_partition(parent):
+                return True
+
+        return False
+
+    def _has_other_quantizable_consumer(
+        tensor: Tensor, quantizable_kgen_heads: List[Node], head_name: str
+    ):
+        # Note. this is kinda approximate analysis,
+        # all quantizable kgen heads may haven't got discovered yet
+        quantizable_ops = [node.name for node in cask_partition_heads + quantizable_kgen_heads]
+
+        # Look for other quantizable consumer than the current kgen head
+        if head_name in quantizable_ops:
+            quantizable_ops.remove(head_name)
+
+        for consumer in tensor.outputs:
+            if consumer.name in quantizable_ops:
+                return True
+
+        return False
+
+    quantizable_kgen_heads = []
+    no_quantize_inputs = []  # list of tuple [(src_node_name, dst_node_name, input_name), ...]
+    output_quantization_candidates = [
+        "AveragePool",
+        "BatchNormalization",
+        "GlobalAveragePool",
+        "MaxPool",
+    ]
+
+    for partition in kgen_partitions:
+        head_node = partition[0]
+        # Check if partition head is of default quantizable type
+        if head_node.op not in quantizable_op_types:
+            continue
+
+        # If the node has cost input, do not quantize
+        if has_const_input(head_node):
+            continue
+
+        head_parents = get_parent_nodes(head_node)
+        no_quantize_inputs_of_head = []
+        has_quantizable_input = False
+
+        # Check each of the parent (input producer for partition head)
+        # or predecessor nodes and see if output quantization is needed for them
+        # and decide which input of kgen head needs quantization
+        for parent in head_parents:
+            # If the head is consuming output of any quantizable op, then it is quantizable
+            if _is_following_cask_partition(parent) or parent.op in output_quantization_candidates:
+                quantizable_kgen_heads.append(partition[0])
+                has_quantizable_input = True
+            # If the input from the current parent has no other quantizable consumer, do not quantize that input
+            elif not _has_other_quantizable_consumer(
+                parent.outputs[0], quantizable_kgen_heads, head_node.name
+            ):
+                no_quantize_inputs_of_head.append((parent, partition[0], parent.outputs[0].name))
+
+        # If at least one input of Add is quantizable, collect if there is any non-quantizable inputs
+        if head_node.op == "Add" and has_quantizable_input:
+            no_quantize_inputs.extend(no_quantize_inputs_of_head)
+
+    return quantizable_kgen_heads, no_quantize_inputs
+
+
+def classify_partition_nodes(
+    partitions: List[List[Node]],
+) -> Tuple[List[Node], List[Node], List[Tuple[Node, Node, str]]]:
+    """We should partially quantize the partition nodes with inputs outside of the partition.
+
+    Args:
+        partitions: Partitions created by modelopt ptq algo.
+
+    Returns:
+        List of non-quantizable nodes.
+        List of quantizable nodes.
+        List of partially-quantizable inputs with non-quantizable input info as (src, dst, input_name)
+    """
+    non_quantizable_partition_nodes = []  # list of Node [node1, ...]
+    quantizable_partition_nodes = []  # list of Node [node1, ...]
+    no_quantize_inputs = []  # list of tuple [(src_node, dst_node, input_name), ...]
+
+    for partition in partitions:
+        partition_root_type = partition[0].op
+        assert is_linear_op(partition_root_type)
+
+        # Collect tensor names produced by partition nodes
+        partition_node_outputs = []
+        for node in partition:
+            for node_output in node.outputs:
+                partition_node_outputs.append(node_output.name)
+
+        for node in partition:
+            has_external_inputs = False
+            internal_inputs = []  # Keeps (producer, consumer, tensor)
+            for tensor in node.inputs:
+                if is_const_input(tensor):
+                    continue
+
+                # If a KGEN op has external non-constant input, it is considered partially quantizable
+                if tensor.name not in partition_node_outputs:
+                    # partition heads will be fully quantizable and added
+                    has_external_inputs = True
+                else:
+                    producer_node = tensor.inputs[0]
+                    # format: source, target, input
+                    # Note. it might happen that this node was not quantized
+                    # We just ignore it from no_quantize_inputs list in post-processing
+                    internal_inputs.append((producer_node, node, tensor.name))
+
+            if not has_external_inputs:
+                non_quantizable_partition_nodes.append(node)
+            elif has_external_inputs and internal_inputs:
+                no_quantize_inputs.extend(internal_inputs)
+            else:
+                # partition head is quantizable
+                quantizable_partition_nodes.append(node)
+
+    return non_quantizable_partition_nodes, quantizable_partition_nodes, no_quantize_inputs
+
+
+def build_non_residual_input_map(graph: Graph) -> Dict[str, str]:
+    """Builds a map of non-residual Add input name to the Add node name from the given graph.
+
+    This assumes that the Add layer only has 2 inputs.
+
+    We will refer to a subgraph which has a Convolution node with a single output that is summed (element-wise)
+    with another non-constant input-tensor as a residual-add subgraph, because it occurs in modern
+    convnets that use residual connections.
+
+    Args:
+        graph: Onnx model graph.
+
+    Returns:
+        Dictionary of Add node names vs their non-residual input name.
+    """
+    non_residual_inputs = {}
+    for node in graph.nodes:
+        if node.op in ["Add"]:
+            # Add nodes with constant or graph input does not have non-residual input
+            # Here, A = node.inputs[0], B = node.inputs[1] and A.inputs means producer nodes of A
+            # TODO: make this check a util?
+            if (
+                has_const_input(node)
+                or len(node.inputs[0].inputs) == 0
+                or len(node.inputs[1].inputs) == 0
+            ):
+                non_residual_inputs[node.name] = None
+                continue
+
+            input1_producer = node.i(0, 0)
+            input2_producer = node.i(1, 0)
+
+            backbone1 = get_fusible_backbone(input1_producer, graph)
+            backbone2 = get_fusible_backbone(input2_producer, graph)
+
+            # Generally if both the inputs have a backbone then both backbones are of the same type
+            if backbone1 and backbone2:
+                if backbone1 == backbone2:
+                    non_residual_inputs[node.name] = None
+                    continue
+
+                assert backbone1.op == backbone2.op, (
+                    f"{backbone1.name} and {backbone2.name} are different types of backbone for"
+                    f" {node.name}!"
+                )
+                # Input in the longest path to LCA is the non-residual input
+                _, d1, d2 = find_lowest_common_ancestor(input1_producer, input2_producer)
+                if d1 > d2:
+                    non_residual_inputs[node.name] = node.inputs[0].name
+                else:
+                    non_residual_inputs[node.name] = node.inputs[1].name
+            elif backbone1:
+                non_residual_inputs[node.name] = node.inputs[0].name
+            elif backbone2:
+                non_residual_inputs[node.name] = node.inputs[1].name
+            else:
+                # Not a residual Add node
+                non_residual_inputs[node.name] = None
+
+    return non_residual_inputs
+
+
+def remove_partial_input_qdq(
+    graph: Graph,
+    no_quantize_inputs: List[Tuple[Node, Node, str]],
+) -> None:
+    """Modifies the onnx model by removing QDQ nodes from the marked inputs, ex. non-residual inputs etc.
+
+    Args:
+        graph: Onnx model graph.
+        no_quantize_inputs: List non-quantizable input info as (src, dst, input_name)
+    """
+    logging.info("Deleting QDQ nodes from marked inputs to make certain operations fusible ...")
+    graph_nodes = {node.name: node for node in graph.nodes}
+    for source, target, non_qdq_input_name in no_quantize_inputs:
+        # Note. no_quantize_inputs objects are from non-quantized input graph
+        # we are deleting some QDQ from the new quantized output graph
+        source_node = graph_nodes[source.name]
+        try:
+            dq_node = source_node.o().o()
+        except Exception:
+            # Reached end of the graph
+            continue
+        if dq_node.op == "DequantizeLinear":
+            dq_node = dq_node.outputs[0]  # source_node->Q->DQ->target_node
+            while len(dq_node.outputs):
+                # Find the input index in the target connecting with source_node
+                target_input_idx_arr = [
+                    idx
+                    for idx, inp in enumerate(dq_node.outputs[0].inputs)
+                    if inp.name == dq_node.name
+                ]
+                target_input_idx = target_input_idx_arr[0] if target_input_idx_arr else 0
+
+                # Connect the output of source_node with the outputs of DQ until DQ is not connected to any other
+                #   layers. Note that when a connection is removed, this is also deleted from dq_node.outputs, thus
+                #   why we keep iterating over the same idx=0 in dq_node.outputs[0].
+                dq_node.outputs[0].inputs[target_input_idx] = source_node.outputs[0]
+
+    graph.cleanup()
+
+
+def print_stat(graph: Graph, verbose: bool) -> None:
+    """Collect and print stats of the quantized model."""
+    count = 0
+    quantized_node_types = set()
+    quantized_nodes = []
+    output_names = [output_node.name for output_node in graph.outputs]
+    for node in graph.nodes:
+        for tensor in node.inputs:
+            if len(tensor.inputs) == 0:
+                continue
+
+            producer_node = tensor.inputs[0]
+            if producer_node.op == "DequantizeLinear":
+                quantized_node_types.add(node.op)
+                quantized_nodes.append(node.name)
+                count += 1
+                break
+            else:
+                # Sometimes "_DequantizeLinear_Output" is not suffix of the "DequantizeLinear" typed node,
+                # if that node is also in final model output. Ex. CLIP-ViT-L-14-opset16.onnx
+                assert tensor.name in output_names or producer_node.op != "DequantizeLinear"
+
+    if verbose:
+        logging.info(f"Quantized nodes: {quantized_nodes}")
+    logging.info(f"Total number of quantized nodes: {count}")
+    logging.info(f"Quantized node types: {quantized_node_types}")
```

## modelopt/onnx/quantization/gs_patching.py

 * *Ordering differences only*

```diff
@@ -1,107 +1,107 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Patch onnx_graphsurgeon to support explicitly setting a dtype."""
-
-from typing import Sequence, Union
-
-import numpy as np
-import onnx
-import onnx_graphsurgeon as gs
-from onnx.mapping import TENSOR_TYPE_MAP
-from onnx_graphsurgeon.ir.tensor import LazyValues
-
-from modelopt.onnx.quantization.quant_utils import pack_float32_to_4bit_optimized
-
-GS_LOGGER = gs.logger.logger.G_LOGGER
-ONNX_MAJOR, ONNX_MINOR, ONNX_REV = tuple(map(int, onnx.__version__.split(".")[:3]))
-
-
-def _onnx_supports_int4():
-    return ONNX_MAJOR > 1 or ONNX_MAJOR == 1 and ONNX_MINOR >= 16
-
-
-def _make_constant(
-    name: str, values: Union[np.ndarray, LazyValues], dtype: onnx.TensorProto.DataType
-) -> gs.Constant:
-    """Creates a constant with a specified dtype."""
-    converted_dtype = (
-        dtype if isinstance(values, LazyValues) else TENSOR_TYPE_MAP[int(dtype)].np_dtype
-    )
-    if values.dtype != converted_dtype:
-        GS_LOGGER.critical(
-            f"Trying to create tensor with incompatible types: `{values.dtype}`, `{dtype}`"
-        )
-
-    t = gs.Constant(name, values)
-    setattr(t, "explicit_dtype", dtype)
-    return t
-
-
-def _make_variable(
-    name: str, dtype: onnx.TensorProto.DataType, shape: Sequence[Union[int, str]]
-) -> gs.Constant:
-    """Creates a variable with a specified dtype."""
-    x = gs.Variable(name, TENSOR_TYPE_MAP[int(dtype)].np_dtype, shape)
-    setattr(x, "explicit_dtype", dtype)
-    return x
-
-
-def _export_tensor_proto(tensor: gs.Constant) -> onnx.TensorProto:
-    if isinstance(tensor._values, LazyValues):
-        onnx_tensor = tensor._values.tensor
-    else:
-        # is numpy array.
-        dtype = getattr(
-            tensor, "explicit_dtype", onnx.helper.np_dtype_to_tensor_dtype(tensor.values.dtype)
-        )
-
-        vals = tensor.values
-        if _onnx_supports_int4() and dtype in [onnx.TensorProto.INT4, onnx.TensorProto.UINT4]:
-            signed = dtype == onnx.TensorProto.INT4
-            np_dtype = onnx.helper.tensor_dtype_to_np_dtype(dtype)
-            vals = pack_float32_to_4bit_optimized(tensor.values, signed=signed).astype(np_dtype)
-
-        onnx_tensor = onnx.helper.make_tensor(
-            tensor.name,
-            dtype,
-            dims=tensor.values.shape,
-            vals=vals.tobytes(),
-            raw=True,
-        )
-        if tensor.data_location is not None:
-            onnx_tensor.data_location = tensor.data_location
-    onnx_tensor.name = tensor.name
-    return onnx_tensor
-
-
-def _export_value_info_proto(tensor: gs.Variable, do_type_check: bool) -> onnx.ValueInfoProto:
-    if do_type_check and tensor.dtype is None:
-        GS_LOGGER.critical(
-            "Graph input and output tensors must include dtype information. Please set the dtype"
-            " attribute for: {:}".format(tensor)
-        )
-
-    if tensor.dtype is not None:
-        dtype = getattr(
-            tensor, "explicit_dtype", onnx.helper.np_dtype_to_tensor_dtype(np.dtype(tensor.dtype))
-        )
-        onnx_tensor = onnx.helper.make_tensor_value_info(tensor.name, dtype, tensor.shape)
-    else:
-        onnx_tensor = onnx.helper.make_empty_tensor_value_info(tensor.name)
-    return onnx_tensor
-
-
-def patch_gs_modules():
-    """Dynamically patch graphsurgeon modules."""
-    gs.make_constant = _make_constant
-    gs.make_variable = _make_variable
-    gs.exporters.onnx_exporter.OnnxExporter.export_tensor_proto = _export_tensor_proto
-    gs.exporters.onnx_exporter.OnnxExporter.export_value_info_proto = _export_value_info_proto
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Patch onnx_graphsurgeon to support explicitly setting a dtype."""
+
+from typing import Sequence, Union
+
+import numpy as np
+import onnx
+import onnx_graphsurgeon as gs
+from onnx.mapping import TENSOR_TYPE_MAP
+from onnx_graphsurgeon.ir.tensor import LazyValues
+
+from modelopt.onnx.quantization.quant_utils import pack_float32_to_4bit_optimized
+
+GS_LOGGER = gs.logger.logger.G_LOGGER
+ONNX_MAJOR, ONNX_MINOR, ONNX_REV = tuple(map(int, onnx.__version__.split(".")[:3]))
+
+
+def _onnx_supports_int4():
+    return ONNX_MAJOR > 1 or ONNX_MAJOR == 1 and ONNX_MINOR >= 16
+
+
+def _make_constant(
+    name: str, values: Union[np.ndarray, LazyValues], dtype: onnx.TensorProto.DataType
+) -> gs.Constant:
+    """Creates a constant with a specified dtype."""
+    converted_dtype = (
+        dtype if isinstance(values, LazyValues) else TENSOR_TYPE_MAP[int(dtype)].np_dtype
+    )
+    if values.dtype != converted_dtype:
+        GS_LOGGER.critical(
+            f"Trying to create tensor with incompatible types: `{values.dtype}`, `{dtype}`"
+        )
+
+    t = gs.Constant(name, values)
+    setattr(t, "explicit_dtype", dtype)
+    return t
+
+
+def _make_variable(
+    name: str, dtype: onnx.TensorProto.DataType, shape: Sequence[Union[int, str]]
+) -> gs.Constant:
+    """Creates a variable with a specified dtype."""
+    x = gs.Variable(name, TENSOR_TYPE_MAP[int(dtype)].np_dtype, shape)
+    setattr(x, "explicit_dtype", dtype)
+    return x
+
+
+def _export_tensor_proto(tensor: gs.Constant) -> onnx.TensorProto:
+    if isinstance(tensor._values, LazyValues):
+        onnx_tensor = tensor._values.tensor
+    else:
+        # is numpy array.
+        dtype = getattr(
+            tensor, "explicit_dtype", onnx.helper.np_dtype_to_tensor_dtype(tensor.values.dtype)
+        )
+
+        vals = tensor.values
+        if _onnx_supports_int4() and dtype in [onnx.TensorProto.INT4, onnx.TensorProto.UINT4]:
+            signed = dtype == onnx.TensorProto.INT4
+            np_dtype = onnx.helper.tensor_dtype_to_np_dtype(dtype)
+            vals = pack_float32_to_4bit_optimized(tensor.values, signed=signed).astype(np_dtype)
+
+        onnx_tensor = onnx.helper.make_tensor(
+            tensor.name,
+            dtype,
+            dims=tensor.values.shape,
+            vals=vals.tobytes(),
+            raw=True,
+        )
+        if tensor.data_location is not None:
+            onnx_tensor.data_location = tensor.data_location
+    onnx_tensor.name = tensor.name
+    return onnx_tensor
+
+
+def _export_value_info_proto(tensor: gs.Variable, do_type_check: bool) -> onnx.ValueInfoProto:
+    if do_type_check and tensor.dtype is None:
+        GS_LOGGER.critical(
+            "Graph input and output tensors must include dtype information. Please set the dtype"
+            " attribute for: {:}".format(tensor)
+        )
+
+    if tensor.dtype is not None:
+        dtype = getattr(
+            tensor, "explicit_dtype", onnx.helper.np_dtype_to_tensor_dtype(np.dtype(tensor.dtype))
+        )
+        onnx_tensor = onnx.helper.make_tensor_value_info(tensor.name, dtype, tensor.shape)
+    else:
+        onnx_tensor = onnx.helper.make_empty_tensor_value_info(tensor.name)
+    return onnx_tensor
+
+
+def patch_gs_modules():
+    """Dynamically patch graphsurgeon modules."""
+    gs.make_constant = _make_constant
+    gs.make_variable = _make_variable
+    gs.exporters.onnx_exporter.OnnxExporter.export_tensor_proto = _export_tensor_proto
+    gs.exporters.onnx_exporter.OnnxExporter.export_value_info_proto = _export_value_info_proto
```

## modelopt/onnx/quantization/int4.py

```diff
@@ -1,472 +1,472 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Perform INT4 WoQ on an ONNX model, and write it back to disk."""
-
-import copy
-import logging
-import math
-import os
-import tempfile
-import time
-import warnings
-from typing import List, Tuple
-
-try:
-    import jax.numpy as np
-
-    has_jax = True
-except ImportError as e:
-    warnings.warn(
-        "Using slower INT4 ONNX quantization using numpy. Install JAX"
-        f" (https://jax.readthedocs.io/en/latest/installation.html) for faster quantization: {e}"
-    )
-    import numpy as np
-
-    has_jax = False
-
-import numpy
-import onnx
-import onnx.numpy_helper as numpy_helper
-import onnx_graphsurgeon as gs
-from onnxruntime.quantization.calibrate import CalibrationDataReader
-
-import modelopt.onnx.quantization.qdq_utils as qdq
-from modelopt.onnx.quantization.gs_patching import patch_gs_modules
-from modelopt.onnx.quantization.ort_utils import create_inference_session
-from modelopt.onnx.utils import save_onnx
-
-# Set logging level to info
-logging.getLogger().setLevel(logging.INFO)
-
-BLOCK_SIZE = 128
-NUM_BITS = 4
-INT4_SCALE = 7.0
-INT4_MIN = -(2 ** (NUM_BITS - 1))  # -8
-INT4_MAX = 2 ** (NUM_BITS - 1) - 1  # 7
-
-
-def _next_block_size_multiple(x: float, block_size: int) -> float:
-    return math.ceil(x / block_size) * block_size
-
-
-def _pad(w: np.ndarray, block_size: int) -> np.ndarray:
-    """Pads `w` to next largest multiple of block_size, on axis 0."""
-    if w.shape[0] % block_size == 0:
-        return w
-
-    pad_width = _next_block_size_multiple(w.shape[0], block_size) - w.shape[0]
-    pads = [(0, 0) for _ in range(len(w.shape))]
-    pads[0] = (0, pad_width)
-    return np.pad(w, pads, mode="constant", constant_values=0)
-
-
-def _depad(w: np.ndarray, orig_shape: tuple) -> np.ndarray:
-    """Depad axis 0 to original shape."""
-    if w.shape == orig_shape:
-        return w
-    return w[0 : orig_shape[0], ...]
-
-
-def find_scales(w: np.ndarray, block_size: int, alpha: float = 1.0) -> np.ndarray:
-    """Find scale factors for `w` via `s = max(w.block(block_size)) / 7`."""
-    w = _pad(w, block_size)
-    w = w.T
-    w_amax = np.abs(w.reshape(-1, block_size)).max(axis=-1)
-    s = (w_amax * alpha) / INT4_SCALE
-    s_last_dim = w.shape[-1] // block_size
-    s_shape = list(w.shape)
-    s_shape[-1] = s_last_dim
-    return s.reshape(s_shape).T
-
-
-def rtn(w: np.ndarray, s: np.ndarray, block_size: int) -> np.ndarray:
-    """Quantizes `w` with scale factors `s` via Round-to-Nearest.
-
-    Ties are broken by rounding to the nearest even number.
-    """
-    w_padded = _pad(w, block_size)
-    num_blocks = w_padded.shape[0] // s.shape[0]
-    w_padded = (
-        np.rint(w_padded / s.repeat(num_blocks, axis=0)).clip(INT4_MIN, INT4_MAX).astype(np.int8)
-    )
-    return _depad(w_padded, w.shape)
-
-
-def dq_tensor(w: np.ndarray, s: np.ndarray, block_size: int) -> np.ndarray:
-    """Dequantizes `w` with scale factors `s`."""
-    w_padded = _pad(w, block_size)
-    num_blocks = w_padded.shape[0] // s.shape[0]
-    w_padded = w_padded * s.repeat(num_blocks, axis=0)
-    return _depad(w_padded, w.shape)
-
-
-def quantize_int4_rtn(
-    onnx_model: onnx.onnx_pb.ModelProto,
-    gemm_io_type: onnx.TensorProto.DataType,
-    dq_only: bool = False,
-) -> onnx.onnx_pb.ModelProto:
-    """Quantizes `onnx_model` using the RTN (Round-to-Nearest) algorithm.
-
-    This algorithm computes scale factors by computing s = max(abs(block)) / 8, for each block. The
-    quantized weights are computed via Q(w) = round_to_even(w / s), where `round_to_even` denotes
-    rounding ties to the nearest even integer (i.e. 1.5, 2.5 both round to 2).
-
-    Always selects the first dimension (0) to block over. This is because we must batch over the Cin
-    dimension, and in ONNX, weights are always plugged into the RHS (i.e. y = x @ W).
-    """
-    graph = gs.import_onnx(onnx_model)
-    gemm_nodes = [node for node in graph.nodes if node.op in ["Gemm", "MatMul"]]
-    gemm_tensors = {}
-    act_tensors = []
-    for gemm in gemm_nodes:
-        for in_tensor in gemm.inputs:
-            if not isinstance(in_tensor, gs.Constant):
-                continue
-            if len(in_tensor.values.shape) == 1:
-                # 1D blocked quantization not supported.
-                continue
-            gemm_tensors[in_tensor.name] = in_tensor
-            act_tensors.append(gemm.inputs[0])
-
-    gemm_weights = {name: tensor.values for name, tensor in gemm_tensors.items()}
-    scales = {name: find_scales(w, BLOCK_SIZE) for name, w in gemm_weights.items()}
-    logging.info("Computed scales.")
-
-    # Change the scale type to the expected type, fp16 by default
-    for name, w in scales.items():
-        s = scales[name]
-        scales[name] = s.astype(onnx.mapping.TENSOR_TYPE_MAP[gemm_io_type].np_dtype)
-
-    # Change the input activation type to the expected type, fp16 by default
-    for act_tensor in act_tensors:
-        _change_input_type(onnx_model.graph, act_tensor.name, gemm_io_type)
-
-    # Import the update graph
-    graph = gs.import_onnx(onnx_model)
-
-    if dq_only:
-        # Calculate actual quantized weights.
-        gemm_weights_quantized = {}
-        for name, w in gemm_weights.items():
-            gemm_weights_quantized[name] = rtn(w, scales[name], BLOCK_SIZE)
-
-        qdq.insert_dq_nodes(graph, scales, quantized_weights=gemm_weights_quantized)
-    else:
-        qdq.insert_qdq_nodes(graph, scales, weight_map=gemm_tensors)
-
-    logging.info(f"Inserted {'DQ' if dq_only else 'Q/DQ'} nodes.")
-    return gs.export_onnx(graph)
-
-
-def quant_tensor(w: np.ndarray, block_size: int, alpha: float = 1.0):
-    """Quantize a tensor using alpha etc. and return the quantized tensor."""
-    scale = find_scales(w, block_size, alpha)
-    wq = rtn(w, scale, block_size)
-    return wq, scale
-
-
-class AWQClipHelper:
-    """AWQ calibration helper class."""
-
-    min_alpha = 0.5
-    alpha_step = 0.05
-    alphas = [round(float(k), 2) for k in np.arange(min_alpha, 1.0, alpha_step)] + [1.0]
-
-    def __init__(self, w, block_size: int):
-        """Initializes AWQClipHelper with a module weight."""
-        ci, co = w.shape
-        self.block_size = block_size if block_size != -1 else w.shape[0]
-        w = _pad(w, block_size).T
-        self.w_amax = np.abs(w.reshape(-1, block_size)).max(axis=-1)
-
-        self.loss = {
-            k: np.zeros((co, math.ceil(ci / self.block_size)), dtype=np.float32)
-            for k in AWQClipHelper.alphas
-        }
-        self.best_loss = np.full_like(self.w_amax, float("inf"))
-        self.best_alpha = np.ones_like(self.w_amax)
-
-    def update_best_params(self):
-        """Updates the loss dictionary."""
-        for alpha, loss in self.loss.items():
-            loss = loss.reshape(self.w_amax.shape)
-            indices = loss < self.best_loss
-            self.best_loss = np.where(indices, loss, self.best_loss)
-            self.best_alpha = np.where(indices, alpha, self.best_alpha)
-
-
-def _clip_search(
-    x: np.ndarray,
-    w: np.ndarray,
-    awq_clip: AWQClipHelper,
-    co_bsz: int = 1024,
-    max_tokens: int = 64,
-):
-    """Apply AWQ algorithm on a weight and return optimum alpha.
-
-    This algorithm defines a simple search space for the optimal scales: S = Sx ^ .
-    S is only related to the magnitude of activation Sx, and a single hyper-parameter  is used to balance
-    between the protection of salient and non-salient channels. The algorithm finds the best  by a fast grid search
-    over the interval of [0, 1] (0 means do not scale; 1 corresponds to the most aggressive scaling).
-    Further weight clipping is also applied by minimizing the MSE error.
-    """
-    # Select max_tokens from input
-    x = np.reshape(x, (-1, x.shape[-1]))  # _, ci
-    x = x[0 :: max(1, x.shape[0] // max_tokens)]  # max_tokens, ci
-
-    ci, co = w.shape
-    block_size = awq_clip.block_size
-
-    # Pad weight and input if necessary
-    if ci % block_size != 0:
-        w = _pad(w, block_size)
-        x = _pad(x.T, block_size).T
-
-    # Make a copy of the original padded weight to quantize with generated scales
-    w_copy = copy.deepcopy(w)
-
-    # Reshape weight and input for batch processing over co dimension
-    w = w.T  # co, ci
-    w = w.reshape(co, 1, -1, block_size)  # co, 1, n_block, block_size
-    x = x.reshape(1, x.shape[0], -1, block_size)  # 1, max_tokens, n_block, block_size
-
-    # Loop over co dimension of the weight and generate scales
-    for co_batch in range(math.ceil(co / co_bsz)):
-        slice_s, slice_e = co_batch * co_bsz, min((co_batch + 1) * co_bsz, co)
-        weight = w[slice_s:slice_e]
-        org_out = np.sum(x * weight, axis=-1)  # co_bsz, max_tokens, n_block
-
-        # Compute loss for each alpha value
-        for alpha in awq_clip.loss.keys():
-            # Perform QDQ on the whole original weight tensor
-            qw, scales = quant_tensor(w_copy, block_size, alpha)
-            cur_w = dq_tensor(qw, scales, block_size)
-
-            # Reshape before getting the batch of size co_bsz to multiply with input
-            cur_w = cur_w.T  # ci, co -> co, ci
-            cur_w = cur_w.reshape(co, 1, -1, block_size)  # co, 1, n_block, block_size
-            cur_w = cur_w[slice_s:slice_e]
-
-            # Compute loss for each batch
-            cur_out = np.sum(x * cur_w, axis=-1)  # co_bsz, max_tokens, n_block
-            loss = np.mean(np.power((org_out - cur_out), 2), axis=1)  # co_bsz, n_block
-            if has_jax:
-                awq_clip.loss[alpha] += awq_clip.loss[alpha].at[slice_s:slice_e].set(loss)
-            else:
-                awq_clip.loss[alpha][slice_s:slice_e] += loss
-
-    # Update the best alpha value for the weight blocks
-    awq_clip.update_best_params()
-
-
-def _find_quantizable_weights(
-    graph: onnx.onnx_pb.GraphProto,
-) -> List[Tuple[onnx.onnx_pb.ValueInfoProto, onnx.onnx_pb.ValueInfoProto, bool]]:
-    """Finds the quantizable weights from the graph."""
-    wa_pack = []
-    gemm_nodes = [node for node in graph.node if node.op_type in ["Gemm", "MatMul"]]
-    initializer_idxs = {initializer.name: idx for idx, initializer in enumerate(graph.initializer)}
-    for gemm in gemm_nodes:
-        if gemm.input[0] in initializer_idxs:
-            # Ex. two const input to MatMul_115 in fastvit0.onnx
-            # Note. RTN algorithm will quantize these weights though
-            continue
-
-        if gemm.input[1] not in initializer_idxs:
-            continue
-
-        weight_tensor = graph.initializer[initializer_idxs[gemm.input[1]]]
-        if len(weight_tensor.dims) == 1:  # 1D blocked quantization not supported
-            continue
-
-        act_tensor = onnx.helper.ValueInfoProto()
-        act_tensor.name = gemm.input[0]
-
-        # TODO: support transA by transposing activation tensors in _clip_search
-        do_transpose = gemm.op_type == "Gemm" and any(
-            [attr.name == "transB" and attr.i > 0 for attr in gemm.attribute]
-        )
-
-        wa_pack.append((act_tensor, weight_tensor, do_transpose))
-
-    return wa_pack
-
-
-def _augment_graph(
-    graph: onnx.onnx_pb.GraphProto, wa_pack: List[Tuple[gs.Tensor, gs.Tensor, bool]]
-):
-    """Extend graph outputs with MatMuls activation input."""
-    augmented_outputs = set([tensor.name for tensor in graph.output])
-    for act_tensor, _, _ in wa_pack:
-        if act_tensor.name not in augmented_outputs:
-            graph.output.append(act_tensor)
-            augmented_outputs.add(act_tensor.name)
-
-
-def _change_input_type(
-    graph: onnx.onnx_pb.GraphProto, input_name: str, gemm_io_type: onnx.TensorProto.DataType
-):
-    # Find the corresponding value info in the graph
-    done = False
-    for value_info in graph.value_info:
-        if value_info.name == input_name:
-            value_info.type.tensor_type.elem_type = gemm_io_type
-            done = True
-            break
-
-    if not done:
-        # If input not in value_info, it must be a graph input
-        for input_info in graph.input:
-            if input_info.name == input_name:
-                input_info.type.tensor_type.elem_type = gemm_io_type
-                break
-
-
-def quantize_int4_awq_clip(
-    onnx_model: onnx.onnx_pb.ModelProto,
-    data_reader: CalibrationDataReader,
-    use_external_data_format: bool,
-    gemm_io_type: onnx.TensorProto.DataType,
-) -> onnx.onnx_pb.ModelProto:
-    """Quantizes `onnx_model` using the Activation aware quantization a.k.a AWQ algorithm."""
-    logging.info("Finding quantizable weights and augmenting graph output with input activations")
-    t = time.time()
-    augmented_model = copy.deepcopy(onnx_model)
-    graph = augmented_model.graph
-
-    # Collect quantizable weight tensors
-    wa_pack = _find_quantizable_weights(graph)
-
-    # Add input activations to graph output
-    _augment_graph(augmented_model.graph, wa_pack)
-    logging.info(f"Augmenting took {time.time() - t} seconds")
-
-    scales = {}
-    gemm_weights_quantized = {}
-
-    t = time.time()
-
-    # Create a temp file for augmented model
-    augmented_onnx_file, augmented_onnx_path = tempfile.mkstemp(suffix=".onnx")
-    os.close(augmented_onnx_file)
-
-    # TODO: ONNX version issue, onnx_export uses current ONNX IR version.
-    augmented_model.ir_version = 9
-    save_onnx(augmented_model, augmented_onnx_path, use_external_data_format)
-    logging.info(f"Saving the model took {time.time() - t} seconds")
-
-    # Creating inference session and preparing inputs for calibration
-    session = create_inference_session(augmented_onnx_path)
-    inputs = []
-    for inp_d in data_reader:
-        inputs.append(inp_d)
-        assert isinstance(inp_d, dict)
-
-    # Apply AWQ clip on selected weights
-    logging.info("Started clip search ...")
-    t = time.time()
-    alphas = {}
-    for act_tensor, weight_tensor, do_transpose in wa_pack:
-        # First capture all the  activation values after calibration data sweep
-        output_dicts = {}
-        for inp_d in inputs:
-            np_inp_d = {name: numpy.asarray(tensor) for name, tensor in inp_d.items()}
-            output = session.run([act_tensor.name], np_inp_d)
-            output_dicts.setdefault(act_tensor.name, []).append(output[0])
-
-        # Concatenating the activation tensors over all calib data
-        x = np.concatenate(output_dicts[act_tensor.name], axis=0)  # n_token, ci
-        w = numpy_helper.to_array(
-            weight_tensor, base_dir=os.path.dirname(augmented_onnx_path)
-        ).copy()
-        if do_transpose:
-            w = w.T
-
-        awq_clip = AWQClipHelper(w, BLOCK_SIZE)
-        _clip_search(x, w, awq_clip)
-        alphas[weight_tensor.name] = awq_clip.best_alpha
-
-    logging.info(f"Clip search for all weights took {time.time() - t} seconds")
-
-    # Compute quantized weights and scales which are needed for DQ nodes
-    logging.info("Quantizing the actual weights ...")
-    t = time.time()
-    for act_tensor, weight_tensor, do_transpose in wa_pack:
-        w = numpy_helper.to_array(
-            weight_tensor, base_dir=os.path.dirname(augmented_onnx_path)
-        ).copy()
-        if do_transpose:
-            w = w.T
-
-        alpha = alphas.get(weight_tensor.name, 1)
-        qw, scale = quant_tensor(w, BLOCK_SIZE, alpha)
-        if do_transpose:
-            qw = qw.T
-            scale = scale.T
-        scales[weight_tensor.name] = scale.astype(
-            onnx.mapping.TENSOR_TYPE_MAP[gemm_io_type].np_dtype
-        )
-        gemm_weights_quantized[weight_tensor.name] = numpy.asarray(qw).astype(numpy.int8)
-
-        # Change the input activation type to the expected type, fp16 by default
-        # TODO: cast input C for Gemm
-        _change_input_type(onnx_model.graph, act_tensor.name, gemm_io_type)
-    logging.info(f"Quantizing actual weights took {time.time() - t} seconds")
-
-    logging.info("Inserting DQ nodes using quantized weights and scales ...")
-    t = time.time()
-    graph_gs = gs.import_onnx(onnx_model)
-    qdq.insert_dq_nodes(graph_gs, scales, quantized_weights=gemm_weights_quantized)
-    logging.info(f"Inserting DQ nodes took {time.time() - t} seconds")
-
-    logging.info("Exporting the quantized graph ...")
-    t = time.time()
-    model = gs.export_onnx(graph_gs)
-    model.ir_version = 9
-    logging.info(f"Exporting took {time.time() - t} seconds")
-
-    try:
-        os.remove(augmented_onnx_path)
-        if use_external_data_format:
-            os.remove(augmented_onnx_path + "_data")
-    except OSError:
-        logging.warn("Augmented ONNX model or external data file was not found!")
-
-    return model
-
-
-def quantize_int4(
-    quantize_mode: str,
-    onnx_model: onnx.onnx_pb.ModelProto,
-    calibration_data_reader: CalibrationDataReader = None,
-    use_external_data_format: bool = True,
-    gemm_io_type: onnx.TensorProto.DataType = onnx.TensorProto.FLOAT16,
-) -> onnx.onnx_pb.ModelProto:
-    """Applies INT4 WoQ (Weight-Only-Quantization) to an ONNX file.
-
-    Currently only GEMM quantization is supported.
-    """
-    logging.info(f"Quantization Mode: {quantize_mode}")
-
-    # Patch GS modules to support INT4.
-    patch_gs_modules()
-
-    if "trt" in quantize_mode:
-        qdq.use_trt_qdq_ops()
-
-    if quantize_mode in ["int4_rtn", "int4_rtn_dq", "int4_rtn_trt", "int4_rtn_trt_dq"]:
-        return quantize_int4_rtn(onnx_model, gemm_io_type, dq_only="dq" in quantize_mode)
-    elif quantize_mode in ["int4_awq_clip", "int4_awq_clip_trt"]:
-        return quantize_int4_awq_clip(
-            onnx_model, calibration_data_reader, use_external_data_format, gemm_io_type
-        )
-    else:
-        raise RuntimeError(f"Unsupported quant mode: '{quantize_mode}'")
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Perform INT4 WoQ on an ONNX model, and write it back to disk."""
+
+import copy
+import logging
+import math
+import os
+import tempfile
+import time
+import warnings
+from typing import List, Tuple
+
+try:
+    import jax.numpy as np
+
+    has_jax = True
+except ImportError as e:
+    warnings.warn(
+        "Using slower INT4 ONNX quantization using numpy. Install JAX"
+        f" (https://jax.readthedocs.io/en/latest/installation.html) for faster quantization: {e}"
+    )
+    import numpy as np
+
+    has_jax = False
+
+import numpy
+import onnx
+import onnx.numpy_helper as numpy_helper
+import onnx_graphsurgeon as gs
+from onnxruntime.quantization.calibrate import CalibrationDataReader
+
+import modelopt.onnx.quantization.qdq_utils as qdq
+from modelopt.onnx.quantization.gs_patching import patch_gs_modules
+from modelopt.onnx.quantization.ort_utils import create_inference_session
+from modelopt.onnx.utils import save_onnx
+
+# Set logging level to info
+logging.getLogger().setLevel(logging.INFO)
+
+BLOCK_SIZE = 128
+NUM_BITS = 4
+INT4_SCALE = 7.0
+INT4_MIN = -(2 ** (NUM_BITS - 1))  # -8
+INT4_MAX = 2 ** (NUM_BITS - 1) - 1  # 7
+
+
+def _next_block_size_multiple(x: float, block_size: int) -> float:
+    return math.ceil(x / block_size) * block_size
+
+
+def _pad(w: np.ndarray, block_size: int) -> np.ndarray:
+    """Pads `w` to next largest multiple of block_size, on axis 0."""
+    if w.shape[0] % block_size == 0:
+        return w
+
+    pad_width = _next_block_size_multiple(w.shape[0], block_size) - w.shape[0]
+    pads = [(0, 0) for _ in range(len(w.shape))]
+    pads[0] = (0, pad_width)
+    return np.pad(w, pads, mode="constant", constant_values=0)
+
+
+def _depad(w: np.ndarray, orig_shape: tuple) -> np.ndarray:
+    """Depad axis 0 to original shape."""
+    if w.shape == orig_shape:
+        return w
+    return w[0 : orig_shape[0], ...]
+
+
+def find_scales(w: np.ndarray, block_size: int, alpha: float = 1.0) -> np.ndarray:
+    """Find scale factors for `w` via `s = max(w.block(block_size)) / 7`."""
+    w = _pad(w, block_size)
+    w = w.T
+    w_amax = np.abs(w.reshape(-1, block_size)).max(axis=-1)
+    s = (w_amax * alpha) / INT4_SCALE
+    s_last_dim = w.shape[-1] // block_size
+    s_shape = list(w.shape)
+    s_shape[-1] = s_last_dim
+    return s.reshape(s_shape).T
+
+
+def rtn(w: np.ndarray, s: np.ndarray, block_size: int) -> np.ndarray:
+    """Quantizes `w` with scale factors `s` via Round-to-Nearest.
+
+    Ties are broken by rounding to the nearest even number.
+    """
+    w_padded = _pad(w, block_size)
+    num_blocks = w_padded.shape[0] // s.shape[0]
+    w_padded = (
+        np.rint(w_padded / s.repeat(num_blocks, axis=0)).clip(INT4_MIN, INT4_MAX).astype(np.int8)
+    )
+    return _depad(w_padded, w.shape)
+
+
+def dq_tensor(w: np.ndarray, s: np.ndarray, block_size: int) -> np.ndarray:
+    """Dequantizes `w` with scale factors `s`."""
+    w_padded = _pad(w, block_size)
+    num_blocks = w_padded.shape[0] // s.shape[0]
+    w_padded = w_padded * s.repeat(num_blocks, axis=0)
+    return _depad(w_padded, w.shape)
+
+
+def quantize_int4_rtn(
+    onnx_model: onnx.onnx_pb.ModelProto,
+    gemm_io_type: onnx.TensorProto.DataType,
+    dq_only: bool = False,
+) -> onnx.onnx_pb.ModelProto:
+    """Quantizes `onnx_model` using the RTN (Round-to-Nearest) algorithm.
+
+    This algorithm computes scale factors by computing s = max(abs(block)) / 8, for each block. The
+    quantized weights are computed via Q(w) = round_to_even(w / s), where `round_to_even` denotes
+    rounding ties to the nearest even integer (i.e. 1.5, 2.5 both round to 2).
+
+    Always selects the first dimension (0) to block over. This is because we must batch over the Cin
+    dimension, and in ONNX, weights are always plugged into the RHS (i.e. y = x @ W).
+    """
+    graph = gs.import_onnx(onnx_model)
+    gemm_nodes = [node for node in graph.nodes if node.op in ["Gemm", "MatMul"]]
+    gemm_tensors = {}
+    act_tensors = []
+    for gemm in gemm_nodes:
+        for in_tensor in gemm.inputs:
+            if not isinstance(in_tensor, gs.Constant):
+                continue
+            if len(in_tensor.values.shape) == 1:
+                # 1D blocked quantization not supported.
+                continue
+            gemm_tensors[in_tensor.name] = in_tensor
+            act_tensors.append(gemm.inputs[0])
+
+    gemm_weights = {name: tensor.values for name, tensor in gemm_tensors.items()}
+    scales = {name: find_scales(w, BLOCK_SIZE) for name, w in gemm_weights.items()}
+    logging.info("Computed scales.")
+
+    # Change the scale type to the expected type, fp16 by default
+    for name, w in scales.items():
+        s = scales[name]
+        scales[name] = s.astype(onnx.mapping.TENSOR_TYPE_MAP[gemm_io_type].np_dtype)
+
+    # Change the input activation type to the expected type, fp16 by default
+    for act_tensor in act_tensors:
+        _change_input_type(onnx_model.graph, act_tensor.name, gemm_io_type)
+
+    # Import the update graph
+    graph = gs.import_onnx(onnx_model)
+
+    if dq_only:
+        # Calculate actual quantized weights.
+        gemm_weights_quantized = {}
+        for name, w in gemm_weights.items():
+            gemm_weights_quantized[name] = rtn(w, scales[name], BLOCK_SIZE)
+
+        qdq.insert_dq_nodes(graph, scales, quantized_weights=gemm_weights_quantized)
+    else:
+        qdq.insert_qdq_nodes(graph, scales, weight_map=gemm_tensors)
+
+    logging.info(f"Inserted {'DQ' if dq_only else 'Q/DQ'} nodes.")
+    return gs.export_onnx(graph)
+
+
+def quant_tensor(w: np.ndarray, block_size: int, alpha: float = 1.0):
+    """Quantize a tensor using alpha etc. and return the quantized tensor."""
+    scale = find_scales(w, block_size, alpha)
+    wq = rtn(w, scale, block_size)
+    return wq, scale
+
+
+class AWQClipHelper:
+    """AWQ calibration helper class."""
+
+    min_alpha = 0.5
+    alpha_step = 0.05
+    alphas = [round(float(k), 2) for k in np.arange(min_alpha, 1.0, alpha_step)] + [1.0]
+
+    def __init__(self, w, block_size: int):
+        """Initializes AWQClipHelper with a module weight."""
+        ci, co = w.shape
+        self.block_size = block_size if block_size != -1 else w.shape[0]
+        w = _pad(w, block_size).T
+        self.w_amax = np.abs(w.reshape(-1, block_size)).max(axis=-1)
+
+        self.loss = {
+            k: np.zeros((co, math.ceil(ci / self.block_size)), dtype=np.float32)
+            for k in AWQClipHelper.alphas
+        }
+        self.best_loss = np.full_like(self.w_amax, float("inf"))
+        self.best_alpha = np.ones_like(self.w_amax)
+
+    def update_best_params(self):
+        """Updates the loss dictionary."""
+        for alpha, loss in self.loss.items():
+            loss = loss.reshape(self.w_amax.shape)
+            indices = loss < self.best_loss
+            self.best_loss = np.where(indices, loss, self.best_loss)
+            self.best_alpha = np.where(indices, alpha, self.best_alpha)
+
+
+def _clip_search(
+    x: np.ndarray,
+    w: np.ndarray,
+    awq_clip: AWQClipHelper,
+    co_bsz: int = 1024,
+    max_tokens: int = 64,
+):
+    """Apply AWQ algorithm on a weight and return optimum alpha.
+
+    This algorithm defines a simple search space for the optimal scales: S = Sx ^ .
+    S is only related to the magnitude of activation Sx, and a single hyper-parameter  is used to balance
+    between the protection of salient and non-salient channels. The algorithm finds the best  by a fast grid search
+    over the interval of [0, 1] (0 means do not scale; 1 corresponds to the most aggressive scaling).
+    Further weight clipping is also applied by minimizing the MSE error.
+    """
+    # Select max_tokens from input
+    x = np.reshape(x, (-1, x.shape[-1]))  # _, ci
+    x = x[0 :: max(1, x.shape[0] // max_tokens)]  # max_tokens, ci
+
+    ci, co = w.shape
+    block_size = awq_clip.block_size
+
+    # Pad weight and input if necessary
+    if ci % block_size != 0:
+        w = _pad(w, block_size)
+        x = _pad(x.T, block_size).T
+
+    # Make a copy of the original padded weight to quantize with generated scales
+    w_copy = copy.deepcopy(w)
+
+    # Reshape weight and input for batch processing over co dimension
+    w = w.T  # co, ci
+    w = w.reshape(co, 1, -1, block_size)  # co, 1, n_block, block_size
+    x = x.reshape(1, x.shape[0], -1, block_size)  # 1, max_tokens, n_block, block_size
+
+    # Loop over co dimension of the weight and generate scales
+    for co_batch in range(math.ceil(co / co_bsz)):
+        slice_s, slice_e = co_batch * co_bsz, min((co_batch + 1) * co_bsz, co)
+        weight = w[slice_s:slice_e]
+        org_out = np.sum(x * weight, axis=-1)  # co_bsz, max_tokens, n_block
+
+        # Compute loss for each alpha value
+        for alpha in awq_clip.loss.keys():
+            # Perform QDQ on the whole original weight tensor
+            qw, scales = quant_tensor(w_copy, block_size, alpha)
+            cur_w = dq_tensor(qw, scales, block_size)
+
+            # Reshape before getting the batch of size co_bsz to multiply with input
+            cur_w = cur_w.T  # ci, co -> co, ci
+            cur_w = cur_w.reshape(co, 1, -1, block_size)  # co, 1, n_block, block_size
+            cur_w = cur_w[slice_s:slice_e]
+
+            # Compute loss for each batch
+            cur_out = np.sum(x * cur_w, axis=-1)  # co_bsz, max_tokens, n_block
+            loss = np.mean(np.power((org_out - cur_out), 2), axis=1)  # co_bsz, n_block
+            if has_jax:
+                awq_clip.loss[alpha] += awq_clip.loss[alpha].at[slice_s:slice_e].set(loss)
+            else:
+                awq_clip.loss[alpha][slice_s:slice_e] += loss
+
+    # Update the best alpha value for the weight blocks
+    awq_clip.update_best_params()
+
+
+def _find_quantizable_weights(
+    graph: onnx.onnx_pb.GraphProto,
+) -> List[Tuple[onnx.onnx_pb.ValueInfoProto, onnx.onnx_pb.ValueInfoProto, bool]]:
+    """Finds the quantizable weights from the graph."""
+    wa_pack = []
+    gemm_nodes = [node for node in graph.node if node.op_type in ["Gemm", "MatMul"]]
+    initializer_idxs = {initializer.name: idx for idx, initializer in enumerate(graph.initializer)}
+    for gemm in gemm_nodes:
+        if gemm.input[0] in initializer_idxs:
+            # Ex. two const input to MatMul_115 in fastvit0.onnx
+            # Note. RTN algorithm will quantize these weights though
+            continue
+
+        if gemm.input[1] not in initializer_idxs:
+            continue
+
+        weight_tensor = graph.initializer[initializer_idxs[gemm.input[1]]]
+        if len(weight_tensor.dims) == 1:  # 1D blocked quantization not supported
+            continue
+
+        act_tensor = onnx.helper.ValueInfoProto()
+        act_tensor.name = gemm.input[0]
+
+        # TODO: support transA by transposing activation tensors in _clip_search
+        do_transpose = gemm.op_type == "Gemm" and any(
+            [attr.name == "transB" and attr.i > 0 for attr in gemm.attribute]
+        )
+
+        wa_pack.append((act_tensor, weight_tensor, do_transpose))
+
+    return wa_pack
+
+
+def _augment_graph(
+    graph: onnx.onnx_pb.GraphProto, wa_pack: List[Tuple[gs.Tensor, gs.Tensor, bool]]
+):
+    """Extend graph outputs with MatMuls activation input."""
+    augmented_outputs = set([tensor.name for tensor in graph.output])
+    for act_tensor, _, _ in wa_pack:
+        if act_tensor.name not in augmented_outputs:
+            graph.output.append(act_tensor)
+            augmented_outputs.add(act_tensor.name)
+
+
+def _change_input_type(
+    graph: onnx.onnx_pb.GraphProto, input_name: str, gemm_io_type: onnx.TensorProto.DataType
+):
+    # Find the corresponding value info in the graph
+    done = False
+    for value_info in graph.value_info:
+        if value_info.name == input_name:
+            value_info.type.tensor_type.elem_type = gemm_io_type
+            done = True
+            break
+
+    if not done:
+        # If input not in value_info, it must be a graph input
+        for input_info in graph.input:
+            if input_info.name == input_name:
+                input_info.type.tensor_type.elem_type = gemm_io_type
+                break
+
+
+def quantize_int4_awq_clip(
+    onnx_model: onnx.onnx_pb.ModelProto,
+    data_reader: CalibrationDataReader,
+    use_external_data_format: bool,
+    gemm_io_type: onnx.TensorProto.DataType,
+) -> onnx.onnx_pb.ModelProto:
+    """Quantizes `onnx_model` using the Activation aware quantization a.k.a AWQ algorithm."""
+    logging.info("Finding quantizable weights and augmenting graph output with input activations")
+    t = time.time()
+    augmented_model = copy.deepcopy(onnx_model)
+    graph = augmented_model.graph
+
+    # Collect quantizable weight tensors
+    wa_pack = _find_quantizable_weights(graph)
+
+    # Add input activations to graph output
+    _augment_graph(augmented_model.graph, wa_pack)
+    logging.info(f"Augmenting took {time.time() - t} seconds")
+
+    scales = {}
+    gemm_weights_quantized = {}
+
+    t = time.time()
+
+    # Create a temp file for augmented model
+    augmented_onnx_file, augmented_onnx_path = tempfile.mkstemp(suffix=".onnx")
+    os.close(augmented_onnx_file)
+
+    # TODO: ONNX version issue, onnx_export uses current ONNX IR version.
+    augmented_model.ir_version = 9
+    save_onnx(augmented_model, augmented_onnx_path, use_external_data_format)
+    logging.info(f"Saving the model took {time.time() - t} seconds")
+
+    # Creating inference session and preparing inputs for calibration
+    session = create_inference_session(augmented_onnx_path)
+    inputs = []
+    for inp_d in data_reader:
+        inputs.append(inp_d)
+        assert isinstance(inp_d, dict)
+
+    # Apply AWQ clip on selected weights
+    logging.info("Started clip search ...")
+    t = time.time()
+    alphas = {}
+    for act_tensor, weight_tensor, do_transpose in wa_pack:
+        # First capture all the  activation values after calibration data sweep
+        output_dicts = {}
+        for inp_d in inputs:
+            np_inp_d = {name: numpy.asarray(tensor) for name, tensor in inp_d.items()}
+            output = session.run([act_tensor.name], np_inp_d)
+            output_dicts.setdefault(act_tensor.name, []).append(output[0])
+
+        # Concatenating the activation tensors over all calib data
+        x = np.concatenate(output_dicts[act_tensor.name], axis=0)  # n_token, ci
+        w = numpy_helper.to_array(
+            weight_tensor, base_dir=os.path.dirname(augmented_onnx_path)
+        ).copy()
+        if do_transpose:
+            w = w.T
+
+        awq_clip = AWQClipHelper(w, BLOCK_SIZE)
+        _clip_search(x, w, awq_clip)
+        alphas[weight_tensor.name] = awq_clip.best_alpha
+
+    logging.info(f"Clip search for all weights took {time.time() - t} seconds")
+
+    # Compute quantized weights and scales which are needed for DQ nodes
+    logging.info("Quantizing the actual weights ...")
+    t = time.time()
+    for act_tensor, weight_tensor, do_transpose in wa_pack:
+        w = numpy_helper.to_array(
+            weight_tensor, base_dir=os.path.dirname(augmented_onnx_path)
+        ).copy()
+        if do_transpose:
+            w = w.T
+
+        alpha = alphas.get(weight_tensor.name, 1)
+        qw, scale = quant_tensor(w, BLOCK_SIZE, alpha)
+        if do_transpose:
+            qw = qw.T
+            scale = scale.T
+        scales[weight_tensor.name] = scale.astype(
+            onnx.mapping.TENSOR_TYPE_MAP[gemm_io_type].np_dtype
+        )
+        gemm_weights_quantized[weight_tensor.name] = numpy.asarray(qw).astype(numpy.int8)
+
+        # Change the input activation type to the expected type, fp16 by default
+        # TODO: cast input C for Gemm
+        _change_input_type(onnx_model.graph, act_tensor.name, gemm_io_type)
+    logging.info(f"Quantizing actual weights took {time.time() - t} seconds")
+
+    logging.info("Inserting DQ nodes using quantized weights and scales ...")
+    t = time.time()
+    graph_gs = gs.import_onnx(onnx_model)
+    qdq.insert_dq_nodes(graph_gs, scales, quantized_weights=gemm_weights_quantized)
+    logging.info(f"Inserting DQ nodes took {time.time() - t} seconds")
+
+    logging.info("Exporting the quantized graph ...")
+    t = time.time()
+    model = gs.export_onnx(graph_gs)
+    model.ir_version = 9
+    logging.info(f"Exporting took {time.time() - t} seconds")
+
+    try:
+        os.remove(augmented_onnx_path)
+        if use_external_data_format:
+            os.remove(augmented_onnx_path + "_data")
+    except OSError:
+        logging.warn("Augmented ONNX model or external data file was not found!")
+
+    return model
+
+
+def quantize_int4(
+    quantize_mode: str,
+    onnx_model: onnx.onnx_pb.ModelProto,
+    calibration_data_reader: CalibrationDataReader = None,
+    use_external_data_format: bool = True,
+    gemm_io_type: onnx.TensorProto.DataType = onnx.TensorProto.FLOAT,
+) -> onnx.onnx_pb.ModelProto:
+    """Applies INT4 WoQ (Weight-Only-Quantization) to an ONNX file.
+
+    Currently only GEMM quantization is supported.
+    """
+    logging.info(f"Quantization Mode: {quantize_mode}")
+
+    # Patch GS modules to support INT4.
+    patch_gs_modules()
+
+    if "trt" in quantize_mode:
+        qdq.use_trt_qdq_ops()
+
+    if quantize_mode in ["int4_rtn", "int4_rtn_dq", "int4_rtn_trt", "int4_rtn_trt_dq"]:
+        return quantize_int4_rtn(onnx_model, gemm_io_type, dq_only="dq" in quantize_mode)
+    elif quantize_mode in ["int4_awq_clip", "int4_awq_clip_trt"]:
+        return quantize_int4_awq_clip(
+            onnx_model, calibration_data_reader, use_external_data_format, gemm_io_type
+        )
+    else:
+        raise RuntimeError(f"Unsupported quant mode: '{quantize_mode}'")
```

## modelopt/onnx/quantization/operators.py

 * *Ordering differences only*

```diff
@@ -1,91 +1,91 @@
-# Adapted from https://github.com/microsoft/onnxruntime/blob/baeece44ba075009c6bfe95891a8c1b3d4571cb3/onnxruntime/python/tools/quantization/operators/conv.py
-# and https://github.com/microsoft/onnxruntime/blob/baeece44ba075009c6bfe95891a8c1b3d4571cb3/onnxruntime/python/tools/quantization/operators/gather.py
-#
-# MIT License
-#
-# Copyright (c) Microsoft Corporation
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in all
-# copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-# SOFTWARE.
-
-# Not a contribution
-# Changes made by NVIDIA CORPORATION & AFFILIATES or otherwise documented as
-# NVIDIA-proprietary are not a contribution and subject to the following terms and conditions:
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Additional or modified QDQ operators on top ORT quantized operators."""
-from onnxruntime.quantization.operators.qdq_base_operator import QDQOperatorBase
-
-from ..op_types import is_normalization_op
-
-
-class QDQNormalization(QDQOperatorBase):
-    """By default, ORT does not quantize Normalization ops. This module is intended to help with that.
-
-    Note. QDQOperatorBase is not sufficient for dynamic input only quantization.
-    """
-
-    def __init__(self, onnx_quantizer, onnx_node):
-        """Normalization quantizer init."""
-        super().__init__(onnx_quantizer, onnx_node)
-
-    def quantize(self):
-        """Main function to quantize the Normalization ops."""
-        node = self.node
-        assert is_normalization_op(node.op_type)
-
-        # Quantize only the dynamic input (first input to the op)
-        self.quantizer.quantize_activation_tensor(node.input[0])
-        if not self.disable_qdq_for_node_output:
-            self.quantizer.quantize_activation_tensor(node.output[0])
-
-
-class QDQConvTranspose(QDQOperatorBase):
-    """QDQ for ConvTranspose operator."""
-
-    def __init__(self, onnx_quantizer, onnx_node):
-        """ConvTranspose quantizer init."""
-        super().__init__(onnx_quantizer, onnx_node)
-
-    def quantize(self):
-        """Main function to quantize the ConvTranspose ops."""
-        node = self.node
-        assert node.op_type == "ConvTranspose"
-
-        self.quantizer.quantize_activation_tensor(node.input[0])
-        if not self.disable_qdq_for_node_output:
-            self.quantizer.quantize_activation_tensor(node.output[0])
-
-        if self.quantizer.is_per_channel():
-            channel_axis = self.quantizer.qdq_op_type_per_channel_support_to_axis.get(
-                node.op_type, 0
-            )
-            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], channel_axis)
-        else:
-            self.quantizer.quantize_weight_tensor(node.input[1])
-
-        if len(node.input) == 3:
-            self.quantizer.quantize_bias_tensor(node.input[2], node.input[0], node.input[1])
+# Adapted from https://github.com/microsoft/onnxruntime/blob/baeece44ba075009c6bfe95891a8c1b3d4571cb3/onnxruntime/python/tools/quantization/operators/conv.py
+# and https://github.com/microsoft/onnxruntime/blob/baeece44ba075009c6bfe95891a8c1b3d4571cb3/onnxruntime/python/tools/quantization/operators/gather.py
+#
+# MIT License
+#
+# Copyright (c) Microsoft Corporation
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+# Not a contribution
+# Changes made by NVIDIA CORPORATION & AFFILIATES or otherwise documented as
+# NVIDIA-proprietary are not a contribution and subject to the following terms and conditions:
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Additional or modified QDQ operators on top ORT quantized operators."""
+from onnxruntime.quantization.operators.qdq_base_operator import QDQOperatorBase
+
+from ..op_types import is_normalization_op
+
+
+class QDQNormalization(QDQOperatorBase):
+    """By default, ORT does not quantize Normalization ops. This module is intended to help with that.
+
+    Note. QDQOperatorBase is not sufficient for dynamic input only quantization.
+    """
+
+    def __init__(self, onnx_quantizer, onnx_node):
+        """Normalization quantizer init."""
+        super().__init__(onnx_quantizer, onnx_node)
+
+    def quantize(self):
+        """Main function to quantize the Normalization ops."""
+        node = self.node
+        assert is_normalization_op(node.op_type)
+
+        # Quantize only the dynamic input (first input to the op)
+        self.quantizer.quantize_activation_tensor(node.input[0])
+        if not self.disable_qdq_for_node_output:
+            self.quantizer.quantize_activation_tensor(node.output[0])
+
+
+class QDQConvTranspose(QDQOperatorBase):
+    """QDQ for ConvTranspose operator."""
+
+    def __init__(self, onnx_quantizer, onnx_node):
+        """ConvTranspose quantizer init."""
+        super().__init__(onnx_quantizer, onnx_node)
+
+    def quantize(self):
+        """Main function to quantize the ConvTranspose ops."""
+        node = self.node
+        assert node.op_type == "ConvTranspose"
+
+        self.quantizer.quantize_activation_tensor(node.input[0])
+        if not self.disable_qdq_for_node_output:
+            self.quantizer.quantize_activation_tensor(node.output[0])
+
+        if self.quantizer.is_per_channel():
+            channel_axis = self.quantizer.qdq_op_type_per_channel_support_to_axis.get(
+                node.op_type, 0
+            )
+            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], channel_axis)
+        else:
+            self.quantizer.quantize_weight_tensor(node.input[1])
+
+        if len(node.input) == 3:
+            self.quantizer.quantize_bias_tensor(node.input[2], node.input[0], node.input[1])
```

## modelopt/onnx/quantization/ort_patching.py

```diff
@@ -1,197 +1,196 @@
-# Adapted from https://github.com/microsoft/onnxruntime/blob/baeece44ba075009c6bfe95891a8c1b3d4571cb3/onnxruntime/python/tools/quantization/quant_utils.py
-# and https://github.com/microsoft/onnxruntime/blob/baeece44ba075009c6bfe95891a8c1b3d4571cb3/onnxruntime/python/tools/quantization/calibrate.py
-# and https://github.com/microsoft/onnxruntime/blob/2ac381c55397dffff327cc6efecf6f95a70f90a1/onnxruntime/python/tools/quantization/onnx_quantizer.py
-#
-# MIT License
-#
-# Copyright (c) Microsoft Corporation
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in all
-# copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-# SOFTWARE.
-
-# Not a contribution
-# Changes made by NVIDIA CORPORATION & AFFILIATES or otherwise documented as
-# NVIDIA-proprietary are not a contribution and subject to the following terms and conditions:
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-
-"""This module contains all the patched functions from ORT."""
-
-from pathlib import Path
-
-import numpy as np
-import onnx
-import onnxruntime as ort
-from onnx import ModelProto, onnx_pb
-from onnxruntime.quantization import quant_utils
-from onnxruntime.quantization.calibrate import CalibraterBase, HistogramCollector
-from onnxruntime.quantization.onnx_quantizer import ONNXQuantizer
-from onnxruntime_extensions import get_library_path as _get_library_path
-from tqdm import tqdm
-
-
-def _compute_scale_zp(rmin, rmax, qmin, qmax, symmetric=False):
-    """Calculates the scale and zero point.
-
-    Calculate the scale s and zero point z for the quantization relation
-    r = s(q-z), where r are the original values and q are the corresponding
-    quantized values.
-
-    r and z are calculated such that every value within [rmin,rmax] has an
-    approximate representation within [qmin,qmax]. In addition, qmin <= z <=
-    qmax is enforced. If the symmetric flag is set to True, the interval
-    [rmin,rmax] is symmetrized to [-absmax, +absmax], where
-    absmax = max(abs(rmin), abs(rmax)).
-
-    Args:
-        rmin: minimum value of r
-        rmax: maximum value of r
-        qmin: minimum value representable by the target quantization data type
-        qmax: maximum value representable by the target quantization data type
-
-    Returns:
-        A tuple zero and scale (z, s)
-    """
-    if qmin > 0 or qmax < 0:
-        raise ValueError(
-            f"qmin and qmax must meet requirement: qmin <= 0 <= qmax while qmin:{qmin},"
-            f" qmmax:{qmax}"
-        )
-
-    # Adjust rmin and rmax such that 0 is included in the range. This is
-    # required to make sure zero can be represented by the quantization data
-    # type (i.e. to make sure qmin <= zero_point <= qmax)
-    rmin = min(rmin, 0)
-    rmax = max(rmax, 0)
-
-    if symmetric:
-        absmax = max(abs(rmin), abs(rmax))
-        rmin = -absmax
-        rmax = +absmax
-
-    scale = (rmax - rmin) / float(qmax - qmin)
-    if scale < 1e-9 or np.isinf(scale):
-        scale = 1.0
-        zero_point = 0
-    else:
-        zero_point = round(qmin - rmin / scale)
-
-    return zero_point, scale
-
-
-def _collect_value(histogram_collector, name_to_arr):
-    """Collect histogram on real value."""
-    for tensor, data_arr in tqdm(name_to_arr.items()):
-        data_arr = np.asarray(data_arr)  # noqa: PLW2901
-        data_arr = data_arr.flatten()  # noqa: PLW2901
-
-        if data_arr.size > 0:
-            min_value = np.min(data_arr)
-            max_value = np.max(data_arr)
-        else:
-            min_value = 0
-            max_value = 0
-
-        # Change the inf and nan values to meaningful min/max
-        min_value = (
-            np.finfo(np.float32).tiny if np.isinf(min_value) or np.isnan(min_value) else min_value
-        )
-        max_value = (
-            np.finfo(np.float32).max if np.isinf(max_value) or np.isnan(max_value) else max_value
-        )
-
-        threshold = max(abs(min_value), abs(max_value))
-
-        if tensor in histogram_collector.histogram_dict:
-            old_histogram = histogram_collector.histogram_dict[tensor]
-            histogram_collector.histogram_dict[tensor] = histogram_collector.merge_histogram(
-                old_histogram, data_arr, min_value, max_value, threshold
-            )
-        else:
-            hist, hist_edges = np.histogram(
-                data_arr, histogram_collector.num_bins, range=(-threshold, threshold)
-            )
-            histogram_collector.histogram_dict[tensor] = (
-                hist,
-                hist_edges,
-                min_value,
-                max_value,
-                threshold,
-            )
-
-
-def _check_opset_version(onnx_quantizer):
-    ai_onnx_domain = [
-        opset
-        for opset in onnx_quantizer.model.model.opset_import
-        if not opset.domain or opset.domain in ["ai.onnx", "ai.onnx.contrib"]
-    ]
-    if len(ai_onnx_domain) != 1:
-        raise ValueError("Failed to find proper ai.onnx domain")
-    opset_version = ai_onnx_domain[0].version
-
-    if opset_version == 10:
-        return 10
-
-    if opset_version < 10:
-        onnx_quantizer.model.model.opset_import.remove(ai_onnx_domain[0])
-        onnx_quantizer.model.model.opset_import.extend([onnx.helper.make_opsetid("", 11)])
-        opset_version = 11
-
-    if opset_version < 19 and onnx_quantizer.weight_qType == onnx_pb.TensorProto.FLOAT8E4M3FN:
-        onnx_quantizer.model.model.opset_import.remove(ai_onnx_domain[0])
-        onnx_quantizer.model.model.opset_import.extend([onnx.helper.make_opsetid("", 19)])
-        onnx_quantizer.model.model.ir_version = 9
-        opset_version = 19
-
-    onnx_quantizer.fuse_dynamic_quant = True
-    return opset_version
-
-
-def _create_inference_session(calibrator):
-    """Create an OnnxRuntime InferenceSession."""
-    sess_options = ort.SessionOptions()
-    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
-    sess_options.register_custom_ops_library(_get_library_path())
-    calibrator.infer_session = ort.InferenceSession(
-        calibrator.augmented_model_path,
-        sess_options=sess_options,
-        providers=calibrator.execution_providers,
-    )
-
-
-def _load_model_with_shape_infer(model_path: Path) -> ModelProto:
-    # TODO: skip shape inference only for models with plugins
-    return onnx.load(model_path.as_posix())
-
-
-def patch_ort_modules():
-    """Patches the ORT modules."""
-    HistogramCollector.collect_value = _collect_value
-    quant_utils.compute_scale_zp = _compute_scale_zp
-    quant_utils.load_model_with_shape_infer = _load_model_with_shape_infer
-    CalibraterBase.create_inference_session = _create_inference_session
-    ONNXQuantizer.check_opset_version = _check_opset_version
+# Adapted from https://github.com/microsoft/onnxruntime/blob/baeece44ba075009c6bfe95891a8c1b3d4571cb3/onnxruntime/python/tools/quantization/quant_utils.py
+# and https://github.com/microsoft/onnxruntime/blob/baeece44ba075009c6bfe95891a8c1b3d4571cb3/onnxruntime/python/tools/quantization/calibrate.py
+# and https://github.com/microsoft/onnxruntime/blob/2ac381c55397dffff327cc6efecf6f95a70f90a1/onnxruntime/python/tools/quantization/onnx_quantizer.py
+#
+# MIT License
+#
+# Copyright (c) Microsoft Corporation
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+# Not a contribution
+# Changes made by NVIDIA CORPORATION & AFFILIATES or otherwise documented as
+# NVIDIA-proprietary are not a contribution and subject to the following terms and conditions:
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+
+"""This module contains all the patched functions from ORT."""
+
+from pathlib import Path
+
+import numpy as np
+import onnx
+import onnxruntime as ort
+from onnx import ModelProto, onnx_pb
+from onnxruntime.quantization import quant_utils
+from onnxruntime.quantization.calibrate import CalibraterBase, HistogramCollector
+from onnxruntime.quantization.onnx_quantizer import ONNXQuantizer
+from onnxruntime_extensions import get_library_path as _get_library_path
+from tqdm import tqdm
+
+
+def _compute_scale_zp(rmin, rmax, qmin, qmax, symmetric=False):
+    """Calculates the scale and zero point.
+
+    Calculate the scale s and zero point z for the quantization relation
+    r = s(q-z), where r are the original values and q are the corresponding
+    quantized values.
+
+    r and z are calculated such that every value within [rmin,rmax] has an
+    approximate representation within [qmin,qmax]. In addition, qmin <= z <=
+    qmax is enforced. If the symmetric flag is set to True, the interval
+    [rmin,rmax] is symmetrized to [-absmax, +absmax], where
+    absmax = max(abs(rmin), abs(rmax)).
+
+    Args:
+        rmin: minimum value of r
+        rmax: maximum value of r
+        qmin: minimum value representable by the target quantization data type
+        qmax: maximum value representable by the target quantization data type
+
+    Returns:
+        A tuple zero and scale (z, s)
+    """
+    if qmin > 0 or qmax < 0:
+        raise ValueError(
+            f"qmin and qmax must meet requirement: qmin <= 0 <= qmax while qmin:{qmin},"
+            f" qmmax:{qmax}"
+        )
+
+    # Adjust rmin and rmax such that 0 is included in the range. This is
+    # required to make sure zero can be represented by the quantization data
+    # type (i.e. to make sure qmin <= zero_point <= qmax)
+    rmin = min(rmin, 0)
+    rmax = max(rmax, 0)
+
+    if symmetric:
+        absmax = max(abs(rmin), abs(rmax))
+        rmin = -absmax
+        rmax = +absmax
+
+    scale = (rmax - rmin) / float(qmax - qmin)
+    if scale < 1e-9 or np.isinf(scale):
+        scale = 1.0
+        zero_point = 0
+    else:
+        zero_point = round(qmin - rmin / scale)
+
+    return zero_point, scale
+
+
+def _collect_value(histogram_collector, name_to_arr):
+    """Collect histogram on real value."""
+    for tensor, data_arr in tqdm(name_to_arr.items()):
+        data_arr = np.asarray(data_arr)  # noqa: PLW2901
+        data_arr = data_arr.flatten()  # noqa: PLW2901
+
+        if data_arr.size > 0:
+            min_value = np.min(data_arr)
+            max_value = np.max(data_arr)
+        else:
+            min_value = 0
+            max_value = 0
+
+        # Change the inf and nan values to meaningful min/max
+        min_value = (
+            np.finfo(np.float32).tiny if np.isinf(min_value) or np.isnan(min_value) else min_value
+        )
+        max_value = (
+            np.finfo(np.float32).max if np.isinf(max_value) or np.isnan(max_value) else max_value
+        )
+
+        threshold = max(abs(min_value), abs(max_value))
+
+        if tensor in histogram_collector.histogram_dict:
+            old_histogram = histogram_collector.histogram_dict[tensor]
+            histogram_collector.histogram_dict[tensor] = histogram_collector.merge_histogram(
+                old_histogram, data_arr, min_value, max_value, threshold
+            )
+        else:
+            hist, hist_edges = np.histogram(
+                data_arr, histogram_collector.num_bins, range=(-threshold, threshold)
+            )
+            histogram_collector.histogram_dict[tensor] = (
+                hist,
+                hist_edges,
+                min_value,
+                max_value,
+                threshold,
+            )
+
+
+def _check_opset_version(onnx_quantizer):
+    ai_onnx_domain = [
+        opset
+        for opset in onnx_quantizer.model.model.opset_import
+        if not opset.domain or opset.domain in ["ai.onnx", "ai.onnx.contrib"]
+    ]
+    if len(ai_onnx_domain) != 1:
+        raise ValueError("Failed to find proper ai.onnx domain")
+    opset_version = ai_onnx_domain[0].version
+
+    if opset_version == 10:
+        return 10
+
+    if opset_version < 10:
+        onnx_quantizer.model.model.opset_import.remove(ai_onnx_domain[0])
+        onnx_quantizer.model.model.opset_import.extend([onnx.helper.make_opsetid("", 11)])
+        opset_version = 11
+
+    if opset_version < 19 and onnx_quantizer.weight_qType == onnx_pb.TensorProto.FLOAT8E4M3FN:
+        onnx_quantizer.model.model.opset_import.remove(ai_onnx_domain[0])
+        onnx_quantizer.model.model.opset_import.extend([onnx.helper.make_opsetid("", 19)])
+        onnx_quantizer.model.model.ir_version = 9
+        opset_version = 19
+
+    onnx_quantizer.fuse_dynamic_quant = True
+    return opset_version
+
+
+def _create_inference_session(calibrator):
+    """Create an OnnxRuntime InferenceSession."""
+    sess_options = ort.SessionOptions()
+    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
+    sess_options.register_custom_ops_library(_get_library_path())
+    calibrator.infer_session = ort.InferenceSession(
+        calibrator.augmented_model_path,
+        sess_options=sess_options,
+        providers=["CUDAExecutionProvider", "CPUExecutionProvider"],
+    )
+
+
+def _load_model_with_shape_infer(model_path: Path) -> ModelProto:
+    # TODO: skip shape inference only for models with plugins
+    return onnx.load(model_path.as_posix())
+
+
+def patch_ort_modules():
+    """Patches the ORT modules."""
+    HistogramCollector.collect_value = _collect_value
+    quant_utils.load_model_with_shape_infer = _load_model_with_shape_infer
+    CalibraterBase.create_inference_session = _create_inference_session
+    ONNXQuantizer.check_opset_version = _check_opset_version
```

## modelopt/onnx/quantization/ort_utils.py

```diff
@@ -1,24 +1,24 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Provides basic ORT inference utils, shoule be replaced by modelopt.torch.ort_client."""
-
-import onnxruntime as ort
-
-
-def create_inference_session(onnx_path: str):
-    """Create an OnnxRuntime InferenceSession."""
-    sess_options = ort.SessionOptions()
-    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
-    return ort.InferenceSession(
-        onnx_path,
-        sess_options=sess_options,
-        providers=["CPUExecutionProvider"],
-    )
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Provides basic ORT inference utils, shoule be replaced by modelopt.torch.ort_client."""
+
+import onnxruntime as ort
+
+
+def create_inference_session(onnx_path: str):
+    """Create an OnnxRuntime InferenceSession."""
+    sess_options = ort.SessionOptions()
+    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
+    return ort.InferenceSession(
+        onnx_path,
+        sess_options=sess_options,
+        providers=["CUDAExecutionProvider", "CPUExecutionProvider"],
+    )
```

## modelopt/onnx/quantization/partitioning.py

 * *Ordering differences only*

```diff
@@ -1,373 +1,373 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utilities related to partitioning the ONNX model to place QDQ nodes."""
-from typing import Dict, List, Set, Tuple
-
-from onnx_graphsurgeon.ir.graph import Graph
-from onnx_graphsurgeon.ir.node import Node
-
-from modelopt.onnx.op_types import (
-    is_copy_op,
-    is_linear_op,
-    is_pointwise_or_elementwise_op,
-    is_pooling_or_window_op,
-)
-from modelopt.onnx.quantization.graph_utils import (
-    get_fusible_backbone,
-    has_const_input,
-    has_path_type,
-    is_const_input,
-)
-from modelopt.onnx.utils import (
-    get_child_nodes,
-    get_variable_inputs,
-)
-
-
-def _build_fusible_partition(
-    cur_node: Node,
-    fusible_partition: List[Node],
-    partitioned_nodes: Set[str],
-    non_residual_inputs: Dict[str, str],
-    graph: Graph,
-) -> None:
-    """Traverses the graph starting from cur_node and updates the fusible_partition list.
-
-    Add a nodes to the partition if any of these holds:
-    1. The node is a unary or binary pointwise operation and fusible by cask
-    2. The node is BN and/or Relu and fusible with preceding Conv op
-    3. The node is a residual Add and fusible with current partition
-
-    Args:
-        cur_node: Current candidate node for the partition.
-        fusible_partition: Current fusible partition.
-        partitioned_nodes: Set of already partitioned nodes.
-        non_residual_inputs: Non-residual input map.
-        graph: ONNX model graph.
-
-    Returns:
-        Backbone node of the given pointwise op, None if not found.
-    """
-
-    def _is_on_non_residual_path(node: Node) -> bool:
-        if (
-            node.op == "Add"  # Input node should be an Add node
-            # The Add node should have a non-residual input
-            and non_residual_inputs[node.name]
-            # Input from the current node is non-residual
-            and cur_node.outputs[0].name == non_residual_inputs[node.name]
-        ):
-            return True
-        return False
-
-    def _get_partition_node_outputs() -> List[str]:
-        # Collect tensor names produced by nodes in fusible_partition
-        # TODO: cache sub-partition outputs and append after them
-        partition_node_outputs = []
-        for partition_node in fusible_partition:
-            for node_output in partition_node.outputs:
-                partition_node_outputs.append(node_output.name)
-
-        return partition_node_outputs
-
-    def _is_cask_fusible(node: Node, partition_node_outputs: List[str]) -> bool:
-        for tensor in node.inputs:
-            if tensor.name not in partition_node_outputs:
-                if not is_const_input(tensor):
-                    return False
-        return True
-
-    def _is_fusible_mul(mul_node: Node) -> bool:
-        # Don't consider Mul as fusible if it has the indicated ancestors.
-        # Otherwise, this causes regressions in:
-        #  - densenet-12 and inception-v2-9 (dangling constants): [mul_node.op, "Unsqueeze"]
-        #  - faster_vit: ["Mul", "Add", "Tanh", "Mul", "Add", "Mul", "Pow"]
-        #  This improves perf in various models:  mobilenet_v3, vovnet19b, coatnet_0, regnety_040.
-
-        var_inps = get_variable_inputs(mul_node)
-        if len(var_inps) <= 1:
-            return True
-
-        # Conv-Sigmoid-Mul chain is fusible
-        if has_path_type(mul_node, graph, ["Mul", "Sigmoid", "Conv"], is_forward=False):
-            return True
-
-        non_fusible_patterns = [["Mul", "Sigmoid"], ["Mul", "HardSigmoid"]]
-        if any([has_path_type(mul_node, graph, p, is_forward=False) for p in non_fusible_patterns]):
-            return False
-
-        return True
-
-    # Check the Mul nodes for their fusion compatibility
-    if cur_node.op == "Mul" and not _is_fusible_mul(cur_node):
-        return
-
-    # Add current node to the partition
-    fusible_partition.append(cur_node)
-    partitioned_nodes.add(cur_node.name)
-
-    # If on non-residual path, return after adding the node to the partition
-    # TODO: can Myelin fuse pointwise ops followed by residual Add?
-    if cur_node.op == "Add" and non_residual_inputs[cur_node.name]:
-        return
-
-    consumer_nodes = get_child_nodes(cur_node)
-    partition_node_outputs = _get_partition_node_outputs()
-
-    # TODO: traverse consumer nodes in topologically sorted order
-    for consumer_node in consumer_nodes:
-        if consumer_node.name in partitioned_nodes:
-            continue
-
-        if (
-            (
-                is_pointwise_or_elementwise_op(consumer_node.op)
-                and _is_cask_fusible(consumer_node, partition_node_outputs)
-            )
-            or (
-                consumer_node.op in ["BatchNormalization", "Relu"]
-                and get_fusible_backbone(consumer_node, graph)
-            )
-            or _is_on_non_residual_path(consumer_node)
-        ):
-            # DFS with the consumer and find more nodes for the partition
-            _build_fusible_partition(
-                consumer_node,
-                fusible_partition,
-                partitioned_nodes,
-                non_residual_inputs,
-                graph,
-            )
-
-
-def find_fusible_partitions(
-    graph: Graph,
-    partitioned_nodes: Set[str],
-    non_residual_inputs: Dict[str, str],
-) -> Tuple[List[List[Node]], List[List[Node]]]:
-    """Traverses the graph and collects all cask/kgen fusible partitions.
-
-    Args:
-        graph: Onnx model graph.
-        partitioned_nodes: Set of already partitioned nodes.
-        non_residual_inputs: Non-residual input map.
-
-    Returns:
-        List of partitions that are fusible by CASK with Conv/MatMul backbone.
-        List of KGEN partitions with pointwise ops only.
-    """
-
-    def _partition_helper(fusible_root_type_checker):
-        all_fusible_partitions = []  # Collects all individual partitions
-        for node in graph.nodes:
-            # Check if the node is already in some other partition
-            if node.name in partitioned_nodes:
-                continue
-
-            # Start a partition with a linear op
-            if not fusible_root_type_checker(node.op):
-                continue
-
-            # Try building a partition starting with this current linear op
-            fusible_partition = []
-            _build_fusible_partition(
-                node,
-                fusible_partition,
-                partitioned_nodes,
-                non_residual_inputs,
-                graph,
-            )
-
-            # Gather the non-empty partitions
-            if fusible_partition:
-                partitioned_nodes.update([node.name for node in fusible_partition])
-                all_fusible_partitions.append(fusible_partition)
-
-        return all_fusible_partitions
-
-    cask_fusible_partitions = _partition_helper(is_linear_op)
-    kgen_partitions = _partition_helper(is_pointwise_or_elementwise_op)
-
-    return cask_fusible_partitions, kgen_partitions
-
-
-def get_skiped_output_layers(graph: Graph, paritially_quantizable_nodes: List[Node]) -> List[str]:
-    """Returns the name of the non-quantizable output layers."""
-    # TODO: see if input producer is already quantized or not
-    # TODO: filter out input layers if consumer is not quantized already
-    output_layers = []
-    paritially_quantizable_node_names = set([node.name for node in paritially_quantizable_nodes])
-    graph_output_names = [tensor.name for tensor in graph.outputs]
-
-    for node in graph.nodes:
-        for tensor in node.outputs:
-            if tensor.name in graph_output_names:
-                if (
-                    node.op not in ["Conv", "Gemm", "MatMul"]
-                    and node.name not in paritially_quantizable_node_names
-                ):
-                    output_layers.append(node.name)
-
-    return output_layers
-
-
-def find_quantizable_nodes(
-    graph: Graph,
-    nodes_to_quantize: List[Node],
-    partitioned_nodes: Set[str],
-    quantizable_op_types: List[str],
-) -> List[Node]:
-    """Return the graph ops which are quantizable but not partitioned yet."""
-
-    # TODO: Check if any BatchNormalization is un-partitioned
-    # Note. Maxpool quantization has +/-
-    # Note. Prologue fusion is not handled, so some pointwise ops might be unnecessarily quantized
-    def _has_quantizable_consumer(node: Node, quantizable_node_set: Set[str]) -> bool:
-        children = get_child_nodes(node)
-        for child_node in children:
-            if (child_node.name in quantizable_node_set) or (
-                is_copy_op(child_node.op)
-                and _has_quantizable_consumer(child_node, quantizable_node_set)
-            ):
-                return True
-
-        return False
-
-    quantizable_nodes = []
-    pooling_and_window_ops = []
-    for node in graph.nodes:
-        if node.name in partitioned_nodes or node.op not in quantizable_op_types:
-            continue
-
-        # Collect pooling and window ops for second pass
-        # as they need to check their neighbor's quantization status
-        if is_pooling_or_window_op(node.op):
-            pooling_and_window_ops.append(node)
-            continue
-
-        if is_pointwise_or_elementwise_op(node.op) and has_const_input(node):
-            continue
-
-        quantizable_nodes.append(node)
-
-    quantizable_node_set = set(
-        [node.name for node in nodes_to_quantize] + [node.name for node in quantizable_nodes]
-    )
-    for node in pooling_and_window_ops:
-        # TODO: Add or _has_quantizable_producer, ex. inception-v1-12.onnx
-        if _has_quantizable_consumer(node, quantizable_node_set):
-            quantizable_nodes.append(node)
-
-    return quantizable_nodes
-
-
-def find_hardcoded_patterns(graph: Graph) -> List[List[Node]]:
-    """Finds some non-quantizable pre-defined patterns!.
-
-    Note. matching this tail pattern causes MTL_v1 -5.5%
-    ["ReduceSum", "Add", "Div", "Mul", "ReduceSum", "Sub", "Pow", "Mul", "ReduceSum", "Sqrt"]
-    """
-    gelu = ["Div", "Erf", "Add", "Mul", "Mul"]
-
-    matched_node_names = []
-    for node in graph.nodes:
-        for path_type in [gelu]:
-            path_nodes = []
-            if has_path_type(
-                node,
-                graph,
-                path_type,
-                is_forward=True,
-                wild_card_types=[],
-                path_nodes=path_nodes,
-            ):
-                matched_node_names.extend(path_nodes)
-
-    return [matched_node_names]
-
-
-def find_layer_norm_partitions(graph: Graph) -> List[List[Node]]:
-    """Finds the layer norm patterns in the graph."""
-    # The most common LayerNorm implementation looks like this:
-    # t -> ReduceMean -> Sub -> Pow -> ReduceMean -> Add -> Sqrt -> Div -> Mul -> Add -> output
-    #   \________________/  \______________________________________/
-    # For simplicity, we do not match the connection between Sub and Div nodes.
-    layer_norm_chain_types = [
-        "ReduceMean",
-        "Sub",
-        "Pow",
-        "ReduceMean",
-        "Add",
-        "Sqrt",
-        "Div",
-        "Mul",
-        "Add",
-    ]
-    mean_var_norm_chain_types = layer_norm_chain_types[:-2]
-    wild_card_types = ["Cast"]
-    layer_norm_partitions = []
-
-    for node in graph.nodes:
-        layer_norm_partition = []
-        if node.op == "LayerNormalization":
-            layer_norm_partitions.append([node])
-        elif node.op == layer_norm_chain_types[0] and has_path_type(
-            node, graph, layer_norm_chain_types, True, wild_card_types, layer_norm_partition
-        ):
-            layer_norm_partitions.append(layer_norm_partition)
-        elif node.op == mean_var_norm_chain_types[0] and has_path_type(
-            node, graph, mean_var_norm_chain_types, True, wild_card_types, layer_norm_partition
-        ):
-            layer_norm_partitions.append(layer_norm_partition)
-
-    return layer_norm_partitions
-
-
-def find_mha_partitions(graph: Graph) -> List[List[Node]]:
-    """Finds the MHA patterns in the graph that should not be quantized.
-
-    A common MHA implementation looks like this:
-    t -> MatMul -> (optional) Pointwise ops (such as Add, Mul, Sub) -> Softmax -> MatMul -> output
-    Patterns that do not look like that should not be quantized (at least for now).
-    """
-    mha_chain_types = [
-        [
-            "MatMul",
-            "MatMul",
-        ],
-    ]
-    mha_partitions = []
-
-    for node in graph.nodes:
-        if node.op == "MatMul":
-            for chain_type in mha_chain_types:
-                mha_partition = []
-                if has_path_type(node, graph, chain_type, True, [], mha_partition):
-                    mha_partitions.append(mha_partition)
-
-    return mha_partitions
-
-
-def find_non_quantizable_partitions_from_patterns(graph: Graph) -> List[List[str]]:
-    """Finds fusible partition from fixed patterns.
-
-    Certain fused kernel counterpart is often a subgraph of native ops in onnx.
-    Those patterns are identified here and quantized to match compiler expectation.
-    """
-    hard_coded_partitions = find_hardcoded_patterns(graph)
-    layer_norm_partitions = find_layer_norm_partitions(graph)
-    mha_partitions = find_mha_partitions(graph)
-
-    partitions = []
-    for partition_nodes in hard_coded_partitions + layer_norm_partitions + mha_partitions:
-        partitions.append([node.name for node in partition_nodes])
-
-    return partitions
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utilities related to partitioning the ONNX model to place QDQ nodes."""
+from typing import Dict, List, Set, Tuple
+
+from onnx_graphsurgeon.ir.graph import Graph
+from onnx_graphsurgeon.ir.node import Node
+
+from modelopt.onnx.op_types import (
+    is_copy_op,
+    is_linear_op,
+    is_pointwise_or_elementwise_op,
+    is_pooling_or_window_op,
+)
+from modelopt.onnx.quantization.graph_utils import (
+    get_fusible_backbone,
+    has_const_input,
+    has_path_type,
+    is_const_input,
+)
+from modelopt.onnx.utils import (
+    get_child_nodes,
+    get_variable_inputs,
+)
+
+
+def _build_fusible_partition(
+    cur_node: Node,
+    fusible_partition: List[Node],
+    partitioned_nodes: Set[str],
+    non_residual_inputs: Dict[str, str],
+    graph: Graph,
+) -> None:
+    """Traverses the graph starting from cur_node and updates the fusible_partition list.
+
+    Add a nodes to the partition if any of these holds:
+    1. The node is a unary or binary pointwise operation and fusible by cask
+    2. The node is BN and/or Relu and fusible with preceding Conv op
+    3. The node is a residual Add and fusible with current partition
+
+    Args:
+        cur_node: Current candidate node for the partition.
+        fusible_partition: Current fusible partition.
+        partitioned_nodes: Set of already partitioned nodes.
+        non_residual_inputs: Non-residual input map.
+        graph: ONNX model graph.
+
+    Returns:
+        Backbone node of the given pointwise op, None if not found.
+    """
+
+    def _is_on_non_residual_path(node: Node) -> bool:
+        if (
+            node.op == "Add"  # Input node should be an Add node
+            # The Add node should have a non-residual input
+            and non_residual_inputs[node.name]
+            # Input from the current node is non-residual
+            and cur_node.outputs[0].name == non_residual_inputs[node.name]
+        ):
+            return True
+        return False
+
+    def _get_partition_node_outputs() -> List[str]:
+        # Collect tensor names produced by nodes in fusible_partition
+        # TODO: cache sub-partition outputs and append after them
+        partition_node_outputs = []
+        for partition_node in fusible_partition:
+            for node_output in partition_node.outputs:
+                partition_node_outputs.append(node_output.name)
+
+        return partition_node_outputs
+
+    def _is_cask_fusible(node: Node, partition_node_outputs: List[str]) -> bool:
+        for tensor in node.inputs:
+            if tensor.name not in partition_node_outputs:
+                if not is_const_input(tensor):
+                    return False
+        return True
+
+    def _is_fusible_mul(mul_node: Node) -> bool:
+        # Don't consider Mul as fusible if it has the indicated ancestors.
+        # Otherwise, this causes regressions in:
+        #  - densenet-12 and inception-v2-9 (dangling constants): [mul_node.op, "Unsqueeze"]
+        #  - faster_vit: ["Mul", "Add", "Tanh", "Mul", "Add", "Mul", "Pow"]
+        #  This improves perf in various models:  mobilenet_v3, vovnet19b, coatnet_0, regnety_040.
+
+        var_inps = get_variable_inputs(mul_node)
+        if len(var_inps) <= 1:
+            return True
+
+        # Conv-Sigmoid-Mul chain is fusible
+        if has_path_type(mul_node, graph, ["Mul", "Sigmoid", "Conv"], is_forward=False):
+            return True
+
+        non_fusible_patterns = [["Mul", "Sigmoid"], ["Mul", "HardSigmoid"]]
+        if any([has_path_type(mul_node, graph, p, is_forward=False) for p in non_fusible_patterns]):
+            return False
+
+        return True
+
+    # Check the Mul nodes for their fusion compatibility
+    if cur_node.op == "Mul" and not _is_fusible_mul(cur_node):
+        return
+
+    # Add current node to the partition
+    fusible_partition.append(cur_node)
+    partitioned_nodes.add(cur_node.name)
+
+    # If on non-residual path, return after adding the node to the partition
+    # TODO: can Myelin fuse pointwise ops followed by residual Add?
+    if cur_node.op == "Add" and non_residual_inputs[cur_node.name]:
+        return
+
+    consumer_nodes = get_child_nodes(cur_node)
+    partition_node_outputs = _get_partition_node_outputs()
+
+    # TODO: traverse consumer nodes in topologically sorted order
+    for consumer_node in consumer_nodes:
+        if consumer_node.name in partitioned_nodes:
+            continue
+
+        if (
+            (
+                is_pointwise_or_elementwise_op(consumer_node.op)
+                and _is_cask_fusible(consumer_node, partition_node_outputs)
+            )
+            or (
+                consumer_node.op in ["BatchNormalization", "Relu"]
+                and get_fusible_backbone(consumer_node, graph)
+            )
+            or _is_on_non_residual_path(consumer_node)
+        ):
+            # DFS with the consumer and find more nodes for the partition
+            _build_fusible_partition(
+                consumer_node,
+                fusible_partition,
+                partitioned_nodes,
+                non_residual_inputs,
+                graph,
+            )
+
+
+def find_fusible_partitions(
+    graph: Graph,
+    partitioned_nodes: Set[str],
+    non_residual_inputs: Dict[str, str],
+) -> Tuple[List[List[Node]], List[List[Node]]]:
+    """Traverses the graph and collects all cask/kgen fusible partitions.
+
+    Args:
+        graph: Onnx model graph.
+        partitioned_nodes: Set of already partitioned nodes.
+        non_residual_inputs: Non-residual input map.
+
+    Returns:
+        List of partitions that are fusible by CASK with Conv/MatMul backbone.
+        List of KGEN partitions with pointwise ops only.
+    """
+
+    def _partition_helper(fusible_root_type_checker):
+        all_fusible_partitions = []  # Collects all individual partitions
+        for node in graph.nodes:
+            # Check if the node is already in some other partition
+            if node.name in partitioned_nodes:
+                continue
+
+            # Start a partition with a linear op
+            if not fusible_root_type_checker(node.op):
+                continue
+
+            # Try building a partition starting with this current linear op
+            fusible_partition = []
+            _build_fusible_partition(
+                node,
+                fusible_partition,
+                partitioned_nodes,
+                non_residual_inputs,
+                graph,
+            )
+
+            # Gather the non-empty partitions
+            if fusible_partition:
+                partitioned_nodes.update([node.name for node in fusible_partition])
+                all_fusible_partitions.append(fusible_partition)
+
+        return all_fusible_partitions
+
+    cask_fusible_partitions = _partition_helper(is_linear_op)
+    kgen_partitions = _partition_helper(is_pointwise_or_elementwise_op)
+
+    return cask_fusible_partitions, kgen_partitions
+
+
+def get_skiped_output_layers(graph: Graph, paritially_quantizable_nodes: List[Node]) -> List[str]:
+    """Returns the name of the non-quantizable output layers."""
+    # TODO: see if input producer is already quantized or not
+    # TODO: filter out input layers if consumer is not quantized already
+    output_layers = []
+    paritially_quantizable_node_names = set([node.name for node in paritially_quantizable_nodes])
+    graph_output_names = [tensor.name for tensor in graph.outputs]
+
+    for node in graph.nodes:
+        for tensor in node.outputs:
+            if tensor.name in graph_output_names:
+                if (
+                    node.op not in ["Conv", "Gemm", "MatMul"]
+                    and node.name not in paritially_quantizable_node_names
+                ):
+                    output_layers.append(node.name)
+
+    return output_layers
+
+
+def find_quantizable_nodes(
+    graph: Graph,
+    nodes_to_quantize: List[Node],
+    partitioned_nodes: Set[str],
+    quantizable_op_types: List[str],
+) -> List[Node]:
+    """Return the graph ops which are quantizable but not partitioned yet."""
+
+    # TODO: Check if any BatchNormalization is un-partitioned
+    # Note. Maxpool quantization has +/-
+    # Note. Prologue fusion is not handled, so some pointwise ops might be unnecessarily quantized
+    def _has_quantizable_consumer(node: Node, quantizable_node_set: Set[str]) -> bool:
+        children = get_child_nodes(node)
+        for child_node in children:
+            if (child_node.name in quantizable_node_set) or (
+                is_copy_op(child_node.op)
+                and _has_quantizable_consumer(child_node, quantizable_node_set)
+            ):
+                return True
+
+        return False
+
+    quantizable_nodes = []
+    pooling_and_window_ops = []
+    for node in graph.nodes:
+        if node.name in partitioned_nodes or node.op not in quantizable_op_types:
+            continue
+
+        # Collect pooling and window ops for second pass
+        # as they need to check their neighbor's quantization status
+        if is_pooling_or_window_op(node.op):
+            pooling_and_window_ops.append(node)
+            continue
+
+        if is_pointwise_or_elementwise_op(node.op) and has_const_input(node):
+            continue
+
+        quantizable_nodes.append(node)
+
+    quantizable_node_set = set(
+        [node.name for node in nodes_to_quantize] + [node.name for node in quantizable_nodes]
+    )
+    for node in pooling_and_window_ops:
+        # TODO: Add or _has_quantizable_producer, ex. inception-v1-12.onnx
+        if _has_quantizable_consumer(node, quantizable_node_set):
+            quantizable_nodes.append(node)
+
+    return quantizable_nodes
+
+
+def find_hardcoded_patterns(graph: Graph) -> List[List[Node]]:
+    """Finds some non-quantizable pre-defined patterns!.
+
+    Note. matching this tail pattern causes MTL_v1 -5.5%
+    ["ReduceSum", "Add", "Div", "Mul", "ReduceSum", "Sub", "Pow", "Mul", "ReduceSum", "Sqrt"]
+    """
+    gelu = ["Div", "Erf", "Add", "Mul", "Mul"]
+
+    matched_node_names = []
+    for node in graph.nodes:
+        for path_type in [gelu]:
+            path_nodes = []
+            if has_path_type(
+                node,
+                graph,
+                path_type,
+                is_forward=True,
+                wild_card_types=[],
+                path_nodes=path_nodes,
+            ):
+                matched_node_names.extend(path_nodes)
+
+    return [matched_node_names]
+
+
+def find_layer_norm_partitions(graph: Graph) -> List[List[Node]]:
+    """Finds the layer norm patterns in the graph."""
+    # The most common LayerNorm implementation looks like this:
+    # t -> ReduceMean -> Sub -> Pow -> ReduceMean -> Add -> Sqrt -> Div -> Mul -> Add -> output
+    #   \________________/  \______________________________________/
+    # For simplicity, we do not match the connection between Sub and Div nodes.
+    layer_norm_chain_types = [
+        "ReduceMean",
+        "Sub",
+        "Pow",
+        "ReduceMean",
+        "Add",
+        "Sqrt",
+        "Div",
+        "Mul",
+        "Add",
+    ]
+    mean_var_norm_chain_types = layer_norm_chain_types[:-2]
+    wild_card_types = ["Cast"]
+    layer_norm_partitions = []
+
+    for node in graph.nodes:
+        layer_norm_partition = []
+        if node.op == "LayerNormalization":
+            layer_norm_partitions.append([node])
+        elif node.op == layer_norm_chain_types[0] and has_path_type(
+            node, graph, layer_norm_chain_types, True, wild_card_types, layer_norm_partition
+        ):
+            layer_norm_partitions.append(layer_norm_partition)
+        elif node.op == mean_var_norm_chain_types[0] and has_path_type(
+            node, graph, mean_var_norm_chain_types, True, wild_card_types, layer_norm_partition
+        ):
+            layer_norm_partitions.append(layer_norm_partition)
+
+    return layer_norm_partitions
+
+
+def find_mha_partitions(graph: Graph) -> List[List[Node]]:
+    """Finds the MHA patterns in the graph that should not be quantized.
+
+    A common MHA implementation looks like this:
+    t -> MatMul -> (optional) Pointwise ops (such as Add, Mul, Sub) -> Softmax -> MatMul -> output
+    Patterns that do not look like that should not be quantized (at least for now).
+    """
+    mha_chain_types = [
+        [
+            "MatMul",
+            "MatMul",
+        ],
+    ]
+    mha_partitions = []
+
+    for node in graph.nodes:
+        if node.op == "MatMul":
+            for chain_type in mha_chain_types:
+                mha_partition = []
+                if has_path_type(node, graph, chain_type, True, [], mha_partition):
+                    mha_partitions.append(mha_partition)
+
+    return mha_partitions
+
+
+def find_non_quantizable_partitions_from_patterns(graph: Graph) -> List[List[str]]:
+    """Finds fusible partition from fixed patterns.
+
+    Certain fused kernel counterpart is often a subgraph of native ops in onnx.
+    Those patterns are identified here and quantized to match compiler expectation.
+    """
+    hard_coded_partitions = find_hardcoded_patterns(graph)
+    layer_norm_partitions = find_layer_norm_partitions(graph)
+    mha_partitions = find_mha_partitions(graph)
+
+    partitions = []
+    for partition_nodes in hard_coded_partitions + layer_norm_partitions + mha_partitions:
+        partitions.append([node.name for node in partition_nodes])
+
+    return partitions
```

## modelopt/onnx/quantization/qdq_utils.py

 * *Ordering differences only*

```diff
@@ -1,242 +1,242 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Various utils to support inserting Q/DQ nodes."""
-
-from typing import Dict, Sequence, Set
-
-import numpy as np
-import onnx
-import onnx_graphsurgeon as gs
-
-QUANTIZE_NODE_NAME = "QuantizeLinear"
-DEQUANTIZE_NODE_NAME = "DequantizeLinear"
-
-
-def use_trt_qdq_ops():
-    """Globally set node names to TRT custom names."""
-    global QUANTIZE_NODE_NAME
-    QUANTIZE_NODE_NAME = "TRT_INT4QuantizeLinear"
-    global DEQUANTIZE_NODE_NAME
-    DEQUANTIZE_NODE_NAME = "TRT_INT4DequantizeLinear"
-
-
-def _wq_name(name: str):
-    return name + "_i4"
-
-
-def _scale_name(name: str):
-    return name + "_scale"
-
-
-def _zp_name(name: str):
-    return name + "_zp"
-
-
-def _q_name(name: str):
-    return name + "_QuantizeLinear"
-
-
-def _q_out_name(name: str):
-    return name + "_QuantizeLinear_Output"
-
-
-def _dq_name(name: str):
-    return name + "_DequantizeLinear"
-
-
-def _dq_out_name(name: str):
-    return name + "_DequantizeLinear_Output"
-
-
-def make_gs_quantized_weight(name: str, wq: np.ndarray, dtype) -> gs.Constant:
-    """Create a GraphSurgeon tensor from a quantized weight tensor.
-
-    `name` is the desired _basename_ of the tensor.
-    """
-    return gs.make_constant(_wq_name(name), np.asarray(wq), dtype)
-
-
-def make_gs_zp(name: str, shape: Sequence[int], dtype) -> gs.Constant:
-    """Create a GraphSurgeon zero-point tensor of all zeroes with the given shape.
-
-    `name` is the desired _basename_ of the tensor.
-    """
-    return gs.make_constant(
-        _zp_name(name),
-        np.zeros(shape, dtype=onnx.mapping.TENSOR_TYPE_MAP[int(dtype)].np_dtype),
-        dtype,
-    )
-
-
-def make_gs_scale(name: str, scale: np.ndarray) -> gs.Constant:
-    """Create a GraphSurgeon scale tensor from the given numpy array.
-
-    `name` is the desired _basename_ of the tensor.
-    """
-    return gs.Constant(_scale_name(name), np.asarray(scale))
-
-
-def make_gs_quantize_output(
-    name: str, shape: Sequence[int], dtype: onnx.TensorProto.DataType
-) -> gs.Variable:
-    """Create a GraphSurgeon variable representing the output of a quantize node.
-
-    `name` is the desired _basename_ of the node.
-    """
-    return gs.make_variable(_q_out_name(name), dtype=dtype, shape=shape)
-
-
-def make_gs_quantize_node(
-    name: str, inputs: Sequence[gs.Tensor], outputs: Sequence[gs.Tensor]
-) -> gs.Node:
-    """Create a GraphSurgeon Quantize node.
-
-    `name` is the desired _basename_ of the node.
-    """
-    return gs.Node(
-        QUANTIZE_NODE_NAME,
-        name=_q_name(name),
-        inputs=inputs,
-        outputs=outputs,
-    )
-
-
-def make_gs_dequantize_output(
-    name: str,
-    shape: Sequence[int],
-    dtype: np.dtype,
-) -> gs.Variable:
-    """Create a GraphSurgeon variable representing the output of a quantize node.
-
-    `name` is the desired _basename_ of the node.
-    """
-    return gs.Variable(_dq_out_name(name), dtype=dtype, shape=shape)
-
-
-def make_gs_dequantize_node(
-    name: str, inputs: Sequence[gs.Tensor], outputs: Sequence[gs.Tensor]
-) -> gs.Node:
-    """Create a GraphSurgeon Dequantize node.
-
-    `name` is the desired _basename_ of the node.
-    """
-    return gs.Node(
-        DEQUANTIZE_NODE_NAME,
-        name=_dq_name(name),
-        inputs=inputs,
-        outputs=outputs,
-    )
-
-
-def _postprocess_qdq(
-    graph: gs.Graph,
-    orig_weight_names: Set[str],
-    q_nodes: Dict[str, gs.Node] = {},
-    dq_nodes: Dict[str, gs.Node] = {},
-):
-    # Inserts all newly created nodes to graph.
-    # Update all consumers of original initializers to point to the DQ nodes.
-    for node in graph.nodes:
-        for i in range(len(node.inputs)):
-            key = node.inputs[i].name
-            if key not in orig_weight_names:
-                continue
-            node.inputs[i] = dq_nodes[key].outputs[0]
-
-    # Insert new nodes.
-    graph.nodes.extend(q_nodes.values())
-    graph.nodes.extend(dq_nodes.values())
-
-    graph.cleanup()
-    graph.toposort()
-
-
-def insert_dq_nodes(
-    graph: gs.Graph, scales: Dict[str, np.ndarray], quantized_weights: Dict[str, np.ndarray]
-):
-    """Insert new initializers and DQ nodes into graph.
-
-    Args:
-        graph: The graph to modify.
-        weights: A map from ONNX initializer name to tensor.
-        scales: A map from ONNX initializer name to desired scale factor for that initializer.
-        dq_only: Whether to only insert dq nodes.
-    """
-
-    def _insert_helper(
-        name: str,
-        wq: np.ndarray,
-        scale: np.ndarray,
-        dq_nodes: Dict[str, gs.Node],
-    ):
-        wq_tensor = make_gs_quantized_weight(name, wq, onnx.TensorProto.INT4)
-        scale_tensor = make_gs_scale(name, scale)
-        zp_tensor = make_gs_zp(name, scale.shape, onnx.TensorProto.INT4)
-        dq_out = make_gs_dequantize_output(name, shape=wq.shape, dtype=scale.dtype)
-        dq_node = make_gs_dequantize_node(
-            name, inputs=[wq_tensor, scale_tensor, zp_tensor], outputs=[dq_out]
-        )
-        dq_nodes[name] = dq_node
-
-    dq_nodes = {}
-    for name, scale in scales.items():
-        _insert_helper(name, quantized_weights[name], scale, dq_nodes)
-
-    _postprocess_qdq(
-        graph,
-        orig_weight_names=set(scales.keys()),
-        dq_nodes=dq_nodes,
-    )
-
-
-def insert_qdq_nodes(
-    graph: gs.Graph,
-    scales: Dict[str, np.ndarray],
-    weight_map: Dict[str, gs.Tensor],
-):
-    """Insert scales and QDQ nodes into graph.
-
-    Args:
-        graph: The graph to modify.
-        scales: A map from ONNX initializer name to desired scale factor for that initializer.
-        weight_map: A map from ONNX initializer name to graphsurgeon tensor.
-    """
-
-    def _insert_helper(
-        name: str,
-        weight_to_quantize: gs.Tensor,
-        scale: np.ndarray,
-        q_nodes: Dict[str, gs.Node],
-        dq_nodes: Dict[str, gs.Node],
-    ):
-        scale_tensor = make_gs_scale(name, scale)
-        zp_tensor = make_gs_zp(name, scale.shape, onnx.TensorProto.INT4)
-        q_out = make_gs_quantize_output(name, weight_to_quantize.shape, onnx.TensorProto.INT4)
-        q_node = make_gs_quantize_node(
-            name, inputs=[weight_to_quantize, scale_tensor, zp_tensor], outputs=[q_out]
-        )
-        dq_out = make_gs_dequantize_output(name, shape=weight_to_quantize.shape, dtype=scale.dtype)
-        dq_node = make_gs_dequantize_node(
-            name, inputs=[q_out, scale_tensor, zp_tensor], outputs=[dq_out]
-        )
-        q_nodes[name] = q_node
-        dq_nodes[name] = dq_node
-
-    q_nodes, dq_nodes = {}, {}
-    for name, scale in scales.items():
-        _insert_helper(name, weight_map[name], scale, q_nodes, dq_nodes)
-
-    _postprocess_qdq(
-        graph,
-        orig_weight_names=set(scales.keys()),
-        q_nodes=q_nodes,
-        dq_nodes=dq_nodes,
-    )
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Various utils to support inserting Q/DQ nodes."""
+
+from typing import Dict, Sequence, Set
+
+import numpy as np
+import onnx
+import onnx_graphsurgeon as gs
+
+QUANTIZE_NODE_NAME = "QuantizeLinear"
+DEQUANTIZE_NODE_NAME = "DequantizeLinear"
+
+
+def use_trt_qdq_ops():
+    """Globally set node names to TRT custom names."""
+    global QUANTIZE_NODE_NAME
+    QUANTIZE_NODE_NAME = "TRT_INT4QuantizeLinear"
+    global DEQUANTIZE_NODE_NAME
+    DEQUANTIZE_NODE_NAME = "TRT_INT4DequantizeLinear"
+
+
+def _wq_name(name: str):
+    return name + "_i4"
+
+
+def _scale_name(name: str):
+    return name + "_scale"
+
+
+def _zp_name(name: str):
+    return name + "_zp"
+
+
+def _q_name(name: str):
+    return name + "_QuantizeLinear"
+
+
+def _q_out_name(name: str):
+    return name + "_QuantizeLinear_Output"
+
+
+def _dq_name(name: str):
+    return name + "_DequantizeLinear"
+
+
+def _dq_out_name(name: str):
+    return name + "_DequantizeLinear_Output"
+
+
+def make_gs_quantized_weight(name: str, wq: np.ndarray, dtype) -> gs.Constant:
+    """Create a GraphSurgeon tensor from a quantized weight tensor.
+
+    `name` is the desired _basename_ of the tensor.
+    """
+    return gs.make_constant(_wq_name(name), np.asarray(wq), dtype)
+
+
+def make_gs_zp(name: str, shape: Sequence[int], dtype) -> gs.Constant:
+    """Create a GraphSurgeon zero-point tensor of all zeroes with the given shape.
+
+    `name` is the desired _basename_ of the tensor.
+    """
+    return gs.make_constant(
+        _zp_name(name),
+        np.zeros(shape, dtype=onnx.mapping.TENSOR_TYPE_MAP[int(dtype)].np_dtype),
+        dtype,
+    )
+
+
+def make_gs_scale(name: str, scale: np.ndarray) -> gs.Constant:
+    """Create a GraphSurgeon scale tensor from the given numpy array.
+
+    `name` is the desired _basename_ of the tensor.
+    """
+    return gs.Constant(_scale_name(name), np.asarray(scale))
+
+
+def make_gs_quantize_output(
+    name: str, shape: Sequence[int], dtype: onnx.TensorProto.DataType
+) -> gs.Variable:
+    """Create a GraphSurgeon variable representing the output of a quantize node.
+
+    `name` is the desired _basename_ of the node.
+    """
+    return gs.make_variable(_q_out_name(name), dtype=dtype, shape=shape)
+
+
+def make_gs_quantize_node(
+    name: str, inputs: Sequence[gs.Tensor], outputs: Sequence[gs.Tensor]
+) -> gs.Node:
+    """Create a GraphSurgeon Quantize node.
+
+    `name` is the desired _basename_ of the node.
+    """
+    return gs.Node(
+        QUANTIZE_NODE_NAME,
+        name=_q_name(name),
+        inputs=inputs,
+        outputs=outputs,
+    )
+
+
+def make_gs_dequantize_output(
+    name: str,
+    shape: Sequence[int],
+    dtype: np.dtype,
+) -> gs.Variable:
+    """Create a GraphSurgeon variable representing the output of a quantize node.
+
+    `name` is the desired _basename_ of the node.
+    """
+    return gs.Variable(_dq_out_name(name), dtype=dtype, shape=shape)
+
+
+def make_gs_dequantize_node(
+    name: str, inputs: Sequence[gs.Tensor], outputs: Sequence[gs.Tensor]
+) -> gs.Node:
+    """Create a GraphSurgeon Dequantize node.
+
+    `name` is the desired _basename_ of the node.
+    """
+    return gs.Node(
+        DEQUANTIZE_NODE_NAME,
+        name=_dq_name(name),
+        inputs=inputs,
+        outputs=outputs,
+    )
+
+
+def _postprocess_qdq(
+    graph: gs.Graph,
+    orig_weight_names: Set[str],
+    q_nodes: Dict[str, gs.Node] = {},
+    dq_nodes: Dict[str, gs.Node] = {},
+):
+    # Inserts all newly created nodes to graph.
+    # Update all consumers of original initializers to point to the DQ nodes.
+    for node in graph.nodes:
+        for i in range(len(node.inputs)):
+            key = node.inputs[i].name
+            if key not in orig_weight_names:
+                continue
+            node.inputs[i] = dq_nodes[key].outputs[0]
+
+    # Insert new nodes.
+    graph.nodes.extend(q_nodes.values())
+    graph.nodes.extend(dq_nodes.values())
+
+    graph.cleanup()
+    graph.toposort()
+
+
+def insert_dq_nodes(
+    graph: gs.Graph, scales: Dict[str, np.ndarray], quantized_weights: Dict[str, np.ndarray]
+):
+    """Insert new initializers and DQ nodes into graph.
+
+    Args:
+        graph: The graph to modify.
+        weights: A map from ONNX initializer name to tensor.
+        scales: A map from ONNX initializer name to desired scale factor for that initializer.
+        dq_only: Whether to only insert dq nodes.
+    """
+
+    def _insert_helper(
+        name: str,
+        wq: np.ndarray,
+        scale: np.ndarray,
+        dq_nodes: Dict[str, gs.Node],
+    ):
+        wq_tensor = make_gs_quantized_weight(name, wq, onnx.TensorProto.INT4)
+        scale_tensor = make_gs_scale(name, scale)
+        zp_tensor = make_gs_zp(name, scale.shape, onnx.TensorProto.INT4)
+        dq_out = make_gs_dequantize_output(name, shape=wq.shape, dtype=scale.dtype)
+        dq_node = make_gs_dequantize_node(
+            name, inputs=[wq_tensor, scale_tensor, zp_tensor], outputs=[dq_out]
+        )
+        dq_nodes[name] = dq_node
+
+    dq_nodes = {}
+    for name, scale in scales.items():
+        _insert_helper(name, quantized_weights[name], scale, dq_nodes)
+
+    _postprocess_qdq(
+        graph,
+        orig_weight_names=set(scales.keys()),
+        dq_nodes=dq_nodes,
+    )
+
+
+def insert_qdq_nodes(
+    graph: gs.Graph,
+    scales: Dict[str, np.ndarray],
+    weight_map: Dict[str, gs.Tensor],
+):
+    """Insert scales and QDQ nodes into graph.
+
+    Args:
+        graph: The graph to modify.
+        scales: A map from ONNX initializer name to desired scale factor for that initializer.
+        weight_map: A map from ONNX initializer name to graphsurgeon tensor.
+    """
+
+    def _insert_helper(
+        name: str,
+        weight_to_quantize: gs.Tensor,
+        scale: np.ndarray,
+        q_nodes: Dict[str, gs.Node],
+        dq_nodes: Dict[str, gs.Node],
+    ):
+        scale_tensor = make_gs_scale(name, scale)
+        zp_tensor = make_gs_zp(name, scale.shape, onnx.TensorProto.INT4)
+        q_out = make_gs_quantize_output(name, weight_to_quantize.shape, onnx.TensorProto.INT4)
+        q_node = make_gs_quantize_node(
+            name, inputs=[weight_to_quantize, scale_tensor, zp_tensor], outputs=[q_out]
+        )
+        dq_out = make_gs_dequantize_output(name, shape=weight_to_quantize.shape, dtype=scale.dtype)
+        dq_node = make_gs_dequantize_node(
+            name, inputs=[q_out, scale_tensor, zp_tensor], outputs=[dq_out]
+        )
+        q_nodes[name] = q_node
+        dq_nodes[name] = dq_node
+
+    q_nodes, dq_nodes = {}, {}
+    for name, scale in scales.items():
+        _insert_helper(name, weight_map[name], scale, q_nodes, dq_nodes)
+
+    _postprocess_qdq(
+        graph,
+        orig_weight_names=set(scales.keys()),
+        q_nodes=q_nodes,
+        dq_nodes=dq_nodes,
+    )
```

## modelopt/onnx/quantization/quant_utils.py

 * *Ordering differences only*

```diff
@@ -1,57 +1,57 @@
-# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Provides some basic utilities that can be used in quantize() methods."""
-
-from typing import Sequence, Union
-
-import numpy as np
-
-INT4_MIN = -8
-INT4_MAX = 7
-UINT4_MIN = 0
-UINT4_MAX = 15
-
-
-def pack_float32_to_4bit_optimized(array: Union[np.ndarray, Sequence], signed: bool) -> np.ndarray:
-    """Convert an array of float32 value to a 4bit data-type and pack every two concecutive elements in a byte.
-
-    This is the optimized version of pack_float32_to_4bit() utility in ONNX helper file. The basic optimizations
-    done here mainly rely on moving some common code out of the per-element function calls or loops, thereby making
-    them per-input-array, instead of per-input-element. The remaining logic should largely remain as is.
-
-    Args:
-        array: array of float to convert and pack
-        signed: Whether the 4 bit variant is signed or unsigned
-
-    Returns:
-        Packed array with size `ceil(farray.size/2)` (single dimension).
-    """
-    if not isinstance(array, np.ndarray):
-        array = np.asarray(array, dtype=np.float32)
-
-    array_flat = array.ravel()
-    is_odd_volume = np.prod(array.shape) % 2 == 1
-    if is_odd_volume:
-        array_flat = np.append(array_flat, np.array([0]))
-
-    inp_arr_len = array_flat.size
-    dtype = np.int8 if signed else np.uint8
-    clip_low = INT4_MIN if signed else UINT4_MIN
-    clip_high = INT4_MAX if signed else UINT4_MAX
-    array_flat = np.clip(array_flat, clip_low, clip_high)
-    array_flat = np.rint(array_flat).astype(dtype)
-    assert len(array_flat) % 2 == 0, "array length must be even at this point"
-    assert len(array_flat) == inp_arr_len, "output-length must match the input-length"
-    output_list = []
-    for i in range(0, inp_arr_len, 2):
-        output_list.append((array_flat[i + 1] << 4) | (array_flat[i] & 0x0F))
-    arr = np.array(output_list)
-    return arr.astype(np.uint8)
+# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Provides some basic utilities that can be used in quantize() methods."""
+
+from typing import Sequence, Union
+
+import numpy as np
+
+INT4_MIN = -8
+INT4_MAX = 7
+UINT4_MIN = 0
+UINT4_MAX = 15
+
+
+def pack_float32_to_4bit_optimized(array: Union[np.ndarray, Sequence], signed: bool) -> np.ndarray:
+    """Convert an array of float32 value to a 4bit data-type and pack every two concecutive elements in a byte.
+
+    This is the optimized version of pack_float32_to_4bit() utility in ONNX helper file. The basic optimizations
+    done here mainly rely on moving some common code out of the per-element function calls or loops, thereby making
+    them per-input-array, instead of per-input-element. The remaining logic should largely remain as is.
+
+    Args:
+        array: array of float to convert and pack
+        signed: Whether the 4 bit variant is signed or unsigned
+
+    Returns:
+        Packed array with size `ceil(farray.size/2)` (single dimension).
+    """
+    if not isinstance(array, np.ndarray):
+        array = np.asarray(array, dtype=np.float32)
+
+    array_flat = array.ravel()
+    is_odd_volume = np.prod(array.shape) % 2 == 1
+    if is_odd_volume:
+        array_flat = np.append(array_flat, np.array([0]))
+
+    inp_arr_len = array_flat.size
+    dtype = np.int8 if signed else np.uint8
+    clip_low = INT4_MIN if signed else UINT4_MIN
+    clip_high = INT4_MAX if signed else UINT4_MAX
+    array_flat = np.clip(array_flat, clip_low, clip_high)
+    array_flat = np.rint(array_flat).astype(dtype)
+    assert len(array_flat) % 2 == 0, "array length must be even at this point"
+    assert len(array_flat) == inp_arr_len, "output-length must match the input-length"
+    output_list = []
+    for i in range(0, inp_arr_len, 2):
+        output_list.append((array_flat[i + 1] << 4) | (array_flat[i] & 0x0F))
+    arr = np.array(output_list)
+    return arr.astype(np.uint8)
```

## modelopt/onnx/quantization/quantize.py

```diff
@@ -1,491 +1,513 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Convert ONNX model without QDQ nodes + calib data into ONNX model with QDQ nodes.
-
-Typically quantizing linear operations like Conv, MatMul etc. gives most of the performance boost.
-But there are many other ops that are quantizable (aka low precision kernels available) and provides
-optimal performance with lower accuracy drop. The default op types that this ONNX ptq tool quantizes
-in different quantization modes are: INT8: ['Add', 'AveragePool', 'BatchNormalization', 'Clip',
-'Conv', 'ConvTranspose', 'Gemm', 'GlobalAveragePool', 'MatMul', 'MaxPool', 'Mul'], INT4: ['MatMul'],
-FP8: ['MatMul']. The tool inserts QDQ nodes following compiler friendly patterns and generates an
-explicit ONNX model.
-"""
-import logging
-import os
-import re
-import shutil
-import tempfile
-from typing import List, Tuple
-
-import onnx
-import onnx.onnx_cpp2py_export.checker as C  # noqa: N812
-import onnx_graphsurgeon as gs
-from onnx_graphsurgeon.ir.graph import Graph
-from onnx_graphsurgeon.ir.node import Node
-from onnxruntime.quantization import (
-    CalibrationMethod,
-    quantize_static,
-)
-from onnxruntime.quantization.operators.qdq_base_operator import QDQOperatorBase
-from onnxruntime.quantization.quant_utils import (
-    QuantType,
-)
-from onnxruntime.quantization.registry import QDQRegistry, QLinearOpsRegistry
-
-from modelopt.onnx.op_types import get_quantizable_op_types
-from modelopt.onnx.quantization.calib_utils import (
-    CalibrationDataProvider,
-    CalibrationDataType,
-    RandomDataProvider,
-)
-from modelopt.onnx.quantization.graph_utils import (
-    build_non_residual_input_map,
-    classify_partition_nodes,
-    filter_quantizable_kgen_heads,
-    print_stat,
-    remove_partial_input_qdq,
-)
-from modelopt.onnx.quantization.int4 import quantize_int4
-from modelopt.onnx.quantization.operators import QDQConvTranspose, QDQNormalization
-from modelopt.onnx.quantization.ort_patching import patch_ort_modules
-from modelopt.onnx.quantization.partitioning import (
-    find_fusible_partitions,
-    find_non_quantizable_partitions_from_patterns,
-    find_quantizable_nodes,
-    get_skiped_output_layers,
-)
-from modelopt.onnx.utils import (
-    duplicate_shared_linear_weights,
-    name_onnx_nodes,
-    save_onnx,
-)
-
-__all__ = ["quantize"]
-
-QUANT_MODES = [
-    "int8",  # INT8 quantization of both scales and activations.
-    "int4_rtn",  # INT4 weight-only quantization. Inserts Q and DQ nodes around each eligible weight tensor.
-    "int4_rtn_dq",  # INT4 weight-only quantization. Directly computes the INT4 weights, and only inserts DQ nodes.
-    "int4_rtn_trt",  # Same as `int4_rtn`, but exports TRT custom Q/DQ nodes instead.
-    "int4_rtn_trt_dq",  # Same as `int4_rtn_dq`, but exports TRT custom DQ nodes instead.
-    "int4_awq_clip",  # INT4 AWQ Clip. Inserts DQ nodes for each eligible weight tensor.
-    "int4_awq_clip_trt",  # Same as `int4_awq_clip`, but exports TRT custom DQ nodes instead.
-    "fp8",
-]
-
-# Set logging level to info
-logging.getLogger().setLevel(logging.INFO)
-
-
-def _load_and_preprocess(
-    onnx_path: str,
-    use_external_data_format: bool,
-    output_path: str,
-):
-    # Load the model and weights
-    onnx_model = onnx.load(onnx_path, load_external_data=use_external_data_format)
-
-    # Per-Channel support with QDQ format requires onnx opset version 13 or above
-    ai_onnx_domain = [
-        opset
-        for opset in onnx_model.opset_import
-        if not opset.domain or opset.domain in ["ai.onnx", "ai.onnx.contrib"]
-    ]
-    opset_version = ai_onnx_domain[0].version
-    logging.info(f"Model {onnx_path} with opset_version {opset_version} is loaded.")
-
-    intermediate_generated_files = []
-    output_dir = os.path.dirname(output_path)
-    model_name = os.path.splitext(os.path.basename(onnx_path))[0]
-
-    required_opset_version = 13
-    if opset_version < required_opset_version and opset_version != 1:
-        opset_version = required_opset_version
-        onnx_model = onnx.version_converter.convert_version(onnx_model, opset_version)
-        onnx_path = os.path.join(output_dir, f"{model_name}_opset{opset_version}.onnx")
-        save_onnx(onnx_model, onnx_path, use_external_data_format)
-        logging.info(f"Model is cloned to {onnx_path} with opset_version {opset_version}.")
-        intermediate_generated_files.append(onnx_path)
-
-    # Sometimes input onnx model does not contain the node names
-    # This tool depends on those names, so we assign names if needed
-    graph = onnx_model.graph
-    is_named = name_onnx_nodes(graph)
-    is_duplicated = duplicate_shared_linear_weights(graph)
-
-    if is_named or is_duplicated:
-        onnx_path = os.path.join(output_dir, f"{model_name}_named.onnx")
-        save_onnx(onnx_model, onnx_path, use_external_data_format)
-        logging.info(f"Model is cloned to {onnx_path} after naming the nodes.")
-        intermediate_generated_files.append(onnx_path)
-
-    return onnx_model, onnx_path, opset_version, intermediate_generated_files
-
-
-def _find_nodes_from_op_types_to_exclude(graph: Graph, op_types_to_exclude=None) -> List[str]:
-    nodes_to_exclude = []
-    if op_types_to_exclude:
-        nodes_to_exclude = [node.name for node in graph.nodes if node.op in op_types_to_exclude]
-    return nodes_to_exclude
-
-
-def _expand_node_names_from_patterns(graph: Graph, name_patterns: List[str]) -> List[str]:
-    matched_node_names = []
-    for pattern in name_patterns:
-        for node in graph.nodes:
-            if re.match(pattern, node.name):
-                matched_node_names.append(node.name)
-
-    return matched_node_names
-
-
-def _configure_ort(op_types: List[str], op_types_to_quantize: List[str]):
-    # Register some new QDQ operators on top of ORT
-    QDQRegistry["BatchNormalization"] = QDQNormalization
-    QDQRegistry["ConvTranspose"] = QDQConvTranspose
-    QDQRegistry["LRN"] = QDQNormalization  # Example: caffenet-12.onnx
-    QDQRegistry["HardSwish"] = (
-        QDQOperatorBase  # Example: mobilenet_v3_opset17, efficientvit_b3_opset17
-    )
-
-    # Patch ORT modules to fix bugs and support some edge cases
-    patch_ort_modules()
-
-    # Remove copy, reduction and activation ops from ORT QDQ registry
-    for op_type in [
-        "ArgMax",
-        "Concat",
-        "EmbedLayerNormalization",
-        "Gather",
-        "InstanceNormalization",
-        "LeakyRelu",
-        "Pad",
-        "Relu",
-        "Reshape",
-        "Resize",
-        "Sigmoid",
-        "Softmax",
-        "Split",
-        "Squeeze",
-        "Transpose",
-        "Unsqueeze",
-        "Where",
-    ]:
-        if op_type in QLinearOpsRegistry:
-            del QLinearOpsRegistry[op_type]
-        if op_type in QDQRegistry:
-            del QDQRegistry[op_type]
-
-    # Prepare TensorRT friendly quantization settings
-    trt_guided_options = {
-        "QuantizeBias": False,
-        "ActivationSymmetric": True,
-        "OpTypesToExcludeOutputQuantization": op_types,  # No output quantization
-        "AddQDQPairToWeight": True,  # Instead of quantizing the weights, add QDQ node
-        "QDQOpTypePerChannelSupportToAxis": {
-            "Conv": 0,
-            "ConvTranspose": 1,
-        },  # per_channel should be True
-        "DedicatedQDQPair": True,
-        "ForceQuantizeNoInputCheck": (
-            # By default, for some latent operators like MaxPool, Transpose, etc.,
-            # ORT does not quantize if their input is not quantized already.
-            True
-        ),
-    }
-
-    quantizable_op_types = get_quantizable_op_types(op_types_to_quantize)
-    return trt_guided_options, quantizable_op_types
-
-
-def _find_nodes_to_quantize(
-    graph: Graph,
-    quantizable_op_types: List[str],
-    verbose: bool,
-) -> Tuple[List[Node], List[Tuple[Node, Node, str]]]:
-    # Build a map of add nodes to their non-residual inputs, i.e. fusible with Conv group
-    logging.info("Building non-residual Add input map ...")
-    non_residual_inputs = build_non_residual_input_map(graph)
-
-    logging.info(
-        "Searching for hard-coded patterns like MHA, LayerNorm, etc. to avoid quantization."
-    )
-    non_quantizable_hard_coded_partitions = find_non_quantizable_partitions_from_patterns(graph)
-
-    logging.info("Building KGEN/CASK targeted partitions ...")
-    # partitioned_nodes keeps track of nodes that are already part of some partition.
-    # Certain nodes of those partitions are quantizable. For example, heads.
-    partitioned_nodes = set(sum(non_quantizable_hard_coded_partitions, []))
-    cask_fusible_partitions, kgen_partitions = find_fusible_partitions(
-        graph,
-        partitioned_nodes,
-        non_residual_inputs,
-    )
-    if verbose:
-        logging.info(
-            "CASK fusible partitions:"
-            f" {[[node.name for node in partition] for partition in cask_fusible_partitions]}"
-        )
-        logging.info(
-            "KGEN partitions:"
-            f" {[[node.name for node in partition] for partition in kgen_partitions]}"
-        )
-
-    logging.info("Classifying the partition nodes ...")
-    _, quantizable_partition_nodes, no_quantize_inputs = classify_partition_nodes(
-        cask_fusible_partitions,
-    )
-    quantizable_kgen_heads, no_quantize_kgen_inputs = filter_quantizable_kgen_heads(
-        cask_fusible_partitions,
-        kgen_partitions,
-        quantizable_op_types,
-    )
-
-    quantizable_nodes = quantizable_kgen_heads + quantizable_partition_nodes
-    paritially_quantizable_nodes = [dst for _, dst, _ in no_quantize_inputs]
-
-    # Quantize all inputs of partially quantizable nodes by ORT
-    # but remove QDQ from non-quantizable inputs in the post-processing step
-    quantizable_nodes.extend(paritially_quantizable_nodes)
-
-    quantizable_nodes.extend(
-        find_quantizable_nodes(graph, quantizable_nodes, partitioned_nodes, quantizable_op_types)
-    )
-
-    skip_list = get_skiped_output_layers(graph, paritially_quantizable_nodes)
-    quantizable_nodes = [node for node in quantizable_nodes if node.name not in skip_list]
-
-    return quantizable_nodes, no_quantize_inputs + no_quantize_kgen_inputs
-
-
-def _find_nodes_to_exclude(
-    graph: Graph, nodes_to_exclude: List[str], op_types_to_exclude: List[str]
-):
-    nodes_to_exclude = nodes_to_exclude or []
-    nodes_to_exclude = _expand_node_names_from_patterns(graph, nodes_to_exclude)
-    nodes_to_exclude.extend(_find_nodes_from_op_types_to_exclude(graph, op_types_to_exclude))
-
-    # Remove duplicates from the exclusion list
-    return [*set(nodes_to_exclude)]
-
-
-def quantize(
-    onnx_path: str,
-    calibration_data: CalibrationDataType = None,
-    calibration_method: str = "entropy",
-    op_types_to_quantize: List[str] = None,
-    op_types_to_exclude: List[str] = None,
-    nodes_to_quantize: List[str] = None,
-    nodes_to_exclude: List[str] = None,
-    use_external_data_format: bool = False,
-    keep_intermediate_files: bool = False,
-    output_path: str = None,
-    verbose: bool = False,
-    quantize_mode: str = "int8",
-) -> None:
-    """Quantize the given onnx model.
-
-    Args:
-        onnx_path:
-            Path to the input onnx model.
-        calibration_data:
-            Calibration data, either a numpy array or list/dict of numpy array.
-        calibration_method:
-            Calibration method. Options={entropy (default), minmax}.
-        op_types_to_quantize:
-            List of types of operators to quantize. When this list is not None, only the types in this list
-            are quantized. Example: ['Conv'] indicates that only ops of type 'Conv' should be quantized.
-            If this list is None (default), all supported operators are quantized.
-            This flag does not support regular expression.
-        op_types_to_exclude:
-            List of types of operators to exclude from quantization.
-            This flag does not support regular expression.
-        nodes_to_quantize:
-            List of node names to quantize. When this list is not None, only the nodes in this list
-            are quantized. Example: ['Conv__224', 'Conv__252'].
-            If this list is None (default), all supported nodes are quantized.
-            This flag does not support regular expression.
-        nodes_to_exclude:
-            List of nodes names to exclude. The nodes in this list will be excluded from quantization
-            when it is not None. This flag supports regular expression.
-        use_external_data_format:
-            If not None, this path will be used to store the weights of the quantized model.
-        keep_intermediate_files:
-            If False, only save the converted ONNX files for the user. Otherwise, keep all intermediate files
-             generated during the ONNX models' conversion/calibration.
-        output_path:
-            Output filename to save the converted ONNX model.
-            If None, save in the same directory as the original ONNX model with .quant suffix.
-        verbose:
-            Prints details of node partition, selection etc. throughout the quantization process.
-        quantize_mode:
-            Quantization mode. One of ['int8', 'int4_rtn', 'int4_rtn_dq', 'int4_rtn_trt', 'int4_rtn_trt_dq',
-            'int4_awq_clip', 'int4_awq_clip_trt', 'fp8']. 'int8' by default. Any INT4-based mode is GEMM weight-only
-            and FP8 mode is MatMul only quantization.
-
-    Returns:
-        None, write the quantized onnx model in the same directory with filename like "<model_name>.quant.onnx".
-    """
-    # quantize_static creates a shape-inferred copy at the input model's directory
-    # Needs to check if we have write permission to this directory
-    assert onnx_path.endswith(".onnx") or onnx_path.endswith(".pb")
-    if not os.access(os.path.dirname(os.path.abspath(onnx_path)), os.W_OK):
-        old_dir = os.path.dirname(os.path.abspath(onnx_path))
-        tmp_dir = tempfile.mkdtemp()
-        logging.info(f"Directory {old_dir} is not writable, store intermediate files in {tmp_dir}")
-        onnx_path = os.path.join(tmp_dir, os.path.basename(onnx_path))
-
-        # We assume that the model directory contains only model related weights and protobuf file
-        # Anything extra in the model directory will be copied unnecessarily
-        for file in os.listdir(old_dir):
-            old_file_path = os.path.join(old_dir, file)
-            new_file_path = os.path.join(tmp_dir, file)
-            if os.path.isfile(old_file_path):
-                shutil.copy(old_file_path, new_file_path)
-
-    model_name = os.path.splitext(os.path.basename(onnx_path))[0]
-    if not output_path:
-        output_dir = os.path.dirname(onnx_path)
-        output_path = os.path.join(output_dir, f"{model_name}.quant.onnx")
-        logging.info(f"No output path specified, save quantized model to {output_path}")
-
-    # We need to preprocess the model with naming, weight duplication etc.
-    onnx_model, onnx_path, opset_version, intermediate_generated_files = _load_and_preprocess(
-        onnx_path, use_external_data_format, output_path
-    )
-
-    # Use random scales if calibration data is not supplied
-    if calibration_data is None:
-        calibration_data_reader = RandomDataProvider(onnx_path)
-    else:
-        calibration_data_reader = CalibrationDataProvider(onnx_path, calibration_data)
-
-    if quantize_mode == "int8" or quantize_mode == "fp8":
-        # Take the onnx graph
-        graph = gs.import_onnx(onnx_model)
-        graph.toposort()
-
-        # Change the default configuration of ORT quantization
-        op_types_to_quantize = op_types_to_quantize or []
-        op_types = set([node.op for node in graph.nodes])
-        trt_guided_options, quantizable_op_types = _configure_ort(
-            list(op_types), op_types_to_quantize
-        )
-
-        if quantize_mode == "fp8":
-            # Quantizable op type is limited to Gemm (Matmul) and Conv for fp8
-            quantizable_op_types = op_types_to_quantize or ["Gemm", "MatMul", "Conv"]
-
-        logging.info(
-            "Quantizable op types in the model:"
-            f" {[t for t in quantizable_op_types if t in op_types]}"
-        )
-
-        no_quantize_inputs = []
-        if not nodes_to_quantize:
-            quantizable_nodes, no_quantize_inputs = _find_nodes_to_quantize(
-                graph,
-                quantizable_op_types,
-                verbose,
-            )
-            nodes_to_quantize = [node.name for node in quantizable_nodes]
-
-        if not op_types_to_quantize and not nodes_to_quantize:
-            logging.info("No node or node type is selected for quantization!")
-            return
-
-        # Collect node names to exclude from quantization
-        nodes_to_exclude = _find_nodes_to_exclude(graph, nodes_to_exclude, op_types_to_exclude)
-
-        logging.info(f"Total number of nodes: {len(graph.nodes)}")
-        logging.info(f"Manually skipped node count: {len(nodes_to_exclude)}")
-        if verbose:
-            logging.info(f"Manually skipped nodes: {nodes_to_exclude}")
-
-        if not op_types_to_quantize and not nodes_to_quantize:
-            logging.info("No node or node type is selected for quantization!")
-            return
-
-        if quantize_mode == "int8":
-            # Use ort api to quantize the onnx model
-            quantize_static(
-                onnx_path,
-                output_path,
-                calibration_data_reader,
-                op_types_to_quantize=op_types_to_quantize,
-                nodes_to_quantize=nodes_to_quantize,
-                nodes_to_exclude=nodes_to_exclude,
-                per_channel=(opset_version >= 13),
-                extra_options=trt_guided_options,
-                use_external_data_format=use_external_data_format,
-                calibrate_method=(
-                    CalibrationMethod.Entropy
-                    if calibration_method == "entropy"
-                    else CalibrationMethod.MinMax
-                ),
-            )
-
-            if use_external_data_format:
-                intermediate_generated_files.append(output_path + ".data")
-
-            # Post-processing of the onnx model after ort quantization
-            onnx_model = onnx.load(output_path)
-            graph = gs.import_onnx(onnx_model)
-
-            remove_partial_input_qdq(graph, no_quantize_inputs)
-            onnx_model = gs.export_onnx(graph)
-        else:  # quantize_mode == "fp8"
-            quantize_static(
-                onnx_path,
-                output_path,
-                calibration_data_reader,
-                op_types_to_quantize=op_types_to_quantize,
-                nodes_to_quantize=nodes_to_quantize,
-                nodes_to_exclude=nodes_to_exclude,
-                # Enabling this causes a model with both FP8 and INT8 nodes which TRT is not happy with
-                per_channel=False,
-                use_external_data_format=use_external_data_format,
-                # This is the only available calibrate method for fp8 now
-                calibrate_method=CalibrationMethod.Distribution,
-                extra_options=trt_guided_options,
-                activation_type=QuantType.QFLOAT8E4M3FN,
-                weight_type=QuantType.QFLOAT8E4M3FN,
-            )
-            # Load the quantized output model for printing stat etc.
-            onnx_model = onnx.load(output_path)
-    else:
-        onnx_model = quantize_int4(
-            quantize_mode,
-            onnx_model,
-            calibration_data_reader,
-            use_external_data_format,
-        )
-
-    # Collect and print stats of the quantized model
-    print_stat(gs.import_onnx(onnx_model), verbose)
-
-    # Check if intermediate files should be deleted
-    if not keep_intermediate_files:
-        for file in intermediate_generated_files:
-            os.remove(file)
-
-    # Save the modified model after post-processing
-    # Note. FP8 quantized model does not have any post-processing steps
-    if quantize_mode != "fp8":
-        save_onnx(onnx_model, output_path, use_external_data_format)
-    logging.info(f"Quantized onnx model is saved as {output_path}")
-
-    # Check if the quantized model is valid
-    try:
-        onnx.checker.check_model(output_path)
-    except C.ValidationError as e:
-        logging.warn("ONNX model checker failed, check your deployment status.")
-        logging.warn(e)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Convert ONNX model without QDQ nodes + calib data into ONNX model with QDQ nodes.
+
+Typically quantizing linear operations like Conv, MatMul etc. gives most of the performance boost.
+But there are many other ops that are quantizable (aka low precision kernels available) and provides
+optimal performance with lower accuracy drop. The default op types that this ONNX ptq tool quantizes
+in different quantization modes are: INT8: ['Add', 'AveragePool', 'BatchNormalization', 'Clip',
+'Conv', 'ConvTranspose', 'Gemm', 'GlobalAveragePool', 'MatMul', 'MaxPool', 'Mul'], INT4: ['MatMul'],
+FP8: ['MatMul']. The tool inserts QDQ nodes following compiler friendly patterns and generates an
+explicit ONNX model.
+"""
+import logging
+import os
+import re
+import shutil
+import tempfile
+from typing import List, Tuple
+
+import onnx
+import onnx.onnx_cpp2py_export.checker as C  # noqa: N812
+import onnx_graphsurgeon as gs
+from onnx_graphsurgeon.ir.graph import Graph
+from onnx_graphsurgeon.ir.node import Node
+from onnxruntime.quantization import (
+    CalibrationMethod,
+    quantize_static,
+)
+from onnxruntime.quantization.operators.qdq_base_operator import QDQOperatorBase
+from onnxruntime.quantization.quant_utils import (
+    QuantType,
+)
+from onnxruntime.quantization.registry import QDQRegistry, QLinearOpsRegistry
+
+from modelopt.onnx.op_types import get_quantizable_op_types
+from modelopt.onnx.quantization.calib_utils import (
+    CalibrationDataProvider,
+    CalibrationDataType,
+    RandomDataProvider,
+)
+from modelopt.onnx.quantization.graph_utils import (
+    build_non_residual_input_map,
+    classify_partition_nodes,
+    filter_quantizable_kgen_heads,
+    print_stat,
+    remove_partial_input_qdq,
+)
+from modelopt.onnx.quantization.int4 import quantize_int4
+from modelopt.onnx.quantization.operators import QDQConvTranspose, QDQNormalization
+from modelopt.onnx.quantization.ort_patching import patch_ort_modules
+from modelopt.onnx.quantization.partitioning import (
+    find_fusible_partitions,
+    find_non_quantizable_partitions_from_patterns,
+    find_quantizable_nodes,
+    get_skiped_output_layers,
+)
+from modelopt.onnx.utils import (
+    duplicate_shared_linear_weights,
+    name_onnx_nodes,
+    save_onnx,
+)
+
+__all__ = ["quantize"]
+
+QUANT_MODES = [
+    "int8",  # INT8 quantization of both scales and activations.
+    "int4_rtn",  # INT4 weight-only quantization. Inserts Q and DQ nodes around each eligible weight tensor.
+    "int4_rtn_dq",  # INT4 weight-only quantization. Directly computes the INT4 weights, and only inserts DQ nodes.
+    "int4_rtn_trt",  # Same as `int4_rtn`, but exports TRT custom Q/DQ nodes instead.
+    "int4_rtn_trt_dq",  # Same as `int4_rtn_dq`, but exports TRT custom DQ nodes instead.
+    "int4_awq_clip",  # INT4 AWQ Clip. Inserts DQ nodes for each eligible weight tensor.
+    "int4_awq_clip_trt",  # Same as `int4_awq_clip`, but exports TRT custom DQ nodes instead.
+    "fp8",
+]
+
+# Set logging level to info
+logging.getLogger().setLevel(logging.INFO)
+
+
+def _load_and_preprocess(
+    onnx_path: str,
+    use_external_data_format: bool,
+    output_path: str,
+):
+    # Load the model and weights
+    onnx_model = onnx.load(onnx_path, load_external_data=use_external_data_format)
+
+    # Per-Channel support with QDQ format requires onnx opset version 13 or above
+    ai_onnx_domain = [
+        opset
+        for opset in onnx_model.opset_import
+        if not opset.domain or opset.domain in ["ai.onnx", "ai.onnx.contrib"]
+    ]
+    opset_version = ai_onnx_domain[0].version
+    logging.info(f"Model {onnx_path} with opset_version {opset_version} is loaded.")
+
+    intermediate_generated_files = []
+    output_dir = os.path.dirname(output_path)
+    model_name = os.path.splitext(os.path.basename(onnx_path))[0]
+
+    required_opset_version = 13
+    if opset_version < required_opset_version and opset_version != 1:
+        opset_version = required_opset_version
+        onnx_model = onnx.version_converter.convert_version(onnx_model, opset_version)
+        onnx_path = os.path.join(output_dir, f"{model_name}_opset{opset_version}.onnx")
+        save_onnx(onnx_model, onnx_path, use_external_data_format)
+        logging.info(f"Model is cloned to {onnx_path} with opset_version {opset_version}.")
+        intermediate_generated_files.append(onnx_path)
+
+    # Sometimes input onnx model does not contain the node names
+    # This tool depends on those names, so we assign names if needed
+    graph = onnx_model.graph
+    is_named = name_onnx_nodes(graph)
+    is_duplicated = duplicate_shared_linear_weights(graph)
+
+    if is_named or is_duplicated:
+        onnx_path = os.path.join(output_dir, f"{model_name}_named.onnx")
+        save_onnx(onnx_model, onnx_path, use_external_data_format)
+        logging.info(f"Model is cloned to {onnx_path} after naming the nodes.")
+        intermediate_generated_files.append(onnx_path)
+
+    return onnx_model, onnx_path, opset_version, intermediate_generated_files
+
+
+def _find_nodes_from_op_types_to_exclude(graph: Graph, op_types_to_exclude=None) -> List[str]:
+    nodes_to_exclude = []
+    if op_types_to_exclude:
+        nodes_to_exclude = [node.name for node in graph.nodes if node.op in op_types_to_exclude]
+    return nodes_to_exclude
+
+
+def _expand_node_names_from_patterns(graph: Graph, name_patterns: List[str]) -> List[str]:
+    matched_node_names = []
+    for pattern in name_patterns:
+        for node in graph.nodes:
+            if re.match(pattern, node.name):
+                matched_node_names.append(node.name)
+
+    return matched_node_names
+
+
+def _configure_ort(op_types: List[str], op_types_to_quantize: List[str]):
+    # Register some new QDQ operators on top of ORT
+    QDQRegistry["BatchNormalization"] = QDQNormalization
+    QDQRegistry["ConvTranspose"] = QDQConvTranspose
+    QDQRegistry["LRN"] = QDQNormalization  # Example: caffenet-12.onnx
+    QDQRegistry["HardSwish"] = (
+        QDQOperatorBase  # Example: mobilenet_v3_opset17, efficientvit_b3_opset17
+    )
+
+    # Patch ORT modules to fix bugs and support some edge cases
+    patch_ort_modules()
+
+    # Remove copy, reduction and activation ops from ORT QDQ registry
+    for op_type in [
+        "ArgMax",
+        "Concat",
+        "EmbedLayerNormalization",
+        "Gather",
+        "InstanceNormalization",
+        "LeakyRelu",
+        "Pad",
+        "Relu",
+        "Reshape",
+        "Resize",
+        "Sigmoid",
+        "Softmax",
+        "Split",
+        "Squeeze",
+        "Transpose",
+        "Unsqueeze",
+        "Where",
+    ]:
+        if op_type in QLinearOpsRegistry:
+            del QLinearOpsRegistry[op_type]
+        if op_type in QDQRegistry:
+            del QDQRegistry[op_type]
+
+    # Prepare TensorRT friendly quantization settings
+    trt_guided_options = {
+        "QuantizeBias": False,
+        "ActivationSymmetric": True,
+        "OpTypesToExcludeOutputQuantization": op_types,  # No output quantization
+        "AddQDQPairToWeight": True,  # Instead of quantizing the weights, add QDQ node
+        "QDQOpTypePerChannelSupportToAxis": {
+            "Conv": 0,
+            "ConvTranspose": 1,
+        },  # per_channel should be True
+        "DedicatedQDQPair": True,
+        "ForceQuantizeNoInputCheck": (
+            # By default, for some latent operators like MaxPool, Transpose, etc.,
+            # ORT does not quantize if their input is not quantized already.
+            True
+        ),
+    }
+
+    quantizable_op_types = get_quantizable_op_types(op_types_to_quantize)
+    return trt_guided_options, quantizable_op_types
+
+
+def _find_nodes_to_quantize(
+    graph: Graph,
+    quantizable_op_types: List[str],
+    verbose: bool,
+) -> Tuple[List[Node], List[Tuple[Node, Node, str]]]:
+    # Build a map of add nodes to their non-residual inputs, i.e. fusible with Conv group
+    logging.info("Building non-residual Add input map ...")
+    non_residual_inputs = build_non_residual_input_map(graph)
+
+    logging.info(
+        "Searching for hard-coded patterns like MHA, LayerNorm, etc. to avoid quantization."
+    )
+    non_quantizable_hard_coded_partitions = find_non_quantizable_partitions_from_patterns(graph)
+
+    logging.info("Building KGEN/CASK targeted partitions ...")
+    # partitioned_nodes keeps track of nodes that are already part of some partition.
+    # Certain nodes of those partitions are quantizable. For example, heads.
+    partitioned_nodes = set(sum(non_quantizable_hard_coded_partitions, []))
+    cask_fusible_partitions, kgen_partitions = find_fusible_partitions(
+        graph,
+        partitioned_nodes,
+        non_residual_inputs,
+    )
+    if verbose:
+        logging.info(
+            "CASK fusible partitions:"
+            f" {[[node.name for node in partition] for partition in cask_fusible_partitions]}"
+        )
+        logging.info(
+            "KGEN partitions:"
+            f" {[[node.name for node in partition] for partition in kgen_partitions]}"
+        )
+
+    logging.info("Classifying the partition nodes ...")
+    _, quantizable_partition_nodes, no_quantize_inputs = classify_partition_nodes(
+        cask_fusible_partitions,
+    )
+    quantizable_kgen_heads, no_quantize_kgen_inputs = filter_quantizable_kgen_heads(
+        cask_fusible_partitions,
+        kgen_partitions,
+        quantizable_op_types,
+    )
+
+    quantizable_nodes = quantizable_kgen_heads + quantizable_partition_nodes
+    paritially_quantizable_nodes = [dst for _, dst, _ in no_quantize_inputs]
+
+    # Quantize all inputs of partially quantizable nodes by ORT
+    # but remove QDQ from non-quantizable inputs in the post-processing step
+    quantizable_nodes.extend(paritially_quantizable_nodes)
+
+    quantizable_nodes.extend(
+        find_quantizable_nodes(graph, quantizable_nodes, partitioned_nodes, quantizable_op_types)
+    )
+
+    skip_list = get_skiped_output_layers(graph, paritially_quantizable_nodes)
+    quantizable_nodes = [node for node in quantizable_nodes if node.name not in skip_list]
+
+    return quantizable_nodes, no_quantize_inputs + no_quantize_kgen_inputs
+
+
+def _find_unpaddable_fp8_convs_to_exclude(graph: Graph):
+    # In CASK 5.8, the input and output channel alignment requirement for FP8
+    # conv kernels for input and output type FP8E4M3 are both 16.
+    # Check myelin/src/compiler/optimizer/cask_impl.cpp::collect_conv_constraints
+    # for detail.
+    unpaddable_conv_nodes = []
+    for node in graph.nodes:
+        if node.op == "Conv":
+            weight = node.inputs[1]
+            output_channel = weight.shape[0]
+            input_channel = weight.shape[1]
+            if output_channel % 16 != input_channel % 16:
+                logging.info(f"Found unpaddable conv for FP8: {node.name}")
+                unpaddable_conv_nodes.append(node.name)
+
+    return unpaddable_conv_nodes
+
+
+def _find_nodes_to_exclude(
+    graph: Graph, nodes_to_exclude: List[str], op_types_to_exclude: List[str], quantize_mode: str
+):
+    nodes_to_exclude = nodes_to_exclude or []
+    nodes_to_exclude = _expand_node_names_from_patterns(graph, nodes_to_exclude)
+    nodes_to_exclude.extend(_find_nodes_from_op_types_to_exclude(graph, op_types_to_exclude))
+    if quantize_mode == "fp8":
+        nodes_to_exclude.extend(_find_unpaddable_fp8_convs_to_exclude(graph))
+
+    # Remove duplicates from the exclusion list
+    return [*set(nodes_to_exclude)]
+
+
+def quantize(
+    onnx_path: str,
+    calibration_data: CalibrationDataType = None,
+    calibration_method: str = "entropy",
+    op_types_to_quantize: List[str] = None,
+    op_types_to_exclude: List[str] = None,
+    nodes_to_quantize: List[str] = None,
+    nodes_to_exclude: List[str] = None,
+    use_external_data_format: bool = False,
+    keep_intermediate_files: bool = False,
+    output_path: str = None,
+    verbose: bool = False,
+    quantize_mode: str = "int8",
+) -> None:
+    """Quantize the given onnx model.
+
+    Args:
+        onnx_path:
+            Path to the input onnx model.
+        calibration_data:
+            Calibration data, either a numpy array or list/dict of numpy array.
+        calibration_method:
+            Calibration method. Options={entropy (default), minmax}.
+        op_types_to_quantize:
+            List of types of operators to quantize. When this list is not None, only the types in this list
+            are quantized. Example: ['Conv'] indicates that only ops of type 'Conv' should be quantized.
+            If this list is None (default), all supported operators are quantized.
+            This flag does not support regular expression.
+        op_types_to_exclude:
+            List of types of operators to exclude from quantization.
+            This flag does not support regular expression.
+        nodes_to_quantize:
+            List of node names to quantize. When this list is not None, only the nodes in this list
+            are quantized. Example: ['Conv__224', 'Conv__252'].
+            If this list is None (default), all supported nodes are quantized.
+            This flag does not support regular expression.
+        nodes_to_exclude:
+            List of nodes names to exclude. The nodes in this list will be excluded from quantization
+            when it is not None. This flag supports regular expression.
+        use_external_data_format:
+            If not None, this path will be used to store the weights of the quantized model.
+        keep_intermediate_files:
+            If False, only save the converted ONNX files for the user. Otherwise, keep all intermediate files
+             generated during the ONNX models' conversion/calibration.
+        output_path:
+            Output filename to save the converted ONNX model.
+            If None, save in the same directory as the original ONNX model with .quant suffix.
+        verbose:
+            Prints details of node partition, selection etc. throughout the quantization process.
+        quantize_mode:
+            Quantization mode. One of ['int8', 'int4_rtn', 'int4_rtn_dq', 'int4_rtn_trt', 'int4_rtn_trt_dq',
+            'int4_awq_clip', 'int4_awq_clip_trt', 'fp8']. 'int8' by default. Any INT4-based mode is Gemm, MatMul
+            weight-only and FP8 mode is Conv, Gemm and MatMul only quantization.
+
+    Returns:
+        None, write the quantized onnx model in the same directory with filename like "<model_name>.quant.onnx".
+    """
+    # quantize_static creates a shape-inferred copy at the input model's directory
+    # Needs to check if we have write permission to this directory
+    assert onnx_path.endswith(".onnx") or onnx_path.endswith(".pb")
+    if not os.access(os.path.dirname(os.path.abspath(onnx_path)), os.W_OK):
+        old_dir = os.path.dirname(os.path.abspath(onnx_path))
+        tmp_dir = tempfile.mkdtemp()
+        logging.info(f"Directory {old_dir} is not writable, store intermediate files in {tmp_dir}")
+        onnx_path = os.path.join(tmp_dir, os.path.basename(onnx_path))
+
+        # We assume that the model directory contains only model related weights and protobuf file
+        # Anything extra in the model directory will be copied unnecessarily
+        for file in os.listdir(old_dir):
+            old_file_path = os.path.join(old_dir, file)
+            new_file_path = os.path.join(tmp_dir, file)
+            if os.path.isfile(old_file_path):
+                shutil.copy(old_file_path, new_file_path)
+
+    model_name = os.path.splitext(os.path.basename(onnx_path))[0]
+    if not output_path:
+        output_dir = os.path.dirname(onnx_path)
+        output_path = os.path.join(output_dir, f"{model_name}.quant.onnx")
+        logging.info(f"No output path specified, save quantized model to {output_path}")
+
+    # We need to preprocess the model with naming, weight duplication etc.
+    onnx_model, onnx_path, opset_version, intermediate_generated_files = _load_and_preprocess(
+        onnx_path, use_external_data_format, output_path
+    )
+
+    # Use random scales if calibration data is not supplied
+    if calibration_data is None:
+        calibration_data_reader = RandomDataProvider(onnx_path)
+    else:
+        calibration_data_reader = CalibrationDataProvider(onnx_path, calibration_data)
+
+    if quantize_mode == "int8" or quantize_mode == "fp8":
+        # Take the onnx graph
+        graph = gs.import_onnx(onnx_model)
+        graph.toposort()
+
+        # Change the default configuration of ORT quantization
+        op_types_to_quantize = op_types_to_quantize or []
+        op_types = set([node.op for node in graph.nodes])
+        trt_guided_options, quantizable_op_types = _configure_ort(
+            list(op_types), op_types_to_quantize
+        )
+
+        if quantize_mode == "fp8":
+            # Quantizable op type is limited to Gemm (Matmul) and Conv for fp8
+            quantizable_op_types = op_types_to_quantize or ["Gemm", "MatMul", "Conv"]
+
+        logging.info(
+            "Quantizable op types in the model:"
+            f" {[t for t in quantizable_op_types if t in op_types]}"
+        )
+
+        no_quantize_inputs = []
+        if not nodes_to_quantize:
+            quantizable_nodes, no_quantize_inputs = _find_nodes_to_quantize(
+                graph,
+                quantizable_op_types,
+                verbose,
+            )
+            nodes_to_quantize = [node.name for node in quantizable_nodes]
+
+        if not op_types_to_quantize and not nodes_to_quantize:
+            logging.info("No node or node type is selected for quantization!")
+            return
+
+        # Collect node names to exclude from quantization
+        nodes_to_exclude = _find_nodes_to_exclude(
+            graph, nodes_to_exclude, op_types_to_exclude, quantize_mode
+        )
+
+        logging.info(f"Total number of nodes: {len(graph.nodes)}")
+        logging.info(f"Skipped node count: {len(nodes_to_exclude)}")
+        if verbose:
+            logging.info(f"Skipped nodes: {nodes_to_exclude}")
+
+        if not op_types_to_quantize and not nodes_to_quantize:
+            logging.info("No node or node type is selected for quantization!")
+            return
+
+        if quantize_mode == "int8":
+            # Use ort api to quantize the onnx model
+            quantize_static(
+                onnx_path,
+                output_path,
+                calibration_data_reader,
+                op_types_to_quantize=op_types_to_quantize,
+                nodes_to_quantize=nodes_to_quantize,
+                nodes_to_exclude=nodes_to_exclude,
+                per_channel=(opset_version >= 13),
+                extra_options=trt_guided_options,
+                use_external_data_format=use_external_data_format,
+                calibrate_method=(
+                    CalibrationMethod.Entropy
+                    if calibration_method == "entropy"
+                    else CalibrationMethod.MinMax
+                ),
+            )
+
+            if use_external_data_format:
+                intermediate_generated_files.append(output_path + ".data")
+
+            # Post-processing of the onnx model after ort quantization
+            onnx_model = onnx.load(output_path)
+            graph = gs.import_onnx(onnx_model)
+
+            remove_partial_input_qdq(graph, no_quantize_inputs)
+            onnx_model = gs.export_onnx(graph)
+        else:  # quantize_mode == "fp8"
+            quantize_static(
+                onnx_path,
+                output_path,
+                calibration_data_reader,
+                op_types_to_quantize=op_types_to_quantize,
+                nodes_to_quantize=nodes_to_quantize,
+                nodes_to_exclude=nodes_to_exclude,
+                # Enabling this causes a model with both FP8 and INT8 nodes which TRT is not happy with
+                per_channel=False,
+                use_external_data_format=use_external_data_format,
+                # This is the only available calibrate method for fp8 now
+                calibrate_method=CalibrationMethod.Distribution,
+                extra_options=trt_guided_options,
+                activation_type=QuantType.QFLOAT8E4M3FN,
+                weight_type=QuantType.QFLOAT8E4M3FN,
+            )
+            # Load the quantized output model for printing stat etc.
+            onnx_model = onnx.load(output_path)
+    else:
+        onnx_model = quantize_int4(
+            quantize_mode,
+            onnx_model,
+            calibration_data_reader,
+            use_external_data_format,
+        )
+
+    # Collect and print stats of the quantized model
+    print_stat(gs.import_onnx(onnx_model), verbose)
+
+    # Check if intermediate files should be deleted
+    if not keep_intermediate_files:
+        for file in intermediate_generated_files:
+            os.remove(file)
+
+    # Save the modified model after post-processing
+    # Note. FP8 quantized model does not have any post-processing steps
+    if quantize_mode != "fp8":
+        save_onnx(onnx_model, output_path, use_external_data_format)
+    logging.info(f"Quantized onnx model is saved as {output_path}")
+
+    # Check if the quantized model is valid
+    try:
+        onnx.checker.check_model(output_path)
+    except C.ValidationError as e:
+        logging.warn("ONNX model checker failed, check your deployment status.")
+        logging.warn(e)
```

## modelopt/torch/__init__.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Model optimization and deployment subpackage for torch."""
-try:
-    from . import opt, quantization, sparsity, utils  # noqa: E402
-except ImportError as e:
-    raise ImportError("Please install optional ``[torch]`` dependencies.") from e
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Model optimization and deployment subpackage for torch."""
+try:
+    from . import opt, quantization, sparsity, utils  # noqa: E402
+except ImportError as e:
+    raise ImportError("Please install optional ``[torch]`` dependencies.") from e
```

## modelopt/torch/_deploy/__init__.py

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Interfaces to benchmark models and convert models into binaries for inference devices."""
-
-try:
-    from . import device_model, utils
-    from .compilation import *
-    from .profiling import *
-except ImportError as e:
-    raise ImportError("Please install optional ``[torch,onnx]`` dependencies.") from e
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Interfaces to benchmark models and convert models into binaries for inference devices."""
+
+try:
+    from . import device_model, utils
+    from .compilation import *
+    from .profiling import *
+except ImportError as e:
+    raise ImportError("Please install optional ``[torch,onnx]`` dependencies.") from e
```

## modelopt/torch/_deploy/compilation.py

 * *Ordering differences only*

```diff
@@ -1,117 +1,117 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Module to compile a model for a target device."""
-from typing import Any, Tuple, Union
-
-import torch.nn as nn
-
-from modelopt.torch.utils import is_channels_last
-
-from ._runtime import Deployment, RuntimeRegistry
-from .device_model import DeviceModel
-from .utils import OnnxBytes, get_onnx_bytes_and_metadata
-
-__all__ = ["compile"]
-
-
-def compile(
-    model: nn.Module, dummy_input: Union[Any, Tuple], deployment: Deployment
-) -> DeviceModel:
-    """Compile a given torch model into a device model according to the provided deployment.
-
-    Args:
-        model: PyTorch model to compile for target device.
-        dummy_input: Arguments of ``model.forward()``. This is used for exporting and calculating
-            inference-based metrics, such as latency/FLOPs. The format of ``dummy_inputs`` follows
-            the convention of the ``args`` argument in
-            `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_.
-            Specifically, ``dummy_input`` can be:
-
-            #. a single argument (``type(dummy_input) != tuple``) corresponding to
-
-               .. code-block:: python
-
-                    model.forward(dummy_input)
-
-            #. a tuple of arguments corresponding to
-
-               .. code-block:: python
-
-                    model.forward(*dummy_input)
-
-            #. a tuple of arguments such that ``type(dummy_input[-1]) == dict`` corresponding to
-
-               .. code-block:: python
-
-                    model.forward(*dummy_input[:-1], **dummy_input[-1])
-
-               .. warning::
-
-                   In this case the model's ``forward()`` method **cannot** contain keyword-only
-                   arguments (e.g. ``forward(..., *, kw_only_args)``) or variable keyword arguments
-                   (e.g. ``forward(..., **kwargs)``) since these cannot be sorted into positional
-                   arguments.
-
-            .. note::
-
-                In order to pass a dict as last non-keyword argument, you need to use a tuple as
-                ``dummy_input`` and add an *empty* dict as the last element, e.g.,
-
-                .. code-block:: python
-
-                    dummy_input = (x, {"y": y, "z": z}, {})
-
-                The empty dict at the end will then be interpreted as the keyword args.
-
-            See `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
-            for more info.
-
-            Note that if you provide a ``{arg_name}`` with batch size ``b``, the results will be
-            computed based on batch size ``b``.
-        deployment: Deployment configuration with keys as specified below.
-
-            * ``runtime``: the desired runtime for deployment (*required*);
-            * ``accelerator``: the accelerator on the device to be used (*optional*);
-            * ``version``: the version of runtime to be used (*optional*);
-            * ``precision``: the desired precision (*optional*);
-            * ``onnx_opset``: the opset version (*optional*).
-
-            An example of a deployment configuration is:
-
-            .. code-block:: python
-
-                deployment = {
-                    "runtime": "ORT",
-                    "accelerator": "CPU",
-                    "version": "1.11",
-                    "precision": "fp32",
-                    "onnx_opset": 13,
-                }
-
-    Returns:
-        An instance of DeviceModel.
-    """
-    # Add check for nhwc format before we formally support NHWC models
-    assert not is_channels_last(model), "Only NCHW models are supported!"
-
-    # Export onnx model and get some required names from it
-    onnx_bytes, metadata = get_onnx_bytes_and_metadata(model, dummy_input, "", True, **deployment)
-
-    client = RuntimeRegistry.get(deployment)
-
-    # For the ORTLocalClient, we need to pass the bytes of the onnx model instead of the OnnxBytes object
-    if client.__class__.__name__ == "ORTLocalClient":
-        onnx_bytes_obj = OnnxBytes.from_bytes(onnx_bytes)
-        onnx_bytes = onnx_bytes_obj.onnx_model[f"{onnx_bytes_obj.model_name}.onnx"]
-
-    compiled_model = client.ir_to_compiled(onnx_bytes)
-
-    return DeviceModel(client, compiled_model, metadata)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Module to compile a model for a target device."""
+from typing import Any, Tuple, Union
+
+import torch.nn as nn
+
+from modelopt.torch.utils import is_channels_last
+
+from ._runtime import Deployment, RuntimeRegistry
+from .device_model import DeviceModel
+from .utils import OnnxBytes, get_onnx_bytes_and_metadata
+
+__all__ = ["compile"]
+
+
+def compile(
+    model: nn.Module, dummy_input: Union[Any, Tuple], deployment: Deployment
+) -> DeviceModel:
+    """Compile a given torch model into a device model according to the provided deployment.
+
+    Args:
+        model: PyTorch model to compile for target device.
+        dummy_input: Arguments of ``model.forward()``. This is used for exporting and calculating
+            inference-based metrics, such as latency/FLOPs. The format of ``dummy_inputs`` follows
+            the convention of the ``args`` argument in
+            `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_.
+            Specifically, ``dummy_input`` can be:
+
+            #. a single argument (``type(dummy_input) != tuple``) corresponding to
+
+               .. code-block:: python
+
+                    model.forward(dummy_input)
+
+            #. a tuple of arguments corresponding to
+
+               .. code-block:: python
+
+                    model.forward(*dummy_input)
+
+            #. a tuple of arguments such that ``type(dummy_input[-1]) == dict`` corresponding to
+
+               .. code-block:: python
+
+                    model.forward(*dummy_input[:-1], **dummy_input[-1])
+
+               .. warning::
+
+                   In this case the model's ``forward()`` method **cannot** contain keyword-only
+                   arguments (e.g. ``forward(..., *, kw_only_args)``) or variable keyword arguments
+                   (e.g. ``forward(..., **kwargs)``) since these cannot be sorted into positional
+                   arguments.
+
+            .. note::
+
+                In order to pass a dict as last non-keyword argument, you need to use a tuple as
+                ``dummy_input`` and add an *empty* dict as the last element, e.g.,
+
+                .. code-block:: python
+
+                    dummy_input = (x, {"y": y, "z": z}, {})
+
+                The empty dict at the end will then be interpreted as the keyword args.
+
+            See `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
+            for more info.
+
+            Note that if you provide a ``{arg_name}`` with batch size ``b``, the results will be
+            computed based on batch size ``b``.
+        deployment: Deployment configuration with keys as specified below.
+
+            * ``runtime``: the desired runtime for deployment (*required*);
+            * ``accelerator``: the accelerator on the device to be used (*optional*);
+            * ``version``: the version of runtime to be used (*optional*);
+            * ``precision``: the desired precision (*optional*);
+            * ``onnx_opset``: the opset version (*optional*).
+
+            An example of a deployment configuration is:
+
+            .. code-block:: python
+
+                deployment = {
+                    "runtime": "ORT",
+                    "accelerator": "CPU",
+                    "version": "1.11",
+                    "precision": "fp32",
+                    "onnx_opset": 13,
+                }
+
+    Returns:
+        An instance of DeviceModel.
+    """
+    # Add check for nhwc format before we formally support NHWC models
+    assert not is_channels_last(model), "Only NCHW models are supported!"
+
+    # Export onnx model and get some required names from it
+    onnx_bytes, metadata = get_onnx_bytes_and_metadata(model, dummy_input, "", True, **deployment)
+
+    client = RuntimeRegistry.get(deployment)
+
+    # For the ORTLocalClient, we need to pass the bytes of the onnx model instead of the OnnxBytes object
+    if client.__class__.__name__ == "ORTLocalClient":
+        onnx_bytes_obj = OnnxBytes.from_bytes(onnx_bytes)
+        onnx_bytes = onnx_bytes_obj.onnx_model[f"{onnx_bytes_obj.model_name}.onnx"]
+
+    compiled_model = client.ir_to_compiled(onnx_bytes)
+
+    return DeviceModel(client, compiled_model, metadata)
```

## modelopt/torch/_deploy/device_model.py

 * *Ordering differences only*

```diff
@@ -1,128 +1,128 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Class representing the compiled model for a particular device."""
-import os
-from typing import Any, Tuple
-
-from modelopt.torch.utils import numpy_to_torch, torch_to_numpy, unflatten_tree
-
-from ._runtime import DetailedResults, RuntimeClient
-from .utils import ModelMetadata, generate_onnx_input
-
-
-class DeviceModel:
-    """On-device model with profiling functions and PyTorch-like inference interface.
-
-    This object should be generated from
-    :meth:`compile <modelopt.torch._deploy.compilation.compile>`.
-    """
-
-    def __init__(self, client: RuntimeClient, compiled_model: bytes, metadata: ModelMetadata):
-        """Initialize a device model with the corresponding model, onnx, and engine model.
-
-        Args:
-            client: the runtime client used to compile the model.
-            compiled_model: Compiled device model created during runtime compilation.
-            metadata: The model's metadata (needed for inference/profiling with compiled model).
-        """
-        self.client = client
-        self.compiled_model = compiled_model
-        self.model_metadata = metadata
-
-    def __call__(self, *args, **kwargs):
-        """Execute forward function of the model on the specified deployment and return output."""
-        return self.forward(*args, **kwargs)
-
-    def get_latency(self) -> float:
-        """Profiling API to let user get model latency with the compiled device model.
-
-        Returns:
-            The latency of the compiled model in ms.
-        """
-        latency, _ = self._profile_device()
-        return latency
-
-    def profile(self, verbose: bool = False) -> Tuple[float, DetailedResults]:
-        """Inference API to let user do inference with the compiled device model.
-
-        Args:
-            verbose: If True, print out the profiling results as a table.
-
-        Returns: A tuple (latency, detailed_result) where
-            ``latency`` is the latency of the compiled model in ms,
-            ``detailed_result`` is a dictionary containing additional benchmarking results
-        """
-        latency, detailed_result = self._profile_device()
-
-        if verbose:
-            print(detailed_result)
-
-        return latency, detailed_result
-
-    def forward(self, *args, **kwargs) -> Any:
-        """Execute forward of the model on the specified deployment and return output.
-
-        Arguments:
-            args: Non-keyword arguments to the model for inference.
-            kwargs: Keyword arguments to the model for inference.
-
-        Returns:
-            The inference result in the same (nested) data structure as the original model.
-
-        .. note::
-
-            This API let the users do inference with the compiled device model.
-
-        .. warning::
-
-            All return values will be of type ``torch.Tensor`` even if the original model returned
-            native python types such as bool/int/float.
-        """
-        if self.compiled_model is None:
-            raise AttributeError("Please compile the model first.")
-
-        # Flatten all args, kwargs into a single list of tensors for onnx/device inference.
-        all_args = args + (kwargs,) if kwargs or (args and isinstance(args[-1], dict)) else args
-
-        # If Model metadata is None then DeviceModel is instantiated with raw ONNX bytes instead of PyTorch module.
-        onnx_inputs = all_args[0]
-        if self.model_metadata:
-            onnx_inputs = list(generate_onnx_input(self.model_metadata, all_args).values())
-
-        # run inference with the engine equivalent of the model
-        np_inputs = torch_to_numpy(onnx_inputs)
-        np_outputs = self.client.inference(compiled_model=self.compiled_model, inputs=np_inputs)
-        onnx_outputs = numpy_to_torch(np_outputs)
-
-        # Note that bool/float/ints will be returned as corresponding tensors
-        # TODO: maybe eventually we want to compare this against the original types
-        # generate expected returned data structure of the model
-        if not self.model_metadata:
-            return onnx_outputs
-        return unflatten_tree(onnx_outputs, self.model_metadata["output_tree_spec"])
-
-    def save_compile_model(self, path: str, remove_hash: bool = False):
-        """Saves the compiled model to a file.
-
-        Args:
-            path: The path to save the compiled model.
-            remove_hash: If True, the hash will be removed from the saved model.
-        """
-        compiled_model = self.compiled_model
-        if remove_hash:
-            compiled_model = compiled_model[32:]
-        os.makedirs(os.path.dirname(path), exist_ok=True)
-        with open(path, "wb") as f:
-            f.write(compiled_model)
-
-    def _profile_device(self) -> Tuple[float, DetailedResults]:
-        """Profile the device model stored in self and return latency results."""
-        return self.client.profile(compiled_model=self.compiled_model)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Class representing the compiled model for a particular device."""
+import os
+from typing import Any, Tuple
+
+from modelopt.torch.utils import numpy_to_torch, torch_to_numpy, unflatten_tree
+
+from ._runtime import DetailedResults, RuntimeClient
+from .utils import ModelMetadata, generate_onnx_input
+
+
+class DeviceModel:
+    """On-device model with profiling functions and PyTorch-like inference interface.
+
+    This object should be generated from
+    :meth:`compile <modelopt.torch._deploy.compilation.compile>`.
+    """
+
+    def __init__(self, client: RuntimeClient, compiled_model: bytes, metadata: ModelMetadata):
+        """Initialize a device model with the corresponding model, onnx, and engine model.
+
+        Args:
+            client: the runtime client used to compile the model.
+            compiled_model: Compiled device model created during runtime compilation.
+            metadata: The model's metadata (needed for inference/profiling with compiled model).
+        """
+        self.client = client
+        self.compiled_model = compiled_model
+        self.model_metadata = metadata
+
+    def __call__(self, *args, **kwargs):
+        """Execute forward function of the model on the specified deployment and return output."""
+        return self.forward(*args, **kwargs)
+
+    def get_latency(self) -> float:
+        """Profiling API to let user get model latency with the compiled device model.
+
+        Returns:
+            The latency of the compiled model in ms.
+        """
+        latency, _ = self._profile_device()
+        return latency
+
+    def profile(self, verbose: bool = False) -> Tuple[float, DetailedResults]:
+        """Inference API to let user do inference with the compiled device model.
+
+        Args:
+            verbose: If True, print out the profiling results as a table.
+
+        Returns: A tuple (latency, detailed_result) where
+            ``latency`` is the latency of the compiled model in ms,
+            ``detailed_result`` is a dictionary containing additional benchmarking results
+        """
+        latency, detailed_result = self._profile_device()
+
+        if verbose:
+            print(detailed_result)
+
+        return latency, detailed_result
+
+    def forward(self, *args, **kwargs) -> Any:
+        """Execute forward of the model on the specified deployment and return output.
+
+        Arguments:
+            args: Non-keyword arguments to the model for inference.
+            kwargs: Keyword arguments to the model for inference.
+
+        Returns:
+            The inference result in the same (nested) data structure as the original model.
+
+        .. note::
+
+            This API let the users do inference with the compiled device model.
+
+        .. warning::
+
+            All return values will be of type ``torch.Tensor`` even if the original model returned
+            native python types such as bool/int/float.
+        """
+        if self.compiled_model is None:
+            raise AttributeError("Please compile the model first.")
+
+        # Flatten all args, kwargs into a single list of tensors for onnx/device inference.
+        all_args = args + (kwargs,) if kwargs or (args and isinstance(args[-1], dict)) else args
+
+        # If Model metadata is None then DeviceModel is instantiated with raw ONNX bytes instead of PyTorch module.
+        onnx_inputs = all_args[0]
+        if self.model_metadata:
+            onnx_inputs = list(generate_onnx_input(self.model_metadata, all_args).values())
+
+        # run inference with the engine equivalent of the model
+        np_inputs = torch_to_numpy(onnx_inputs)
+        np_outputs = self.client.inference(compiled_model=self.compiled_model, inputs=np_inputs)
+        onnx_outputs = numpy_to_torch(np_outputs)
+
+        # Note that bool/float/ints will be returned as corresponding tensors
+        # TODO: maybe eventually we want to compare this against the original types
+        # generate expected returned data structure of the model
+        if not self.model_metadata:
+            return onnx_outputs
+        return unflatten_tree(onnx_outputs, self.model_metadata["output_tree_spec"])
+
+    def save_compile_model(self, path: str, remove_hash: bool = False):
+        """Saves the compiled model to a file.
+
+        Args:
+            path: The path to save the compiled model.
+            remove_hash: If True, the hash will be removed from the saved model.
+        """
+        compiled_model = self.compiled_model
+        if remove_hash:
+            compiled_model = compiled_model[32:]
+        os.makedirs(os.path.dirname(path), exist_ok=True)
+        with open(path, "wb") as f:
+            f.write(compiled_model)
+
+    def _profile_device(self) -> Tuple[float, DetailedResults]:
+        """Profile the device model stored in self and return latency results."""
+        return self.client.profile(compiled_model=self.compiled_model)
```

## modelopt/torch/_deploy/profiling.py

 * *Ordering differences only*

```diff
@@ -1,189 +1,189 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Module to profile a model on a target device."""
-from typing import Any, Tuple, Union
-
-import torch.nn as nn
-
-from ._runtime import Deployment, DetailedResults
-from .compilation import compile
-
-__all__ = ["get_latency", "profile"]
-
-
-def get_latency(
-    model: nn.Module,
-    dummy_input: Union[Any, Tuple],
-    deployment: Deployment,
-) -> float:
-    """Obtain deployment latency of model by compiling and sending model to engine for profiling.
-
-    Args:
-        model: PyTorch model to compile for target device.
-        dummy_input: Arguments of ``model.forward()``. This is used for exporting and calculating
-            inference-based metrics, such as latency/FLOPs. The format of ``dummy_inputs`` follows
-            the convention of the ``args`` argument in
-            `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_.
-            Specifically, ``dummy_input`` can be:
-
-            #. a single argument (``type(dummy_input) != tuple``) corresponding to
-
-               .. code-block:: python
-
-                    model.forward(dummy_input)
-
-            #. a tuple of arguments corresponding to
-
-               .. code-block:: python
-
-                    model.forward(*dummy_input)
-
-            #. a tuple of arguments such that ``type(dummy_input[-1]) == dict`` corresponding to
-
-               .. code-block:: python
-
-                    model.forward(*dummy_input[:-1], **dummy_input[-1])
-
-               .. warning::
-
-                   In this case the model's ``forward()`` method **cannot** contain keyword-only
-                   arguments (e.g. ``forward(..., *, kw_only_args)``) or variable keyword arguments
-                   (e.g. ``forward(..., **kwargs)``) since these cannot be sorted into positional
-                   arguments.
-
-            .. note::
-
-                In order to pass a dict as last non-keyword argument, you need to use a tuple as
-                ``dummy_input`` and add an *empty* dict as the last element, e.g.,
-
-                .. code-block:: python
-
-                    dummy_input = (x, {"y": y, "z": z}, {})
-
-                The empty dict at the end will then be interpreted as the keyword args.
-
-            See `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
-            for more info.
-
-            Note that if you provide a ``{arg_name}`` with batch size ``b``, the results will be
-            computed based on batch size ``b``.
-
-        deployment: Deployment configuration with keys as specified below.
-
-            * ``runtime``: the desired runtime for deployment (*required*);
-            * ``accelerator``: the accelerator on the device to be used (*optional*);
-            * ``version``: the version of runtime to be used (*optional*);
-            * ``precision``: the desired precision (*optional*);
-            * ``onnx_opset``: the opset version (*optional*).
-
-            An example of a deployment configuration is:
-
-            .. code-block:: python
-
-                deployment = {
-                    "runtime": "ORT",
-                    "accelerator": "CPU",
-                    "version": "1.11",
-                    "precision": "fp32",
-                    "onnx_opset": 13,
-                }
-
-    Returns:
-        The latency of the compiled model in ms.
-    """
-    device_model = compile(model, dummy_input, deployment)
-    return device_model.get_latency()
-
-
-def profile(
-    model: nn.Module,
-    dummy_input: Union[Any, Tuple],
-    deployment: Deployment,
-    verbose: bool = False,
-) -> Tuple[float, DetailedResults]:
-    """Model profile method to help user to profile their model on target device.
-
-    Args:
-        model: PyTorch model to compile for target device.
-        dummy_input: Arguments of ``model.forward()``. This is used for exporting and calculating
-            inference-based metrics, such as latency/FLOPs. The format of ``dummy_inputs`` follows
-            the convention of the ``args`` argument in
-            `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_.
-            Specifically, ``dummy_input`` can be:
-
-            #. a single argument (``type(dummy_input) != tuple``) corresponding to
-
-               .. code-block:: python
-
-                    model.forward(dummy_input)
-
-            #. a tuple of arguments corresponding to
-
-               .. code-block:: python
-
-                    model.forward(*dummy_input)
-
-            #. a tuple of arguments such that ``type(dummy_input[-1]) == dict`` corresponding to
-
-               .. code-block:: python
-
-                    model.forward(*dummy_input[:-1], **dummy_input[-1])
-
-               .. warning::
-
-                   In this case the model's ``forward()`` method **cannot** contain keyword-only
-                   arguments (e.g. ``forward(..., *, kw_only_args)``) or variable keyword arguments
-                   (e.g. ``forward(..., **kwargs)``) since these cannot be sorted into positional
-                   arguments.
-
-            .. note::
-
-                In order to pass a dict as last non-keyword argument, you need to use a tuple as
-                ``dummy_input`` and add an *empty* dict as the last element, e.g.,
-
-                .. code-block:: python
-
-                    dummy_input = (x, {"y": y, "z": z}, {})
-
-                The empty dict at the end will then be interpreted as the keyword args.
-
-            See `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
-            for more info.
-
-            Note that if you provide a ``{arg_name}`` with batch size ``b``, the results will be
-            computed based on batch size ``b``.
-        deployment: Deployment configuration with keys as specified below.
-
-            * ``runtime``: the desired runtime for deployment (*required*);
-            * ``accelerator``: the accelerator on the device to be used (*optional*);
-            * ``version``: the version of runtime to be used (*optional*);
-            * ``precision``: the desired precision (*optional*);
-            * ``onnx_opset``: the opset version (*optional*).
-
-            An example of a deployment configuration is:
-
-            .. code-block:: python
-
-                deployment = {
-                    "runtime": "ORT",
-                    "accelerator": "CPU",
-                    "version": "1.11",
-                    "precision": "fp32",
-                    "onnx_opset": 13,
-                }
-        verbose: If True, print out the profiling results as a table.
-
-        Returns: A tuple (latency, detailed_result) where
-            ``latency`` is the latency of the compiled model in ms,
-            ``detailed_result`` is a dictionary containing additional benchmarking results
-    """
-    device_model = compile(model, dummy_input, deployment)
-    return device_model.profile(verbose)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Module to profile a model on a target device."""
+from typing import Any, Tuple, Union
+
+import torch.nn as nn
+
+from ._runtime import Deployment, DetailedResults
+from .compilation import compile
+
+__all__ = ["get_latency", "profile"]
+
+
+def get_latency(
+    model: nn.Module,
+    dummy_input: Union[Any, Tuple],
+    deployment: Deployment,
+) -> float:
+    """Obtain deployment latency of model by compiling and sending model to engine for profiling.
+
+    Args:
+        model: PyTorch model to compile for target device.
+        dummy_input: Arguments of ``model.forward()``. This is used for exporting and calculating
+            inference-based metrics, such as latency/FLOPs. The format of ``dummy_inputs`` follows
+            the convention of the ``args`` argument in
+            `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_.
+            Specifically, ``dummy_input`` can be:
+
+            #. a single argument (``type(dummy_input) != tuple``) corresponding to
+
+               .. code-block:: python
+
+                    model.forward(dummy_input)
+
+            #. a tuple of arguments corresponding to
+
+               .. code-block:: python
+
+                    model.forward(*dummy_input)
+
+            #. a tuple of arguments such that ``type(dummy_input[-1]) == dict`` corresponding to
+
+               .. code-block:: python
+
+                    model.forward(*dummy_input[:-1], **dummy_input[-1])
+
+               .. warning::
+
+                   In this case the model's ``forward()`` method **cannot** contain keyword-only
+                   arguments (e.g. ``forward(..., *, kw_only_args)``) or variable keyword arguments
+                   (e.g. ``forward(..., **kwargs)``) since these cannot be sorted into positional
+                   arguments.
+
+            .. note::
+
+                In order to pass a dict as last non-keyword argument, you need to use a tuple as
+                ``dummy_input`` and add an *empty* dict as the last element, e.g.,
+
+                .. code-block:: python
+
+                    dummy_input = (x, {"y": y, "z": z}, {})
+
+                The empty dict at the end will then be interpreted as the keyword args.
+
+            See `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
+            for more info.
+
+            Note that if you provide a ``{arg_name}`` with batch size ``b``, the results will be
+            computed based on batch size ``b``.
+
+        deployment: Deployment configuration with keys as specified below.
+
+            * ``runtime``: the desired runtime for deployment (*required*);
+            * ``accelerator``: the accelerator on the device to be used (*optional*);
+            * ``version``: the version of runtime to be used (*optional*);
+            * ``precision``: the desired precision (*optional*);
+            * ``onnx_opset``: the opset version (*optional*).
+
+            An example of a deployment configuration is:
+
+            .. code-block:: python
+
+                deployment = {
+                    "runtime": "ORT",
+                    "accelerator": "CPU",
+                    "version": "1.11",
+                    "precision": "fp32",
+                    "onnx_opset": 13,
+                }
+
+    Returns:
+        The latency of the compiled model in ms.
+    """
+    device_model = compile(model, dummy_input, deployment)
+    return device_model.get_latency()
+
+
+def profile(
+    model: nn.Module,
+    dummy_input: Union[Any, Tuple],
+    deployment: Deployment,
+    verbose: bool = False,
+) -> Tuple[float, DetailedResults]:
+    """Model profile method to help user to profile their model on target device.
+
+    Args:
+        model: PyTorch model to compile for target device.
+        dummy_input: Arguments of ``model.forward()``. This is used for exporting and calculating
+            inference-based metrics, such as latency/FLOPs. The format of ``dummy_inputs`` follows
+            the convention of the ``args`` argument in
+            `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_.
+            Specifically, ``dummy_input`` can be:
+
+            #. a single argument (``type(dummy_input) != tuple``) corresponding to
+
+               .. code-block:: python
+
+                    model.forward(dummy_input)
+
+            #. a tuple of arguments corresponding to
+
+               .. code-block:: python
+
+                    model.forward(*dummy_input)
+
+            #. a tuple of arguments such that ``type(dummy_input[-1]) == dict`` corresponding to
+
+               .. code-block:: python
+
+                    model.forward(*dummy_input[:-1], **dummy_input[-1])
+
+               .. warning::
+
+                   In this case the model's ``forward()`` method **cannot** contain keyword-only
+                   arguments (e.g. ``forward(..., *, kw_only_args)``) or variable keyword arguments
+                   (e.g. ``forward(..., **kwargs)``) since these cannot be sorted into positional
+                   arguments.
+
+            .. note::
+
+                In order to pass a dict as last non-keyword argument, you need to use a tuple as
+                ``dummy_input`` and add an *empty* dict as the last element, e.g.,
+
+                .. code-block:: python
+
+                    dummy_input = (x, {"y": y, "z": z}, {})
+
+                The empty dict at the end will then be interpreted as the keyword args.
+
+            See `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
+            for more info.
+
+            Note that if you provide a ``{arg_name}`` with batch size ``b``, the results will be
+            computed based on batch size ``b``.
+        deployment: Deployment configuration with keys as specified below.
+
+            * ``runtime``: the desired runtime for deployment (*required*);
+            * ``accelerator``: the accelerator on the device to be used (*optional*);
+            * ``version``: the version of runtime to be used (*optional*);
+            * ``precision``: the desired precision (*optional*);
+            * ``onnx_opset``: the opset version (*optional*).
+
+            An example of a deployment configuration is:
+
+            .. code-block:: python
+
+                deployment = {
+                    "runtime": "ORT",
+                    "accelerator": "CPU",
+                    "version": "1.11",
+                    "precision": "fp32",
+                    "onnx_opset": 13,
+                }
+        verbose: If True, print out the profiling results as a table.
+
+        Returns: A tuple (latency, detailed_result) where
+            ``latency`` is the latency of the compiled model in ms,
+            ``detailed_result`` is a dictionary containing additional benchmarking results
+    """
+    device_model = compile(model, dummy_input, deployment)
+    return device_model.profile(verbose)
```

## modelopt/torch/_deploy/_runtime/__init__.py

 * *Ordering differences only*

```diff
@@ -1,25 +1,25 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-from .registry import *
-from .runtime_client import *
-
-# no runtime_client_impl will be available if 'deploy' is not installed
-try:
-    from .ort_client import *
-except ImportError:
-    pass
-
-try:
-    from .trt_client import *
-except (ImportError, ModuleNotFoundError):
-    # ImportError if tensorrt is not installed
-    # ModuleNotFoundError if .tensorrt/ is not available
-    pass
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+from .registry import *
+from .runtime_client import *
+
+# no runtime_client_impl will be available if 'deploy' is not installed
+try:
+    from .ort_client import *
+except ImportError:
+    pass
+
+try:
+    from .trt_client import *
+except (ImportError, ModuleNotFoundError):
+    # ImportError if tensorrt is not installed
+    # ModuleNotFoundError if .tensorrt/ is not available
+    pass
```

## modelopt/torch/_deploy/_runtime/common.py

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-import logging
-import time
-from typing import Callable
-
-
-def timeit(method: Callable) -> Callable:
-    """This function is supposed to use as a decorator to measure the execution time of another function.
-
-    If the decorator is applied and no changes are done at the call site, this will print out the
-    timing information on the log console. If the call site wants to get the time info returned, they
-    should pass a dictionary named log_time like below-
-
-    (regular_returns, ...), func_exec_time = func(regular_params, ..., log_time={})
-    """
-
-    def timed(*args, **kw):
-        ts = time.time()
-        result = method(*args, **kw)
-        te = time.time()
-        if "log_time" in kw:
-            name = kw.get("log_name", method.__name__.upper())
-            kw["log_time"][name] = (te - ts) * 1000
-            return result, kw["log_time"][name]
-        else:
-            logging.info(f"{method.__name__}: {(te - ts) * 1000} ms")
-            return result
-
-    return timed
-
-
-def init_logging() -> None:
-    logging.basicConfig(
-        format="%(asctime)s P%(process)d T%(thread)d %(levelname)-8s %(message)s",
-        level=logging.INFO,
-        datefmt="%Y-%m-%d %H:%M:%S",
-    )
-
-
-def read_bytes(file_path: str) -> bytes:
-    with open(file_path, "rb") as file:
-        file_bytes = file.read()
-        return file_bytes
-
-
-def read_string(file_path: str) -> str:
-    with open(file_path, "r") as file:
-        data = file.read()
-        return data
-
-
-def write_bytes(data: bytes, file_path: str) -> None:
-    with open(file_path, "wb") as file:
-        file.write(data)
-
-
-def write_string(data: str, file_path: str) -> None:
-    with open(file_path, "w") as file:
-        file.write(data)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+import logging
+import time
+from typing import Callable
+
+
+def timeit(method: Callable) -> Callable:
+    """This function is supposed to use as a decorator to measure the execution time of another function.
+
+    If the decorator is applied and no changes are done at the call site, this will print out the
+    timing information on the log console. If the call site wants to get the time info returned, they
+    should pass a dictionary named log_time like below-
+
+    (regular_returns, ...), func_exec_time = func(regular_params, ..., log_time={})
+    """
+
+    def timed(*args, **kw):
+        ts = time.time()
+        result = method(*args, **kw)
+        te = time.time()
+        if "log_time" in kw:
+            name = kw.get("log_name", method.__name__.upper())
+            kw["log_time"][name] = (te - ts) * 1000
+            return result, kw["log_time"][name]
+        else:
+            logging.info(f"{method.__name__}: {(te - ts) * 1000} ms")
+            return result
+
+    return timed
+
+
+def init_logging() -> None:
+    logging.basicConfig(
+        format="%(asctime)s P%(process)d T%(thread)d %(levelname)-8s %(message)s",
+        level=logging.INFO,
+        datefmt="%Y-%m-%d %H:%M:%S",
+    )
+
+
+def read_bytes(file_path: str) -> bytes:
+    with open(file_path, "rb") as file:
+        file_bytes = file.read()
+        return file_bytes
+
+
+def read_string(file_path: str) -> str:
+    with open(file_path, "r") as file:
+        data = file.read()
+        return data
+
+
+def write_bytes(data: bytes, file_path: str) -> None:
+    with open(file_path, "wb") as file:
+        file.write(data)
+
+
+def write_string(data: str, file_path: str) -> None:
+    with open(file_path, "w") as file:
+        file.write(data)
```

## modelopt/torch/_deploy/_runtime/ort_client.py

 * *Ordering differences only*

```diff
@@ -1,176 +1,176 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-import json
-import os
-import tempfile
-import time
-from typing import Any, Dict, List, Optional, Tuple, Union
-
-import numpy as np
-import onnxruntime as ort
-
-from .registry import RuntimeRegistry
-from .runtime_client import Deployment, DeploymentTable, DetailedResults, RuntimeClient
-
-__all__ = ["ORTLocalClient"]
-
-
-@RuntimeRegistry.register("ORT")
-class ORTLocalClient(RuntimeClient):
-    """A client for using the local onnx runtime with CPU backend."""
-
-    @property
-    def _profile_defaults(self) -> Dict[str, Union[int, float]]:
-        """Default profiling parameters."""
-        return {
-            "iterations": 10,
-            "iterations_max": 1000,
-            "warm_up": 0.2,
-            "duration": 3.0,
-        }
-
-    @property
-    def _accelerator_to_provider(self) -> Dict[str, str]:
-        """Maps accelerator to ORT execution provider."""
-        return {
-            "CPU": "CPUExecutionProvider",
-        }
-
-    @property
-    def default_deployment(self) -> Deployment:
-        return {k: v[0] for k, v in self.deployment_table.items()}
-
-    @property
-    def deployment_table(self) -> DeploymentTable:
-        return {
-            "version": ["1.16"],
-            "accelerator": list(self._accelerator_to_provider.keys()),
-            "precision": ["fp32"],
-            "onnx_opset": ["13"],
-        }
-
-    def _ir_to_compiled(self, ir_bytes: bytes, compilation_args: Dict[str, Any] = None) -> bytes:
-        """Converts an ONNX model to a compiled device model."""
-        return ir_bytes  # ir_bytes (onnx) are also compiled model for ORT
-
-    def _onnx_to_np_dtype(self, onnx_type: str) -> np.dtype:
-        """Maps an ONNX data type to a numpy data type."""
-        return {
-            "tensor(float16)": np.float16,
-            "tensor(float)": np.float32,
-            "tensor(double)": np.float64,
-            "tensor(int8)": np.int8,
-            "tensor(int16)": np.int16,
-            "tensor(int32)": np.int32,
-            "tensor(int64)": np.int64,
-            "tensor(uint8)": np.uint8,
-            "tensor(uint16)": np.uint16,
-            "tensor(uint32)": np.uint32,
-            "tensor(uint64)": np.uint64,
-        }[onnx_type]
-
-    def _init_session(
-        self, compiled_model: bytes, session_options: Optional[ort.SessionOptions] = None
-    ) -> ort.InferenceSession:
-        """Initializes an inference session with the compiled model and returns the session."""
-        provider = self._accelerator_to_provider[self.deployment["accelerator"]]
-        return ort.InferenceSession(compiled_model, session_options, providers=[provider])
-
-    def _profile(self, compiled_model: bytes) -> Tuple[float, DetailedResults]:
-        """Profiles a compiled device model and returns the latency & detailed profiling results."""
-        # use a temp folder for profiling results
-        with tempfile.TemporaryDirectory() as temp_dir:
-            # initialize session + options
-            session_options = ort.SessionOptions()
-            session_options.enable_profiling = True
-            session_options.profile_file_prefix = os.path.join(temp_dir, "ort_profile")
-            ort_session = self._init_session(compiled_model, session_options)
-
-            # generate dummy inputs from ort_session.get_inputs()
-            inputs = [
-                np.asarray(np.random.rand(*x.shape)).astype(self._onnx_to_np_dtype(x.type))
-                for x in ort_session.get_inputs()
-            ]
-
-            # run session with dummy inputs
-            self._run_session(ort_session, inputs, **self._profile_defaults)  # type: ignore[arg-type]
-
-            # end profiling and load results
-            prof_file = ort_session.end_profiling()
-            with open(prof_file, "r") as p_file:
-                results = json.load(p_file)
-
-        # get latency from profiling results (latencies are in nano-seconds)
-        # We generally use milliseconds for latency, so divide by 1e3
-        latencies = [x["dur"] for x in results if x.get("name") == "model_run"]
-        avg_latency = np.mean(latencies) / 1e3
-
-        # return latency & detailed profiling results
-        return avg_latency, {"ort_results": results}
-
-    def _inference(self, compiled_model: bytes, inputs: List[np.ndarray]) -> List[np.ndarray]:
-        """Run inference with the compiled model and return the output as list of numpy arrays."""
-        # initialize session, run session, and return session outputs
-        ort_session = self._init_session(compiled_model)
-        return self._run_session(ort_session, inputs)
-
-    def _run_session(
-        self,
-        ort_session: ort.InferenceSession,
-        inputs: List[np.ndarray],
-        iterations: int = 1,
-        iterations_max: int = 1000,
-        warm_up: float = 0.0,
-        duration: float = 0.0,
-    ) -> List[np.ndarray]:
-        """Run inference with the compiled model and return the output as list of numpy arrays.
-
-        Args:
-            ort_session: The ONNX runtime inference session.
-            inputs: The input tensors to the session.
-            iterations: The minimum number of iterations to run the session for.
-            iterations_max: The maximum number of iterations to run the session for.
-            warm_up: The number of seconds to warm up the session for.
-            duration: The minimum number of seconds to run the session for.
-
-        Returns:
-            The outputs of the session as a list of numpy arrays.
-        """
-        assert iterations > 0, "Number of iterations must be positive!"
-
-        # basics of the session
-        ort_inputs = {ort_session.get_inputs()[i].name: inp for i, inp in enumerate(inputs)}
-        time_elapsed = 0.0
-
-        # small utility for running the session once with time measurement
-        def _run_session_once():
-            nonlocal time_elapsed
-            time_elapsed -= time.perf_counter()
-            ort_outputs = ort_session.run(None, ort_inputs)
-            time_elapsed += time.perf_counter()
-            return ort_outputs
-
-        # warm-up
-        while time_elapsed < warm_up:
-            _run_session_once()
-
-        # run inference
-        time_elapsed = 0.0
-        iter = 0
-        total_time = -time.time()
-        while (iter < iterations or time_elapsed < duration) and iter < iterations_max:
-            ort_outputs = _run_session_once()
-            iter += 1
-        total_time += time.time()
-        print(f"ORT: {iter} iterations in {total_time:.3f} seconds ({total_time/iter:.3f} s/iter)")
-
-        # return session outputs of final iteration
-        return ort_outputs
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+import json
+import os
+import tempfile
+import time
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+import numpy as np
+import onnxruntime as ort
+
+from .registry import RuntimeRegistry
+from .runtime_client import Deployment, DeploymentTable, DetailedResults, RuntimeClient
+
+__all__ = ["ORTLocalClient"]
+
+
+@RuntimeRegistry.register("ORT")
+class ORTLocalClient(RuntimeClient):
+    """A client for using the local onnx runtime with CPU backend."""
+
+    @property
+    def _profile_defaults(self) -> Dict[str, Union[int, float]]:
+        """Default profiling parameters."""
+        return {
+            "iterations": 10,
+            "iterations_max": 1000,
+            "warm_up": 0.2,
+            "duration": 3.0,
+        }
+
+    @property
+    def _accelerator_to_provider(self) -> Dict[str, str]:
+        """Maps accelerator to ORT execution provider."""
+        return {
+            "CPU": "CPUExecutionProvider",
+        }
+
+    @property
+    def default_deployment(self) -> Deployment:
+        return {k: v[0] for k, v in self.deployment_table.items()}
+
+    @property
+    def deployment_table(self) -> DeploymentTable:
+        return {
+            "version": ["1.16"],
+            "accelerator": list(self._accelerator_to_provider.keys()),
+            "precision": ["fp32"],
+            "onnx_opset": ["13"],
+        }
+
+    def _ir_to_compiled(self, ir_bytes: bytes, compilation_args: Dict[str, Any] = None) -> bytes:
+        """Converts an ONNX model to a compiled device model."""
+        return ir_bytes  # ir_bytes (onnx) are also compiled model for ORT
+
+    def _onnx_to_np_dtype(self, onnx_type: str) -> np.dtype:
+        """Maps an ONNX data type to a numpy data type."""
+        return {
+            "tensor(float16)": np.float16,
+            "tensor(float)": np.float32,
+            "tensor(double)": np.float64,
+            "tensor(int8)": np.int8,
+            "tensor(int16)": np.int16,
+            "tensor(int32)": np.int32,
+            "tensor(int64)": np.int64,
+            "tensor(uint8)": np.uint8,
+            "tensor(uint16)": np.uint16,
+            "tensor(uint32)": np.uint32,
+            "tensor(uint64)": np.uint64,
+        }[onnx_type]
+
+    def _init_session(
+        self, compiled_model: bytes, session_options: Optional[ort.SessionOptions] = None
+    ) -> ort.InferenceSession:
+        """Initializes an inference session with the compiled model and returns the session."""
+        provider = self._accelerator_to_provider[self.deployment["accelerator"]]
+        return ort.InferenceSession(compiled_model, session_options, providers=[provider])
+
+    def _profile(self, compiled_model: bytes) -> Tuple[float, DetailedResults]:
+        """Profiles a compiled device model and returns the latency & detailed profiling results."""
+        # use a temp folder for profiling results
+        with tempfile.TemporaryDirectory() as temp_dir:
+            # initialize session + options
+            session_options = ort.SessionOptions()
+            session_options.enable_profiling = True
+            session_options.profile_file_prefix = os.path.join(temp_dir, "ort_profile")
+            ort_session = self._init_session(compiled_model, session_options)
+
+            # generate dummy inputs from ort_session.get_inputs()
+            inputs = [
+                np.asarray(np.random.rand(*x.shape)).astype(self._onnx_to_np_dtype(x.type))
+                for x in ort_session.get_inputs()
+            ]
+
+            # run session with dummy inputs
+            self._run_session(ort_session, inputs, **self._profile_defaults)  # type: ignore[arg-type]
+
+            # end profiling and load results
+            prof_file = ort_session.end_profiling()
+            with open(prof_file, "r") as p_file:
+                results = json.load(p_file)
+
+        # get latency from profiling results (latencies are in nano-seconds)
+        # We generally use milliseconds for latency, so divide by 1e3
+        latencies = [x["dur"] for x in results if x.get("name") == "model_run"]
+        avg_latency = np.mean(latencies) / 1e3
+
+        # return latency & detailed profiling results
+        return avg_latency, {"ort_results": results}
+
+    def _inference(self, compiled_model: bytes, inputs: List[np.ndarray]) -> List[np.ndarray]:
+        """Run inference with the compiled model and return the output as list of numpy arrays."""
+        # initialize session, run session, and return session outputs
+        ort_session = self._init_session(compiled_model)
+        return self._run_session(ort_session, inputs)
+
+    def _run_session(
+        self,
+        ort_session: ort.InferenceSession,
+        inputs: List[np.ndarray],
+        iterations: int = 1,
+        iterations_max: int = 1000,
+        warm_up: float = 0.0,
+        duration: float = 0.0,
+    ) -> List[np.ndarray]:
+        """Run inference with the compiled model and return the output as list of numpy arrays.
+
+        Args:
+            ort_session: The ONNX runtime inference session.
+            inputs: The input tensors to the session.
+            iterations: The minimum number of iterations to run the session for.
+            iterations_max: The maximum number of iterations to run the session for.
+            warm_up: The number of seconds to warm up the session for.
+            duration: The minimum number of seconds to run the session for.
+
+        Returns:
+            The outputs of the session as a list of numpy arrays.
+        """
+        assert iterations > 0, "Number of iterations must be positive!"
+
+        # basics of the session
+        ort_inputs = {ort_session.get_inputs()[i].name: inp for i, inp in enumerate(inputs)}
+        time_elapsed = 0.0
+
+        # small utility for running the session once with time measurement
+        def _run_session_once():
+            nonlocal time_elapsed
+            time_elapsed -= time.perf_counter()
+            ort_outputs = ort_session.run(None, ort_inputs)
+            time_elapsed += time.perf_counter()
+            return ort_outputs
+
+        # warm-up
+        while time_elapsed < warm_up:
+            _run_session_once()
+
+        # run inference
+        time_elapsed = 0.0
+        iter = 0
+        total_time = -time.time()
+        while (iter < iterations or time_elapsed < duration) and iter < iterations_max:
+            ort_outputs = _run_session_once()
+            iter += 1
+        total_time += time.time()
+        print(f"ORT: {iter} iterations in {total_time:.3f} seconds ({total_time/iter:.3f} s/iter)")
+
+        # return session outputs of final iteration
+        return ort_outputs
```

## modelopt/torch/_deploy/_runtime/registry.py

 * *Ordering differences only*

```diff
@@ -1,60 +1,60 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-from typing import Callable, Dict, Type
-
-from .runtime_client import Deployment, RuntimeClient
-
-__all__ = ["RuntimeRegistry"]
-
-
-class RuntimeRegistry:
-    """Registry to store and retrieve various runtime client implementations."""
-
-    _runtime_client_lookup: Dict[str, Type[RuntimeClient]] = {}
-
-    @classmethod
-    def register(cls, runtime: str) -> Callable[[Type[RuntimeClient]], Type[RuntimeClient]]:
-        """A decorator to register a RuntimeClient with its relevant runtime.
-
-        For example:
-
-        .. code-block:: python
-
-            @RuntimeRegistry.register("my_runtime")
-            class MyRuntimeClient(RuntimeClient):
-                ...
-        """
-
-        def _register_runtime_client(new_type: Type[RuntimeClient]) -> Type[RuntimeClient]:
-            cls._runtime_client_lookup[runtime] = new_type
-            new_type._runtime = runtime
-            return new_type
-
-        return _register_runtime_client
-
-    @classmethod
-    def get(cls, deployment: Deployment) -> RuntimeClient:
-        """Get the runtime client for the given deployment.
-
-        Args:
-            deployment: Deployment configuration.
-
-        Returns:
-            The runtime client for the given deployment.
-        """
-        # check for valid runtime
-        if "runtime" not in deployment:
-            raise KeyError("Runtime was not set.")
-        if deployment["runtime"] not in cls._runtime_client_lookup:
-            raise ValueError(f"Runtime {deployment['runtime']} is not supported.")
-
-        # initialize runtime client
-        return cls._runtime_client_lookup[deployment["runtime"]](deployment)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+from typing import Callable, Dict, Type
+
+from .runtime_client import Deployment, RuntimeClient
+
+__all__ = ["RuntimeRegistry"]
+
+
+class RuntimeRegistry:
+    """Registry to store and retrieve various runtime client implementations."""
+
+    _runtime_client_lookup: Dict[str, Type[RuntimeClient]] = {}
+
+    @classmethod
+    def register(cls, runtime: str) -> Callable[[Type[RuntimeClient]], Type[RuntimeClient]]:
+        """A decorator to register a RuntimeClient with its relevant runtime.
+
+        For example:
+
+        .. code-block:: python
+
+            @RuntimeRegistry.register("my_runtime")
+            class MyRuntimeClient(RuntimeClient):
+                ...
+        """
+
+        def _register_runtime_client(new_type: Type[RuntimeClient]) -> Type[RuntimeClient]:
+            cls._runtime_client_lookup[runtime] = new_type
+            new_type._runtime = runtime
+            return new_type
+
+        return _register_runtime_client
+
+    @classmethod
+    def get(cls, deployment: Deployment) -> RuntimeClient:
+        """Get the runtime client for the given deployment.
+
+        Args:
+            deployment: Deployment configuration.
+
+        Returns:
+            The runtime client for the given deployment.
+        """
+        # check for valid runtime
+        if "runtime" not in deployment:
+            raise KeyError("Runtime was not set.")
+        if deployment["runtime"] not in cls._runtime_client_lookup:
+            raise ValueError(f"Runtime {deployment['runtime']} is not supported.")
+
+        # initialize runtime client
+        return cls._runtime_client_lookup[deployment["runtime"]](deployment)
```

## modelopt/torch/_deploy/_runtime/runtime_client.py

 * *Ordering differences only*

```diff
@@ -1,139 +1,139 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-from abc import ABC, abstractmethod
-from typing import Any, Dict, List, Tuple
-
-import numpy as np
-
-__all__ = ["Deployment", "DeploymentTable", "DetailedResults", "RuntimeClient"]
-
-Deployment = Dict[str, str]
-DeploymentTable = Dict[str, List[str]]
-DetailedResults = Dict[str, Any]
-
-
-class RuntimeClient(ABC):
-    """A abstract client class for implementing various runtimes to be used for deployment.
-
-    The RuntimeClient defines a common interfaces for accessing various runtimes within modelopt.
-    """
-
-    _runtime: str  # runtime of the client --> set by RuntimeRegistry.register
-
-    def __init__(self, deployment: Deployment):
-        super().__init__()
-        self.deployment = self.sanitize_deployment_config(deployment)
-
-    @property
-    def runtime(self) -> str:
-        return self._runtime
-
-    def sanitize_deployment_config(self, deployment: Deployment) -> Deployment:
-        """Cleans/checks the deployment config & fills in runtime-specific default values.
-
-        Args:
-            deployment: Deployment config with at least the ``runtime`` key specified.
-
-        Returns:
-            The sanitized deployment config with all runtime-specific default values filled
-            in for missing keys.
-        """
-        # check runtime
-        assert self.runtime == deployment["runtime"], "Runtime mismatch!"
-
-        # check that version was provided
-        if "version" not in deployment:
-            raise KeyError("Runtime version must be provided!")
-
-        # fill in default values and update
-        deployment = {**self.default_deployment, **deployment}
-
-        # sanity check on keys (inverse doesn't have to be checked since we fill in defaults)
-        table = self.deployment_table
-        extra_keys = deployment.keys() - table.keys() - {"runtime"}
-        assert not extra_keys, f"Invalid deployment config keys detected: {extra_keys}!"
-
-        # sanity checks on values
-        invalid_values = {(k, deployment[k]): t for k, t in table.items() if deployment[k] not in t}
-        assert not invalid_values, f"Invalid deployment config values detected: {invalid_values}!"
-
-        return deployment
-
-    def ir_to_compiled(self, ir_bytes: bytes, compilation_args: Dict[str, Any] = {}) -> bytes:
-        """Converts a model from its intermediate representation (IR) to a compiled device model.
-        Args:
-            ir_bytes: Intermediate representation (IR) of the model.
-            compilation_args: Additional arguments for the compilation process.
-
-        Returns: The compiled device model that can be used for further downstream tasks such as
-            on-device inference and profiling,
-        """
-        # run model compilation
-        compiled_model = self._ir_to_compiled(ir_bytes, compilation_args)
-        assert compiled_model, "Device conversion failed!"
-
-        return compiled_model
-
-    def profile(self, compiled_model: bytes) -> Tuple[float, DetailedResults]:
-        """Profiles a compiled device model and returns the latency & detailed profiling results.
-
-        Args:
-            compiled_model: Compiled device model from compilation service.
-
-        Returns: A tuple (latency, detailed_result) where
-            ``latency`` is the latency of the compiled model in ms,
-            ``detailed_result`` is a dictionary containing additional benchmarking results
-        """
-        # get latency & detailed results from client
-        latency, detailed_result = self._profile(compiled_model)
-        assert latency > 0.0, "Profiling failed!"
-
-        return latency, detailed_result
-
-    def inference(self, compiled_model: bytes, inputs: List[np.ndarray]) -> List[np.ndarray]:
-        """Run inference with the compiled model and return the output as list of numpy arrays.
-
-        Args:
-            compiled_model: Compiled device model from compilation service.
-            inputs: Inputs to do inference on server.
-
-        Returns:
-            A list of torch tensors from the inference outputs
-        """
-        # run inference
-        outputs = self._inference(compiled_model, inputs)
-        assert len(outputs) > 0, "Inference failed!"
-
-        return outputs
-
-    @property
-    @abstractmethod
-    def default_deployment(self) -> Deployment:
-        """Provides the default deployment config without the device key."""
-        raise NotImplementedError
-
-    @property
-    @abstractmethod
-    def deployment_table(self) -> DeploymentTable:
-        """Provides a set of supported values for each deployment config key."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def _ir_to_compiled(self, ir_bytes: bytes, compilation_args: Dict[str, Any] = {}) -> bytes:
-        """Converts a model from its intermediate representation (IR) to a compiled device model."""
-
-    @abstractmethod
-    def _profile(self, compiled_model: bytes) -> Tuple[float, DetailedResults]:
-        """Profiles a compiled device model and returns the latency & detailed profiling results."""
-
-    @abstractmethod
-    def _inference(self, compiled_model: bytes, inputs: List[np.ndarray]) -> List[np.ndarray]:
-        """Run inference with the compiled model and return the output as list of numpy arrays."""
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+from abc import ABC, abstractmethod
+from typing import Any, Dict, List, Tuple
+
+import numpy as np
+
+__all__ = ["Deployment", "DeploymentTable", "DetailedResults", "RuntimeClient"]
+
+Deployment = Dict[str, str]
+DeploymentTable = Dict[str, List[str]]
+DetailedResults = Dict[str, Any]
+
+
+class RuntimeClient(ABC):
+    """A abstract client class for implementing various runtimes to be used for deployment.
+
+    The RuntimeClient defines a common interfaces for accessing various runtimes within modelopt.
+    """
+
+    _runtime: str  # runtime of the client --> set by RuntimeRegistry.register
+
+    def __init__(self, deployment: Deployment):
+        super().__init__()
+        self.deployment = self.sanitize_deployment_config(deployment)
+
+    @property
+    def runtime(self) -> str:
+        return self._runtime
+
+    def sanitize_deployment_config(self, deployment: Deployment) -> Deployment:
+        """Cleans/checks the deployment config & fills in runtime-specific default values.
+
+        Args:
+            deployment: Deployment config with at least the ``runtime`` key specified.
+
+        Returns:
+            The sanitized deployment config with all runtime-specific default values filled
+            in for missing keys.
+        """
+        # check runtime
+        assert self.runtime == deployment["runtime"], "Runtime mismatch!"
+
+        # check that version was provided
+        if "version" not in deployment:
+            raise KeyError("Runtime version must be provided!")
+
+        # fill in default values and update
+        deployment = {**self.default_deployment, **deployment}
+
+        # sanity check on keys (inverse doesn't have to be checked since we fill in defaults)
+        table = self.deployment_table
+        extra_keys = deployment.keys() - table.keys() - {"runtime"}
+        assert not extra_keys, f"Invalid deployment config keys detected: {extra_keys}!"
+
+        # sanity checks on values
+        invalid_values = {(k, deployment[k]): t for k, t in table.items() if deployment[k] not in t}
+        assert not invalid_values, f"Invalid deployment config values detected: {invalid_values}!"
+
+        return deployment
+
+    def ir_to_compiled(self, ir_bytes: bytes, compilation_args: Dict[str, Any] = {}) -> bytes:
+        """Converts a model from its intermediate representation (IR) to a compiled device model.
+        Args:
+            ir_bytes: Intermediate representation (IR) of the model.
+            compilation_args: Additional arguments for the compilation process.
+
+        Returns: The compiled device model that can be used for further downstream tasks such as
+            on-device inference and profiling,
+        """
+        # run model compilation
+        compiled_model = self._ir_to_compiled(ir_bytes, compilation_args)
+        assert compiled_model, "Device conversion failed!"
+
+        return compiled_model
+
+    def profile(self, compiled_model: bytes) -> Tuple[float, DetailedResults]:
+        """Profiles a compiled device model and returns the latency & detailed profiling results.
+
+        Args:
+            compiled_model: Compiled device model from compilation service.
+
+        Returns: A tuple (latency, detailed_result) where
+            ``latency`` is the latency of the compiled model in ms,
+            ``detailed_result`` is a dictionary containing additional benchmarking results
+        """
+        # get latency & detailed results from client
+        latency, detailed_result = self._profile(compiled_model)
+        assert latency > 0.0, "Profiling failed!"
+
+        return latency, detailed_result
+
+    def inference(self, compiled_model: bytes, inputs: List[np.ndarray]) -> List[np.ndarray]:
+        """Run inference with the compiled model and return the output as list of numpy arrays.
+
+        Args:
+            compiled_model: Compiled device model from compilation service.
+            inputs: Inputs to do inference on server.
+
+        Returns:
+            A list of torch tensors from the inference outputs
+        """
+        # run inference
+        outputs = self._inference(compiled_model, inputs)
+        assert len(outputs) > 0, "Inference failed!"
+
+        return outputs
+
+    @property
+    @abstractmethod
+    def default_deployment(self) -> Deployment:
+        """Provides the default deployment config without the device key."""
+        raise NotImplementedError
+
+    @property
+    @abstractmethod
+    def deployment_table(self) -> DeploymentTable:
+        """Provides a set of supported values for each deployment config key."""
+        raise NotImplementedError
+
+    @abstractmethod
+    def _ir_to_compiled(self, ir_bytes: bytes, compilation_args: Dict[str, Any] = {}) -> bytes:
+        """Converts a model from its intermediate representation (IR) to a compiled device model."""
+
+    @abstractmethod
+    def _profile(self, compiled_model: bytes) -> Tuple[float, DetailedResults]:
+        """Profiles a compiled device model and returns the latency & detailed profiling results."""
+
+    @abstractmethod
+    def _inference(self, compiled_model: bytes, inputs: List[np.ndarray]) -> List[np.ndarray]:
+        """Run inference with the compiled model and return the output as list of numpy arrays."""
```

## modelopt/torch/_deploy/_runtime/trt_client.py

 * *Ordering differences only*

```diff
@@ -1,170 +1,170 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-from typing import Any, Dict, List, Tuple
-
-import numpy as np
-import tensorrt as trt
-import torch
-
-from modelopt.onnx.utils import get_node_names_from_bytes
-
-from ..utils import OnnxBytes
-from .registry import RuntimeRegistry
-from .runtime_client import Deployment, DeploymentTable, DetailedResults, RuntimeClient
-from .tensorrt.constants import SHA_256_HASH_LENGTH
-from .tensorrt.engine_builder import build_and_profile_engine, profile_engine
-from .tensorrt.parse_trtexec_log import parse_profiling_log
-from .tensorrt.tensorrt_utils import convert_trt_dtype_to_torch
-
-__all__ = ["TRTLocalClient"]
-
-
-@RuntimeRegistry.register("TRT")
-class TRTLocalClient(RuntimeClient):
-    """A client for using the local TRT runtime with GPU backend."""
-
-    @property
-    def default_deployment(self) -> Deployment:
-        return {k: v[0] for k, v in self.deployment_table.items()}
-
-    @property
-    def deployment_table(self) -> DeploymentTable:
-        return {
-            "version": ["8.6", "9.1", "9.2", "9.3", "10.0"],
-            "accelerator": ["GPU"],
-            "precision": ["fp32", "fp16", "fp8", "int8", "int4"],
-            # Support ONNX opsets 13-19
-            "onnx_opset": [str(i) for i in range(13, 20)],
-        }
-
-    def __init__(self, deployment: Deployment):
-        """Initialize a TRTLocalClient with the given deployment."""
-        super().__init__(deployment)
-        self.inference_sessions = {}
-        logger = trt.Logger(trt.Logger.WARNING)
-        self.trt_runtime = trt.Runtime(logger)
-        assert trt.init_libnvinfer_plugins(logger, ""), "Failed to initialize nvinfer plugins."
-        self.stream = torch.cuda.Stream()
-
-    def _ir_to_compiled(self, ir_bytes: bytes, compilation_args: Dict[str, Any] = None) -> bytes:
-        """Converts an ONNX model to a compiled TRT model."""
-        onnx_bytes = OnnxBytes.from_bytes(ir_bytes)
-        onnx_model_file_bytes = onnx_bytes.get_onnx_model_file_bytes()
-        self.node_names = get_node_names_from_bytes(onnx_model_file_bytes)
-        engine_bytes, _, _, _ = build_and_profile_engine(
-            onnx_bytes,
-            dynamic_shapes=compilation_args.get("dynamic_shapes", None),
-            trt_mode=self.deployment["precision"],
-        )
-        self.engine_bytes = engine_bytes
-        return engine_bytes
-
-    def _profile(self, compiled_model: bytes) -> Tuple[float, DetailedResults]:
-        _, trtexec_log = profile_engine(
-            compiled_model, self.node_names, enable_layerwise_profiling=True
-        )
-        profiling_results = parse_profiling_log(trtexec_log.decode())
-        latency = 0.0
-        detailed_results = {}
-        if profiling_results is not None:
-            latency = profiling_results["performance_summary"]["Latency"][1]
-            detailed_results = profiling_results
-        return latency, detailed_results
-
-    def _inference(self, compiled_model: bytes, inputs: List[np.ndarray]) -> List[np.ndarray]:
-        """Run inference with the compiled model and return the output as list of numpy arrays."""
-        assert compiled_model is not None, "Engine bytes are not set."
-
-        model_hash = compiled_model[:SHA_256_HASH_LENGTH]
-        if model_hash not in self.inference_sessions:
-            model_bytes = compiled_model[SHA_256_HASH_LENGTH:]
-            self.inference_sessions[model_hash] = self.TRTSession(
-                model_bytes, self.trt_runtime, self.stream
-            )
-        return self.inference_sessions[model_hash].run(inputs)
-
-    def _teardown_all_sessions(self):
-        """Clean up all TRT sessions."""
-        for session in self.inference_sessions.values():
-            del session
-        self.inference_sessions = {}
-
-    class TRTSession:
-        def __init__(self, compiled_model, trt_runtime, stream):
-            self.engine = trt_runtime.deserialize_cuda_engine(compiled_model)
-            assert self.engine is not None, "Engine deserialization failed."
-            self.execution_context = self.engine.create_execution_context()
-            self.stream = stream
-            self.input_tensors, self.output_tensors = self.initialize_input_output_tensors(
-                self.engine
-            )
-
-        def initialize_input_output_tensors(self, engine):
-            # Allocate torch tensors for inputs and outputs
-            input_tensors = []
-            output_tensors = []
-            for idx in range(engine.num_io_tensors):
-                tensor_name = engine.get_tensor_name(idx)
-                tensor_shape = engine.get_tensor_profile_shape(tensor_name, 0)[0]
-                tensor_dtype = convert_trt_dtype_to_torch(engine.get_tensor_dtype(tensor_name))
-                torch_tensor = torch.empty(tuple(tensor_shape), dtype=tensor_dtype, device="cuda")
-                if engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:
-                    input_tensors.append(torch_tensor)
-                    self.execution_context.set_tensor_address(
-                        tensor_name, input_tensors[idx].data_ptr()
-                    )
-                else:
-                    output_tensors.append(torch_tensor)
-                    self.execution_context.set_tensor_address(
-                        tensor_name,
-                        output_tensors[idx - len(input_tensors)].data_ptr(),
-                    )
-            assert (
-                self.execution_context.all_shape_inputs_specified
-            ), "Not all shape inputs are specified."
-
-            # Set selected profile idx
-            self.execution_context.set_optimization_profile_async(0, self.stream.cuda_stream)
-
-            # Assertion: to ensure all the inputs are set
-            assert (
-                len(self.execution_context.infer_shapes()) == 0
-            ), "Shapes of all the bindings cannot be inferred."
-
-            return input_tensors, output_tensors
-
-        def run(self, inputs):
-            assert self.engine is not None, "Engine is not set."
-
-            # Copy inputs to GPU
-            with torch.cuda.stream(self.stream):
-                for i, input_np in enumerate(inputs):
-                    input_t = torch.from_numpy(input_np)
-
-                    # Pad the input tensor with zeros if the input tensor is smaller than the expected size
-                    zero_tensor = torch.zeros(self.input_tensors[i].shape)
-                    slices = tuple(slice(0, input_t.size(dim)) for dim in range(input_t.dim()))
-                    zero_tensor[slices] = input_t
-
-                    # Copy the input tensor to the GPU
-                    self.input_tensors[i].copy_(zero_tensor, non_blocking=True)
-
-            # Run inference
-            self.execution_context.execute_async_v3(stream_handle=self.stream.cuda_stream)
-
-            # Copy outputs to CPU
-            with torch.cuda.stream(self.stream):
-                for t in self.output_tensors:
-                    t.to(device="cpu", non_blocking=True)
-
-            self.stream.synchronize()
-
-            return [t.detach().cpu().numpy() for t in self.output_tensors]
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+from typing import Any, Dict, List, Tuple
+
+import numpy as np
+import tensorrt as trt
+import torch
+
+from modelopt.onnx.utils import get_node_names_from_bytes
+
+from ..utils import OnnxBytes
+from .registry import RuntimeRegistry
+from .runtime_client import Deployment, DeploymentTable, DetailedResults, RuntimeClient
+from .tensorrt.constants import SHA_256_HASH_LENGTH
+from .tensorrt.engine_builder import build_and_profile_engine, profile_engine
+from .tensorrt.parse_trtexec_log import parse_profiling_log
+from .tensorrt.tensorrt_utils import convert_trt_dtype_to_torch
+
+__all__ = ["TRTLocalClient"]
+
+
+@RuntimeRegistry.register("TRT")
+class TRTLocalClient(RuntimeClient):
+    """A client for using the local TRT runtime with GPU backend."""
+
+    @property
+    def default_deployment(self) -> Deployment:
+        return {k: v[0] for k, v in self.deployment_table.items()}
+
+    @property
+    def deployment_table(self) -> DeploymentTable:
+        return {
+            "version": ["8.6", "9.1", "9.2", "9.3", "10.0"],
+            "accelerator": ["GPU"],
+            "precision": ["fp32", "fp16", "fp8", "int8", "int4"],
+            # Support ONNX opsets 13-19
+            "onnx_opset": [str(i) for i in range(13, 20)],
+        }
+
+    def __init__(self, deployment: Deployment):
+        """Initialize a TRTLocalClient with the given deployment."""
+        super().__init__(deployment)
+        self.inference_sessions = {}
+        logger = trt.Logger(trt.Logger.WARNING)
+        self.trt_runtime = trt.Runtime(logger)
+        assert trt.init_libnvinfer_plugins(logger, ""), "Failed to initialize nvinfer plugins."
+        self.stream = torch.cuda.Stream()
+
+    def _ir_to_compiled(self, ir_bytes: bytes, compilation_args: Dict[str, Any] = None) -> bytes:
+        """Converts an ONNX model to a compiled TRT model."""
+        onnx_bytes = OnnxBytes.from_bytes(ir_bytes)
+        onnx_model_file_bytes = onnx_bytes.get_onnx_model_file_bytes()
+        self.node_names = get_node_names_from_bytes(onnx_model_file_bytes)
+        engine_bytes, _, _, _ = build_and_profile_engine(
+            onnx_bytes,
+            dynamic_shapes=compilation_args.get("dynamic_shapes", None),
+            trt_mode=self.deployment["precision"],
+        )
+        self.engine_bytes = engine_bytes
+        return engine_bytes
+
+    def _profile(self, compiled_model: bytes) -> Tuple[float, DetailedResults]:
+        _, trtexec_log = profile_engine(
+            compiled_model, self.node_names, enable_layerwise_profiling=True
+        )
+        profiling_results = parse_profiling_log(trtexec_log.decode())
+        latency = 0.0
+        detailed_results = {}
+        if profiling_results is not None:
+            latency = profiling_results["performance_summary"]["Latency"][1]
+            detailed_results = profiling_results
+        return latency, detailed_results
+
+    def _inference(self, compiled_model: bytes, inputs: List[np.ndarray]) -> List[np.ndarray]:
+        """Run inference with the compiled model and return the output as list of numpy arrays."""
+        assert compiled_model is not None, "Engine bytes are not set."
+
+        model_hash = compiled_model[:SHA_256_HASH_LENGTH]
+        if model_hash not in self.inference_sessions:
+            model_bytes = compiled_model[SHA_256_HASH_LENGTH:]
+            self.inference_sessions[model_hash] = self.TRTSession(
+                model_bytes, self.trt_runtime, self.stream
+            )
+        return self.inference_sessions[model_hash].run(inputs)
+
+    def _teardown_all_sessions(self):
+        """Clean up all TRT sessions."""
+        for session in self.inference_sessions.values():
+            del session
+        self.inference_sessions = {}
+
+    class TRTSession:
+        def __init__(self, compiled_model, trt_runtime, stream):
+            self.engine = trt_runtime.deserialize_cuda_engine(compiled_model)
+            assert self.engine is not None, "Engine deserialization failed."
+            self.execution_context = self.engine.create_execution_context()
+            self.stream = stream
+            self.input_tensors, self.output_tensors = self.initialize_input_output_tensors(
+                self.engine
+            )
+
+        def initialize_input_output_tensors(self, engine):
+            # Allocate torch tensors for inputs and outputs
+            input_tensors = []
+            output_tensors = []
+            for idx in range(engine.num_io_tensors):
+                tensor_name = engine.get_tensor_name(idx)
+                tensor_shape = engine.get_tensor_profile_shape(tensor_name, 0)[0]
+                tensor_dtype = convert_trt_dtype_to_torch(engine.get_tensor_dtype(tensor_name))
+                torch_tensor = torch.empty(tuple(tensor_shape), dtype=tensor_dtype, device="cuda")
+                if engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:
+                    input_tensors.append(torch_tensor)
+                    self.execution_context.set_tensor_address(
+                        tensor_name, input_tensors[idx].data_ptr()
+                    )
+                else:
+                    output_tensors.append(torch_tensor)
+                    self.execution_context.set_tensor_address(
+                        tensor_name,
+                        output_tensors[idx - len(input_tensors)].data_ptr(),
+                    )
+            assert (
+                self.execution_context.all_shape_inputs_specified
+            ), "Not all shape inputs are specified."
+
+            # Set selected profile idx
+            self.execution_context.set_optimization_profile_async(0, self.stream.cuda_stream)
+
+            # Assertion: to ensure all the inputs are set
+            assert (
+                len(self.execution_context.infer_shapes()) == 0
+            ), "Shapes of all the bindings cannot be inferred."
+
+            return input_tensors, output_tensors
+
+        def run(self, inputs):
+            assert self.engine is not None, "Engine is not set."
+
+            # Copy inputs to GPU
+            with torch.cuda.stream(self.stream):
+                for i, input_np in enumerate(inputs):
+                    input_t = torch.from_numpy(input_np)
+
+                    # Pad the input tensor with zeros if the input tensor is smaller than the expected size
+                    zero_tensor = torch.zeros(self.input_tensors[i].shape)
+                    slices = tuple(slice(0, input_t.size(dim)) for dim in range(input_t.dim()))
+                    zero_tensor[slices] = input_t
+
+                    # Copy the input tensor to the GPU
+                    self.input_tensors[i].copy_(zero_tensor, non_blocking=True)
+
+            # Run inference
+            self.execution_context.execute_async_v3(stream_handle=self.stream.cuda_stream)
+
+            # Copy outputs to CPU
+            with torch.cuda.stream(self.stream):
+                for t in self.output_tensors:
+                    t.to(device="cpu", non_blocking=True)
+
+            self.stream.synchronize()
+
+            return [t.detach().cpu().numpy() for t in self.output_tensors]
```

## modelopt/torch/_deploy/_runtime/tensorrt/constants.py

 * *Ordering differences only*

```diff
@@ -1,91 +1,91 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""TensorRT server specific constants.
-
-If any constant is shared by both the TensorRT client and TensorRT server,
-that should go into client/constants.py module and server should import from there.
-Also if we open some of these settings for the clients, they should be moved there.
-"""
-
-# Versions
-TENSORRT_7_MAJOR_VERSION = 7
-TENSORRT_8_MAJOR_VERSION = 8
-
-# Sizes (unit: MB)
-ONE_MEBI = 1
-ONE_GIBI = 1024
-
-ONE_MEBI_IN_BYTES = 1 << 20
-ONE_GIBI_IN_BYTES = 1 << 30
-
-# TensorRT conversion tool names
-TRTEXEC = "trtexec"
-
-# trtexec path within docker
-TRTEXEC_PATH = "/usr/src/tensorrt/bin/trtexec"
-
-# Default conversion params
-DEFAULT_VALIDATION_THRESHOLD = 1e-4
-DEFAULT_MAX_BATCH_SIZE = 1
-DEFAULT_ACCELERATOR = "GPU"
-
-# With empty tactic source string trtexec will use all the available sources
-DEFAULT_TACTIC_SOURCES = ""
-
-# NVTX annotation verbosity (default, verbose or none)
-DEFAULT_NVTX_MODE = "none"
-
-# The minimum number of iterations used in kernel selection
-DEFAULT_MIN_TIMING = 1
-
-# The number of times averaged in each iteration for kernel selection
-DEFAULT_AVG_TIMING = 1
-
-# Is inference perf measurement required by default
-ENABLE_PERF_MEASUREMENT = False
-
-# Is layerwise profiling required by default
-ENABLE_LAYERWISE_PROFILING = False
-
-# Default batch size for inference
-DEFAULT_BATCH_SIZE = 1
-
-# Default GPU Id
-DEFAULT_GPU_ID = 0
-
-# Default maximum workspace size
-DEFAULT_MAX_WORKSPACE_SIZE = 256
-
-# trtexec settings.
-# Run for N milliseconds to warmup before measuring performance
-WARMUP_TIME_MS = 500
-
-# Profiling parameters
-DEFAULT_PROFILING_RUNS = 1
-DEFAULT_NUM_INFERENCE_PER_RUN = 100
-
-# Layerwise profiling parameters
-INPUT_DATA_KEY = "input_data"
-OUTPUT_DATA_KEY = "output_data"
-UNNAMED_LAYER_KEY = "other"
-NODE_NAME_DELIMITER = " + "
-
-
-# Class representing all TensorRT modes expressed as text
-class TRTMode:
-    FLOAT32 = "fp32"
-    FLOAT16 = "fp16"
-    FLOAT8 = "fp8"
-    INT8 = "int8"
-    INT4 = "int4"
-
-
-SHA_256_HASH_LENGTH = 32
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""TensorRT server specific constants.
+
+If any constant is shared by both the TensorRT client and TensorRT server,
+that should go into client/constants.py module and server should import from there.
+Also if we open some of these settings for the clients, they should be moved there.
+"""
+
+# Versions
+TENSORRT_7_MAJOR_VERSION = 7
+TENSORRT_8_MAJOR_VERSION = 8
+
+# Sizes (unit: MB)
+ONE_MEBI = 1
+ONE_GIBI = 1024
+
+ONE_MEBI_IN_BYTES = 1 << 20
+ONE_GIBI_IN_BYTES = 1 << 30
+
+# TensorRT conversion tool names
+TRTEXEC = "trtexec"
+
+# trtexec path within docker
+TRTEXEC_PATH = "/usr/src/tensorrt/bin/trtexec"
+
+# Default conversion params
+DEFAULT_VALIDATION_THRESHOLD = 1e-4
+DEFAULT_MAX_BATCH_SIZE = 1
+DEFAULT_ACCELERATOR = "GPU"
+
+# With empty tactic source string trtexec will use all the available sources
+DEFAULT_TACTIC_SOURCES = ""
+
+# NVTX annotation verbosity (default, verbose or none)
+DEFAULT_NVTX_MODE = "none"
+
+# The minimum number of iterations used in kernel selection
+DEFAULT_MIN_TIMING = 1
+
+# The number of times averaged in each iteration for kernel selection
+DEFAULT_AVG_TIMING = 1
+
+# Is inference perf measurement required by default
+ENABLE_PERF_MEASUREMENT = False
+
+# Is layerwise profiling required by default
+ENABLE_LAYERWISE_PROFILING = False
+
+# Default batch size for inference
+DEFAULT_BATCH_SIZE = 1
+
+# Default GPU Id
+DEFAULT_GPU_ID = 0
+
+# Default maximum workspace size
+DEFAULT_MAX_WORKSPACE_SIZE = 256
+
+# trtexec settings.
+# Run for N milliseconds to warmup before measuring performance
+WARMUP_TIME_MS = 500
+
+# Profiling parameters
+DEFAULT_PROFILING_RUNS = 1
+DEFAULT_NUM_INFERENCE_PER_RUN = 100
+
+# Layerwise profiling parameters
+INPUT_DATA_KEY = "input_data"
+OUTPUT_DATA_KEY = "output_data"
+UNNAMED_LAYER_KEY = "other"
+NODE_NAME_DELIMITER = " + "
+
+
+# Class representing all TensorRT modes expressed as text
+class TRTMode:
+    FLOAT32 = "fp32"
+    FLOAT16 = "fp16"
+    FLOAT8 = "fp8"
+    INT8 = "int8"
+    INT4 = "int4"
+
+
+SHA_256_HASH_LENGTH = 32
```

## modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py

 * *Ordering differences only*

```diff
@@ -1,45 +1,45 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Hardware specific parameters.
-
-All the hardware parameters will be treated as dictionary type for omniengine client APIs.
-The key name and the suggested value range are listed with name TENSORRT_HW_PARAMS_SUGGESTED_OPTIONS.
-The key name and the optimum value are listed with name TENSORRT_HW_PARAMS_OPT_OPTIONS.
-"""
-from .constants import (
-    DEFAULT_AVG_TIMING,
-    DEFAULT_MAX_WORKSPACE_SIZE,
-    DEFAULT_MIN_TIMING,
-    DEFAULT_TACTIC_SOURCES,
-)
-
-# Key names
-# Workspace unit: MB
-MAX_WORKSPACE_SIZE = [16, 32, 64, 128, 256, 512, 1024]
-TACTIC_SOURCES = ["cublasLt", "cublas", "cudnn"]
-ALL_TATIC_SOURCES_COMPONENT = []
-for source in TACTIC_SOURCES:
-    ALL_TATIC_SOURCES_COMPONENT.append("+" + source)
-    ALL_TATIC_SOURCES_COMPONENT.append("-" + source)
-
-TENSORRT_HW_PARAMS_SUGGESTED_OPTIONS = {
-    "tacticSources": ALL_TATIC_SOURCES_COMPONENT,
-    "minTiming": range(1, 5),
-    "avgTiming": range(1, 5),
-    "workspace": MAX_WORKSPACE_SIZE,
-}
-
-TENSORRT_HW_PARAMS_OPT_OPTIONS = {
-    "tacticSources": DEFAULT_TACTIC_SOURCES,
-    "minTiming": str(DEFAULT_MIN_TIMING),
-    "avgTiming": str(DEFAULT_AVG_TIMING),
-    "workspace": str(DEFAULT_MAX_WORKSPACE_SIZE),
-}
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Hardware specific parameters.
+
+All the hardware parameters will be treated as dictionary type for omniengine client APIs.
+The key name and the suggested value range are listed with name TENSORRT_HW_PARAMS_SUGGESTED_OPTIONS.
+The key name and the optimum value are listed with name TENSORRT_HW_PARAMS_OPT_OPTIONS.
+"""
+from .constants import (
+    DEFAULT_AVG_TIMING,
+    DEFAULT_MAX_WORKSPACE_SIZE,
+    DEFAULT_MIN_TIMING,
+    DEFAULT_TACTIC_SOURCES,
+)
+
+# Key names
+# Workspace unit: MB
+MAX_WORKSPACE_SIZE = [16, 32, 64, 128, 256, 512, 1024]
+TACTIC_SOURCES = ["cublasLt", "cublas", "cudnn"]
+ALL_TATIC_SOURCES_COMPONENT = []
+for source in TACTIC_SOURCES:
+    ALL_TATIC_SOURCES_COMPONENT.append("+" + source)
+    ALL_TATIC_SOURCES_COMPONENT.append("-" + source)
+
+TENSORRT_HW_PARAMS_SUGGESTED_OPTIONS = {
+    "tacticSources": ALL_TATIC_SOURCES_COMPONENT,
+    "minTiming": range(1, 5),
+    "avgTiming": range(1, 5),
+    "workspace": MAX_WORKSPACE_SIZE,
+}
+
+TENSORRT_HW_PARAMS_OPT_OPTIONS = {
+    "tacticSources": DEFAULT_TACTIC_SOURCES,
+    "minTiming": str(DEFAULT_MIN_TIMING),
+    "avgTiming": str(DEFAULT_AVG_TIMING),
+    "workspace": str(DEFAULT_MAX_WORKSPACE_SIZE),
+}
```

## modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py

 * *Ordering differences only*

```diff
@@ -1,176 +1,176 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-import json
-import logging
-from typing import Dict, List, Tuple
-
-from ..common import read_string
-from .constants import (
-    INPUT_DATA_KEY,
-    NODE_NAME_DELIMITER,
-    OUTPUT_DATA_KEY,
-    UNNAMED_LAYER_KEY,
-)
-
-
-def _merge_reformatters(layer_latency_dict: Dict[str, float]) -> Dict[str, float]:
-    reformatter_delimiters = [" input reformatter ", " output reformatter ", " to "]
-    keys2merge = [k for k in layer_latency_dict if any(rn in k for rn in reformatter_delimiters)]
-
-    for key in keys2merge:
-        layer_latency = layer_latency_dict.pop(key)
-        for delimiter in reformatter_delimiters:
-            if delimiter in key:
-                split_key = key.split(delimiter)
-                if len(split_key) != 2:
-                    logging.error(
-                        f"Key splitting in layerwise profiling failed for reformatter node: {key}."
-                    )
-                else:
-                    key = split_key[1] if delimiter == " to " else split_key[0]
-                break
-
-        layer_latency_dict[key] = layer_latency_dict.get(key, 0.0) + layer_latency
-
-    return layer_latency_dict
-
-
-def process_layerwise_result(profile_path: str, onnx_node_names: List[str]) -> Dict[str, float]:
-    """This module process the layerwise profiling result to make them mappable to PyTorch.
-
-    Args:
-        profile_path: Path to the layerwise profiling dump from trtexec.
-        onnx_node_names: List of node names in the onnx model.
-
-    Returns:
-        A dictionary mapping layer names to latency in ms. For example,
-
-            .. code-block:: python
-
-                {
-                    "Conv_0 + Clip_3": 0.016,
-                    "Conv_4": 0.008,
-                    "Gemm_169": 0.015,
-                    "input_data": 0.010,
-                    "output_data": 0.040,
-                }
-    """
-    layerwise_profiling = read_string(profile_path)
-    layerwise_results = json.loads(layerwise_profiling)[1:]
-
-    # Just keep averageMs from layerwise profiling result
-    layer_latency_dict = {}
-    for results in layerwise_results:
-        layer_latency_dict[results["name"].replace("onnx::", "")] = results["averageMs"]
-
-    # Merge the input/output reformatter op into its following op
-    layer_latency_dict = _merge_reformatters(layer_latency_dict)
-
-    return map_trt_layers_to_onnx(layer_latency_dict, onnx_node_names)
-
-
-def map_trt_layers_to_onnx(
-    layerwise_result: Dict[str, float], onnx_node_names: List[str]
-) -> Dict[str, float]:
-    """This module maps the TensorRT layers from profiling result with onnx nodes.
-
-    Args:
-        layerwise_result: Layerwise profiling result.
-        onnx_node_names: Onnx node names.
-
-    Returns:
-        A dictionary mapping layer names to latency in ms.
-    """
-
-    def _group_split(key: str, delimiter: str):
-        parenthesis_balance = 0
-        key_group = []
-        current_key = ""
-
-        for ch in key:
-            if ch == "(":
-                parenthesis_balance += 1
-            elif ch == ")":
-                parenthesis_balance -= 1
-            elif ch == delimiter and parenthesis_balance == 0:
-                key_group.append(current_key.strip())
-                current_key = ""
-                continue
-
-            current_key += ch
-
-        # Insert the last key in group
-        key_group.append(current_key.strip())
-
-        return key_group
-
-    def _remove_non_onnx_nodes(layer: str) -> Tuple[str]:
-        keys = []
-
-        # To match the longer name first, match from the last node
-        for node_name in reversed(onnx_node_names):
-            if node_name in layer:
-                keys.append(node_name)
-                layer = layer.replace(node_name, "")  # Skip from further matching
-
-        cleaned_layer = NODE_NAME_DELIMITER.join(reversed(keys))
-
-        # Lets check if the layer has any input/output data timing
-        if not cleaned_layer and "input" in layer:
-            return (INPUT_DATA_KEY,)
-        if not cleaned_layer and "output" in layer:
-            return (OUTPUT_DATA_KEY,)
-
-        if not onnx_node_names:  # empty list if omnimizer<=v0.4.1
-            return (layer,)
-
-        return (cleaned_layer,) if len(cleaned_layer) > 0 else ()  # type: ignore[return-value]
-
-    def _iterative_split_key(key: str) -> Tuple:
-        if ": " in key:
-            # e.g., 2-layer MLP: Conv_68 + Relu_69 -> Conv_70
-            if len(key.split(": ")) != 2:
-                logging.error(f"Key splitting in layerwise profiling failed for key: {key}.")
-                return _remove_non_onnx_nodes(key)
-            return _iterative_split_key(key.split(": ")[1])
-        elif key.startswith("PWN"):
-            # e.g., PWN(Clip_589 + (Unnamed Layer* 16) [Shuffle], PWN(PWN(PWN(Add_586 + (Unnamed Layer* 11) [Shuffle]
-            # + Add_43, Clip_46), Mul_47), Div_49))
-            if not key.endswith(")"):
-                logging.error(f"Key: {key} starts with 'PWN' but does not end with a ')'.")
-                key = key[4:]
-            else:
-                key = key[4:-1]
-            return sum((_iterative_split_key(k) for k in _group_split(key, ",")), ())
-        elif " + " in key:
-            # e.g. Add_586 + (Unnamed Layer* 11) [Shuffle] + Add_43
-            return sum((_iterative_split_key(k) for k in _group_split(key, "+")), ())
-        elif " -> " in key:
-            # e.g. Conv_68 + Relu_69 -> Conv_70
-            key = key.split(" -> ")
-            if len(key) != 2:
-                logging.error(f"Key splitting in layerwise profiling failed for key: {key}.")
-                return _remove_non_onnx_nodes(key)
-
-            covered_keys = _group_split(key[0], "+") + [key[1]]
-            if onnx_node_names:
-                idx1, idx2 = onnx_node_names.index(key[0]), onnx_node_names.index(key[1])
-                covered_keys = tuple(onnx_node_names[idx1 : idx2 + 1])
-            return sum((_iterative_split_key(k) for k in covered_keys), ())
-
-        return _remove_non_onnx_nodes(key)
-
-    mapped_layerwise_result = {}
-    for k, v in layerwise_result.items():
-        new_key = NODE_NAME_DELIMITER.join(_iterative_split_key(k)) or UNNAMED_LAYER_KEY
-        mapped_layerwise_result[new_key] = mapped_layerwise_result.get(new_key, 0.0) + v
-
-    return mapped_layerwise_result
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+import json
+import logging
+from typing import Dict, List, Tuple
+
+from ..common import read_string
+from .constants import (
+    INPUT_DATA_KEY,
+    NODE_NAME_DELIMITER,
+    OUTPUT_DATA_KEY,
+    UNNAMED_LAYER_KEY,
+)
+
+
+def _merge_reformatters(layer_latency_dict: Dict[str, float]) -> Dict[str, float]:
+    reformatter_delimiters = [" input reformatter ", " output reformatter ", " to "]
+    keys2merge = [k for k in layer_latency_dict if any(rn in k for rn in reformatter_delimiters)]
+
+    for key in keys2merge:
+        layer_latency = layer_latency_dict.pop(key)
+        for delimiter in reformatter_delimiters:
+            if delimiter in key:
+                split_key = key.split(delimiter)
+                if len(split_key) != 2:
+                    logging.error(
+                        f"Key splitting in layerwise profiling failed for reformatter node: {key}."
+                    )
+                else:
+                    key = split_key[1] if delimiter == " to " else split_key[0]
+                break
+
+        layer_latency_dict[key] = layer_latency_dict.get(key, 0.0) + layer_latency
+
+    return layer_latency_dict
+
+
+def process_layerwise_result(profile_path: str, onnx_node_names: List[str]) -> Dict[str, float]:
+    """This module process the layerwise profiling result to make them mappable to PyTorch.
+
+    Args:
+        profile_path: Path to the layerwise profiling dump from trtexec.
+        onnx_node_names: List of node names in the onnx model.
+
+    Returns:
+        A dictionary mapping layer names to latency in ms. For example,
+
+            .. code-block:: python
+
+                {
+                    "Conv_0 + Clip_3": 0.016,
+                    "Conv_4": 0.008,
+                    "Gemm_169": 0.015,
+                    "input_data": 0.010,
+                    "output_data": 0.040,
+                }
+    """
+    layerwise_profiling = read_string(profile_path)
+    layerwise_results = json.loads(layerwise_profiling)[1:]
+
+    # Just keep averageMs from layerwise profiling result
+    layer_latency_dict = {}
+    for results in layerwise_results:
+        layer_latency_dict[results["name"].replace("onnx::", "")] = results["averageMs"]
+
+    # Merge the input/output reformatter op into its following op
+    layer_latency_dict = _merge_reformatters(layer_latency_dict)
+
+    return map_trt_layers_to_onnx(layer_latency_dict, onnx_node_names)
+
+
+def map_trt_layers_to_onnx(
+    layerwise_result: Dict[str, float], onnx_node_names: List[str]
+) -> Dict[str, float]:
+    """This module maps the TensorRT layers from profiling result with onnx nodes.
+
+    Args:
+        layerwise_result: Layerwise profiling result.
+        onnx_node_names: Onnx node names.
+
+    Returns:
+        A dictionary mapping layer names to latency in ms.
+    """
+
+    def _group_split(key: str, delimiter: str):
+        parenthesis_balance = 0
+        key_group = []
+        current_key = ""
+
+        for ch in key:
+            if ch == "(":
+                parenthesis_balance += 1
+            elif ch == ")":
+                parenthesis_balance -= 1
+            elif ch == delimiter and parenthesis_balance == 0:
+                key_group.append(current_key.strip())
+                current_key = ""
+                continue
+
+            current_key += ch
+
+        # Insert the last key in group
+        key_group.append(current_key.strip())
+
+        return key_group
+
+    def _remove_non_onnx_nodes(layer: str) -> Tuple[str]:
+        keys = []
+
+        # To match the longer name first, match from the last node
+        for node_name in reversed(onnx_node_names):
+            if node_name in layer:
+                keys.append(node_name)
+                layer = layer.replace(node_name, "")  # Skip from further matching
+
+        cleaned_layer = NODE_NAME_DELIMITER.join(reversed(keys))
+
+        # Lets check if the layer has any input/output data timing
+        if not cleaned_layer and "input" in layer:
+            return (INPUT_DATA_KEY,)
+        if not cleaned_layer and "output" in layer:
+            return (OUTPUT_DATA_KEY,)
+
+        if not onnx_node_names:  # empty list if omnimizer<=v0.4.1
+            return (layer,)
+
+        return (cleaned_layer,) if len(cleaned_layer) > 0 else ()  # type: ignore[return-value]
+
+    def _iterative_split_key(key: str) -> Tuple:
+        if ": " in key:
+            # e.g., 2-layer MLP: Conv_68 + Relu_69 -> Conv_70
+            if len(key.split(": ")) != 2:
+                logging.error(f"Key splitting in layerwise profiling failed for key: {key}.")
+                return _remove_non_onnx_nodes(key)
+            return _iterative_split_key(key.split(": ")[1])
+        elif key.startswith("PWN"):
+            # e.g., PWN(Clip_589 + (Unnamed Layer* 16) [Shuffle], PWN(PWN(PWN(Add_586 + (Unnamed Layer* 11) [Shuffle]
+            # + Add_43, Clip_46), Mul_47), Div_49))
+            if not key.endswith(")"):
+                logging.error(f"Key: {key} starts with 'PWN' but does not end with a ')'.")
+                key = key[4:]
+            else:
+                key = key[4:-1]
+            return sum((_iterative_split_key(k) for k in _group_split(key, ",")), ())
+        elif " + " in key:
+            # e.g. Add_586 + (Unnamed Layer* 11) [Shuffle] + Add_43
+            return sum((_iterative_split_key(k) for k in _group_split(key, "+")), ())
+        elif " -> " in key:
+            # e.g. Conv_68 + Relu_69 -> Conv_70
+            key = key.split(" -> ")
+            if len(key) != 2:
+                logging.error(f"Key splitting in layerwise profiling failed for key: {key}.")
+                return _remove_non_onnx_nodes(key)
+
+            covered_keys = _group_split(key[0], "+") + [key[1]]
+            if onnx_node_names:
+                idx1, idx2 = onnx_node_names.index(key[0]), onnx_node_names.index(key[1])
+                covered_keys = tuple(onnx_node_names[idx1 : idx2 + 1])
+            return sum((_iterative_split_key(k) for k in covered_keys), ())
+
+        return _remove_non_onnx_nodes(key)
+
+    mapped_layerwise_result = {}
+    for k, v in layerwise_result.items():
+        new_key = NODE_NAME_DELIMITER.join(_iterative_split_key(k)) or UNNAMED_LAYER_KEY
+        mapped_layerwise_result[new_key] = mapped_layerwise_result.get(new_key, 0.0) + v
+
+    return mapped_layerwise_result
```

## modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py

 * *Ordering differences only*

```diff
@@ -1,152 +1,152 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-# Reference: https://github.com/NVIDIA/TensorRT/tree/release/9.1/tools/experimental/trt-engine-explorer/utils
-
-
-"""
-trtexec log file parsing
-"""
-
-
-import re
-from typing import Any, Dict, List, Tuple
-
-
-def __to_float(line: str) -> float:
-    """Scan the input string and extract the first float instance."""
-    # https://docs.python.org/3/library/re.html#simulating-scanf
-    float_match = re.search(r"[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?", line)
-    if float_match is None:
-        raise ValueError
-    start, end = float_match.span()
-    return float(line[start:end])
-
-
-def __get_stats(line: str) -> List[float]:
-    """Parse a string containing pairs of "key = value" and return the list of values.
-
-    Here's a sample input line: "min = 0.87854 ms, max = 0.894043 ms, mean = 0.881251 ms"
-    The values are expected to be floats.
-    Split the kv list to "k = v" substrings, then split each substring to
-    k, v and return float(v)
-    """
-    return [__to_float(substr.split("=")[1]) for substr in line.split(",")]
-
-
-class FileSection:
-    def __init__(self, section_header: str):
-        self.section_header = section_header
-        self.dict = {}
-
-    def entered_section(self, line: str):
-        s = re.search(self.section_header, line)
-        return s is not None
-
-    def parse_line(self, line: str):
-        def parse_kv_line(line: str) -> Tuple[Any, Any]:
-            """Parse a log line that reports a key-value pair.
-
-            The log line has this format: [mm/dd/yyyy-hh:mm:ss] [I] key_name: key_value
-            """
-            match = re.search(r"(\[\d+/\d+/\d+-\d+:\d+:\d+\] \[I\] )", line)
-            if match is not None:
-                match_end = match.span()[1]
-                kv_line = line[match_end:].strip()
-                kv = kv_line.split(": ")
-                if len(kv) > 1:
-                    return kv[0], kv[1]
-            return None, None
-
-        k, v = parse_kv_line(line)
-        if k is not None and v is not None:
-            self.dict[k] = v
-            return True
-        if k is not None:
-            return True
-        return False
-
-
-def __parse_log_file(trtexec_log: str, sections: List) -> List[Dict]:
-    current_section = None
-    for line in trtexec_log.split("\n"):
-        if current_section is None:
-            for section in sections:
-                if section.entered_section(line):
-                    current_section = section
-                    break
-        else:
-            if not current_section.parse_line(line):
-                current_section = None
-    dicts = [section.dict for section in sections]
-    return dicts
-
-
-def parse_build_log(trtexec_log: str) -> Dict[str, Any]:
-    """Parse the TensorRT engine build log and extract the builder configuration.
-
-    Args:
-        trtexec_log: The log file generated by trtexec.
-
-    Returns:
-        A dictionary containing the model options and build options.
-    """
-    model_options = FileSection("=== Model Options ===")
-    build_options = FileSection("=== Build Options ===")
-    sections = [model_options, build_options]
-    __parse_log_file(trtexec_log, sections)
-    return {
-        "model_options": model_options.dict,
-        "build_options": build_options.dict,
-    }
-
-
-def parse_profiling_log(trtexec_log: str) -> Dict[str, Any]:
-    """Parse the TensorRT engine profiling log and extract the performance summary.
-
-    Args:
-        trtexec_log: The log file generated by trtexec.
-
-    Returns:
-        A dictionary containing the performance summary, inference options and device information.
-    """
-    performance_summary = FileSection("=== Performance summary ===")
-    inference_options = FileSection("=== Inference Options ===")
-    device_information = FileSection("=== Device Information ===")
-    sections = [performance_summary, inference_options, device_information]
-    __parse_log_file(trtexec_log, sections)
-
-    def post_process_perf(perf_summary: dict):
-        """Normalize the log results to a standard format"""
-        for k, v in perf_summary.items():
-            if k in ["Throughput", "Total Host Walltime", "Total GPU Compute Time"]:
-                perf_summary[k] = __to_float(v)
-            if k in ["Latency", "Enqueue Time", "H2D Latency", "GPU Compute Time", "D2H Latency"]:
-                perf_summary[k] = __get_stats(v)
-        return perf_summary
-
-    def post_process_device_info(device_info: dict):
-        """Convert some value fields to float"""
-        for k, v in device_info.items():
-            if k in [
-                "Compute Clock Rate",
-                "Memory Bus Width",
-                "Memory Clock Rate",
-                "Compute Capability",
-                "SMs",
-            ]:
-                device_info[k] = __to_float(v)
-        return device_info
-
-    return {
-        "performance_summary": post_process_perf(performance_summary.dict),
-        "inference_options": inference_options.dict,
-        "device_information": post_process_device_info(device_information.dict),
-    }
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+# Reference: https://github.com/NVIDIA/TensorRT/tree/release/9.1/tools/experimental/trt-engine-explorer/utils
+
+
+"""
+trtexec log file parsing
+"""
+
+
+import re
+from typing import Any, Dict, List, Tuple
+
+
+def __to_float(line: str) -> float:
+    """Scan the input string and extract the first float instance."""
+    # https://docs.python.org/3/library/re.html#simulating-scanf
+    float_match = re.search(r"[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?", line)
+    if float_match is None:
+        raise ValueError
+    start, end = float_match.span()
+    return float(line[start:end])
+
+
+def __get_stats(line: str) -> List[float]:
+    """Parse a string containing pairs of "key = value" and return the list of values.
+
+    Here's a sample input line: "min = 0.87854 ms, max = 0.894043 ms, mean = 0.881251 ms"
+    The values are expected to be floats.
+    Split the kv list to "k = v" substrings, then split each substring to
+    k, v and return float(v)
+    """
+    return [__to_float(substr.split("=")[1]) for substr in line.split(",")]
+
+
+class FileSection:
+    def __init__(self, section_header: str):
+        self.section_header = section_header
+        self.dict = {}
+
+    def entered_section(self, line: str):
+        s = re.search(self.section_header, line)
+        return s is not None
+
+    def parse_line(self, line: str):
+        def parse_kv_line(line: str) -> Tuple[Any, Any]:
+            """Parse a log line that reports a key-value pair.
+
+            The log line has this format: [mm/dd/yyyy-hh:mm:ss] [I] key_name: key_value
+            """
+            match = re.search(r"(\[\d+/\d+/\d+-\d+:\d+:\d+\] \[I\] )", line)
+            if match is not None:
+                match_end = match.span()[1]
+                kv_line = line[match_end:].strip()
+                kv = kv_line.split(": ")
+                if len(kv) > 1:
+                    return kv[0], kv[1]
+            return None, None
+
+        k, v = parse_kv_line(line)
+        if k is not None and v is not None:
+            self.dict[k] = v
+            return True
+        if k is not None:
+            return True
+        return False
+
+
+def __parse_log_file(trtexec_log: str, sections: List) -> List[Dict]:
+    current_section = None
+    for line in trtexec_log.split("\n"):
+        if current_section is None:
+            for section in sections:
+                if section.entered_section(line):
+                    current_section = section
+                    break
+        else:
+            if not current_section.parse_line(line):
+                current_section = None
+    dicts = [section.dict for section in sections]
+    return dicts
+
+
+def parse_build_log(trtexec_log: str) -> Dict[str, Any]:
+    """Parse the TensorRT engine build log and extract the builder configuration.
+
+    Args:
+        trtexec_log: The log file generated by trtexec.
+
+    Returns:
+        A dictionary containing the model options and build options.
+    """
+    model_options = FileSection("=== Model Options ===")
+    build_options = FileSection("=== Build Options ===")
+    sections = [model_options, build_options]
+    __parse_log_file(trtexec_log, sections)
+    return {
+        "model_options": model_options.dict,
+        "build_options": build_options.dict,
+    }
+
+
+def parse_profiling_log(trtexec_log: str) -> Dict[str, Any]:
+    """Parse the TensorRT engine profiling log and extract the performance summary.
+
+    Args:
+        trtexec_log: The log file generated by trtexec.
+
+    Returns:
+        A dictionary containing the performance summary, inference options and device information.
+    """
+    performance_summary = FileSection("=== Performance summary ===")
+    inference_options = FileSection("=== Inference Options ===")
+    device_information = FileSection("=== Device Information ===")
+    sections = [performance_summary, inference_options, device_information]
+    __parse_log_file(trtexec_log, sections)
+
+    def post_process_perf(perf_summary: dict):
+        """Normalize the log results to a standard format"""
+        for k, v in perf_summary.items():
+            if k in ["Throughput", "Total Host Walltime", "Total GPU Compute Time"]:
+                perf_summary[k] = __to_float(v)
+            if k in ["Latency", "Enqueue Time", "H2D Latency", "GPU Compute Time", "D2H Latency"]:
+                perf_summary[k] = __get_stats(v)
+        return perf_summary
+
+    def post_process_device_info(device_info: dict):
+        """Convert some value fields to float"""
+        for k, v in device_info.items():
+            if k in [
+                "Compute Clock Rate",
+                "Memory Bus Width",
+                "Memory Clock Rate",
+                "Compute Capability",
+                "SMs",
+            ]:
+                device_info[k] = __to_float(v)
+        return device_info
+
+    return {
+        "performance_summary": post_process_perf(performance_summary.dict),
+        "inference_options": inference_options.dict,
+        "device_information": post_process_device_info(device_information.dict),
+    }
```

## modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py

 * *Ordering differences only*

```diff
@@ -1,196 +1,196 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-import hashlib
-import logging
-from typing import Dict, List
-
-import numpy as np
-import onnx
-import tensorrt as trt
-import torch
-
-from modelopt.onnx.utils import get_batch_size
-from modelopt.onnx.utils import get_input_names as get_onnx_input_names
-
-from .constants import (
-    TENSORRT_8_MAJOR_VERSION,
-    TRTMode,
-)
-
-
-def is_trt8():
-    return int(trt.__version__.split(".", maxsplit=1)[0]) == TENSORRT_8_MAJOR_VERSION
-
-
-class HostDeviceMem(object):
-    """Simple helper data class to hold host and device memory pointers."""
-
-    def __init__(self, host_mem, device_mem, dtype):
-        self.host = host_mem
-        self.device = device_mem
-        self.dtype = dtype
-
-    def __str__(self):
-        return (
-            "Host:\n"
-            + str(self.host)
-            + "\nDevice:\n"
-            + str(self.device)
-            + "\nType:\n"
-            + str(self.dtype)
-        )
-
-    def __repr__(self):
-        return self.__str__()
-
-    def __del__(self):
-        del self.host
-        del self.device
-
-
-def get_engine_bytes(engine: trt.tensorrt.ICudaEngine) -> bytes:
-    """Return serialized TensorRT engine bytes."""
-    return bytearray(engine.serialize())
-
-
-def load_engine(buffer: bytes, log_level: int = trt.Logger.ERROR) -> trt.tensorrt.ICudaEngine:
-    """Load a TensorRT engine from engine data and return."""
-    try:
-        trt_logger = trt.Logger(log_level)
-        with trt.Runtime(trt_logger) as runtime:
-            return runtime.deserialize_cuda_engine(buffer), ""
-    except Exception as e:
-        logging.exception(str(e))
-        return None, str(e)
-
-
-def get_input_names(engine: trt.tensorrt.ICudaEngine) -> List[str]:
-    """Gather the input names from an ICudaEngine.
-
-    Args:
-        engine: TensorRT engine object.
-
-    Returns:
-        List of engine input names.
-    """
-    input_names = []
-    for binding_index in range(engine.num_bindings):
-        if engine.binding_is_input(binding_index):
-            input_names.append(engine.get_binding_name(binding_index))
-    return input_names
-
-
-def get_output_names(engine: trt.tensorrt.ICudaEngine) -> List[str]:
-    """Gather the output names from an ICudaEngine.
-
-    Args:
-        engine: TensorRT engine object.
-
-    Returns:
-        List of engine output names.
-    """
-    output_names = []
-    for binding_index in range(engine.num_bindings):
-        if not engine.binding_is_input(binding_index):
-            output_names.append(engine.get_binding_name(binding_index))
-    return output_names
-
-
-def get_output_shapes(
-    engine: trt.tensorrt.ICudaEngine,
-    context: trt.tensorrt.IExecutionContext,
-) -> List[List[int]]:
-    """Gather the output shapes from an ICudaEngine.
-
-    Args:
-        engine: TensorRT engine object.
-        context: Current execution context for the inference.
-
-    Returns:
-        List of shapes of outputs which are list of integers.
-    """
-    assert context.all_binding_shapes_specified
-    assert context.all_shape_inputs_specified
-
-    output_shapes = []
-    for binding_index in range(engine.num_bindings):
-        if not engine.binding_is_input(binding_index):
-            shape = context.get_binding_shape(binding_index)
-            output_shapes.append(shape)
-    return output_shapes
-
-
-def validate_precision(precision: str) -> bool:
-    """Returns whether an input precision is in supported set."""
-    return precision in [TRTMode.FLOAT32, TRTMode.FLOAT16, TRTMode.INT8]
-
-
-def calib_data_generator(onnx_bytes: bytes, input_tensors: List[np.ndarray]):
-    """The calibation data generator that yields calibration feed_dict to tensorrt."""
-    input_names = get_onnx_input_names(onnx.load_from_string(onnx_bytes))
-
-    batch_size = get_batch_size(onnx.load_from_string(onnx_bytes))
-    if not batch_size or batch_size <= 0:
-        batch_size = 1
-    # If input tensor batch % batch_size != 0, we don't use all input tensors for calibration.
-    num_batches = int(input_tensors[0].shape[0] / batch_size)
-
-    for i in range(num_batches):
-        feed_dict = {}
-        tensor_batch_dim_index = i * batch_size
-        for idx, input_name in enumerate(input_names):
-            feed_dict[input_name] = input_tensors[idx][
-                tensor_batch_dim_index : (tensor_batch_dim_index + batch_size)
-            ]
-        yield feed_dict
-
-
-def convert_trt_dtype_to_torch(trt_dtype: trt.tensorrt.DataType) -> torch.dtype:
-    """Convert TensorRT data type to torch data type."""
-    trt_to_torch_dtype_map = {
-        trt.DataType.FLOAT: torch.float32,
-        trt.DataType.HALF: torch.float16,
-        trt.DataType.BF16: torch.bfloat16,
-        trt.DataType.INT8: torch.int8,
-        trt.DataType.INT32: torch.int32,
-        trt.DataType.INT64: torch.int64,
-        trt.DataType.BOOL: torch.bool,
-    }
-
-    assert trt_dtype in trt_to_torch_dtype_map, f"Unsupported TensorRT data type: {trt_dtype}"
-    return trt_to_torch_dtype_map[trt_dtype]
-
-
-def prepend_hash_to_bytes(engine_bytes: bytes) -> bytes:
-    """Prepend the engine bytes with the SHA256 hash of the engine bytes
-    This has will serve as a unique identifier for the engine and will be used to manage
-    TRTSessions in the TRTClient."""
-    hash_object = hashlib.sha256(engine_bytes)
-    hash_object.update(engine_bytes)
-    hash_bytes = hash_object.digest()
-    engine_bytes = hash_bytes + engine_bytes
-    return engine_bytes
-
-
-def convert_shape_to_string(shape: Dict[str, List]) -> str:
-    """Convert a shape dictionary to a string.
-    For example, if the shape is:
-        {
-            "input": [1, 3, 224, 224],
-            "output": [1, 1000]
-        }.
-    The output string will be:
-        input:1x3x244x244,output:1x1000
-    """
-    result = ""
-    for key, value in shape.items():
-        result += f"{key}:{'x'.join(map(str, value))},"
-    return result[:-1]
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+import hashlib
+import logging
+from typing import Dict, List
+
+import numpy as np
+import onnx
+import tensorrt as trt
+import torch
+
+from modelopt.onnx.utils import get_batch_size
+from modelopt.onnx.utils import get_input_names as get_onnx_input_names
+
+from .constants import (
+    TENSORRT_8_MAJOR_VERSION,
+    TRTMode,
+)
+
+
+def is_trt8():
+    return int(trt.__version__.split(".", maxsplit=1)[0]) == TENSORRT_8_MAJOR_VERSION
+
+
+class HostDeviceMem(object):
+    """Simple helper data class to hold host and device memory pointers."""
+
+    def __init__(self, host_mem, device_mem, dtype):
+        self.host = host_mem
+        self.device = device_mem
+        self.dtype = dtype
+
+    def __str__(self):
+        return (
+            "Host:\n"
+            + str(self.host)
+            + "\nDevice:\n"
+            + str(self.device)
+            + "\nType:\n"
+            + str(self.dtype)
+        )
+
+    def __repr__(self):
+        return self.__str__()
+
+    def __del__(self):
+        del self.host
+        del self.device
+
+
+def get_engine_bytes(engine: trt.tensorrt.ICudaEngine) -> bytes:
+    """Return serialized TensorRT engine bytes."""
+    return bytearray(engine.serialize())
+
+
+def load_engine(buffer: bytes, log_level: int = trt.Logger.ERROR) -> trt.tensorrt.ICudaEngine:
+    """Load a TensorRT engine from engine data and return."""
+    try:
+        trt_logger = trt.Logger(log_level)
+        with trt.Runtime(trt_logger) as runtime:
+            return runtime.deserialize_cuda_engine(buffer), ""
+    except Exception as e:
+        logging.exception(str(e))
+        return None, str(e)
+
+
+def get_input_names(engine: trt.tensorrt.ICudaEngine) -> List[str]:
+    """Gather the input names from an ICudaEngine.
+
+    Args:
+        engine: TensorRT engine object.
+
+    Returns:
+        List of engine input names.
+    """
+    input_names = []
+    for binding_index in range(engine.num_bindings):
+        if engine.binding_is_input(binding_index):
+            input_names.append(engine.get_binding_name(binding_index))
+    return input_names
+
+
+def get_output_names(engine: trt.tensorrt.ICudaEngine) -> List[str]:
+    """Gather the output names from an ICudaEngine.
+
+    Args:
+        engine: TensorRT engine object.
+
+    Returns:
+        List of engine output names.
+    """
+    output_names = []
+    for binding_index in range(engine.num_bindings):
+        if not engine.binding_is_input(binding_index):
+            output_names.append(engine.get_binding_name(binding_index))
+    return output_names
+
+
+def get_output_shapes(
+    engine: trt.tensorrt.ICudaEngine,
+    context: trt.tensorrt.IExecutionContext,
+) -> List[List[int]]:
+    """Gather the output shapes from an ICudaEngine.
+
+    Args:
+        engine: TensorRT engine object.
+        context: Current execution context for the inference.
+
+    Returns:
+        List of shapes of outputs which are list of integers.
+    """
+    assert context.all_binding_shapes_specified
+    assert context.all_shape_inputs_specified
+
+    output_shapes = []
+    for binding_index in range(engine.num_bindings):
+        if not engine.binding_is_input(binding_index):
+            shape = context.get_binding_shape(binding_index)
+            output_shapes.append(shape)
+    return output_shapes
+
+
+def validate_precision(precision: str) -> bool:
+    """Returns whether an input precision is in supported set."""
+    return precision in [TRTMode.FLOAT32, TRTMode.FLOAT16, TRTMode.INT8]
+
+
+def calib_data_generator(onnx_bytes: bytes, input_tensors: List[np.ndarray]):
+    """The calibation data generator that yields calibration feed_dict to tensorrt."""
+    input_names = get_onnx_input_names(onnx.load_from_string(onnx_bytes))
+
+    batch_size = get_batch_size(onnx.load_from_string(onnx_bytes))
+    if not batch_size or batch_size <= 0:
+        batch_size = 1
+    # If input tensor batch % batch_size != 0, we don't use all input tensors for calibration.
+    num_batches = int(input_tensors[0].shape[0] / batch_size)
+
+    for i in range(num_batches):
+        feed_dict = {}
+        tensor_batch_dim_index = i * batch_size
+        for idx, input_name in enumerate(input_names):
+            feed_dict[input_name] = input_tensors[idx][
+                tensor_batch_dim_index : (tensor_batch_dim_index + batch_size)
+            ]
+        yield feed_dict
+
+
+def convert_trt_dtype_to_torch(trt_dtype: trt.tensorrt.DataType) -> torch.dtype:
+    """Convert TensorRT data type to torch data type."""
+    trt_to_torch_dtype_map = {
+        trt.DataType.FLOAT: torch.float32,
+        trt.DataType.HALF: torch.float16,
+        trt.DataType.BF16: torch.bfloat16,
+        trt.DataType.INT8: torch.int8,
+        trt.DataType.INT32: torch.int32,
+        trt.DataType.INT64: torch.int64,
+        trt.DataType.BOOL: torch.bool,
+    }
+
+    assert trt_dtype in trt_to_torch_dtype_map, f"Unsupported TensorRT data type: {trt_dtype}"
+    return trt_to_torch_dtype_map[trt_dtype]
+
+
+def prepend_hash_to_bytes(engine_bytes: bytes) -> bytes:
+    """Prepend the engine bytes with the SHA256 hash of the engine bytes
+    This has will serve as a unique identifier for the engine and will be used to manage
+    TRTSessions in the TRTClient."""
+    hash_object = hashlib.sha256(engine_bytes)
+    hash_object.update(engine_bytes)
+    hash_bytes = hash_object.digest()
+    engine_bytes = hash_bytes + engine_bytes
+    return engine_bytes
+
+
+def convert_shape_to_string(shape: Dict[str, List]) -> str:
+    """Convert a shape dictionary to a string.
+    For example, if the shape is:
+        {
+            "input": [1, 3, 224, 224],
+            "output": [1, 1000]
+        }.
+    The output string will be:
+        input:1x3x244x244,output:1x1000
+    """
+    result = ""
+    for key, value in shape.items():
+        result += f"{key}:{'x'.join(map(str, value))},"
+    return result[:-1]
```

## modelopt/torch/_deploy/utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utilities for modelopt.torch._deploy."""
-
-from .torch_onnx import *
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utilities for modelopt.torch._deploy."""
+
+from .torch_onnx import *
```

## modelopt/torch/_deploy/utils/onnx_optimizer.py

 * *Ordering differences only*

```diff
@@ -1,112 +1,112 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility to optimze onnx graphs."""
-import os
-import tempfile
-
-import onnx
-import onnx_graphsurgeon as gs
-
-
-class Optimizer:
-    """Optimizer for onnx graphs."""
-
-    def __init__(self, onnx_graph, verbose=False):
-        """Initializes optimizer."""
-        self.graph = gs.import_onnx(onnx_graph)
-        self.verbose = verbose
-
-    def info(self, prefix):
-        """Prints the graph information."""
-        if self.verbose:
-            print(
-                f"{prefix} .. {len(self.graph.nodes)} nodes,"
-                f" {len(self.graph.tensors().keys())} tensors,"
-                f" {len(self.graph.inputs)} inputs, {len(self.graph.outputs)} outputs"
-            )
-
-    def cleanup(self, return_onnx=False):
-        """Cleans the onnx graph.
-
-        Args:
-            return_onnx (bool): If True, returns the onnx graph.
-
-        Returns:
-            onnx_graph: The cleaned onnx graph.
-        """
-        self.graph.cleanup().toposort()
-        if return_onnx:
-            return gs.export_onnx(self.graph)
-
-    def select_outputs(self, keep, names=None):
-        """Selects the output nodes."""
-        self.graph.outputs = [self.graph.outputs[o] for o in keep]
-        if names:
-            for i, name in enumerate(names):
-                self.graph.outputs[i].name = name
-
-    def infer_shapes(self, return_onnx=False):
-        """Infers shapes of the onnx graph."""
-        onnx_graph = gs.export_onnx(self.graph)
-        if onnx_graph.ByteSize() > 2147483648:
-            temp_dir = tempfile.TemporaryDirectory().name
-            os.makedirs(temp_dir, exist_ok=True)
-            onnx_orig_path = os.path.join(temp_dir, "model.onnx")
-            onnx_inferred_path = os.path.join(temp_dir, "inferred.onnx")
-            onnx.save_model(
-                onnx_graph,
-                onnx_orig_path,
-                save_as_external_data=True,
-                all_tensors_to_one_file=True,
-                convert_attribute=False,
-            )
-            onnx.shape_inference.infer_shapes_path(onnx_orig_path, onnx_inferred_path)
-            onnx_graph = onnx.load(onnx_inferred_path)
-        else:
-            onnx_graph = onnx.shape_inference.infer_shapes(onnx_graph)
-
-        if return_onnx:
-            return onnx_graph
-
-    # TODO: Move this fuctionality to the diffusion runner as it is specific to CLIP.
-    def clip_add_hidden_states(self, return_onnx=False):
-        """Adds hidden states to the CLIP model graph."""
-        hidden_layers = -1
-        onnx_graph = gs.export_onnx(self.graph)
-        for i in range(len(onnx_graph.graph.node)):
-            for j in range(len(onnx_graph.graph.node[i].output)):
-                name = onnx_graph.graph.node[i].output[j]
-                if "layers" in name:
-                    hidden_layers = max(int(name.split(".")[1].split("/")[0]), hidden_layers)
-        for i in range(len(onnx_graph.graph.node)):
-            for j in range(len(onnx_graph.graph.node[i].output)):
-                if onnx_graph.graph.node[i].output[
-                    j
-                ] == "/text_model/encoder/layers.{}/Add_1_output_0".format(hidden_layers - 1):
-                    onnx_graph.graph.node[i].output[j] = "hidden_states"
-            for j in range(len(onnx_graph.graph.node[i].input)):
-                if onnx_graph.graph.node[i].input[
-                    j
-                ] == "/text_model/encoder/layers.{}/Add_1_output_0".format(hidden_layers - 1):
-                    onnx_graph.graph.node[i].input[j] = "hidden_states"
-        if return_onnx:
-            return onnx_graph
-
-    def fold_constants(self, return_onnx=False):
-        """Folds constants in the onnx graph for multiple iterations till prefolded nodes == post folded nodes."""
-        prefold_num_nodes = len(self.graph.nodes)
-        post_fold_num_nodes = -1
-        while prefold_num_nodes != post_fold_num_nodes:
-            prefold_num_nodes = len(self.graph.nodes)
-            self.graph.fold_constants()
-            post_fold_num_nodes = len(self.graph.nodes)
-        if return_onnx:
-            return gs.export_onnx(self.graph)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility to optimze onnx graphs."""
+import os
+import tempfile
+
+import onnx
+import onnx_graphsurgeon as gs
+
+
+class Optimizer:
+    """Optimizer for onnx graphs."""
+
+    def __init__(self, onnx_graph, verbose=False):
+        """Initializes optimizer."""
+        self.graph = gs.import_onnx(onnx_graph)
+        self.verbose = verbose
+
+    def info(self, prefix):
+        """Prints the graph information."""
+        if self.verbose:
+            print(
+                f"{prefix} .. {len(self.graph.nodes)} nodes,"
+                f" {len(self.graph.tensors().keys())} tensors,"
+                f" {len(self.graph.inputs)} inputs, {len(self.graph.outputs)} outputs"
+            )
+
+    def cleanup(self, return_onnx=False):
+        """Cleans the onnx graph.
+
+        Args:
+            return_onnx (bool): If True, returns the onnx graph.
+
+        Returns:
+            onnx_graph: The cleaned onnx graph.
+        """
+        self.graph.cleanup().toposort()
+        if return_onnx:
+            return gs.export_onnx(self.graph)
+
+    def select_outputs(self, keep, names=None):
+        """Selects the output nodes."""
+        self.graph.outputs = [self.graph.outputs[o] for o in keep]
+        if names:
+            for i, name in enumerate(names):
+                self.graph.outputs[i].name = name
+
+    def infer_shapes(self, return_onnx=False):
+        """Infers shapes of the onnx graph."""
+        onnx_graph = gs.export_onnx(self.graph)
+        if onnx_graph.ByteSize() > 2147483648:
+            temp_dir = tempfile.TemporaryDirectory().name
+            os.makedirs(temp_dir, exist_ok=True)
+            onnx_orig_path = os.path.join(temp_dir, "model.onnx")
+            onnx_inferred_path = os.path.join(temp_dir, "inferred.onnx")
+            onnx.save_model(
+                onnx_graph,
+                onnx_orig_path,
+                save_as_external_data=True,
+                all_tensors_to_one_file=True,
+                convert_attribute=False,
+            )
+            onnx.shape_inference.infer_shapes_path(onnx_orig_path, onnx_inferred_path)
+            onnx_graph = onnx.load(onnx_inferred_path)
+        else:
+            onnx_graph = onnx.shape_inference.infer_shapes(onnx_graph)
+
+        if return_onnx:
+            return onnx_graph
+
+    # TODO: Move this fuctionality to the diffusion runner as it is specific to CLIP.
+    def clip_add_hidden_states(self, return_onnx=False):
+        """Adds hidden states to the CLIP model graph."""
+        hidden_layers = -1
+        onnx_graph = gs.export_onnx(self.graph)
+        for i in range(len(onnx_graph.graph.node)):
+            for j in range(len(onnx_graph.graph.node[i].output)):
+                name = onnx_graph.graph.node[i].output[j]
+                if "layers" in name:
+                    hidden_layers = max(int(name.split(".")[1].split("/")[0]), hidden_layers)
+        for i in range(len(onnx_graph.graph.node)):
+            for j in range(len(onnx_graph.graph.node[i].output)):
+                if onnx_graph.graph.node[i].output[
+                    j
+                ] == "/text_model/encoder/layers.{}/Add_1_output_0".format(hidden_layers - 1):
+                    onnx_graph.graph.node[i].output[j] = "hidden_states"
+            for j in range(len(onnx_graph.graph.node[i].input)):
+                if onnx_graph.graph.node[i].input[
+                    j
+                ] == "/text_model/encoder/layers.{}/Add_1_output_0".format(hidden_layers - 1):
+                    onnx_graph.graph.node[i].input[j] = "hidden_states"
+        if return_onnx:
+            return onnx_graph
+
+    def fold_constants(self, return_onnx=False):
+        """Folds constants in the onnx graph for multiple iterations till prefolded nodes == post folded nodes."""
+        prefold_num_nodes = len(self.graph.nodes)
+        post_fold_num_nodes = -1
+        while prefold_num_nodes != post_fold_num_nodes:
+            prefold_num_nodes = len(self.graph.nodes)
+            self.graph.fold_constants()
+            post_fold_num_nodes = len(self.graph.nodes)
+        if return_onnx:
+            return gs.export_onnx(self.graph)
```

## modelopt/torch/_deploy/utils/torch_onnx.py

 * *Ordering differences only*

```diff
@@ -1,480 +1,480 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions related to Onnx."""
-import inspect
-import os
-import shutil
-from typing import Any, Dict, List, Tuple, Union
-
-import cloudpickle
-import onnx
-import torch
-import torch.nn as nn
-from onnx import ModelProto
-from torch.nn.parallel import DataParallel, DistributedDataParallel
-
-from modelopt.onnx.utils import (
-    get_input_names,
-    get_input_shapes,
-    get_node_names,
-    get_output_names,
-    get_output_shapes,
-)
-from modelopt.torch.utils import flatten_tree, standardize_named_model_args
-from modelopt.torch.utils._pytree import TreeSpec
-
-from .._runtime.common import write_bytes
-from .onnx_optimizer import Optimizer
-
-ModelMetadata = Dict[str, Any]
-ModelType = Any
-ValueInfoType = Any
-
-# a few constants...
-DEFAULT_ONNX_OPSET = 13
-ONNX_EXPORT_IN_PREFIX = "in"
-ONNX_EXPORT_OUT_PREFIX = "out"
-TWO_GB = 2147483648
-
-
-class OnnxBytes:
-    """A class to save and load onnx models as bytes."""
-
-    def __init__(self, onnx_load_path: str, external_data_format: bool = False) -> None:
-        """Loads the model from the specified path.
-
-        If the model is loaded without external data format, then it is saved as a dictionary where
-        the key is the model name and the value is the model bytes.
-        If the model is loaded with external data format, then the model is saved as a dictionary
-        where the keys include all the file names in the model directory and the value are the corresponding file bytes.
-        For external data format, we assume that the external data for the model is saved in the same directory
-        as the model file.
-
-        Args:
-            onnx_load_path: The path to load the .onnx model file.
-            external_data_format: If True, the onnx model is loaded from the external data format.
-        """
-        self.onnx_model = {}
-        self.model_name = ""
-        print("Loading onnx model from path:", onnx_load_path)
-        if external_data_format:
-            onnx_load_path = os.path.dirname(onnx_load_path)
-            for onnx_model_file in os.listdir(onnx_load_path):
-                with open(os.path.join(onnx_load_path, onnx_model_file), "rb") as f:
-                    self.onnx_model[onnx_model_file] = f.read()
-                if onnx_model_file.endswith(".onnx"):
-                    if self.model_name != "":
-                        raise ValueError("Multiple onnx files found in the directory")
-                    self.model_name = onnx_model_file.replace(".onnx", "")
-        else:
-            onnx_model_file = os.path.basename(onnx_load_path)
-            if not onnx_model_file.endswith(".onnx"):
-                raise ValueError("The file should be a .onnx file")
-            with open(onnx_load_path, "rb") as f:
-                self.onnx_model[onnx_model_file] = f.read()
-            self.model_name = onnx_model_file.replace(".onnx", "")
-
-    def write_to_disk(self, onnx_save_path: str) -> None:
-        """Writes the onnx model to the specified path."""
-        if os.path.exists(onnx_save_path):
-            print(f"Removing existing directory: {onnx_save_path}")
-            shutil.rmtree(onnx_save_path)
-        os.makedirs(onnx_save_path)
-        print("Writing onnx model to path:", onnx_save_path)
-        for onnx_model_file, onnx_model_bytes in self.onnx_model.items():
-            with open(os.path.join(onnx_save_path, onnx_model_file), "wb") as f:
-                f.write(onnx_model_bytes)
-
-    def to_bytes(self) -> bytes:
-        """Returns the bytes of the object."""
-        return cloudpickle.dumps(self)
-
-    def get_onnx_model_file_bytes(self) -> bytes:
-        """Returns the bytes of the onnx model file."""
-        return self.onnx_model[self.model_name + ".onnx"]
-
-    @classmethod
-    def from_bytes(cls, onnx_bytes: bytes) -> "OnnxBytes":
-        """Returns the OnnxBytes object from the bytes."""
-        return cloudpickle.loads(onnx_bytes)
-
-
-def _to_expected_onnx_type(val: Any) -> Any:
-    """Convert the given value to the expected onnx type.
-
-    During the onnx export process, plain numeric types (floats and ints) are converted to torch
-    tensors. This function pre-converts the given val to a tensor in case val is a int or float for
-    easier handling of such input values during the onnx export process.
-    """
-    if isinstance(val, (int, float)):
-        return torch.tensor(val).to(type(val))
-    return val
-
-
-def generate_onnx_input(model_metadata: ModelMetadata, input: Union[Any, Tuple]) -> Dict[str, Any]:
-    """Generate input for onnx model from model's forward signature and provided input.
-
-    Args:
-        model_metadata: The model's metadata.
-        input: A tuple of args/kwargs or torch.Tensor feed into the model's ``forward()`` method,
-            see :meth:`standardize_model_args() <modelopt.torch.utils.network.standardize_model_args>`
-            for more info on the convention.
-
-    Returns:
-        Args flattened into one dictionary with serialized keys compatible with provided onnx.
-
-    .. note::
-
-        This function performs a sanity check on the provided input data to filter out args that
-        are constants (instead of input nodes) in the onnx graph.
-
-
-    Some more relevant background of why we want to flatten the input pytree here:
-
-        * In the onnx export process, nested python data structures (like nested lists, tuples,
-            dictionaries) are being recursed into until leaf objects corresponding to tensors are
-            encountered.
-
-        * This is used to flatten the input in an onnx model to a list of tensors.
-
-        * However, this is a fairly complex process for the user to understand in case their models
-            takes a nested data structure. They have to understand how to manually flatten the data
-            structure in the *correct* order in order for them to run inference on a device_model or
-            onnx model.
-
-        * With this function this additional complexity can be abstracted away from the user.
-
-        * Example: if the original model took ``[x, {"y":y, "z" : [z1,z2]}]`` they can still provide
-            this nested data structure instead of the expected onnx input list of ``[x, y, z1, z2]``
-            --> flattening and unflattering is handled internally.
-    """
-    # get named args and set of params where we added default values
-    named_args, args_with_default = standardize_named_model_args(model_metadata["signature"], input)
-
-    # retrieve onnx input names
-    onnx_input_names = model_metadata["input_onnx_names"]
-    input_none_names = model_metadata["input_none_names"]
-
-    # capture flattened names of args from default values
-    named_default_args = {k: v for k, v in named_args.items() if k in args_with_default}
-    _, tree_spec_default_args = flatten_tree(named_default_args, prefix=ONNX_EXPORT_IN_PREFIX)
-
-    # capture flattened args without default args that do not appear in onnx graph
-    values, tree_spec = flatten_tree(named_args, prefix=ONNX_EXPORT_IN_PREFIX)
-    flat_kv = {k: v for k, v in zip(tree_spec.names, values)}
-
-    # We wanna consider four types of flattened args:
-    # 1. Args that appear in the onnx graph
-    # 2. Args that are not their default value
-    # 3. Args that were provided as None during conversion but are not None right now
-    # 4. Args that were provided as None during conversion and are None right now
-
-    args_in_onnx = {k for k in flat_kv if k in onnx_input_names}
-    args_not_default = {k for k in flat_kv if k not in tree_spec_default_args.names}
-    args_not_none = {k for k, v in flat_kv.items() if k in input_none_names and v is not None}
-    args_none = {k for k, v in flat_kv.items() if k in input_none_names and v is None}
-
-    # identify unexpected args from these 4 types
-    unexpected_args = ((args_not_default - args_none) | args_not_none) - args_in_onnx
-    if unexpected_args:
-        raise ValueError(
-            "The following args were provided that do not appear in the onnx graph of your model "
-            "since they are treated as constants in the onnx graph:"
-            + "\t\n".join(unexpected_args)
-            + "\nConsider removing these args from your input that are constants in the onnx model "
-            "or recompiling your onnx model with new constant values!"
-        )
-
-    # return the args that are relevant for the onnx graph in the right type
-    return {k: _to_expected_onnx_type(v) for k, v in flat_kv.items() if k in args_in_onnx}
-
-
-def optimize(name, onnx_graph, verbose=False):
-    """Optimizes onnx graph."""
-    opt = Optimizer(onnx_graph, verbose=verbose)
-    opt.info(name + ": original")
-    opt.cleanup()
-    opt.info(name + ": cleanup")
-    # TODO: fold constants is not working for some models from deploy_models(NestedOutModel, ArgsKwargsModel1)
-    # opt.fold_constants()
-    # opt.info(name + ": fold_constants")
-    onnx_graph = opt.infer_shapes(return_onnx=True)
-    opt.info(name + ": shape inference")
-    return onnx_graph
-
-
-def get_onnx_bytes_and_metadata(
-    model: nn.Module,
-    dummy_input: Union[Any, Tuple],
-    onnx_load_path: str = "",
-    remove_exported_model: bool = True,
-    **kwargs,
-) -> Tuple[bytes, ModelMetadata]:
-    """Get onnx model in bytes from input pytorch model together with the input/output of model.
-
-    Arguments:
-        model: PyTorch model to export to onnx.
-        dummy_input: A tuple of args/kwargs or torch.Tensor, see
-            `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
-            for more info on the convention.
-        onnx_load_path: The path to load the onnx model.
-        remove_exported_model: If True, the onnx model will be cleared from the disk after the
-            export process
-        **kwargs: Additional arguments to pass, e.g., ``onnx_opset``.
-
-    Returns:
-        bytes: Onnx model in bytes.
-        ModelMetadata: The model's meta data.
-
-    Raises:
-        ValueError: If nn.Module is not passed as model.
-    """
-    if not isinstance(model, nn.Module):
-        raise ValueError("Only PyTorch model compilation is supported.")
-
-    # unwrap DDP and DP models
-    if isinstance(model, (DataParallel, DistributedDataParallel)):
-        model = model.module
-
-    # process onnx_opset
-    onnx_opset = int(kwargs.get("onnx_opset", DEFAULT_ONNX_OPSET))
-
-    # Standardize model args and also tensorize them so they also appear in the onnx graph!
-    # Floats/ints are tensorized when they are provided, but not tensorized when they are not
-    # provided which is somewhat inconsistent (we always tensorize them!)
-    named_args, _ = standardize_named_model_args(model, dummy_input)
-    named_args = {k: _to_expected_onnx_type(v) for k, v in named_args.items()}
-
-    # Also standardize dummy_input again so we can use it
-    dummy_input = tuple(named_args.values())
-    if dummy_input and isinstance(dummy_input[-1], dict):
-        dummy_input = dummy_input + ({},)  # we need to add an extra dict for the fake kwargs!
-
-    # Get input tree spec, see generate_onnx_input for more info as well on this
-    flat_input, tree_spec_input = flatten_tree(named_args, prefix=ONNX_EXPORT_IN_PREFIX)
-
-    # input names are the names of the flattened input tree spec but without None values
-    input_names = [k for k, v in zip(tree_spec_input.names, flat_input) if v is not None]
-
-    # we also want to record the input names that are None so we can remove them from the input
-    # during inference.
-    input_none_names = list(set(tree_spec_input.names) - set(input_names))
-
-    # Get output once (we export in eval mode - so also using eval mode here!)
-    is_training = model.training
-    model.eval()
-    output = model(*named_args.values())
-    model.train(is_training)
-
-    # Get output tree spec
-    flat_output, tree_spec_output = flatten_tree(output, prefix=ONNX_EXPORT_OUT_PREFIX)
-
-    # output names are the names of the flattened input tree spec but without None values
-    output_names = [k for k, v in zip(tree_spec_output.names, flat_output) if v is not None]
-
-    model_name = model.__class__.__name__
-    onnx_build_folder = "./build/onnx/"
-    onnx_path = os.path.join(onnx_build_folder, model_name)
-    os.makedirs(onnx_path, exist_ok=True)
-    onnx_save_path = os.path.join(onnx_path, f"{model_name}.onnx")
-
-    # If the onnx_load path is specified by the user or if an onnx model exists at the default path
-    # then the model is loaded from this path and returned along with the metadata
-    if os.path.exists(onnx_save_path):
-        print(f"Overriding onnx load path to {onnx_save_path}")
-        onnx_load_path = onnx_save_path
-
-    if onnx_load_path != "":
-        onnx_model = OnnxBytes(onnx_load_path)
-        onnx_model_graph = onnx.load(os.path.join(onnx_load_path))
-        model_metadata = create_model_metadata(
-            tree_spec_input, tree_spec_output, input_none_names, onnx_model_graph, model
-        )
-        return onnx_model.to_bytes(), model_metadata
-
-    # Export onnx model from pytorch model
-    # As the maximum size of protobuf is 2GB, we cannot use io.BytesIO() buffer during export.
-    with torch.inference_mode():
-        torch.onnx.export(
-            model,
-            dummy_input,
-            onnx_save_path,
-            input_names=input_names,
-            output_names=output_names,
-            opset_version=onnx_opset,
-        )
-
-    # Check that export worked
-    assert len(os.listdir(onnx_path)) > 0, "Torch to onnx export failed."
-
-    # Load the onnx graph for optimizaiton
-    onnx_graph = onnx.load(onnx_save_path)
-
-    # Optimize the onnx graph
-    onnx_opt_graph = optimize(model.__class__.__name__, onnx_graph)
-
-    # Remove training_mode attribute from BatchNormalization nodes
-    onnx_opt_graph = remove_node_training_mode(onnx_opt_graph, "BatchNormalization")
-
-    model_metadata = create_model_metadata(
-        tree_spec_input, tree_spec_output, input_none_names, onnx_opt_graph, model
-    )
-
-    # Change the ONNX IR version to 9 to be compatible with ONNXRuntime
-    onnx_opt_graph.ir_version = 9
-
-    # If the onnx model is larger than 2GB, we need to save it in multiple files using the
-    # external data format.
-    if onnx_opt_graph.ByteSize() > TWO_GB:
-        onnx.save_model(
-            onnx_opt_graph,
-            onnx_save_path,
-            save_as_external_data=True,
-            all_tensors_to_one_file=True,
-            convert_attribute=False,
-        )
-    else:
-        onnx.save(onnx_opt_graph, onnx_save_path)
-    onnx_bytes = OnnxBytes(onnx_save_path)
-
-    if remove_exported_model:
-        shutil.rmtree(os.path.dirname(onnx_build_folder))
-    return onnx_bytes.to_bytes(), model_metadata
-
-
-def create_onnx_model_dict(
-    onnx_load_path: str = None, model_metadata: ModelMetadata = None, model_dict: dict = None
-) -> Tuple[bytes, ModelMetadata]:
-    """Creates onnx model dictionary and model metadata.
-
-    Args:
-        onnx_load_path: The path to load the onnx model.
-        model_metadata: Metadata to be stored in the DeviceModel.
-        model_dict: A dictionaty containing the Pytorch model and the input and output treespec for the model.
-
-    Returns:
-        A tuple of onnx model dictionary and model metadata.
-    """
-    onnx_model_dict = {"onnx_load_path": onnx_load_path}
-    onnx_model = {}
-    for onnx_model_file in os.listdir(onnx_load_path):
-        with open(os.path.join(onnx_load_path, onnx_model_file), "rb") as f:
-            onnx_model[onnx_model_file] = f.read()
-        if onnx_model_file.endswith(".onnx") and model_metadata is None:
-            onnx_model_file_path = os.path.join(onnx_load_path, onnx_model_file)
-            onnx_graph = onnx.load(onnx_model_file_path)
-            onnx_input_names = [input.name for input in onnx_graph.graph.input]
-            torch_input_names = model_dict["tree_spec_input"].names
-            assert set(onnx_input_names).issubset(
-                set(torch_input_names)
-            ), "One or more inputs in the onnx model are not present in the torch model."
-    onnx_model_dict["onnx_model"] = onnx_model
-    if model_metadata is None:
-        model_metadata = create_model_metadata(
-            model_dict["tree_spec_input"],
-            model_dict["tree_spec_output"],
-            model_dict["input_none_names"],
-            onnx_graph,
-            model_dict["model"],
-        )
-    return cloudpickle.dumps(onnx_model_dict), model_metadata
-
-
-def create_model_metadata(
-    tree_spec_input: TreeSpec,
-    tree_spec_output: TreeSpec,
-    input_none_names: List[str],
-    onnx_graph: ModelProto,
-    model: nn.Module,
-) -> ModelMetadata:
-    """Create model metadata from the given input.
-
-    Args:
-        tree_spec_input: pytree spec describing the structure of the pytree for the model input.
-        tree_spec_output: pytree spec describing the structure of the pytree for the model output.
-        input_none_names: List of input names with values that are None.
-        onnx_opt_graph: Graph of the onnx model.
-        model: Pytorch model.
-
-    Returns:
-        ModelMetadata: The DeviceModel metadata.
-    """
-    return {
-        "input_tree_spec": tree_spec_input,
-        "input_shapes": get_input_shapes(onnx_graph),
-        "input_onnx_names": get_input_names(onnx_graph),
-        "input_none_names": input_none_names,
-        "output_tree_spec": tree_spec_output,
-        "output_shapes": get_output_shapes(onnx_graph),
-        "output_onnx_names": get_output_names(onnx_graph),
-        "signature": inspect.signature(model.forward),
-        "onnx_node_names": get_node_names(onnx_graph),
-        "is_bytes_pickled": onnx_graph.ByteSize() > TWO_GB,
-    }
-
-
-def write_onnx_bytes(onnx_model_bytes: Union[bytes, dict], onnx_save_path: str) -> Tuple[str, str]:
-    """Write onnx bytes to the specified path.
-
-    Args:
-        onnx_model_bytes: Can be one of the following:
-            1. A pickled dictionary of onnx model files. The keys of this dictionary are file names and\
-            the values are the corresponding bytes.
-            2. A single onnx model bytes.
-        onnx_save_path: The onnx path to save the model.
-
-    Returns:
-        The path to the onnx model.
-        Name of the onnx model.
-    """
-    if isinstance(onnx_model_bytes, dict):
-        if not os.path.exists(onnx_save_path):
-            os.makedirs(onnx_save_path)
-        for key in onnx_model_bytes["onnx_model"]:
-            write_bytes(onnx_model_bytes["onnx_model"][key], os.path.join(onnx_save_path, key))
-        onnx_model_file = [s for s in onnx_model_bytes["onnx_model"].keys() if s.endswith(".onnx")]
-        assert len(onnx_model_file) == 1, "There should be only one onnx model file"
-        model_name = onnx_model_file[0].split(".")[0]
-        return os.path.join(onnx_save_path, onnx_model_file[0]), model_name
-    write_bytes(onnx_model_bytes, os.path.join(onnx_save_path, "model.onnx"))
-    return os.path.join(onnx_save_path, "model.onnx"), ""
-
-
-def get_onnx_bytes(*args, **kwargs):
-    """Return onnx bytes only.
-
-    See ``get_onnx_bytes_and_metadata()`` for more info.
-    """
-    onnx_bytes = get_onnx_bytes_and_metadata(*args, **kwargs)[0]
-    onnx_bytes_obj = OnnxBytes.from_bytes(onnx_bytes)
-    return onnx_bytes_obj.onnx_model[f"{onnx_bytes_obj.model_name}.onnx"]
-
-
-def remove_node_training_mode(onnx_model: ModelProto, node_op_type: str) -> ModelProto:
-    """Remove training_mode attribute from selected node type.
-
-    Args:
-        onnx_model: The onnx model.
-        node_op_type: The node type to remove training_mode attribute from.
-
-    Returns:
-        The onnx model with the training_mode attribute removed.
-    """
-    for node in onnx_model.graph.node:
-        if node.op_type == node_op_type:
-            for attribute in node.attribute:
-                if attribute.name == "training_mode":
-                    if attribute.i == 1:
-                        node.output.remove(node.output[1])
-                        node.output.remove(node.output[1])
-                    attribute.i = 0
-
-    return onnx_model
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions related to Onnx."""
+import inspect
+import os
+import shutil
+from typing import Any, Dict, List, Tuple, Union
+
+import cloudpickle
+import onnx
+import torch
+import torch.nn as nn
+from onnx import ModelProto
+from torch.nn.parallel import DataParallel, DistributedDataParallel
+
+from modelopt.onnx.utils import (
+    get_input_names,
+    get_input_shapes,
+    get_node_names,
+    get_output_names,
+    get_output_shapes,
+)
+from modelopt.torch.utils import flatten_tree, standardize_named_model_args
+from modelopt.torch.utils._pytree import TreeSpec
+
+from .._runtime.common import write_bytes
+from .onnx_optimizer import Optimizer
+
+ModelMetadata = Dict[str, Any]
+ModelType = Any
+ValueInfoType = Any
+
+# a few constants...
+DEFAULT_ONNX_OPSET = 13
+ONNX_EXPORT_IN_PREFIX = "in"
+ONNX_EXPORT_OUT_PREFIX = "out"
+TWO_GB = 2147483648
+
+
+class OnnxBytes:
+    """A class to save and load onnx models as bytes."""
+
+    def __init__(self, onnx_load_path: str, external_data_format: bool = False) -> None:
+        """Loads the model from the specified path.
+
+        If the model is loaded without external data format, then it is saved as a dictionary where
+        the key is the model name and the value is the model bytes.
+        If the model is loaded with external data format, then the model is saved as a dictionary
+        where the keys include all the file names in the model directory and the value are the corresponding file bytes.
+        For external data format, we assume that the external data for the model is saved in the same directory
+        as the model file.
+
+        Args:
+            onnx_load_path: The path to load the .onnx model file.
+            external_data_format: If True, the onnx model is loaded from the external data format.
+        """
+        self.onnx_model = {}
+        self.model_name = ""
+        print("Loading onnx model from path:", onnx_load_path)
+        if external_data_format:
+            onnx_load_path = os.path.dirname(onnx_load_path)
+            for onnx_model_file in os.listdir(onnx_load_path):
+                with open(os.path.join(onnx_load_path, onnx_model_file), "rb") as f:
+                    self.onnx_model[onnx_model_file] = f.read()
+                if onnx_model_file.endswith(".onnx"):
+                    if self.model_name != "":
+                        raise ValueError("Multiple onnx files found in the directory")
+                    self.model_name = onnx_model_file.replace(".onnx", "")
+        else:
+            onnx_model_file = os.path.basename(onnx_load_path)
+            if not onnx_model_file.endswith(".onnx"):
+                raise ValueError("The file should be a .onnx file")
+            with open(onnx_load_path, "rb") as f:
+                self.onnx_model[onnx_model_file] = f.read()
+            self.model_name = onnx_model_file.replace(".onnx", "")
+
+    def write_to_disk(self, onnx_save_path: str) -> None:
+        """Writes the onnx model to the specified path."""
+        if os.path.exists(onnx_save_path):
+            print(f"Removing existing directory: {onnx_save_path}")
+            shutil.rmtree(onnx_save_path)
+        os.makedirs(onnx_save_path)
+        print("Writing onnx model to path:", onnx_save_path)
+        for onnx_model_file, onnx_model_bytes in self.onnx_model.items():
+            with open(os.path.join(onnx_save_path, onnx_model_file), "wb") as f:
+                f.write(onnx_model_bytes)
+
+    def to_bytes(self) -> bytes:
+        """Returns the bytes of the object."""
+        return cloudpickle.dumps(self)
+
+    def get_onnx_model_file_bytes(self) -> bytes:
+        """Returns the bytes of the onnx model file."""
+        return self.onnx_model[self.model_name + ".onnx"]
+
+    @classmethod
+    def from_bytes(cls, onnx_bytes: bytes) -> "OnnxBytes":
+        """Returns the OnnxBytes object from the bytes."""
+        return cloudpickle.loads(onnx_bytes)
+
+
+def _to_expected_onnx_type(val: Any) -> Any:
+    """Convert the given value to the expected onnx type.
+
+    During the onnx export process, plain numeric types (floats and ints) are converted to torch
+    tensors. This function pre-converts the given val to a tensor in case val is a int or float for
+    easier handling of such input values during the onnx export process.
+    """
+    if isinstance(val, (int, float)):
+        return torch.tensor(val).to(type(val))
+    return val
+
+
+def generate_onnx_input(model_metadata: ModelMetadata, input: Union[Any, Tuple]) -> Dict[str, Any]:
+    """Generate input for onnx model from model's forward signature and provided input.
+
+    Args:
+        model_metadata: The model's metadata.
+        input: A tuple of args/kwargs or torch.Tensor feed into the model's ``forward()`` method,
+            see :meth:`standardize_model_args() <modelopt.torch.utils.network.standardize_model_args>`
+            for more info on the convention.
+
+    Returns:
+        Args flattened into one dictionary with serialized keys compatible with provided onnx.
+
+    .. note::
+
+        This function performs a sanity check on the provided input data to filter out args that
+        are constants (instead of input nodes) in the onnx graph.
+
+
+    Some more relevant background of why we want to flatten the input pytree here:
+
+        * In the onnx export process, nested python data structures (like nested lists, tuples,
+            dictionaries) are being recursed into until leaf objects corresponding to tensors are
+            encountered.
+
+        * This is used to flatten the input in an onnx model to a list of tensors.
+
+        * However, this is a fairly complex process for the user to understand in case their models
+            takes a nested data structure. They have to understand how to manually flatten the data
+            structure in the *correct* order in order for them to run inference on a device_model or
+            onnx model.
+
+        * With this function this additional complexity can be abstracted away from the user.
+
+        * Example: if the original model took ``[x, {"y":y, "z" : [z1,z2]}]`` they can still provide
+            this nested data structure instead of the expected onnx input list of ``[x, y, z1, z2]``
+            --> flattening and unflattering is handled internally.
+    """
+    # get named args and set of params where we added default values
+    named_args, args_with_default = standardize_named_model_args(model_metadata["signature"], input)
+
+    # retrieve onnx input names
+    onnx_input_names = model_metadata["input_onnx_names"]
+    input_none_names = model_metadata["input_none_names"]
+
+    # capture flattened names of args from default values
+    named_default_args = {k: v for k, v in named_args.items() if k in args_with_default}
+    _, tree_spec_default_args = flatten_tree(named_default_args, prefix=ONNX_EXPORT_IN_PREFIX)
+
+    # capture flattened args without default args that do not appear in onnx graph
+    values, tree_spec = flatten_tree(named_args, prefix=ONNX_EXPORT_IN_PREFIX)
+    flat_kv = {k: v for k, v in zip(tree_spec.names, values)}
+
+    # We wanna consider four types of flattened args:
+    # 1. Args that appear in the onnx graph
+    # 2. Args that are not their default value
+    # 3. Args that were provided as None during conversion but are not None right now
+    # 4. Args that were provided as None during conversion and are None right now
+
+    args_in_onnx = {k for k in flat_kv if k in onnx_input_names}
+    args_not_default = {k for k in flat_kv if k not in tree_spec_default_args.names}
+    args_not_none = {k for k, v in flat_kv.items() if k in input_none_names and v is not None}
+    args_none = {k for k, v in flat_kv.items() if k in input_none_names and v is None}
+
+    # identify unexpected args from these 4 types
+    unexpected_args = ((args_not_default - args_none) | args_not_none) - args_in_onnx
+    if unexpected_args:
+        raise ValueError(
+            "The following args were provided that do not appear in the onnx graph of your model "
+            "since they are treated as constants in the onnx graph:"
+            + "\t\n".join(unexpected_args)
+            + "\nConsider removing these args from your input that are constants in the onnx model "
+            "or recompiling your onnx model with new constant values!"
+        )
+
+    # return the args that are relevant for the onnx graph in the right type
+    return {k: _to_expected_onnx_type(v) for k, v in flat_kv.items() if k in args_in_onnx}
+
+
+def optimize(name, onnx_graph, verbose=False):
+    """Optimizes onnx graph."""
+    opt = Optimizer(onnx_graph, verbose=verbose)
+    opt.info(name + ": original")
+    opt.cleanup()
+    opt.info(name + ": cleanup")
+    # TODO: fold constants is not working for some models from deploy_models(NestedOutModel, ArgsKwargsModel1)
+    # opt.fold_constants()
+    # opt.info(name + ": fold_constants")
+    onnx_graph = opt.infer_shapes(return_onnx=True)
+    opt.info(name + ": shape inference")
+    return onnx_graph
+
+
+def get_onnx_bytes_and_metadata(
+    model: nn.Module,
+    dummy_input: Union[Any, Tuple],
+    onnx_load_path: str = "",
+    remove_exported_model: bool = True,
+    **kwargs,
+) -> Tuple[bytes, ModelMetadata]:
+    """Get onnx model in bytes from input pytorch model together with the input/output of model.
+
+    Arguments:
+        model: PyTorch model to export to onnx.
+        dummy_input: A tuple of args/kwargs or torch.Tensor, see
+            `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
+            for more info on the convention.
+        onnx_load_path: The path to load the onnx model.
+        remove_exported_model: If True, the onnx model will be cleared from the disk after the
+            export process
+        **kwargs: Additional arguments to pass, e.g., ``onnx_opset``.
+
+    Returns:
+        bytes: Onnx model in bytes.
+        ModelMetadata: The model's meta data.
+
+    Raises:
+        ValueError: If nn.Module is not passed as model.
+    """
+    if not isinstance(model, nn.Module):
+        raise ValueError("Only PyTorch model compilation is supported.")
+
+    # unwrap DDP and DP models
+    if isinstance(model, (DataParallel, DistributedDataParallel)):
+        model = model.module
+
+    # process onnx_opset
+    onnx_opset = int(kwargs.get("onnx_opset", DEFAULT_ONNX_OPSET))
+
+    # Standardize model args and also tensorize them so they also appear in the onnx graph!
+    # Floats/ints are tensorized when they are provided, but not tensorized when they are not
+    # provided which is somewhat inconsistent (we always tensorize them!)
+    named_args, _ = standardize_named_model_args(model, dummy_input)
+    named_args = {k: _to_expected_onnx_type(v) for k, v in named_args.items()}
+
+    # Also standardize dummy_input again so we can use it
+    dummy_input = tuple(named_args.values())
+    if dummy_input and isinstance(dummy_input[-1], dict):
+        dummy_input = dummy_input + ({},)  # we need to add an extra dict for the fake kwargs!
+
+    # Get input tree spec, see generate_onnx_input for more info as well on this
+    flat_input, tree_spec_input = flatten_tree(named_args, prefix=ONNX_EXPORT_IN_PREFIX)
+
+    # input names are the names of the flattened input tree spec but without None values
+    input_names = [k for k, v in zip(tree_spec_input.names, flat_input) if v is not None]
+
+    # we also want to record the input names that are None so we can remove them from the input
+    # during inference.
+    input_none_names = list(set(tree_spec_input.names) - set(input_names))
+
+    # Get output once (we export in eval mode - so also using eval mode here!)
+    is_training = model.training
+    model.eval()
+    output = model(*named_args.values())
+    model.train(is_training)
+
+    # Get output tree spec
+    flat_output, tree_spec_output = flatten_tree(output, prefix=ONNX_EXPORT_OUT_PREFIX)
+
+    # output names are the names of the flattened input tree spec but without None values
+    output_names = [k for k, v in zip(tree_spec_output.names, flat_output) if v is not None]
+
+    model_name = model.__class__.__name__
+    onnx_build_folder = "./build/onnx/"
+    onnx_path = os.path.join(onnx_build_folder, model_name)
+    os.makedirs(onnx_path, exist_ok=True)
+    onnx_save_path = os.path.join(onnx_path, f"{model_name}.onnx")
+
+    # If the onnx_load path is specified by the user or if an onnx model exists at the default path
+    # then the model is loaded from this path and returned along with the metadata
+    if os.path.exists(onnx_save_path):
+        print(f"Overriding onnx load path to {onnx_save_path}")
+        onnx_load_path = onnx_save_path
+
+    if onnx_load_path != "":
+        onnx_model = OnnxBytes(onnx_load_path)
+        onnx_model_graph = onnx.load(os.path.join(onnx_load_path))
+        model_metadata = create_model_metadata(
+            tree_spec_input, tree_spec_output, input_none_names, onnx_model_graph, model
+        )
+        return onnx_model.to_bytes(), model_metadata
+
+    # Export onnx model from pytorch model
+    # As the maximum size of protobuf is 2GB, we cannot use io.BytesIO() buffer during export.
+    with torch.inference_mode():
+        torch.onnx.export(
+            model,
+            dummy_input,
+            onnx_save_path,
+            input_names=input_names,
+            output_names=output_names,
+            opset_version=onnx_opset,
+        )
+
+    # Check that export worked
+    assert len(os.listdir(onnx_path)) > 0, "Torch to onnx export failed."
+
+    # Load the onnx graph for optimizaiton
+    onnx_graph = onnx.load(onnx_save_path)
+
+    # Optimize the onnx graph
+    onnx_opt_graph = optimize(model.__class__.__name__, onnx_graph)
+
+    # Remove training_mode attribute from BatchNormalization nodes
+    onnx_opt_graph = remove_node_training_mode(onnx_opt_graph, "BatchNormalization")
+
+    model_metadata = create_model_metadata(
+        tree_spec_input, tree_spec_output, input_none_names, onnx_opt_graph, model
+    )
+
+    # Change the ONNX IR version to 9 to be compatible with ONNXRuntime
+    onnx_opt_graph.ir_version = 9
+
+    # If the onnx model is larger than 2GB, we need to save it in multiple files using the
+    # external data format.
+    if onnx_opt_graph.ByteSize() > TWO_GB:
+        onnx.save_model(
+            onnx_opt_graph,
+            onnx_save_path,
+            save_as_external_data=True,
+            all_tensors_to_one_file=True,
+            convert_attribute=False,
+        )
+    else:
+        onnx.save(onnx_opt_graph, onnx_save_path)
+    onnx_bytes = OnnxBytes(onnx_save_path)
+
+    if remove_exported_model:
+        shutil.rmtree(os.path.dirname(onnx_build_folder))
+    return onnx_bytes.to_bytes(), model_metadata
+
+
+def create_onnx_model_dict(
+    onnx_load_path: str = None, model_metadata: ModelMetadata = None, model_dict: dict = None
+) -> Tuple[bytes, ModelMetadata]:
+    """Creates onnx model dictionary and model metadata.
+
+    Args:
+        onnx_load_path: The path to load the onnx model.
+        model_metadata: Metadata to be stored in the DeviceModel.
+        model_dict: A dictionaty containing the Pytorch model and the input and output treespec for the model.
+
+    Returns:
+        A tuple of onnx model dictionary and model metadata.
+    """
+    onnx_model_dict = {"onnx_load_path": onnx_load_path}
+    onnx_model = {}
+    for onnx_model_file in os.listdir(onnx_load_path):
+        with open(os.path.join(onnx_load_path, onnx_model_file), "rb") as f:
+            onnx_model[onnx_model_file] = f.read()
+        if onnx_model_file.endswith(".onnx") and model_metadata is None:
+            onnx_model_file_path = os.path.join(onnx_load_path, onnx_model_file)
+            onnx_graph = onnx.load(onnx_model_file_path)
+            onnx_input_names = [input.name for input in onnx_graph.graph.input]
+            torch_input_names = model_dict["tree_spec_input"].names
+            assert set(onnx_input_names).issubset(
+                set(torch_input_names)
+            ), "One or more inputs in the onnx model are not present in the torch model."
+    onnx_model_dict["onnx_model"] = onnx_model
+    if model_metadata is None:
+        model_metadata = create_model_metadata(
+            model_dict["tree_spec_input"],
+            model_dict["tree_spec_output"],
+            model_dict["input_none_names"],
+            onnx_graph,
+            model_dict["model"],
+        )
+    return cloudpickle.dumps(onnx_model_dict), model_metadata
+
+
+def create_model_metadata(
+    tree_spec_input: TreeSpec,
+    tree_spec_output: TreeSpec,
+    input_none_names: List[str],
+    onnx_graph: ModelProto,
+    model: nn.Module,
+) -> ModelMetadata:
+    """Create model metadata from the given input.
+
+    Args:
+        tree_spec_input: pytree spec describing the structure of the pytree for the model input.
+        tree_spec_output: pytree spec describing the structure of the pytree for the model output.
+        input_none_names: List of input names with values that are None.
+        onnx_opt_graph: Graph of the onnx model.
+        model: Pytorch model.
+
+    Returns:
+        ModelMetadata: The DeviceModel metadata.
+    """
+    return {
+        "input_tree_spec": tree_spec_input,
+        "input_shapes": get_input_shapes(onnx_graph),
+        "input_onnx_names": get_input_names(onnx_graph),
+        "input_none_names": input_none_names,
+        "output_tree_spec": tree_spec_output,
+        "output_shapes": get_output_shapes(onnx_graph),
+        "output_onnx_names": get_output_names(onnx_graph),
+        "signature": inspect.signature(model.forward),
+        "onnx_node_names": get_node_names(onnx_graph),
+        "is_bytes_pickled": onnx_graph.ByteSize() > TWO_GB,
+    }
+
+
+def write_onnx_bytes(onnx_model_bytes: Union[bytes, dict], onnx_save_path: str) -> Tuple[str, str]:
+    """Write onnx bytes to the specified path.
+
+    Args:
+        onnx_model_bytes: Can be one of the following:
+            1. A pickled dictionary of onnx model files. The keys of this dictionary are file names and\
+            the values are the corresponding bytes.
+            2. A single onnx model bytes.
+        onnx_save_path: The onnx path to save the model.
+
+    Returns:
+        The path to the onnx model.
+        Name of the onnx model.
+    """
+    if isinstance(onnx_model_bytes, dict):
+        if not os.path.exists(onnx_save_path):
+            os.makedirs(onnx_save_path)
+        for key in onnx_model_bytes["onnx_model"]:
+            write_bytes(onnx_model_bytes["onnx_model"][key], os.path.join(onnx_save_path, key))
+        onnx_model_file = [s for s in onnx_model_bytes["onnx_model"].keys() if s.endswith(".onnx")]
+        assert len(onnx_model_file) == 1, "There should be only one onnx model file"
+        model_name = onnx_model_file[0].split(".")[0]
+        return os.path.join(onnx_save_path, onnx_model_file[0]), model_name
+    write_bytes(onnx_model_bytes, os.path.join(onnx_save_path, "model.onnx"))
+    return os.path.join(onnx_save_path, "model.onnx"), ""
+
+
+def get_onnx_bytes(*args, **kwargs):
+    """Return onnx bytes only.
+
+    See ``get_onnx_bytes_and_metadata()`` for more info.
+    """
+    onnx_bytes = get_onnx_bytes_and_metadata(*args, **kwargs)[0]
+    onnx_bytes_obj = OnnxBytes.from_bytes(onnx_bytes)
+    return onnx_bytes_obj.onnx_model[f"{onnx_bytes_obj.model_name}.onnx"]
+
+
+def remove_node_training_mode(onnx_model: ModelProto, node_op_type: str) -> ModelProto:
+    """Remove training_mode attribute from selected node type.
+
+    Args:
+        onnx_model: The onnx model.
+        node_op_type: The node type to remove training_mode attribute from.
+
+    Returns:
+        The onnx model with the training_mode attribute removed.
+    """
+    for node in onnx_model.graph.node:
+        if node.op_type == node_op_type:
+            for attribute in node.attribute:
+                if attribute.name == "training_mode":
+                    if attribute.i == 1:
+                        node.output.remove(node.output[1])
+                        node.output.remove(node.output[1])
+                    attribute.i = 0
+
+    return onnx_model
```

## modelopt/torch/export/__init__.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Export package. So far it only supports selected nemo and huggingface LLMs."""
-
-from .model_config import *
-from .model_config_export import *
-from .model_config_utils import *
-from .postprocess import postprocess_tensors as postprocess_tensors
-from .transformer_engine import *
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Export package. So far it only supports selected nemo and huggingface LLMs."""
+
+from .model_config import *
+from .model_config_export import *
+from .model_config_utils import *
+from .postprocess import postprocess_tensors as postprocess_tensors
+from .transformer_engine import *
```

## modelopt/torch/export/distribute.py

 * *Ordering differences only*

```diff
@@ -1,330 +1,330 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""torch.distribute utils."""
-
-import json
-from contextlib import contextmanager
-from io import BytesIO
-from multiprocessing.shared_memory import SharedMemory
-from pathlib import Path
-from typing import Any, Dict, List, Optional, Tuple, Union
-
-import torch
-
-from .model_config_utils import (
-    model_config_from_dict,
-    model_config_to_dict,
-    restore_model_config,
-    split_config_and_weights,
-)
-
-
-def get_world_size() -> int:
-    """Safe method to get world size."""
-    if torch.distributed.is_initialized():
-        return torch.distributed.get_world_size()
-    else:
-        print("torch.distributed not initialized, assuming single world_size.")
-        return 1
-
-
-def get_rank() -> int:
-    """Safe method to get local rank."""
-    if torch.distributed.is_initialized():
-        return torch.distributed.get_rank()
-    else:
-        print("torch.distributed not initialized, assuming single world_size.")
-        return 0
-
-
-def get_group(ranks: List[int]):
-    """Returns the process group if torch.distributed.is_initialized()."""
-    # NCCL has an issue with calling barrier. So we just use the gloo backebnd for group barriers.
-    return (
-        torch.distributed.new_group(ranks, backend="gloo")
-        if torch.distributed.is_initialized()
-        else None
-    )
-
-
-def barrier(group=None):
-    """Set a parallel barrier."""
-    if torch.distributed.is_initialized():
-        torch.distributed.barrier(group=group)
-
-
-class NFSWorkspace:
-    """A shared workspace implementation using Network File Storage (NFS).
-
-    NOTE: all read/write/modifition to the NFS dir do not involve any collective
-          communication nor barrier. It is users' responsibility to synchronize
-          all ranks (local and remove processes).
-
-    This implementation uses `torch.save` and `torch.load` for serialization.
-
-    Args:
-        workspace_path: the path to the NFS directory for postprocess cross rank communication.
-            If not provided, SharedMemory will be used instead.
-    """
-
-    def __init__(self, workspace_path: Optional[Union[Path, str]] = None):
-        """Create the NFS work dir and clean up existing existing state files."""
-        self.path = Path("") if workspace_path is None else Path(workspace_path)
-        self._is_initialized = workspace_path is not None
-        self.rank = get_rank()
-        if self.is_initialized:
-            if self.rank == 0:
-                self.path.mkdir(parents=True, exist_ok=True)
-            self.state_path = self._get_state_path(self.rank)
-            self._clean_up()
-
-    @property
-    def is_initialized(self):
-        """Whether the workspace is intialized."""
-        return self._is_initialized
-
-    def write_configs_and_weights(self, config_json: Dict[str, Any], weights: Dict[str, Any]):
-        """All ranks write the state file to the shared NFS dir.
-
-        Args:
-            config_json: model or module config in json
-            weights: module weights in torch's state_dict format
-        """
-        if not self.is_initialized:
-            raise ValueError("NFSWorkspace is not initialized!")
-        self._clean_up()
-        torch.save({"config": config_json, "weight": weights}, self.state_path)
-
-    def read_configs_and_weights_from_rank(
-        self, target_rank: int
-    ) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
-        """All ranks read the target_rank state file.
-
-        Args:
-            target_rank: the target rank
-
-        Returns:
-            the model/module config and the weights
-        """
-        if not self.is_initialized:
-            raise ValueError("NFSWorkspace is not initialized!")
-        state_path = self._get_state_path(target_rank)
-        if state_path.exists():
-            state = torch.load(state_path)
-            return state["config"], state["weight"]
-        else:
-            return None, None
-
-    def _get_state_path(self, target_rank: int) -> Path:
-        """Return the state file name of a particular rank.
-
-        Args:
-            target_rank: the target rank
-
-        Returns:
-            the state file path of the target rank
-        """
-        if not self.is_initialized:
-            raise ValueError("NFSWorkspace is not initialized!")
-        return self.path.joinpath("rank_{}_state.pth".format(target_rank))
-
-    def _clean_up(self):
-        """Remove existing state files."""
-        if not self.is_initialized:
-            raise ValueError("NFSWorkspace is not initialized!")
-        self.state_path.unlink(missing_ok=True)
-
-
-@contextmanager
-def get_tensors_parallel(tensor: torch.Tensor, ranks: List[int], group=None):
-    """Gathers the tensors across distributed processes using shm.
-
-    Args:
-        tensor: the tensor that each rank want to pass to the first rank.
-            The tensors across the ranks need to have the same size.
-        ranks: the list of the ranks
-        group: the barrier sync group.
-
-    Yields:
-        the first rank in the ranks has the full access of the tensors across all the ranks.
-        the other ranks returns an empty list
-
-    The shm will be destroyed after consumption.
-    """
-    assert tensor is not None
-    assert len(ranks) > 1
-    local_rank = get_rank()
-    shm_writer = None
-    shm_readers = []
-    tensor = tensor.cpu()
-
-    is_merged_rank = local_rank == ranks[0]
-    # Create shm and copy the tensor to the shm if not the merged rank.
-    # Assume each tensor need up to 2KB additional space for metadata.
-    if not is_merged_rank:
-        shm_writer = SharedMemory(name=f"rank_{local_rank}", create=True, size=tensor.nbytes + 2048)
-        torch.save(tensor, shm_writer._mmap)  # type: ignore[attr-defined]
-    # All ranks wait for this to complete.
-    barrier(group)
-
-    tensors = []
-    # The merged rank gather the tensor from the other ranks (including itself).
-    if is_merged_rank:
-        for rank in ranks:
-            if rank == ranks[0]:
-                tensors.append(tensor)
-            else:
-                shm = SharedMemory(name=f"rank_{rank}", create=False)
-                shared_tensor = torch.load(BytesIO(shm.buf))
-                tensors.append(shared_tensor)
-                shm_readers.append(shm)
-    try:
-        # Send the tensor list to the consumer.
-        # The merged rank will get a valid tensor while the other ranks an empty tensor.
-        yield tensors
-    finally:
-        # Reader closes the shms.
-        if shm_readers:
-            for shm in shm_readers:
-                shm.close()
-
-        # All ranks wait for the reader to close the shms.
-        barrier(group)
-
-        # Writer frees the shm resource.
-        if shm_writer is not None:
-            shm_writer.close()
-            shm_writer.unlink()
-
-
-@contextmanager
-def get_configs_parallel(
-    config, ranks: List[int], group, workspace_path: Optional[Union[Path, str]] = None
-):
-    """Gathers the layer config across distributed processes using shm or NFS.
-
-    Args:
-        config: the config (nullable) that each rank want to pass to the first rank.
-        ranks: the list of the ranks
-        group: the barrier sync group.
-        workspace_path: the path to the NFS directory for postprocess cross rank communication.
-
-    Yields:
-        the first rank in the ranks has the full access of the configs across all the ranks.
-        the other ranks returns an empty list
-
-    When workspace_path is provided, an NFSWorkspace object is created to perform communication
-    across ranks. Otherwise, `SharedMemory` is used for local multi-process communication.
-    The shm will be destroyed after consumption.
-    """
-    assert len(ranks) > 1
-    local_rank = get_rank()
-    shm_writer = None
-    shm_readers = []
-    nfs_workspace = NFSWorkspace(workspace_path)
-
-    is_merged_rank = local_rank == ranks[0]
-
-    def _get_weights_nbytes(weights_dict: Dict[str, torch.Tensor]):
-        total_nbytes = 0
-        for k, v in weights_dict.items():
-            # Assume each tensor need up to 2KB additional space for metadata.
-            # In reality this should be much smaller.
-            total_nbytes = total_nbytes + len(k) + v.nbytes + 2048
-
-        return total_nbytes
-
-    # Create shm and copy the serialized config to the shm if not the merged rank.
-    if not is_merged_rank:
-        if config is not None:
-            config_dict = model_config_to_dict(config)
-            # Add additional config type name to the dict so we can later pick the right config type.
-            config_dict["__name__"] = str(type(config).__name__)
-            weights = {}
-            split_config_and_weights(config_dict, weights)
-
-            config_json = json.dumps(config_dict)
-
-            if nfs_workspace.is_initialized:
-                # All ranks except for the master merge rank write to the NFS dir.
-                nfs_workspace.write_configs_and_weights(config_dict, weights)
-            else:
-                # SHM data structure: 8B json size, serialized json bytes and the weights dict.
-                shm_writer = SharedMemory(
-                    name=f"rank_{local_rank}_config",
-                    create=True,
-                    size=(8 + len(config_json) + _get_weights_nbytes(weights)),
-                )
-
-                # Write json length to the shm
-                shm_writer.buf[:8] = len(config_json).to_bytes(8, "little")
-
-                # Write json to the shm
-                shm_writer.buf[8 : len(config_json) + 8] = config_json.encode()
-
-                # Write np tensors to the shm.
-                shm_writer._mmap.seek(len(config_json) + 8)  # type: ignore[attr-defined]
-                torch.save(weights, shm_writer._mmap)  # type: ignore[attr-defined]
-        else:
-            # If the config is None, we just store the empty 0.
-            shm_writer = SharedMemory(
-                name=f"rank_{local_rank}_config",
-                create=True,
-                size=8,
-            )
-
-            shm_writer.buf[:8] = int(0).to_bytes(8, "little")
-
-    # All ranks wait for this to complete.
-    barrier(group)
-
-    configs = []
-    if is_merged_rank:
-        for rank in ranks:
-            if rank == ranks[0]:
-                configs.append(config)
-            else:
-                if nfs_workspace.is_initialized:
-                    # The master merge rank read other configs from the NFS dir.
-                    config_dict, weights = nfs_workspace.read_configs_and_weights_from_rank(rank)
-                    if config_dict is not None:
-                        restore_model_config(config_dict, weights)
-                        config = model_config_from_dict(config_dict)
-                        configs.append(config)
-                else:
-                    shm = SharedMemory(name=f"rank_{rank}_config", create=False)
-                    len_json = int.from_bytes(shm.buf[:8], "little")
-
-                    if len_json != 0:
-                        config_dict = json.loads(shm.buf[8 : 8 + len_json].tobytes().decode())
-                        weights = torch.load(BytesIO(shm.buf[8 + len_json :]), allow_pickle=True)
-                        restore_model_config(config_dict, weights)
-                        config = model_config_from_dict(config_dict)
-
-                        configs.append(config)
-                        shm_readers.append(shm)
-    try:
-        # Send the config list to the consumer.
-        # The merged rank will get a valid config list while the other ranks an empty list.
-        yield configs
-    finally:
-        # Reader closes the shms.
-        if shm_readers:
-            for shm in shm_readers:
-                shm.close()
-
-        # All ranks wait for the reader to close the shms.
-        barrier(group)
-
-        # Writer frees the shm resource.
-        if shm_writer is not None:
-            shm_writer.close()
-            shm_writer.unlink()
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""torch.distribute utils."""
+
+import json
+from contextlib import contextmanager
+from io import BytesIO
+from multiprocessing.shared_memory import SharedMemory
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+import torch
+
+from .model_config_utils import (
+    model_config_from_dict,
+    model_config_to_dict,
+    restore_model_config,
+    split_config_and_weights,
+)
+
+
+def get_world_size() -> int:
+    """Safe method to get world size."""
+    if torch.distributed.is_initialized():
+        return torch.distributed.get_world_size()
+    else:
+        print("torch.distributed not initialized, assuming single world_size.")
+        return 1
+
+
+def get_rank() -> int:
+    """Safe method to get local rank."""
+    if torch.distributed.is_initialized():
+        return torch.distributed.get_rank()
+    else:
+        print("torch.distributed not initialized, assuming single world_size.")
+        return 0
+
+
+def get_group(ranks: List[int]):
+    """Returns the process group if torch.distributed.is_initialized()."""
+    # NCCL has an issue with calling barrier. So we just use the gloo backebnd for group barriers.
+    return (
+        torch.distributed.new_group(ranks, backend="gloo")
+        if torch.distributed.is_initialized()
+        else None
+    )
+
+
+def barrier(group=None):
+    """Set a parallel barrier."""
+    if torch.distributed.is_initialized():
+        torch.distributed.barrier(group=group)
+
+
+class NFSWorkspace:
+    """A shared workspace implementation using Network File Storage (NFS).
+
+    NOTE: all read/write/modifition to the NFS dir do not involve any collective
+          communication nor barrier. It is users' responsibility to synchronize
+          all ranks (local and remove processes).
+
+    This implementation uses `torch.save` and `torch.load` for serialization.
+
+    Args:
+        workspace_path: the path to the NFS directory for postprocess cross rank communication.
+            If not provided, SharedMemory will be used instead.
+    """
+
+    def __init__(self, workspace_path: Optional[Union[Path, str]] = None):
+        """Create the NFS work dir and clean up existing existing state files."""
+        self.path = Path("") if workspace_path is None else Path(workspace_path)
+        self._is_initialized = workspace_path is not None
+        self.rank = get_rank()
+        if self.is_initialized:
+            if self.rank == 0:
+                self.path.mkdir(parents=True, exist_ok=True)
+            self.state_path = self._get_state_path(self.rank)
+            self._clean_up()
+
+    @property
+    def is_initialized(self):
+        """Whether the workspace is intialized."""
+        return self._is_initialized
+
+    def write_configs_and_weights(self, config_json: Dict[str, Any], weights: Dict[str, Any]):
+        """All ranks write the state file to the shared NFS dir.
+
+        Args:
+            config_json: model or module config in json
+            weights: module weights in torch's state_dict format
+        """
+        if not self.is_initialized:
+            raise ValueError("NFSWorkspace is not initialized!")
+        self._clean_up()
+        torch.save({"config": config_json, "weight": weights}, self.state_path)
+
+    def read_configs_and_weights_from_rank(
+        self, target_rank: int
+    ) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
+        """All ranks read the target_rank state file.
+
+        Args:
+            target_rank: the target rank
+
+        Returns:
+            the model/module config and the weights
+        """
+        if not self.is_initialized:
+            raise ValueError("NFSWorkspace is not initialized!")
+        state_path = self._get_state_path(target_rank)
+        if state_path.exists():
+            state = torch.load(state_path)
+            return state["config"], state["weight"]
+        else:
+            return None, None
+
+    def _get_state_path(self, target_rank: int) -> Path:
+        """Return the state file name of a particular rank.
+
+        Args:
+            target_rank: the target rank
+
+        Returns:
+            the state file path of the target rank
+        """
+        if not self.is_initialized:
+            raise ValueError("NFSWorkspace is not initialized!")
+        return self.path.joinpath("rank_{}_state.pth".format(target_rank))
+
+    def _clean_up(self):
+        """Remove existing state files."""
+        if not self.is_initialized:
+            raise ValueError("NFSWorkspace is not initialized!")
+        self.state_path.unlink(missing_ok=True)
+
+
+@contextmanager
+def get_tensors_parallel(tensor: torch.Tensor, ranks: List[int], group=None):
+    """Gathers the tensors across distributed processes using shm.
+
+    Args:
+        tensor: the tensor that each rank want to pass to the first rank.
+            The tensors across the ranks need to have the same size.
+        ranks: the list of the ranks
+        group: the barrier sync group.
+
+    Yields:
+        the first rank in the ranks has the full access of the tensors across all the ranks.
+        the other ranks returns an empty list
+
+    The shm will be destroyed after consumption.
+    """
+    assert tensor is not None
+    assert len(ranks) > 1
+    local_rank = get_rank()
+    shm_writer = None
+    shm_readers = []
+    tensor = tensor.cpu()
+
+    is_merged_rank = local_rank == ranks[0]
+    # Create shm and copy the tensor to the shm if not the merged rank.
+    # Assume each tensor need up to 2KB additional space for metadata.
+    if not is_merged_rank:
+        shm_writer = SharedMemory(name=f"rank_{local_rank}", create=True, size=tensor.nbytes + 2048)
+        torch.save(tensor, shm_writer._mmap)  # type: ignore[attr-defined]
+    # All ranks wait for this to complete.
+    barrier(group)
+
+    tensors = []
+    # The merged rank gather the tensor from the other ranks (including itself).
+    if is_merged_rank:
+        for rank in ranks:
+            if rank == ranks[0]:
+                tensors.append(tensor)
+            else:
+                shm = SharedMemory(name=f"rank_{rank}", create=False)
+                shared_tensor = torch.load(BytesIO(shm.buf))
+                tensors.append(shared_tensor)
+                shm_readers.append(shm)
+    try:
+        # Send the tensor list to the consumer.
+        # The merged rank will get a valid tensor while the other ranks an empty tensor.
+        yield tensors
+    finally:
+        # Reader closes the shms.
+        if shm_readers:
+            for shm in shm_readers:
+                shm.close()
+
+        # All ranks wait for the reader to close the shms.
+        barrier(group)
+
+        # Writer frees the shm resource.
+        if shm_writer is not None:
+            shm_writer.close()
+            shm_writer.unlink()
+
+
+@contextmanager
+def get_configs_parallel(
+    config, ranks: List[int], group, workspace_path: Optional[Union[Path, str]] = None
+):
+    """Gathers the layer config across distributed processes using shm or NFS.
+
+    Args:
+        config: the config (nullable) that each rank want to pass to the first rank.
+        ranks: the list of the ranks
+        group: the barrier sync group.
+        workspace_path: the path to the NFS directory for postprocess cross rank communication.
+
+    Yields:
+        the first rank in the ranks has the full access of the configs across all the ranks.
+        the other ranks returns an empty list
+
+    When workspace_path is provided, an NFSWorkspace object is created to perform communication
+    across ranks. Otherwise, `SharedMemory` is used for local multi-process communication.
+    The shm will be destroyed after consumption.
+    """
+    assert len(ranks) > 1
+    local_rank = get_rank()
+    shm_writer = None
+    shm_readers = []
+    nfs_workspace = NFSWorkspace(workspace_path)
+
+    is_merged_rank = local_rank == ranks[0]
+
+    def _get_weights_nbytes(weights_dict: Dict[str, torch.Tensor]):
+        total_nbytes = 0
+        for k, v in weights_dict.items():
+            # Assume each tensor need up to 2KB additional space for metadata.
+            # In reality this should be much smaller.
+            total_nbytes = total_nbytes + len(k) + v.nbytes + 2048
+
+        return total_nbytes
+
+    # Create shm and copy the serialized config to the shm if not the merged rank.
+    if not is_merged_rank:
+        if config is not None:
+            config_dict = model_config_to_dict(config)
+            # Add additional config type name to the dict so we can later pick the right config type.
+            config_dict["__name__"] = str(type(config).__name__)
+            weights = {}
+            split_config_and_weights(config_dict, weights)
+
+            config_json = json.dumps(config_dict)
+
+            if nfs_workspace.is_initialized:
+                # All ranks except for the master merge rank write to the NFS dir.
+                nfs_workspace.write_configs_and_weights(config_dict, weights)
+            else:
+                # SHM data structure: 8B json size, serialized json bytes and the weights dict.
+                shm_writer = SharedMemory(
+                    name=f"rank_{local_rank}_config",
+                    create=True,
+                    size=(8 + len(config_json) + _get_weights_nbytes(weights)),
+                )
+
+                # Write json length to the shm
+                shm_writer.buf[:8] = len(config_json).to_bytes(8, "little")
+
+                # Write json to the shm
+                shm_writer.buf[8 : len(config_json) + 8] = config_json.encode()
+
+                # Write np tensors to the shm.
+                shm_writer._mmap.seek(len(config_json) + 8)  # type: ignore[attr-defined]
+                torch.save(weights, shm_writer._mmap)  # type: ignore[attr-defined]
+        else:
+            # If the config is None, we just store the empty 0.
+            shm_writer = SharedMemory(
+                name=f"rank_{local_rank}_config",
+                create=True,
+                size=8,
+            )
+
+            shm_writer.buf[:8] = int(0).to_bytes(8, "little")
+
+    # All ranks wait for this to complete.
+    barrier(group)
+
+    configs = []
+    if is_merged_rank:
+        for rank in ranks:
+            if rank == ranks[0]:
+                configs.append(config)
+            else:
+                if nfs_workspace.is_initialized:
+                    # The master merge rank read other configs from the NFS dir.
+                    config_dict, weights = nfs_workspace.read_configs_and_weights_from_rank(rank)
+                    if config_dict is not None:
+                        restore_model_config(config_dict, weights)
+                        config = model_config_from_dict(config_dict)
+                        configs.append(config)
+                else:
+                    shm = SharedMemory(name=f"rank_{rank}_config", create=False)
+                    len_json = int.from_bytes(shm.buf[:8], "little")
+
+                    if len_json != 0:
+                        config_dict = json.loads(shm.buf[8 : 8 + len_json].tobytes().decode())
+                        weights = torch.load(BytesIO(shm.buf[8 + len_json :]), allow_pickle=True)
+                        restore_model_config(config_dict, weights)
+                        config = model_config_from_dict(config_dict)
+
+                        configs.append(config)
+                        shm_readers.append(shm)
+    try:
+        # Send the config list to the consumer.
+        # The merged rank will get a valid config list while the other ranks an empty list.
+        yield configs
+    finally:
+        # Reader closes the shms.
+        if shm_readers:
+            for shm in shm_readers:
+                shm.close()
+
+        # All ranks wait for the reader to close the shms.
+        barrier(group)
+
+        # Writer frees the shm resource.
+        if shm_writer is not None:
+            shm_writer.close()
+            shm_writer.unlink()
```

## modelopt/torch/export/layer_utils.py

 * *Ordering differences only*

```diff
@@ -1,1313 +1,1313 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utils for model_config export.
-
-Some of the logics in this file are empirical and needs constant update if exceptions occur.
-"""
-
-from typing import List, Optional, Tuple
-
-import torch
-import torch.nn as nn
-
-try:
-    from transformers.activations import ACT2FN
-except Exception:
-    print("Cannot find transformers package. Hugginface modules cannot be exported.")
-    pass
-
-from ..quantization.nn import SequentialQuantizer, TensorQuantizer
-from .distribute import get_rank, get_world_size
-from .model_config import (
-    KV_CACHE_FP8,
-    KV_CACHE_INT8,
-    LAYERNORM_DEFAULT,
-    LAYERNORM_RMS,
-    LINEAR_COLUMN,
-    LINEAR_ROW,
-    QUANTIZATION_FP8,
-    QUANTIZATION_INT4_AWQ,
-    QUANTIZATION_INT8_SQ,
-    QUANTIZATION_NONE,
-    QUANTIZATION_W4A8_AWQ,
-    AttentionConfig,
-    DecoderLayerConfig,
-    EmbeddingConfig,
-    ExpertConfig,
-    LayernormConfig,
-    LinearConfig,
-    MLPConfig,
-    MOEConfig,
-    QKVConfig,
-)
-from .model_config_utils import pad_weights
-from .scaling_factor_utils import resmooth_and_get_scale
-
-
-def check_model_compatibility(module_list: List[nn.Module]) -> Tuple[bool, bool, bool]:
-    """Returns whether the list of modules is compatible with the export logic.
-
-    And if positional embedding and embedding layernorm exists.
-
-    We assumes the model to be assembled with one or two embedding layers,
-    a ModuleList of transformer decoders,
-    and a final layernorm with optional embedding layernorm.
-    Otherwise it will not be supported.
-    """
-    num_embeddings = 0
-    num_module_list = 0
-    num_layer_norm = 0
-    for module in module_list:
-        if is_embedding(module):
-            num_embeddings += 1
-        elif is_decoder_list(module):
-            num_module_list += 1
-        elif is_layernorm(module):
-            num_layer_norm += 1
-
-    return (
-        1 <= num_embeddings
-        and num_embeddings <= 2
-        and num_module_list == 1
-        and 1 <= num_layer_norm
-        and num_layer_norm <= 2,
-        num_embeddings > 1,
-        num_layer_norm > 1,
-    )
-
-
-def get_transformer_layers(model: nn.Module) -> List[nn.Module]:
-    """Returns the root module of the transformer model."""
-    if "Megatron" in type(model).__name__:
-        if hasattr(model, "model") and "GPTModel" in type(model.model).__name__:
-            # NEMO mcore models can be handled with the following branch.
-            model = model.model
-
-        # NEMO non mcore models, we need to find the language_model module first.
-        children = [model]
-        language_model = None
-        while children and not language_model:
-            next_children = []
-            for child in children:
-                if type(child).__name__ == "TransformerLanguageModel":
-                    language_model = child
-                    break
-                for m in child.children():
-                    next_children.append(m)
-            children = next_children
-        if language_model:
-            print("Warning: this is an old NEMO checkpoint format and will be deprecated soon.")
-            layers = [m for m in language_model.embedding.children()] + [
-                m for m in language_model.encoder.children()
-            ]
-
-            if hasattr(language_model, "output_layer"):
-                layers.append(language_model.output_layer)
-
-            return layers
-
-    if "GPTModel" in type(model).__name__:
-        # mcore models
-        layers = []
-        if hasattr(model, "embedding"):
-            layers = layers + [m for m in model.embedding.children()]
-        layers = layers + [m for m in model.decoder.children()]
-        if hasattr(model, "output_layer"):
-            layers.append(model.output_layer)
-        return layers
-
-    if hasattr(model, "transformer"):
-        # This is a LMHead model
-        # Add lm_head to be processed along with transformer layers
-        modules = []
-        for m in model.transformer.children():
-            if "Transformer" in type(m).__name__ and is_decoder_list(m.layers):
-                modules.append(m.layers)
-                modules.append(m.final_layernorm)
-            else:
-                modules.append(m)
-        if hasattr(model, "lm_head"):
-            modules += [model.lm_head]
-        return modules
-
-    if hasattr(model, "model"):
-        # LLAMA
-        modules = [m for m in model.model.children()]
-        if hasattr(model, "lm_head"):
-            modules += [model.lm_head]
-
-        return modules
-
-    return [m for m in model.children()]
-
-
-def is_linear(module: nn.Module) -> bool:
-    """Returns whether the module is a linear layer."""
-    return any([k in type(module).__name__ for k in ["Linear", "Conv1D", "NormHead"]])
-
-
-def is_embedding(module: nn.Module) -> bool:
-    """Returns whether the module is an embedding layer."""
-    module_type_name = type(module).__name__
-    return (
-        "Embedding" in module_type_name
-        and "Rotary" not in module_type_name
-        and "PhiImage" not in module_type_name
-    )
-
-
-def build_embedding_config(
-    module: nn.Module, dtype: torch.dtype, normalization_constant: float = 1
-) -> EmbeddingConfig:
-    """Builds the embedding config from the module."""
-    assert is_embedding(module)
-
-    world_size = get_world_size()
-    rank = get_rank()
-
-    # Special case for chatglm
-    if hasattr(module, "word_embeddings"):
-        module = module.word_embeddings
-
-    weight = module.weight.detach().type(dtype)
-    normalized_weight = weight * normalization_constant
-    if "Parallel" in type(module).__name__:
-        local_weight = normalized_weight
-    else:
-        padded_weight = pad_weights(normalized_weight, get_world_size())
-        local_weight = torch.chunk(padded_weight, world_size, dim=0)[rank]
-    return EmbeddingConfig(
-        weight=local_weight,
-    )
-
-
-def is_layernorm(module: nn.Module) -> bool:
-    """Returns whether the module is a layernorm layer."""
-    module_name = type(module).__name__
-    return any(norm in module_name for norm in ["LayerNorm", "RMSNorm"])
-
-
-def build_layernorm_config(module: nn.Module, dtype: torch.dtype) -> LayernormConfig:
-    """Builds the layernorm config from the module."""
-    assert is_layernorm(module)
-
-    layernorm_type = LAYERNORM_DEFAULT
-    if "RMS" in type(module).__name__:
-        layernorm_type = LAYERNORM_RMS
-
-    weight = module.weight.detach()
-
-    def _weights_plus_one(module):
-        if any(name in type(module).__name__ for name in ["LayerNorm1P", "GemmaRMSNorm"]):
-            return True
-
-        if hasattr(module, "zero_centered_gamma") and module.zero_centered_gamma:
-            return True
-
-        return False
-
-    if _weights_plus_one(module):
-        # megatron layernorm's weight needs to be updated.
-        weight = weight.float() + 1.0
-
-    config = LayernormConfig(
-        weight=weight.type(dtype),
-        bias=(
-            module.bias.detach().type(dtype)
-            if hasattr(module, "bias") and module.bias is not None
-            else None
-        ),
-        layernorm_type=layernorm_type,
-    )
-
-    # TODO: handle the nemo llama eps config.
-    for eps_key in ["eps", "variance_epsilon"]:
-        if hasattr(module, eps_key):
-            config.eps = getattr(module, eps_key)
-            break
-
-    return config
-
-
-def is_decoder_list(module: nn.Module) -> bool:
-    """Returns whether the module is a decoder list."""
-    return type(module) == nn.ModuleList
-
-
-def is_attention(module: nn.Module) -> bool:
-    """Returns whether the module is an attention layer."""
-    return "Attention" in type(module).__name__
-
-
-def is_mlp(module: nn.Module) -> bool:
-    """Returns whether the module is an MLP layer."""
-    return "MLP" in type(module).__name__
-
-
-def is_moe(module: nn.Module) -> bool:
-    """Returns whether the module is an MOE layer."""
-    return type(module).__name__ in ["MixtralSparseMoeBlock", "ArcticMoE"]
-
-
-def get_scaling_factor(quantizer: TensorQuantizer) -> torch.Tensor:
-    """Returns scaling factor from the quantizer as torch.Tensor."""
-    if not quantizer.is_enabled:
-        return None
-
-    amax = quantizer.export_amax()
-    if amax is None:
-        return None
-
-    # tensorrt_llm uses float as the scaling_factors.
-    scaling_factor = amax.float() / quantizer.maxbound
-
-    assert torch.all(scaling_factor > 0), f"scaling factor {scaling_factor} not positive."
-
-    return scaling_factor
-
-
-def get_activation_scaling_factor(module: nn.Module) -> torch.Tensor:
-    """Returns the activation scaling factor."""
-    return (
-        get_scaling_factor(module.input_quantizer) if hasattr(module, "input_quantizer") else None
-    )
-
-
-def get_weight_scaling_factor(module: nn.Module) -> torch.Tensor:
-    """Returns the weight scaling factor."""
-    # module.weight_quantizer could be a TensorQuantizer (for algorithms except W4A8) or
-    # a SequentialQuantizer (for W4A8). In the latter case, we need to get the scaling factor from the
-    # first quantizer of the SequentialQuantizer instance.
-    if hasattr(module, "weight_quantizer") and isinstance(
-        module.weight_quantizer, SequentialQuantizer
-    ):
-        return get_scaling_factor(module.weight_quantizer[0])
-    return (
-        get_scaling_factor(module.weight_quantizer) if hasattr(module, "weight_quantizer") else None
-    )
-
-
-def get_weight_block_size(module: nn.Module) -> int:
-    """Returns the weight block size."""
-    if not hasattr(module, "weight_quantizer"):
-        return 0
-
-    weight_quantizer = module.weight_quantizer
-
-    if isinstance(weight_quantizer, SequentialQuantizer):
-        weight_quantizer = weight_quantizer[0]
-
-    if not weight_quantizer.is_enabled:
-        return 0
-
-    block_sizes = weight_quantizer.block_sizes
-
-    if block_sizes:
-        return block_sizes[-1]
-    return 0
-
-
-def get_weight_scaling_factor_2(module: nn.Module) -> torch.Tensor:
-    """Returns the secondary weight scaling factor."""
-    if (
-        not hasattr(module, "weight_quantizer")
-        or not isinstance(module.weight_quantizer, SequentialQuantizer)
-        or not module.weight_quantizer[-1].is_enabled
-    ):
-        return None
-    assert (
-        len(module.weight_quantizer) == 2
-    ), "modelopt only supports 2 sequential quantization layers for now"
-    return get_scaling_factor(module.weight_quantizer[-1])
-
-
-def get_prequant_scaling_factor(module: nn.Module, dtype: torch.dtype) -> torch.Tensor:
-    """Returns the prequant scaling factor."""
-    prequant_scaling_factor = (
-        module.input_quantizer._pre_quant_scale.squeeze().type(dtype)
-        if hasattr(module, "input_quantizer")
-        and hasattr(module.input_quantizer, "_pre_quant_scale")
-        else None
-    )
-
-    if prequant_scaling_factor is not None:
-        assert torch.all(
-            prequant_scaling_factor > 0
-        ), f"prequant scaling factor {prequant_scaling_factor} not positive."
-    return prequant_scaling_factor
-
-
-def get_kv_cache_scaling_factor(qkv_modules: List[nn.Module]) -> torch.Tensor:
-    """Returns the kv_cache scaling factor if output quantizer is set. Else returns None by default."""
-    # For FP8, we recommend default kv cache scaling factor to be 1.
-    if get_kv_cache_dtype(qkv_modules) == KV_CACHE_FP8:
-        return torch.tensor([1.0], dtype=torch.float)
-
-    scaling_factors = [
-        get_scaling_factor(module.output_quantizer)
-        for module in qkv_modules
-        if hasattr(module, "output_quantizer")
-    ]
-
-    scaling_factors = [
-        scaling_factor for scaling_factor in scaling_factors if scaling_factor is not None
-    ]
-
-    if not scaling_factors:
-        return None
-
-    return torch.stack(scaling_factors).max(dim=0).values
-
-
-def get_kv_cache_dtype(qkv_modules: List[nn.Module]) -> str:
-    """Returns the kv_cache dtype.
-
-    If num_bits of output_quantizer is (4, 3) then returns FP8; if it is 8, returns int8,
-    otherwise returns None.
-    """
-    num_bits_list = [
-        qkv_module.output_quantizer.num_bits
-        for qkv_module in qkv_modules
-        if hasattr(qkv_module, "output_quantizer") and qkv_module.output_quantizer.is_enabled
-    ]
-
-    if (4, 3) in num_bits_list:
-        return KV_CACHE_FP8
-    elif 8 in num_bits_list:
-        return KV_CACHE_INT8
-    else:
-        return QUANTIZATION_NONE
-
-
-def build_qkv(
-    qkv_modules: List[nn.Module],
-    model_metadata_config,
-    dtype: torch.dtype,
-    ext_config: DecoderLayerConfig = None,
-) -> QKVConfig:
-    """Converts the qkv modules to the config."""
-    config = QKVConfig()
-    q_bias = None
-    k_bias = None
-    v_bias = None
-
-    block_size = get_weight_block_size(qkv_modules[0])
-
-    num_heads = ext_config.num_attention_heads
-    training_tp = model_metadata_config["training_tensor_parallel"]
-    if len(qkv_modules) == 1:
-        # QKV layers combined as a single module, e.g. GPT2, GPTNext
-        qkv_module = qkv_modules[0]
-        assert ext_config is not None, "ext_config is None"
-        num_kv_heads = ext_config.num_kv_heads
-
-        if "ColumnParallelLinear" in type(qkv_module).__name__:
-            # For NEMO model, num_kv_heads/num_attention_heads is the first dimension of QKV
-            model_metadata_config["head_is_first_dim"] = True
-
-        qkv_weight = qkv_module.weight.detach()
-        if type(qkv_module).__name__ == "Conv1D":
-            if not hasattr(qkv_module, "input_quantizer") and not hasattr(
-                qkv_module, "output_quantizer"
-            ):
-                # For unquantized nn.Conv1D, the weights are transposed compared with the nn.Linear
-                qkv_weight = qkv_weight.T
-
-        # Handle the case that num_kv_heads/num_attention_heads is the first dimension of QKV.
-        # This logic covers MQA and GQA as well.
-        keep_channel_order = not model_metadata_config.get("head_is_first_dim", False)
-
-        hidden_size = qkv_module.weight.shape[1]
-        q_weight, k_weight, v_weight = _split_fused_qkv_weight_and_scaling(
-            qkv_weight,
-            hidden_size,
-            num_heads,
-            num_kv_heads,
-            training_tp,
-            False,
-            keep_channel_order,
-        )
-        qkv_activation_scaling_factor = get_activation_scaling_factor(qkv_module)
-        q_activation_scaling_factor = qkv_activation_scaling_factor
-        k_activation_scaling_factor = qkv_activation_scaling_factor
-        v_activation_scaling_factor = qkv_activation_scaling_factor
-
-        qkv_weight_scaling_factor = get_weight_scaling_factor(qkv_module)
-
-        if qkv_weight_scaling_factor is not None and qkv_weight_scaling_factor.numel() != 1:
-            # INT8 sq case
-            q_weight_scaling_factor, k_weight_scaling_factor, v_weight_scaling_factor = (
-                _split_fused_qkv_weight_and_scaling(
-                    qkv_weight_scaling_factor,
-                    hidden_size,
-                    num_heads,
-                    num_kv_heads,
-                    training_tp,
-                    True,
-                    keep_channel_order,
-                )
-            )
-        else:
-            q_weight_scaling_factor = qkv_weight_scaling_factor
-            k_weight_scaling_factor = qkv_weight_scaling_factor
-            v_weight_scaling_factor = qkv_weight_scaling_factor
-
-        # bias
-        if qkv_module.bias is not None:
-            q_bias, k_bias, v_bias = _split_fused_qkv_weight_and_scaling(
-                qkv_module.bias.detach(),
-                hidden_size,
-                num_heads,
-                num_kv_heads,
-                training_tp,
-                True,
-                keep_channel_order,
-            )
-
-        q_weight_scaling_factor_2 = k_weight_scaling_factor_2 = v_weight_scaling_factor_2 = (
-            get_weight_scaling_factor_2(qkv_module)
-        )
-
-        q_prequant_scaling_factor = k_prequant_scaling_factor = v_prequant_scaling_factor = (
-            get_prequant_scaling_factor(qkv_module, dtype)
-        )
-
-    elif len(qkv_modules) == 3:
-        # Separate QKV layers
-        q_weight = qkv_modules[0].weight.detach()
-        q_activation_scaling_factor = get_activation_scaling_factor(qkv_modules[0])
-        q_weight_scaling_factor = get_weight_scaling_factor(qkv_modules[0])
-        k_weight = qkv_modules[1].weight.detach()
-        k_activation_scaling_factor = get_activation_scaling_factor(qkv_modules[1])
-        k_weight_scaling_factor = get_weight_scaling_factor(qkv_modules[1])
-        v_weight = qkv_modules[2].weight.detach()
-        v_activation_scaling_factor = get_activation_scaling_factor(qkv_modules[2])
-        v_weight_scaling_factor = get_weight_scaling_factor(qkv_modules[2])
-
-        q_weight_scaling_factor_2 = get_weight_scaling_factor_2(qkv_modules[0])
-        k_weight_scaling_factor_2 = get_weight_scaling_factor_2(qkv_modules[1])
-        v_weight_scaling_factor_2 = get_weight_scaling_factor_2(qkv_modules[2])
-
-        q_prequant_scaling_factor = get_prequant_scaling_factor(qkv_modules[0], dtype)
-        k_prequant_scaling_factor = get_prequant_scaling_factor(qkv_modules[1], dtype)
-        v_prequant_scaling_factor = get_prequant_scaling_factor(qkv_modules[2], dtype)
-
-        if hasattr(qkv_modules[0], "bias"):
-            q_bias = qkv_modules[0].bias
-
-        if hasattr(qkv_modules[1], "bias"):
-            k_bias = qkv_modules[1].bias
-
-        if hasattr(qkv_modules[2], "bias"):
-            v_bias = qkv_modules[2].bias
-
-    else:
-        raise NotImplementedError(f"QKV modules format {qkv_modules} not supported")
-
-    # Adopt the implementation from examples/llama/weight.py in the tekit repo for INT4 AWQ
-    # Resmooth q, k, v for int4_awq, as they share the same pre_quant_scale during compulation
-    # This logic is implemented at the export stage to reduce resource requirement for model building/deployment
-    if all(
-        pre_quant_scale is not None
-        for pre_quant_scale in [
-            q_prequant_scaling_factor,
-            k_prequant_scaling_factor,
-            v_prequant_scaling_factor,
-        ]
-    ):
-        pre_quant_scale = (
-            q_prequant_scaling_factor + k_prequant_scaling_factor + v_prequant_scaling_factor
-        ) / 3.0
-        # Resmooth q, k, v with average pre_quant_scale for AWQ
-        q_weight, q_weight_scaling_factor, _ = resmooth_and_get_scale(
-            merged_weights=q_weight,
-            pre_quant_scales=[q_prequant_scaling_factor],
-            ranks=1,
-            group_size=block_size,
-            avg_pre_quant_scale=pre_quant_scale,
-        )
-        k_weight, k_weight_scaling_factor, _ = resmooth_and_get_scale(
-            merged_weights=k_weight,
-            pre_quant_scales=[k_prequant_scaling_factor],
-            ranks=1,
-            group_size=block_size,
-            avg_pre_quant_scale=pre_quant_scale,
-        )
-        v_weight, v_weight_scaling_factor, _ = resmooth_and_get_scale(
-            merged_weights=v_weight,
-            pre_quant_scales=[v_prequant_scaling_factor],
-            ranks=1,
-            group_size=block_size,
-            avg_pre_quant_scale=pre_quant_scale,
-        )
-        q_prequant_scaling_factor = k_prequant_scaling_factor = v_prequant_scaling_factor = (
-            pre_quant_scale
-        )
-
-    config.q = LinearConfig(linear_type=LINEAR_COLUMN)
-    config.q.weight = q_weight.type(dtype).cpu()
-    config.q.bias = q_bias.type(dtype) if q_bias is not None else None
-    config.q.activation_scaling_factor = q_activation_scaling_factor
-    config.q.weights_scaling_factor = q_weight_scaling_factor
-    config.q.weights_scaling_factor_2 = q_weight_scaling_factor_2
-    config.q.prequant_scaling_factor = q_prequant_scaling_factor
-    config.q.awq_block_size = block_size
-
-    config.k = LinearConfig(linear_type=LINEAR_COLUMN)
-    config.k.weight = k_weight.type(dtype).cpu()
-    config.k.bias = k_bias.type(dtype) if k_bias is not None else None
-    config.k.activation_scaling_factor = k_activation_scaling_factor
-    config.k.weights_scaling_factor = k_weight_scaling_factor
-    config.k.weights_scaling_factor_2 = k_weight_scaling_factor_2
-    config.k.prequant_scaling_factor = k_prequant_scaling_factor
-    config.k.awq_block_size = block_size
-
-    config.v = LinearConfig(linear_type=LINEAR_COLUMN)
-    config.v.weight = v_weight.type(dtype).cpu()
-    config.v.bias = v_bias.type(dtype) if v_bias is not None else None
-    config.v.activation_scaling_factor = v_activation_scaling_factor
-    config.v.weights_scaling_factor = v_weight_scaling_factor
-    config.v.weights_scaling_factor_2 = v_weight_scaling_factor_2
-    config.v.prequant_scaling_factor = v_prequant_scaling_factor
-    config.v.awq_block_size = block_size
-
-    if not ext_config.attention_head_size:
-        ext_config.attention_head_size = config.q.weight.shape[0] * training_tp // num_heads
-
-    return config
-
-
-def build_linear_config(module: nn.Module, linear_type: str, dtype: torch.dtype) -> LinearConfig:
-    """Builds the linear config for the module."""
-    assert is_linear(module)
-
-    torch_weight = module.weight.detach()
-
-    if "NormHead" in type(module).__name__:
-        torch_weight = torch.nn.functional.normalize(torch_weight)
-    elif "Conv1D" in type(module).__name__ and not (
-        hasattr(module, "input_quantizer") or hasattr(module, "output_quantizer")
-    ):
-        # Transpose Conv1D weights to linear unless it has been transposed by the quantization.
-        torch_weight = torch_weight.T
-
-    weight = torch_weight.type(dtype)
-
-    config = LinearConfig(linear_type=linear_type)
-    config.weight = weight.cpu()
-
-    if hasattr(module, "bias") and module.bias is not None:
-        config.bias = module.bias.detach().type(dtype)
-
-    config.activation_scaling_factor = get_activation_scaling_factor(module)
-    config.weights_scaling_factor = get_weight_scaling_factor(module)
-    config.weights_scaling_factor_2 = get_weight_scaling_factor_2(module)
-    config.prequant_scaling_factor = get_prequant_scaling_factor(module, dtype)
-    config.awq_block_size = get_weight_block_size(module)
-    return config
-
-
-def build_attention_config(
-    module: nn.Module,
-    model_metadata_config,
-    dtype: torch.dtype,
-    ext_config: DecoderLayerConfig = None,
-) -> AttentionConfig:
-    """Builds the attention config from the module."""
-    assert is_attention(module)
-
-    config = AttentionConfig()
-    if hasattr(module, "rotary_dim"):
-        config.rotary_dim = module.rotary_dim
-    if hasattr(module, "clip_qkv"):
-        config.clip_qkv = module.clip_qkv
-
-    qkv_modules = []
-    q = None
-    k = None
-    v = None
-    for name, layer in module.named_children():
-        if is_linear(layer):
-            if _is_qkv(name):
-                qkv_modules.append(layer)
-            elif "q" in name:
-                q = layer
-            elif "k" in name:
-                k = layer
-            elif "v" in name:
-                v = layer
-            else:
-                # The dense layer
-                config.dense = build_linear_config(layer, LINEAR_ROW, dtype)
-
-    if not qkv_modules:
-        assert q
-        assert k
-        assert v
-        qkv_modules = [q, k, v]
-
-    config.qkv = build_qkv(qkv_modules, model_metadata_config, dtype, ext_config)
-
-    config.kv_cache_scaling_factor = get_kv_cache_scaling_factor(qkv_modules)
-    if config.kv_cache_scaling_factor is not None:
-        config.kv_cache_dtype = get_kv_cache_dtype(qkv_modules)
-
-    return config
-
-
-def _is_qkv(name) -> bool:
-    return all([k in name for k in ["q", "k", "v"]]) or "W_pack" in name or "c_attn" in name
-
-
-def _get_hidden_act(act_func) -> str:
-    """Returns the name of the hidden activation functon based on ACT2FN."""
-    if isinstance(act_func, str):
-        return act_func
-
-    # Falcon activation, "nn.GELU" is equivalent to "gelu" in ACT2FN
-    if isinstance(act_func, nn.GELU):
-        return "gelu"
-
-    if hasattr(act_func, "func") and act_func.func == nn.functional.gelu:
-        return "gelu"
-
-    for name, func in ACT2FN.items():
-        if isinstance(func, tuple):
-            if isinstance(act_func, func[0]):
-                return name
-        elif isinstance(act_func, func):
-            return name
-
-    return act_func.__name__
-
-
-def build_mlp_config(module: nn.Module, decoder_type, dtype: torch.dtype) -> MLPConfig:
-    """Builds the MLP config for the module."""
-    assert is_mlp(module)
-
-    config = MLPConfig()
-
-    # fc1 and gate will be merged to the same layer for chatglm
-    if decoder_type == "chatglm":
-        config.merged_fc1_gate = True
-
-    def _split_gate_from_fc(decoder_type, module, fc_name, fc_layer):
-        if (
-            "ColumnParallelLinear" in type(fc_layer).__name__
-            and hasattr(module.config, "gated_linear_unit")
-            and module.config.gated_linear_unit
-        ):
-            return True
-
-        if decoder_type == "chatglm":
-            return True
-
-        if decoder_type != "gptnext":
-            return False
-
-        if "dense_h_to_4h" in fc_name and "dense_h_to_4h_2" not in fc_name:
-            return True
-
-        return False
-
-    # TODO: We may want to refactor these keywords/model mapping
-    fc_keywords = set(
-        [
-            "c_fc",  # gpt2
-            "fc_in",  # gptj
-            "gate_proj",  # llama, baichuan
-            "dense_h_to_4h",  # falcon, chatglm, bloom
-            "linear_fc1",
-            "w2",  # qwen
-            "fc1",  # phi, gemma
-            "gate_up_proj",  # phi
-        ]
-    )
-    proj_keywords = set(
-        [
-            "c_proj",  # gpt2, qwen
-            "fc_out",  # gptj
-            "dense_4h_to_h",  # falcon, chatglm, bloom
-            "4h_to_h",
-            "down_proj",  # llama, baichuan, mpt, phi
-            "linear_fc2",
-            "proj",
-            "fc2",  # phi, gemma
-        ]
-    )
-    gate_keywords = set(
-        [
-            "up_proj",  # llama, baichuan
-            "dense_h_to_4h_2",
-            "w1",  # qwen
-        ]
-    )
-
-    for name, layer in module.named_children():
-        if is_linear(layer):
-            # Arctic (llama-based MoE, decoder_type is "llama") has MLP keyword conflicts with Qwen
-            # Arctic's residual MLP use w1 for fc, w2 for proj, w3 for gate
-            if type(module).__name__ == "ArcticMLP":
-                fc_keywords.discard("w2")
-                gate_keywords.discard("w1")
-                fc_keywords.add("w1")
-                proj_keywords.add("w2")
-                gate_keywords.add("w3")
-
-            if decoder_type == "mpt":
-                fc_keywords.add("up_proj")
-                gate_keywords.discard("up_proj")
-
-            if type(module).__name__ == "TLGv4MLP":  # for TLGv4ForCausalLM
-                fc_keywords.add("up_proj")
-                gate_keywords.discard("up_proj")
-
-            split_gate = _split_gate_from_fc(decoder_type, module, name, layer)
-
-            if any([keyword == name for keyword in fc_keywords]):
-                if split_gate:
-                    # We have to split the gate from the fc
-                    weights = torch.chunk(layer.weight.detach().type(dtype), 2, dim=0)
-                    if decoder_type == "chatglm":
-                        # Chatglm's weight has gate first then fc in the HF model
-                        weights = (weights[1], weights[0])
-
-                    activation_scaling_factor = get_activation_scaling_factor(layer)
-                    weight_scaling_factor = get_weight_scaling_factor(layer)
-                    weight_scaling_factor_2 = get_weight_scaling_factor_2(layer)
-                    prequant_scaling_factor = get_prequant_scaling_factor(layer, dtype)
-
-                    weight_scaling_factors = [None, None]
-
-                    if weight_scaling_factor is not None:
-                        if weight_scaling_factor.numel() != 1:
-                            # for Int8 SQ case, we split the weight scaling factor into two parts.
-                            weight_scaling_factors = torch.chunk(weight_scaling_factor, 2, dim=0)
-                            if decoder_type == "chatglm":
-                                weight_scaling_factors = (
-                                    weight_scaling_factors[1],
-                                    weight_scaling_factors[0],
-                                )
-                        else:
-                            # for FP8 case that weight_scaling_factor is a scalar, we repeat it for the gate.
-                            weight_scaling_factors = (
-                                [weight_scaling_factor] * 2
-                                if weight_scaling_factor is not None
-                                else [None, None]
-                            )
-
-                    config.fc = LinearConfig()
-                    config.fc.linear_type = LINEAR_COLUMN
-                    config.fc.weight = weights[0]
-                    config.fc.weights_scaling_factor = weight_scaling_factors[0]
-                    config.fc.weights_scaling_factor_2 = weight_scaling_factor_2
-                    config.fc.activation_scaling_factor = activation_scaling_factor
-                    config.fc.prequant_scaling_factor = prequant_scaling_factor
-                    config.fc.awq_block_size = get_weight_block_size(layer)
-
-                    config.gate = LinearConfig()
-                    config.gate.linear_type = LINEAR_COLUMN
-                    config.gate.weight = weights[1]
-                    config.gate.weights_scaling_factor = weight_scaling_factors[1]
-                    config.gate.weights_scaling_factor_2 = weight_scaling_factor_2
-                    config.gate.activation_scaling_factor = activation_scaling_factor
-                    config.gate.prequant_scaling_factor = prequant_scaling_factor
-                    config.gate.awq_block_size = get_weight_block_size(layer)
-                else:
-                    config.fc = build_linear_config(layer, LINEAR_COLUMN, dtype)
-
-            elif any([keyword == name for keyword in proj_keywords]):
-                config.proj = build_linear_config(layer, LINEAR_ROW, dtype)
-            elif any([keyword == name for keyword in gate_keywords]):
-                config.gate = build_linear_config(layer, LINEAR_COLUMN, dtype)
-
-    assert config.proj is not None and config.fc is not None, "proj or fc can not be found"
-
-    hidden_act = None
-
-    if hasattr(module, "activation"):
-        hidden_act = module.activation
-    elif hasattr(module, "activation_func"):
-        # MCore activation_func can be swiglu (gated silu) or squared_relu.
-        hidden_act = module.activation_func.__name__.replace("_", "-")
-        if hidden_act in ["glu", "silu"]:
-            hidden_act = "swiglu" if decoder_type == "gptnext" else "silu"
-    else:
-        for act in ["act", "act_fn", "activation_fn"]:
-            if hasattr(module, act):
-                hidden_act = _get_hidden_act(getattr(module, act)).split("_")[0]
-                break
-
-    if decoder_type == "bloom":
-        hidden_act = "gelu"
-
-    if decoder_type == "qwen":
-        hidden_act = "silu"
-
-    if type(module).__name__ == "TLGv4MLP":
-        hidden_act = module.config.hidden_act  # The support from TRT LLM is still on-going.
-
-    if hidden_act is None:
-        raise NotImplementedError(f"{module} not supported.")
-
-    config.hidden_act = hidden_act
-    return config
-
-
-def _build_stacked_linear(experts: nn.Module, module_name, linear_type, dtype):
-    config = LinearConfig(linear_type=linear_type)
-
-    first_module = getattr(experts[0], module_name)
-    # weights
-    config.weight = torch.stack(
-        [getattr(e, module_name).weight.detach().type(dtype).to("cpu") for e in experts]
-    )
-
-    # bias
-    if hasattr(first_module, "bias") and first_module.bias is not None:
-        raise ValueError("Unexpected bias tensors inside MOE modules.")
-
-    # scaling factors
-    def get_stacked_scaling_factors(experts, get_function, module_name, karg={}):
-        if get_function(getattr(experts[0], module_name), **karg) is not None:
-            return torch.stack(
-                [get_function(getattr(e, module_name), **karg).to("cpu") for e in experts]
-            )
-        return None
-
-    config.activation_scaling_factor = get_stacked_scaling_factors(
-        experts, get_activation_scaling_factor, module_name
-    )
-    # The moe plugin only supports a single activation scaling factor for all experts
-    if config.activation_scaling_factor is not None:
-        config.activation_scaling_factor = config.activation_scaling_factor.max().unsqueeze(0)
-    config.weights_scaling_factor = get_stacked_scaling_factors(
-        experts, get_weight_scaling_factor, module_name
-    )
-    config.weights_scaling_factor_2 = get_stacked_scaling_factors(
-        experts, get_weight_scaling_factor_2, module_name
-    )
-    config.prequant_scaling_factor = get_stacked_scaling_factors(
-        experts, get_prequant_scaling_factor, module_name, {"dtype": dtype}
-    )
-    config.awq_block_size = get_weight_block_size(getattr(experts[0], module_name))
-
-    return config
-
-
-def build_stacked_experts(experts: nn.Module, dtype: torch.dtype):
-    """Builds the experts_weight_1 and experts_weight_2 configs for the experts."""
-    experts_weight_1 = _build_stacked_linear(experts, "w1", LINEAR_COLUMN, dtype)
-    experts_weight_2 = _build_stacked_linear(experts, "w2", LINEAR_ROW, dtype)
-    experts_weight_3 = _build_stacked_linear(experts, "w3", LINEAR_COLUMN, dtype)
-
-    # Concat w1 and w3 into w1
-    experts_weight_1.weight = torch.concat(
-        [experts_weight_3.weight, experts_weight_1.weight], dim=-2
-    )
-
-    def _resmooth_stack_weights(experts_weight_1, experts_weight_3):
-        """Resmooth stacked weights for experts for int4_awq."""
-        resmooth_weights = []
-        resmooth_weights_scaling_factors = []
-        resmooth_prequant_scaling_factors = []
-        group_size = experts_weight_1.awq_block_size
-        # resmooth each expert as w1 and w3 will share the average prequant_scaling_factor
-        for idx in range(experts_weight_1.weight.shape[0]):
-            merged_weight = experts_weight_1.weight[idx]  # weights should be concated already
-            pre_quant_scales = [
-                experts_weight_3.prequant_scaling_factor[idx],
-                experts_weight_1.prequant_scaling_factor[idx],
-            ]
-            (
-                resmooth_weight,
-                resmooth_scaling_factor,
-                resmooth_prequant_scaling_factor,
-            ) = resmooth_and_get_scale(
-                merged_weight,
-                pre_quant_scales,
-                len(pre_quant_scales),
-                group_size,
-            )
-            resmooth_weights.append(resmooth_weight)
-            resmooth_weights_scaling_factors.append(resmooth_scaling_factor)
-            resmooth_prequant_scaling_factors.append(resmooth_prequant_scaling_factor)
-        return (
-            torch.stack(resmooth_weights),
-            torch.stack(resmooth_weights_scaling_factors),
-            torch.stack(resmooth_prequant_scaling_factors),
-        )
-
-    def _max_stack_scaling_factor(weights_scaling_factor_1, weights_scaling_factor_3):
-        return torch.stack([weights_scaling_factor_3, weights_scaling_factor_1]).max(dim=0).values
-
-    # scaling factors
-    # TODO: check if this works with int8_sq
-    if experts_weight_1.weights_scaling_factor is not None:
-        # fp8 case, per-tensor quantization
-        if experts_weight_1.weights_scaling_factor.dim() == 2:
-            experts_weight_1.weights_scaling_factor = _max_stack_scaling_factor(
-                experts_weight_3.weights_scaling_factor, experts_weight_1.weights_scaling_factor
-            )
-        # int4 awq case, group_wise quantization, and per tensor quantization for prequant_scaling_factor
-        elif (
-            experts_weight_1.weights_scaling_factor.dim() == 3
-            and experts_weight_1.awq_block_size > 0
-            and experts_weight_1.prequant_scaling_factor.dim() == 2
-        ):
-            (
-                experts_weight_1.weight,
-                experts_weight_1.weights_scaling_factor,
-                experts_weight_1.prequant_scaling_factor,
-            ) = _resmooth_stack_weights(experts_weight_1, experts_weight_3)
-        else:
-            raise NotImplementedError(
-                "Unexpected shape of weights_scaling_factor. The quantization algorithm is not"
-                " supported."
-            )
-    if experts_weight_1.weights_scaling_factor_2 is not None:
-        if experts_weight_1.weights_scaling_factor_2.dim() == 2:
-            experts_weight_1.weights_scaling_factor_2 = _max_stack_scaling_factor(
-                experts_weight_3.weights_scaling_factor_2, experts_weight_1.weights_scaling_factor_2
-            )
-        else:
-            raise NotImplementedError(
-                "Unexpected shape of weights_scaling_factor_2. The quantization algorithm is not"
-                " supported."
-            )
-    return experts_weight_1, experts_weight_2
-
-
-def build_moe_config(module: nn.Module, decoder_type, dtype: torch.dtype) -> MOEConfig:
-    """Builds the MOE config for the module."""
-    assert is_moe(module)
-    # We only support mixtral that use the llama decoder for now
-    assert decoder_type in ["llama"]
-
-    config = MOEConfig()
-
-    # Router: TRT-LLM uses fp32 for router to keep precision
-    config.router = build_linear_config(module.gate, LINEAR_ROW, torch.float32)
-
-    # Experts
-    experts = ExpertConfig()
-    experts.fc, experts.proj = build_stacked_experts(module.experts, dtype)
-    config.experts = experts
-
-    # activation for mixtral
-    config.hidden_act = "swiglu"
-
-    return config
-
-
-def build_decoder_config(
-    module: nn.Module,
-    model_metadata_config,
-    decoder_type: str,
-    dtype: torch.dtype,
-) -> DecoderLayerConfig:
-    """Builds the full decoder config from the module."""
-    quantization = _get_quantization(module)
-    config = DecoderLayerConfig(decoder_type=decoder_type, quantization=quantization)
-    for k in ["n_head", "num_attention_heads", "n_heads"]:
-        if k in model_metadata_config:
-            config.num_attention_heads = model_metadata_config[k]
-
-    for k in [
-        "num_key_value_heads",
-        "num_kv_heads",
-        "num_kv",
-        "n_head_kv",
-        "multi_query_group_num",
-        "num_query_groups",
-    ]:
-        if k in model_metadata_config:
-            config.num_kv_heads = model_metadata_config[k]
-
-    # Supporting different attention layer config in MCoreGPTModel. If per layer config
-    # exists, override the global config.
-    if hasattr(module, "self_attention"):
-        if hasattr(module.self_attention, "config"):
-            if hasattr(module.self_attention.config, "num_attention_heads"):
-                config.num_attention_heads = module.self_attention.config.num_attention_heads
-            if hasattr(module.self_attention.config, "kv_channels"):
-                config.attention_head_size = module.self_attention.config.kv_channels
-            if hasattr(module.self_attention.config, "num_query_groups"):
-                config.num_kv_heads = module.self_attention.config.num_query_groups
-
-    for k in ["n_positions", "max_position_embeddings"]:
-        if k in model_metadata_config:
-            config.max_position_embeddings = model_metadata_config[k]
-    # 2048 is the default TRT LLM max_position_embeddings
-    if config.max_position_embeddings == 0:
-        config.max_position_embeddings = 2048
-
-    for k in ["rotary_percentage", "rotary_percent", "rotary_pct"]:
-        if k in model_metadata_config:
-            config.rotary_pct = model_metadata_config[k]
-
-    for k in ["alibi"]:
-        if k in model_metadata_config:
-            config.use_alibi = model_metadata_config[k]
-
-    for k in ["alibi_bias_max"]:
-        if k in model_metadata_config:
-            config.alibi_bias_max = model_metadata_config[k]
-
-    for k in ["new_decoder_architecture"]:
-        if k in model_metadata_config:
-            config.new_decoder_architecture = model_metadata_config[k]
-
-    for k in ["apply_residual_connection_post_layernorm"]:
-        if k in model_metadata_config:
-            config.apply_residual_connection_post_layernorm = model_metadata_config[k]
-
-    for k in ["use_cache"]:
-        if k in model_metadata_config:
-            config.use_cache = model_metadata_config[k]
-
-    for k in ["_name_or_path"]:
-        if k in model_metadata_config:
-            model_path_name = model_metadata_config[k]
-            model_version = ""
-            model_scale = ""
-            for n in ["chatglm3", "chatglm2", "chatglm"]:
-                if n in model_path_name:
-                    model_version = n
-                    break
-            for n in ["6b-32k", "6b-base", "6b"]:
-                if n in model_path_name:
-                    model_scale = n.replace("-", "_")
-                    break
-            config.model_name = model_version + "_" + model_scale
-            if "chatglm" in config.model_name:
-                assert config.model_name in (
-                    [
-                        "chatglm3_6b",
-                        "chatglm3_6b_32k",
-                        "chatglm3_6b_base",
-                        "chatglm2_6b",
-                        "chatglm2_6b_32k",
-                    ]
-                ), f"{config.model_name} is not supported now."
-
-    # For chatglm
-    for k in ["rope_ratio"]:
-        if k in model_metadata_config:
-            config.rope_ratio = model_metadata_config[k]
-
-    # For Falcon variants do not provide new_decoder_architecture, but we can infer it from the model_type
-    if "model_type" in model_metadata_config:
-        if model_metadata_config["model_type"] == "RefinedWeb":
-            # Case 1. Falcon-40B / Falcon-40B-instruct
-            # https://huggingface.co/tiiuae/falcon-40b/blob/main/config.json
-            config.new_decoder_architecture = True
-        elif model_metadata_config["model_type"] == "RefinedWebModel":
-            # Case 2. Falcon-7B / Falcon-7B-instruct
-            # https://huggingface.co/tiiuae/falcon-7b/blob/main/config.json
-            config.new_decoder_architecture = False
-    for k in ["parallel_attn"]:
-        if k in model_metadata_config:
-            config.parallel_attention = model_metadata_config[k]
-
-    # For Falcon variants, they might not specify the number of kv heads with MQA models, e.g., 7b
-    if (
-        not config.new_decoder_architecture
-        and "multi_query" in model_metadata_config
-        and model_metadata_config["multi_query"]
-    ):
-        config.num_kv_heads = 1
-
-    # For Qwen
-    for k in ["seq_length"]:
-        if k in model_metadata_config:
-            config.seq_length = model_metadata_config[k]
-
-    # For Qwen rotary_emb_base
-    # CodeLlama using different rotary_base in comparison to LLaMA v1/v2 models
-    for k in ["rotary_emb_base", "rope_theta", "rotary_base"]:
-        if k in model_metadata_config:
-            config.rotary_base = model_metadata_config[k]
-
-    # For Phi
-    config.partial_rotary_factor = model_metadata_config.get("partial_rotary_factor", 0)
-
-    # Mixture of Experts
-    for k in ["num_local_experts"]:
-        if k in model_metadata_config:
-            config.moe_num_experts = model_metadata_config[k]
-    for k in ["num_experts_per_tok"]:
-        if k in model_metadata_config:
-            config.moe_top_k = model_metadata_config[k]
-
-    # Mixture of Experts
-    for k in ["num_local_experts"]:
-        if k in model_metadata_config:
-            config.moe_num_experts = model_metadata_config[k]
-    for k in ["num_experts_per_tok"]:
-        if k in model_metadata_config:
-            config.moe_top_k = model_metadata_config[k]
-
-    for name, layer in module.named_children():
-        # We assume input_layernorm should be before the post_layernorm in decoder block layout,
-        # and residual_layernorm could be after post_layernorm
-        if is_layernorm(layer):
-            layernorm_config = build_layernorm_config(layer, dtype)
-            if name in ["ln_mlp"]:
-                config.mlp_layernorm = layernorm_config
-            elif config.input_layernorm is None:
-                config.input_layernorm = layernorm_config
-            elif config.post_layernorm is None:
-                config.post_layernorm = layernorm_config
-            else:
-                assert model_metadata_config[
-                    "parallel_attn_mlp_res"
-                ], "Unexpected layernorm in a layer"
-                config.residual_layernorm = layernorm_config
-
-        elif is_attention(layer):
-            if decoder_type in ["bloom", "falcon"]:
-                model_metadata_config["head_is_first_dim"] = True
-            config.attention = build_attention_config(layer, model_metadata_config, dtype, config)
-
-        elif is_moe(layer):
-            config.mlp = build_moe_config(layer, decoder_type, dtype)
-
-        # We assume MoE layer should be before the residual MLP layer in decoder block layout,
-        # so MLP layer after a MoE layer is treated as a residual MLP layer
-        elif is_mlp(layer):
-            if config.mlp is None:
-                config.mlp = build_mlp_config(layer, decoder_type, dtype)
-            else:
-                assert model_metadata_config["parallel_attn_mlp_res"], "Unexpected mlp in a layer"
-                config.residual_mlp = build_mlp_config(layer, decoder_type, dtype)
-
-    return config
-
-
-def _split_fused_qkv_weight_and_scaling(
-    weight: torch.Tensor,
-    hidden_size: int,
-    num_heads: int,
-    num_kv_heads: Optional[int] = None,
-    training_tp: int = 1,
-    is_scaling_factor: bool = False,
-    keep_channel_order: bool = False,
-):
-    """Reorder the qkv weight for spliting QKV weights.
-
-    The shape of the fused QKV weights in HF is different from the shape that
-    TRT-LLM requires. In particular, the weight of HF consists of interleaved
-    q, k, v head weights, while that of TRT-LLM is contigous.
-        HF     : [q1, k1, v1, ..., qh, kh, vh]
-        TRT-LLM: [q1, ..., qh, k1, ..., kh, v1, vh]
-    where qi, vi, ki are weight vectors corresponding to attention head i.
-    It's similar to multi/grouped query attention cases.
-    if keep_channel_order
-        HF     : [q1, ..., qh, k1, ..., kh, v1, ..., vh]
-        TRT-LLM: [q1, ..., qh, k1, ..., kh, v1, ..., vh]
-    """
-    # Query types and expected kv heads.
-    #  - Conventional MHA: num_heads = num_kv_heads
-    #  - Multi-Query Attention: num_kv_heads = 1
-    #  - Grouped-Query Attention: num_heads % num_kv_heads = 0
-    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
-    assert (
-        num_heads % num_kv_heads == 0
-    ), f"num_heads({num_heads}) must be divisible by num_kv_heads({num_kv_heads}))."
-
-    # The number of attention heads per group: N q head + 1 k head + 1 v head.
-    num_group_heads = num_heads // num_kv_heads + 2
-    num_kv_heads_single_tp = max(num_kv_heads // training_tp, 1)
-    size_per_head = weight.shape[0] // num_kv_heads_single_tp // num_group_heads
-
-    if size_per_head != hidden_size // num_heads:
-        print("Warning: qkv have different hidden size than the input.")
-
-    if is_scaling_factor:
-        # For AWQ, weight scaling facotrs will be a 2D array (out_feat, in_feat/group_size)
-        # Int8_sq can be regarded as a specific instance where the group_size is equal to in_feat.
-        output_ch = num_kv_heads_single_tp * num_group_heads * size_per_head
-        weight_size = weight.numel()
-
-        qkv_in = weight_size // output_ch
-    else:
-        qkv_in = hidden_size
-
-    if keep_channel_order:
-        kv_size = num_kv_heads * size_per_head
-        q_w = weight[: -2 * kv_size, ...]
-        k_w = weight[-2 * kv_size : -1 * kv_size, ...]
-        v_w = weight[-1 * kv_size :, ...]
-
-        if is_scaling_factor:
-            q_w = q_w.reshape(-1)
-            k_w = k_w.reshape(-1)
-            v_w = v_w.reshape(-1)
-    else:
-        # Split Q/K/V weights
-        weight = weight.reshape(num_kv_heads_single_tp, num_group_heads, size_per_head, qkv_in)
-        q_w = weight[:, :-2, ...]  # (nKV, num_heads // nKV, size_per_head, qkv_in)
-        k_w = weight[:, -2:-1, ...]  # (nKV, 1, size_per_head, qkv_in)
-        v_w = weight[:, -1:, ...]  # (nKV, 1, size_per_head, qkv_in)
-
-        q_w = q_w.reshape(-1, qkv_in)
-        k_w = k_w.reshape(-1, qkv_in)
-        v_w = v_w.reshape(-1, qkv_in)
-
-        if not is_scaling_factor:
-            q_w = q_w.reshape(-1, qkv_in)
-            k_w = k_w.reshape(-1, qkv_in)
-            v_w = v_w.reshape(-1, qkv_in)
-        else:
-            q_w = q_w.reshape(-1)
-            k_w = k_w.reshape(-1)
-            v_w = v_w.reshape(-1)
-
-    return q_w, k_w, v_w
-
-
-def _get_quantization(decoder_layer) -> str:
-    """Gets the quantization string."""
-    for _, decoder_sub_module in decoder_layer.named_children():
-        if is_attention(decoder_sub_module):
-            for _, layer in decoder_sub_module.named_children():
-                if is_linear(layer):
-                    if not hasattr(layer, "weight_quantizer"):
-                        return QUANTIZATION_NONE
-                    w_quantizer = layer.weight_quantizer
-                    if isinstance(w_quantizer, SequentialQuantizer):
-                        assert (
-                            len(w_quantizer) == 2
-                            and w_quantizer[0].num_bits == 4
-                            and w_quantizer[1].num_bits == (4, 3)
-                        ), "Unsupported quantizer"
-                        assert (
-                            w_quantizer[0].block_sizes
-                            and len(w_quantizer[0].block_sizes) > 0
-                            and w_quantizer[0].block_sizes[-1] > 0
-                        ), "Invalid block_sizes"
-                        return QUANTIZATION_W4A8_AWQ
-                    if w_quantizer.num_bits == 4:
-                        assert (
-                            len(w_quantizer.block_sizes) > 0 and w_quantizer.block_sizes[-1] > 0
-                        ), "Invalid block_sizes"
-                        return QUANTIZATION_INT4_AWQ
-                    elif w_quantizer.num_bits == 8:
-                        return QUANTIZATION_INT8_SQ
-                    elif w_quantizer.num_bits == (4, 3):
-                        return QUANTIZATION_FP8
-                    else:
-                        raise NotImplementedError(
-                            f"Unsupported quantizer with num_bits: {w_quantizer.num_bits}"
-                        )
-
-    return QUANTIZATION_NONE
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utils for model_config export.
+
+Some of the logics in this file are empirical and needs constant update if exceptions occur.
+"""
+
+from typing import List, Optional, Tuple
+
+import torch
+import torch.nn as nn
+
+try:
+    from transformers.activations import ACT2FN
+except Exception:
+    print("Cannot find transformers package. Hugginface modules cannot be exported.")
+    pass
+
+from ..quantization.nn import SequentialQuantizer, TensorQuantizer
+from .distribute import get_rank, get_world_size
+from .model_config import (
+    KV_CACHE_FP8,
+    KV_CACHE_INT8,
+    LAYERNORM_DEFAULT,
+    LAYERNORM_RMS,
+    LINEAR_COLUMN,
+    LINEAR_ROW,
+    QUANTIZATION_FP8,
+    QUANTIZATION_INT4_AWQ,
+    QUANTIZATION_INT8_SQ,
+    QUANTIZATION_NONE,
+    QUANTIZATION_W4A8_AWQ,
+    AttentionConfig,
+    DecoderLayerConfig,
+    EmbeddingConfig,
+    ExpertConfig,
+    LayernormConfig,
+    LinearConfig,
+    MLPConfig,
+    MOEConfig,
+    QKVConfig,
+)
+from .model_config_utils import pad_weights
+from .scaling_factor_utils import resmooth_and_get_scale
+
+
+def check_model_compatibility(module_list: List[nn.Module]) -> Tuple[bool, bool, bool]:
+    """Returns whether the list of modules is compatible with the export logic.
+
+    And if positional embedding and embedding layernorm exists.
+
+    We assumes the model to be assembled with one or two embedding layers,
+    a ModuleList of transformer decoders,
+    and a final layernorm with optional embedding layernorm.
+    Otherwise it will not be supported.
+    """
+    num_embeddings = 0
+    num_module_list = 0
+    num_layer_norm = 0
+    for module in module_list:
+        if is_embedding(module):
+            num_embeddings += 1
+        elif is_decoder_list(module):
+            num_module_list += 1
+        elif is_layernorm(module):
+            num_layer_norm += 1
+
+    return (
+        1 <= num_embeddings
+        and num_embeddings <= 2
+        and num_module_list == 1
+        and 1 <= num_layer_norm
+        and num_layer_norm <= 2,
+        num_embeddings > 1,
+        num_layer_norm > 1,
+    )
+
+
+def get_transformer_layers(model: nn.Module) -> List[nn.Module]:
+    """Returns the root module of the transformer model."""
+    if "Megatron" in type(model).__name__:
+        if hasattr(model, "model") and "GPTModel" in type(model.model).__name__:
+            # NEMO mcore models can be handled with the following branch.
+            model = model.model
+
+        # NEMO non mcore models, we need to find the language_model module first.
+        children = [model]
+        language_model = None
+        while children and not language_model:
+            next_children = []
+            for child in children:
+                if type(child).__name__ == "TransformerLanguageModel":
+                    language_model = child
+                    break
+                for m in child.children():
+                    next_children.append(m)
+            children = next_children
+        if language_model:
+            print("Warning: this is an old NEMO checkpoint format and will be deprecated soon.")
+            layers = [m for m in language_model.embedding.children()] + [
+                m for m in language_model.encoder.children()
+            ]
+
+            if hasattr(language_model, "output_layer"):
+                layers.append(language_model.output_layer)
+
+            return layers
+
+    if "GPTModel" in type(model).__name__:
+        # mcore models
+        layers = []
+        if hasattr(model, "embedding"):
+            layers = layers + [m for m in model.embedding.children()]
+        layers = layers + [m for m in model.decoder.children()]
+        if hasattr(model, "output_layer"):
+            layers.append(model.output_layer)
+        return layers
+
+    if hasattr(model, "transformer"):
+        # This is a LMHead model
+        # Add lm_head to be processed along with transformer layers
+        modules = []
+        for m in model.transformer.children():
+            if "Transformer" in type(m).__name__ and is_decoder_list(m.layers):
+                modules.append(m.layers)
+                modules.append(m.final_layernorm)
+            else:
+                modules.append(m)
+        if hasattr(model, "lm_head"):
+            modules += [model.lm_head]
+        return modules
+
+    if hasattr(model, "model"):
+        # LLAMA
+        modules = [m for m in model.model.children()]
+        if hasattr(model, "lm_head"):
+            modules += [model.lm_head]
+
+        return modules
+
+    return [m for m in model.children()]
+
+
+def is_linear(module: nn.Module) -> bool:
+    """Returns whether the module is a linear layer."""
+    return any([k in type(module).__name__ for k in ["Linear", "Conv1D", "NormHead"]])
+
+
+def is_embedding(module: nn.Module) -> bool:
+    """Returns whether the module is an embedding layer."""
+    module_type_name = type(module).__name__
+    return (
+        "Embedding" in module_type_name
+        and "Rotary" not in module_type_name
+        and "PhiImage" not in module_type_name
+    )
+
+
+def build_embedding_config(
+    module: nn.Module, dtype: torch.dtype, normalization_constant: float = 1
+) -> EmbeddingConfig:
+    """Builds the embedding config from the module."""
+    assert is_embedding(module)
+
+    world_size = get_world_size()
+    rank = get_rank()
+
+    # Special case for chatglm
+    if hasattr(module, "word_embeddings"):
+        module = module.word_embeddings
+
+    weight = module.weight.detach().type(dtype)
+    normalized_weight = weight * normalization_constant
+    if "Parallel" in type(module).__name__:
+        local_weight = normalized_weight
+    else:
+        padded_weight = pad_weights(normalized_weight, get_world_size())
+        local_weight = torch.chunk(padded_weight, world_size, dim=0)[rank]
+    return EmbeddingConfig(
+        weight=local_weight,
+    )
+
+
+def is_layernorm(module: nn.Module) -> bool:
+    """Returns whether the module is a layernorm layer."""
+    module_name = type(module).__name__
+    return any(norm in module_name for norm in ["LayerNorm", "RMSNorm"])
+
+
+def build_layernorm_config(module: nn.Module, dtype: torch.dtype) -> LayernormConfig:
+    """Builds the layernorm config from the module."""
+    assert is_layernorm(module)
+
+    layernorm_type = LAYERNORM_DEFAULT
+    if "RMS" in type(module).__name__:
+        layernorm_type = LAYERNORM_RMS
+
+    weight = module.weight.detach()
+
+    def _weights_plus_one(module):
+        if any(name in type(module).__name__ for name in ["LayerNorm1P", "GemmaRMSNorm"]):
+            return True
+
+        if hasattr(module, "zero_centered_gamma") and module.zero_centered_gamma:
+            return True
+
+        return False
+
+    if _weights_plus_one(module):
+        # megatron layernorm's weight needs to be updated.
+        weight = weight.float() + 1.0
+
+    config = LayernormConfig(
+        weight=weight.type(dtype),
+        bias=(
+            module.bias.detach().type(dtype)
+            if hasattr(module, "bias") and module.bias is not None
+            else None
+        ),
+        layernorm_type=layernorm_type,
+    )
+
+    # TODO: handle the nemo llama eps config.
+    for eps_key in ["eps", "variance_epsilon"]:
+        if hasattr(module, eps_key):
+            config.eps = getattr(module, eps_key)
+            break
+
+    return config
+
+
+def is_decoder_list(module: nn.Module) -> bool:
+    """Returns whether the module is a decoder list."""
+    return type(module) == nn.ModuleList
+
+
+def is_attention(module: nn.Module) -> bool:
+    """Returns whether the module is an attention layer."""
+    return "Attention" in type(module).__name__
+
+
+def is_mlp(module: nn.Module) -> bool:
+    """Returns whether the module is an MLP layer."""
+    return "MLP" in type(module).__name__
+
+
+def is_moe(module: nn.Module) -> bool:
+    """Returns whether the module is an MOE layer."""
+    return type(module).__name__ in ["MixtralSparseMoeBlock", "ArcticMoE"]
+
+
+def get_scaling_factor(quantizer: TensorQuantizer) -> torch.Tensor:
+    """Returns scaling factor from the quantizer as torch.Tensor."""
+    if not quantizer.is_enabled:
+        return None
+
+    amax = quantizer.export_amax()
+    if amax is None:
+        return None
+
+    # tensorrt_llm uses float as the scaling_factors.
+    scaling_factor = amax.float() / quantizer.maxbound
+
+    assert torch.all(scaling_factor > 0), f"scaling factor {scaling_factor} not positive."
+
+    return scaling_factor
+
+
+def get_activation_scaling_factor(module: nn.Module) -> torch.Tensor:
+    """Returns the activation scaling factor."""
+    return (
+        get_scaling_factor(module.input_quantizer) if hasattr(module, "input_quantizer") else None
+    )
+
+
+def get_weight_scaling_factor(module: nn.Module) -> torch.Tensor:
+    """Returns the weight scaling factor."""
+    # module.weight_quantizer could be a TensorQuantizer (for algorithms except W4A8) or
+    # a SequentialQuantizer (for W4A8). In the latter case, we need to get the scaling factor from the
+    # first quantizer of the SequentialQuantizer instance.
+    if hasattr(module, "weight_quantizer") and isinstance(
+        module.weight_quantizer, SequentialQuantizer
+    ):
+        return get_scaling_factor(module.weight_quantizer[0])
+    return (
+        get_scaling_factor(module.weight_quantizer) if hasattr(module, "weight_quantizer") else None
+    )
+
+
+def get_weight_block_size(module: nn.Module) -> int:
+    """Returns the weight block size."""
+    if not hasattr(module, "weight_quantizer"):
+        return 0
+
+    weight_quantizer = module.weight_quantizer
+
+    if isinstance(weight_quantizer, SequentialQuantizer):
+        weight_quantizer = weight_quantizer[0]
+
+    if not weight_quantizer.is_enabled:
+        return 0
+
+    block_sizes = weight_quantizer.block_sizes
+
+    if block_sizes:
+        return block_sizes[-1]
+    return 0
+
+
+def get_weight_scaling_factor_2(module: nn.Module) -> torch.Tensor:
+    """Returns the secondary weight scaling factor."""
+    if (
+        not hasattr(module, "weight_quantizer")
+        or not isinstance(module.weight_quantizer, SequentialQuantizer)
+        or not module.weight_quantizer[-1].is_enabled
+    ):
+        return None
+    assert (
+        len(module.weight_quantizer) == 2
+    ), "modelopt only supports 2 sequential quantization layers for now"
+    return get_scaling_factor(module.weight_quantizer[-1])
+
+
+def get_prequant_scaling_factor(module: nn.Module, dtype: torch.dtype) -> torch.Tensor:
+    """Returns the prequant scaling factor."""
+    prequant_scaling_factor = (
+        module.input_quantizer._pre_quant_scale.squeeze().type(dtype)
+        if hasattr(module, "input_quantizer")
+        and hasattr(module.input_quantizer, "_pre_quant_scale")
+        else None
+    )
+
+    if prequant_scaling_factor is not None:
+        assert torch.all(
+            prequant_scaling_factor > 0
+        ), f"prequant scaling factor {prequant_scaling_factor} not positive."
+    return prequant_scaling_factor
+
+
+def get_kv_cache_scaling_factor(qkv_modules: List[nn.Module]) -> torch.Tensor:
+    """Returns the kv_cache scaling factor if output quantizer is set. Else returns None by default."""
+    # For FP8, we recommend default kv cache scaling factor to be 1.
+    if get_kv_cache_dtype(qkv_modules) == KV_CACHE_FP8:
+        return torch.tensor([1.0], dtype=torch.float)
+
+    scaling_factors = [
+        get_scaling_factor(module.output_quantizer)
+        for module in qkv_modules
+        if hasattr(module, "output_quantizer")
+    ]
+
+    scaling_factors = [
+        scaling_factor for scaling_factor in scaling_factors if scaling_factor is not None
+    ]
+
+    if not scaling_factors:
+        return None
+
+    return torch.stack(scaling_factors).max(dim=0).values
+
+
+def get_kv_cache_dtype(qkv_modules: List[nn.Module]) -> str:
+    """Returns the kv_cache dtype.
+
+    If num_bits of output_quantizer is (4, 3) then returns FP8; if it is 8, returns int8,
+    otherwise returns None.
+    """
+    num_bits_list = [
+        qkv_module.output_quantizer.num_bits
+        for qkv_module in qkv_modules
+        if hasattr(qkv_module, "output_quantizer") and qkv_module.output_quantizer.is_enabled
+    ]
+
+    if (4, 3) in num_bits_list:
+        return KV_CACHE_FP8
+    elif 8 in num_bits_list:
+        return KV_CACHE_INT8
+    else:
+        return QUANTIZATION_NONE
+
+
+def build_qkv(
+    qkv_modules: List[nn.Module],
+    model_metadata_config,
+    dtype: torch.dtype,
+    ext_config: DecoderLayerConfig = None,
+) -> QKVConfig:
+    """Converts the qkv modules to the config."""
+    config = QKVConfig()
+    q_bias = None
+    k_bias = None
+    v_bias = None
+
+    block_size = get_weight_block_size(qkv_modules[0])
+
+    num_heads = ext_config.num_attention_heads
+    training_tp = model_metadata_config["training_tensor_parallel"]
+    if len(qkv_modules) == 1:
+        # QKV layers combined as a single module, e.g. GPT2, GPTNext
+        qkv_module = qkv_modules[0]
+        assert ext_config is not None, "ext_config is None"
+        num_kv_heads = ext_config.num_kv_heads
+
+        if "ColumnParallelLinear" in type(qkv_module).__name__:
+            # For NEMO model, num_kv_heads/num_attention_heads is the first dimension of QKV
+            model_metadata_config["head_is_first_dim"] = True
+
+        qkv_weight = qkv_module.weight.detach()
+        if type(qkv_module).__name__ == "Conv1D":
+            if not hasattr(qkv_module, "input_quantizer") and not hasattr(
+                qkv_module, "output_quantizer"
+            ):
+                # For unquantized nn.Conv1D, the weights are transposed compared with the nn.Linear
+                qkv_weight = qkv_weight.T
+
+        # Handle the case that num_kv_heads/num_attention_heads is the first dimension of QKV.
+        # This logic covers MQA and GQA as well.
+        keep_channel_order = not model_metadata_config.get("head_is_first_dim", False)
+
+        hidden_size = qkv_module.weight.shape[1]
+        q_weight, k_weight, v_weight = _split_fused_qkv_weight_and_scaling(
+            qkv_weight,
+            hidden_size,
+            num_heads,
+            num_kv_heads,
+            training_tp,
+            False,
+            keep_channel_order,
+        )
+        qkv_activation_scaling_factor = get_activation_scaling_factor(qkv_module)
+        q_activation_scaling_factor = qkv_activation_scaling_factor
+        k_activation_scaling_factor = qkv_activation_scaling_factor
+        v_activation_scaling_factor = qkv_activation_scaling_factor
+
+        qkv_weight_scaling_factor = get_weight_scaling_factor(qkv_module)
+
+        if qkv_weight_scaling_factor is not None and qkv_weight_scaling_factor.numel() != 1:
+            # INT8 sq case
+            q_weight_scaling_factor, k_weight_scaling_factor, v_weight_scaling_factor = (
+                _split_fused_qkv_weight_and_scaling(
+                    qkv_weight_scaling_factor,
+                    hidden_size,
+                    num_heads,
+                    num_kv_heads,
+                    training_tp,
+                    True,
+                    keep_channel_order,
+                )
+            )
+        else:
+            q_weight_scaling_factor = qkv_weight_scaling_factor
+            k_weight_scaling_factor = qkv_weight_scaling_factor
+            v_weight_scaling_factor = qkv_weight_scaling_factor
+
+        # bias
+        if qkv_module.bias is not None:
+            q_bias, k_bias, v_bias = _split_fused_qkv_weight_and_scaling(
+                qkv_module.bias.detach(),
+                hidden_size,
+                num_heads,
+                num_kv_heads,
+                training_tp,
+                True,
+                keep_channel_order,
+            )
+
+        q_weight_scaling_factor_2 = k_weight_scaling_factor_2 = v_weight_scaling_factor_2 = (
+            get_weight_scaling_factor_2(qkv_module)
+        )
+
+        q_prequant_scaling_factor = k_prequant_scaling_factor = v_prequant_scaling_factor = (
+            get_prequant_scaling_factor(qkv_module, dtype)
+        )
+
+    elif len(qkv_modules) == 3:
+        # Separate QKV layers
+        q_weight = qkv_modules[0].weight.detach()
+        q_activation_scaling_factor = get_activation_scaling_factor(qkv_modules[0])
+        q_weight_scaling_factor = get_weight_scaling_factor(qkv_modules[0])
+        k_weight = qkv_modules[1].weight.detach()
+        k_activation_scaling_factor = get_activation_scaling_factor(qkv_modules[1])
+        k_weight_scaling_factor = get_weight_scaling_factor(qkv_modules[1])
+        v_weight = qkv_modules[2].weight.detach()
+        v_activation_scaling_factor = get_activation_scaling_factor(qkv_modules[2])
+        v_weight_scaling_factor = get_weight_scaling_factor(qkv_modules[2])
+
+        q_weight_scaling_factor_2 = get_weight_scaling_factor_2(qkv_modules[0])
+        k_weight_scaling_factor_2 = get_weight_scaling_factor_2(qkv_modules[1])
+        v_weight_scaling_factor_2 = get_weight_scaling_factor_2(qkv_modules[2])
+
+        q_prequant_scaling_factor = get_prequant_scaling_factor(qkv_modules[0], dtype)
+        k_prequant_scaling_factor = get_prequant_scaling_factor(qkv_modules[1], dtype)
+        v_prequant_scaling_factor = get_prequant_scaling_factor(qkv_modules[2], dtype)
+
+        if hasattr(qkv_modules[0], "bias"):
+            q_bias = qkv_modules[0].bias
+
+        if hasattr(qkv_modules[1], "bias"):
+            k_bias = qkv_modules[1].bias
+
+        if hasattr(qkv_modules[2], "bias"):
+            v_bias = qkv_modules[2].bias
+
+    else:
+        raise NotImplementedError(f"QKV modules format {qkv_modules} not supported")
+
+    # Adopt the implementation from examples/llama/weight.py in the tekit repo for INT4 AWQ
+    # Resmooth q, k, v for int4_awq, as they share the same pre_quant_scale during compulation
+    # This logic is implemented at the export stage to reduce resource requirement for model building/deployment
+    if all(
+        pre_quant_scale is not None
+        for pre_quant_scale in [
+            q_prequant_scaling_factor,
+            k_prequant_scaling_factor,
+            v_prequant_scaling_factor,
+        ]
+    ):
+        pre_quant_scale = (
+            q_prequant_scaling_factor + k_prequant_scaling_factor + v_prequant_scaling_factor
+        ) / 3.0
+        # Resmooth q, k, v with average pre_quant_scale for AWQ
+        q_weight, q_weight_scaling_factor, _ = resmooth_and_get_scale(
+            merged_weights=q_weight,
+            pre_quant_scales=[q_prequant_scaling_factor],
+            ranks=1,
+            group_size=block_size,
+            avg_pre_quant_scale=pre_quant_scale,
+        )
+        k_weight, k_weight_scaling_factor, _ = resmooth_and_get_scale(
+            merged_weights=k_weight,
+            pre_quant_scales=[k_prequant_scaling_factor],
+            ranks=1,
+            group_size=block_size,
+            avg_pre_quant_scale=pre_quant_scale,
+        )
+        v_weight, v_weight_scaling_factor, _ = resmooth_and_get_scale(
+            merged_weights=v_weight,
+            pre_quant_scales=[v_prequant_scaling_factor],
+            ranks=1,
+            group_size=block_size,
+            avg_pre_quant_scale=pre_quant_scale,
+        )
+        q_prequant_scaling_factor = k_prequant_scaling_factor = v_prequant_scaling_factor = (
+            pre_quant_scale
+        )
+
+    config.q = LinearConfig(linear_type=LINEAR_COLUMN)
+    config.q.weight = q_weight.type(dtype).cpu()
+    config.q.bias = q_bias.type(dtype) if q_bias is not None else None
+    config.q.activation_scaling_factor = q_activation_scaling_factor
+    config.q.weights_scaling_factor = q_weight_scaling_factor
+    config.q.weights_scaling_factor_2 = q_weight_scaling_factor_2
+    config.q.prequant_scaling_factor = q_prequant_scaling_factor
+    config.q.awq_block_size = block_size
+
+    config.k = LinearConfig(linear_type=LINEAR_COLUMN)
+    config.k.weight = k_weight.type(dtype).cpu()
+    config.k.bias = k_bias.type(dtype) if k_bias is not None else None
+    config.k.activation_scaling_factor = k_activation_scaling_factor
+    config.k.weights_scaling_factor = k_weight_scaling_factor
+    config.k.weights_scaling_factor_2 = k_weight_scaling_factor_2
+    config.k.prequant_scaling_factor = k_prequant_scaling_factor
+    config.k.awq_block_size = block_size
+
+    config.v = LinearConfig(linear_type=LINEAR_COLUMN)
+    config.v.weight = v_weight.type(dtype).cpu()
+    config.v.bias = v_bias.type(dtype) if v_bias is not None else None
+    config.v.activation_scaling_factor = v_activation_scaling_factor
+    config.v.weights_scaling_factor = v_weight_scaling_factor
+    config.v.weights_scaling_factor_2 = v_weight_scaling_factor_2
+    config.v.prequant_scaling_factor = v_prequant_scaling_factor
+    config.v.awq_block_size = block_size
+
+    if not ext_config.attention_head_size:
+        ext_config.attention_head_size = config.q.weight.shape[0] * training_tp // num_heads
+
+    return config
+
+
+def build_linear_config(module: nn.Module, linear_type: str, dtype: torch.dtype) -> LinearConfig:
+    """Builds the linear config for the module."""
+    assert is_linear(module)
+
+    torch_weight = module.weight.detach()
+
+    if "NormHead" in type(module).__name__:
+        torch_weight = torch.nn.functional.normalize(torch_weight)
+    elif "Conv1D" in type(module).__name__ and not (
+        hasattr(module, "input_quantizer") or hasattr(module, "output_quantizer")
+    ):
+        # Transpose Conv1D weights to linear unless it has been transposed by the quantization.
+        torch_weight = torch_weight.T
+
+    weight = torch_weight.type(dtype)
+
+    config = LinearConfig(linear_type=linear_type)
+    config.weight = weight.cpu()
+
+    if hasattr(module, "bias") and module.bias is not None:
+        config.bias = module.bias.detach().type(dtype)
+
+    config.activation_scaling_factor = get_activation_scaling_factor(module)
+    config.weights_scaling_factor = get_weight_scaling_factor(module)
+    config.weights_scaling_factor_2 = get_weight_scaling_factor_2(module)
+    config.prequant_scaling_factor = get_prequant_scaling_factor(module, dtype)
+    config.awq_block_size = get_weight_block_size(module)
+    return config
+
+
+def build_attention_config(
+    module: nn.Module,
+    model_metadata_config,
+    dtype: torch.dtype,
+    ext_config: DecoderLayerConfig = None,
+) -> AttentionConfig:
+    """Builds the attention config from the module."""
+    assert is_attention(module)
+
+    config = AttentionConfig()
+    if hasattr(module, "rotary_dim"):
+        config.rotary_dim = module.rotary_dim
+    if hasattr(module, "clip_qkv"):
+        config.clip_qkv = module.clip_qkv
+
+    qkv_modules = []
+    q = None
+    k = None
+    v = None
+    for name, layer in module.named_children():
+        if is_linear(layer):
+            if _is_qkv(name):
+                qkv_modules.append(layer)
+            elif "q" in name:
+                q = layer
+            elif "k" in name:
+                k = layer
+            elif "v" in name:
+                v = layer
+            else:
+                # The dense layer
+                config.dense = build_linear_config(layer, LINEAR_ROW, dtype)
+
+    if not qkv_modules:
+        assert q
+        assert k
+        assert v
+        qkv_modules = [q, k, v]
+
+    config.qkv = build_qkv(qkv_modules, model_metadata_config, dtype, ext_config)
+
+    config.kv_cache_scaling_factor = get_kv_cache_scaling_factor(qkv_modules)
+    if config.kv_cache_scaling_factor is not None:
+        config.kv_cache_dtype = get_kv_cache_dtype(qkv_modules)
+
+    return config
+
+
+def _is_qkv(name) -> bool:
+    return all([k in name for k in ["q", "k", "v"]]) or "W_pack" in name or "c_attn" in name
+
+
+def _get_hidden_act(act_func) -> str:
+    """Returns the name of the hidden activation functon based on ACT2FN."""
+    if isinstance(act_func, str):
+        return act_func
+
+    # Falcon activation, "nn.GELU" is equivalent to "gelu" in ACT2FN
+    if isinstance(act_func, nn.GELU):
+        return "gelu"
+
+    if hasattr(act_func, "func") and act_func.func == nn.functional.gelu:
+        return "gelu"
+
+    for name, func in ACT2FN.items():
+        if isinstance(func, tuple):
+            if isinstance(act_func, func[0]):
+                return name
+        elif isinstance(act_func, func):
+            return name
+
+    return act_func.__name__
+
+
+def build_mlp_config(module: nn.Module, decoder_type, dtype: torch.dtype) -> MLPConfig:
+    """Builds the MLP config for the module."""
+    assert is_mlp(module)
+
+    config = MLPConfig()
+
+    # fc1 and gate will be merged to the same layer for chatglm
+    if decoder_type == "chatglm":
+        config.merged_fc1_gate = True
+
+    def _split_gate_from_fc(decoder_type, module, fc_name, fc_layer):
+        if (
+            "ColumnParallelLinear" in type(fc_layer).__name__
+            and hasattr(module.config, "gated_linear_unit")
+            and module.config.gated_linear_unit
+        ):
+            return True
+
+        if decoder_type == "chatglm":
+            return True
+
+        if decoder_type != "gptnext":
+            return False
+
+        if "dense_h_to_4h" in fc_name and "dense_h_to_4h_2" not in fc_name:
+            return True
+
+        return False
+
+    # TODO: We may want to refactor these keywords/model mapping
+    fc_keywords = set(
+        [
+            "c_fc",  # gpt2
+            "fc_in",  # gptj
+            "gate_proj",  # llama, baichuan
+            "dense_h_to_4h",  # falcon, chatglm, bloom
+            "linear_fc1",
+            "w2",  # qwen
+            "fc1",  # phi, gemma
+            "gate_up_proj",  # phi
+        ]
+    )
+    proj_keywords = set(
+        [
+            "c_proj",  # gpt2, qwen
+            "fc_out",  # gptj
+            "dense_4h_to_h",  # falcon, chatglm, bloom
+            "4h_to_h",
+            "down_proj",  # llama, baichuan, mpt, phi
+            "linear_fc2",
+            "proj",
+            "fc2",  # phi, gemma
+        ]
+    )
+    gate_keywords = set(
+        [
+            "up_proj",  # llama, baichuan
+            "dense_h_to_4h_2",
+            "w1",  # qwen
+        ]
+    )
+
+    for name, layer in module.named_children():
+        if is_linear(layer):
+            # Arctic (llama-based MoE, decoder_type is "llama") has MLP keyword conflicts with Qwen
+            # Arctic's residual MLP use w1 for fc, w2 for proj, w3 for gate
+            if type(module).__name__ == "ArcticMLP":
+                fc_keywords.discard("w2")
+                gate_keywords.discard("w1")
+                fc_keywords.add("w1")
+                proj_keywords.add("w2")
+                gate_keywords.add("w3")
+
+            if decoder_type == "mpt":
+                fc_keywords.add("up_proj")
+                gate_keywords.discard("up_proj")
+
+            if type(module).__name__ == "TLGv4MLP":  # for TLGv4ForCausalLM
+                fc_keywords.add("up_proj")
+                gate_keywords.discard("up_proj")
+
+            split_gate = _split_gate_from_fc(decoder_type, module, name, layer)
+
+            if any([keyword == name for keyword in fc_keywords]):
+                if split_gate:
+                    # We have to split the gate from the fc
+                    weights = torch.chunk(layer.weight.detach().type(dtype), 2, dim=0)
+                    if decoder_type == "chatglm":
+                        # Chatglm's weight has gate first then fc in the HF model
+                        weights = (weights[1], weights[0])
+
+                    activation_scaling_factor = get_activation_scaling_factor(layer)
+                    weight_scaling_factor = get_weight_scaling_factor(layer)
+                    weight_scaling_factor_2 = get_weight_scaling_factor_2(layer)
+                    prequant_scaling_factor = get_prequant_scaling_factor(layer, dtype)
+
+                    weight_scaling_factors = [None, None]
+
+                    if weight_scaling_factor is not None:
+                        if weight_scaling_factor.numel() != 1:
+                            # for Int8 SQ case, we split the weight scaling factor into two parts.
+                            weight_scaling_factors = torch.chunk(weight_scaling_factor, 2, dim=0)
+                            if decoder_type == "chatglm":
+                                weight_scaling_factors = (
+                                    weight_scaling_factors[1],
+                                    weight_scaling_factors[0],
+                                )
+                        else:
+                            # for FP8 case that weight_scaling_factor is a scalar, we repeat it for the gate.
+                            weight_scaling_factors = (
+                                [weight_scaling_factor] * 2
+                                if weight_scaling_factor is not None
+                                else [None, None]
+                            )
+
+                    config.fc = LinearConfig()
+                    config.fc.linear_type = LINEAR_COLUMN
+                    config.fc.weight = weights[0]
+                    config.fc.weights_scaling_factor = weight_scaling_factors[0]
+                    config.fc.weights_scaling_factor_2 = weight_scaling_factor_2
+                    config.fc.activation_scaling_factor = activation_scaling_factor
+                    config.fc.prequant_scaling_factor = prequant_scaling_factor
+                    config.fc.awq_block_size = get_weight_block_size(layer)
+
+                    config.gate = LinearConfig()
+                    config.gate.linear_type = LINEAR_COLUMN
+                    config.gate.weight = weights[1]
+                    config.gate.weights_scaling_factor = weight_scaling_factors[1]
+                    config.gate.weights_scaling_factor_2 = weight_scaling_factor_2
+                    config.gate.activation_scaling_factor = activation_scaling_factor
+                    config.gate.prequant_scaling_factor = prequant_scaling_factor
+                    config.gate.awq_block_size = get_weight_block_size(layer)
+                else:
+                    config.fc = build_linear_config(layer, LINEAR_COLUMN, dtype)
+
+            elif any([keyword == name for keyword in proj_keywords]):
+                config.proj = build_linear_config(layer, LINEAR_ROW, dtype)
+            elif any([keyword == name for keyword in gate_keywords]):
+                config.gate = build_linear_config(layer, LINEAR_COLUMN, dtype)
+
+    assert config.proj is not None and config.fc is not None, "proj or fc can not be found"
+
+    hidden_act = None
+
+    if hasattr(module, "activation"):
+        hidden_act = module.activation
+    elif hasattr(module, "activation_func"):
+        # MCore activation_func can be swiglu (gated silu) or squared_relu.
+        hidden_act = module.activation_func.__name__.replace("_", "-")
+        if hidden_act in ["glu", "silu"]:
+            hidden_act = "swiglu" if decoder_type == "gptnext" else "silu"
+    else:
+        for act in ["act", "act_fn", "activation_fn"]:
+            if hasattr(module, act):
+                hidden_act = _get_hidden_act(getattr(module, act)).split("_")[0]
+                break
+
+    if decoder_type == "bloom":
+        hidden_act = "gelu"
+
+    if decoder_type == "qwen":
+        hidden_act = "silu"
+
+    if type(module).__name__ == "TLGv4MLP":
+        hidden_act = module.config.hidden_act  # The support from TRT LLM is still on-going.
+
+    if hidden_act is None:
+        raise NotImplementedError(f"{module} not supported.")
+
+    config.hidden_act = hidden_act
+    return config
+
+
+def _build_stacked_linear(experts: nn.Module, module_name, linear_type, dtype):
+    config = LinearConfig(linear_type=linear_type)
+
+    first_module = getattr(experts[0], module_name)
+    # weights
+    config.weight = torch.stack(
+        [getattr(e, module_name).weight.detach().type(dtype).to("cpu") for e in experts]
+    )
+
+    # bias
+    if hasattr(first_module, "bias") and first_module.bias is not None:
+        raise ValueError("Unexpected bias tensors inside MOE modules.")
+
+    # scaling factors
+    def get_stacked_scaling_factors(experts, get_function, module_name, karg={}):
+        if get_function(getattr(experts[0], module_name), **karg) is not None:
+            return torch.stack(
+                [get_function(getattr(e, module_name), **karg).to("cpu") for e in experts]
+            )
+        return None
+
+    config.activation_scaling_factor = get_stacked_scaling_factors(
+        experts, get_activation_scaling_factor, module_name
+    )
+    # The moe plugin only supports a single activation scaling factor for all experts
+    if config.activation_scaling_factor is not None:
+        config.activation_scaling_factor = config.activation_scaling_factor.max().unsqueeze(0)
+    config.weights_scaling_factor = get_stacked_scaling_factors(
+        experts, get_weight_scaling_factor, module_name
+    )
+    config.weights_scaling_factor_2 = get_stacked_scaling_factors(
+        experts, get_weight_scaling_factor_2, module_name
+    )
+    config.prequant_scaling_factor = get_stacked_scaling_factors(
+        experts, get_prequant_scaling_factor, module_name, {"dtype": dtype}
+    )
+    config.awq_block_size = get_weight_block_size(getattr(experts[0], module_name))
+
+    return config
+
+
+def build_stacked_experts(experts: nn.Module, dtype: torch.dtype):
+    """Builds the experts_weight_1 and experts_weight_2 configs for the experts."""
+    experts_weight_1 = _build_stacked_linear(experts, "w1", LINEAR_COLUMN, dtype)
+    experts_weight_2 = _build_stacked_linear(experts, "w2", LINEAR_ROW, dtype)
+    experts_weight_3 = _build_stacked_linear(experts, "w3", LINEAR_COLUMN, dtype)
+
+    # Concat w1 and w3 into w1
+    experts_weight_1.weight = torch.concat(
+        [experts_weight_3.weight, experts_weight_1.weight], dim=-2
+    )
+
+    def _resmooth_stack_weights(experts_weight_1, experts_weight_3):
+        """Resmooth stacked weights for experts for int4_awq."""
+        resmooth_weights = []
+        resmooth_weights_scaling_factors = []
+        resmooth_prequant_scaling_factors = []
+        group_size = experts_weight_1.awq_block_size
+        # resmooth each expert as w1 and w3 will share the average prequant_scaling_factor
+        for idx in range(experts_weight_1.weight.shape[0]):
+            merged_weight = experts_weight_1.weight[idx]  # weights should be concated already
+            pre_quant_scales = [
+                experts_weight_3.prequant_scaling_factor[idx],
+                experts_weight_1.prequant_scaling_factor[idx],
+            ]
+            (
+                resmooth_weight,
+                resmooth_scaling_factor,
+                resmooth_prequant_scaling_factor,
+            ) = resmooth_and_get_scale(
+                merged_weight,
+                pre_quant_scales,
+                len(pre_quant_scales),
+                group_size,
+            )
+            resmooth_weights.append(resmooth_weight)
+            resmooth_weights_scaling_factors.append(resmooth_scaling_factor)
+            resmooth_prequant_scaling_factors.append(resmooth_prequant_scaling_factor)
+        return (
+            torch.stack(resmooth_weights),
+            torch.stack(resmooth_weights_scaling_factors),
+            torch.stack(resmooth_prequant_scaling_factors),
+        )
+
+    def _max_stack_scaling_factor(weights_scaling_factor_1, weights_scaling_factor_3):
+        return torch.stack([weights_scaling_factor_3, weights_scaling_factor_1]).max(dim=0).values
+
+    # scaling factors
+    # TODO: check if this works with int8_sq
+    if experts_weight_1.weights_scaling_factor is not None:
+        # fp8 case, per-tensor quantization
+        if experts_weight_1.weights_scaling_factor.dim() == 2:
+            experts_weight_1.weights_scaling_factor = _max_stack_scaling_factor(
+                experts_weight_3.weights_scaling_factor, experts_weight_1.weights_scaling_factor
+            )
+        # int4 awq case, group_wise quantization, and per tensor quantization for prequant_scaling_factor
+        elif (
+            experts_weight_1.weights_scaling_factor.dim() == 3
+            and experts_weight_1.awq_block_size > 0
+            and experts_weight_1.prequant_scaling_factor.dim() == 2
+        ):
+            (
+                experts_weight_1.weight,
+                experts_weight_1.weights_scaling_factor,
+                experts_weight_1.prequant_scaling_factor,
+            ) = _resmooth_stack_weights(experts_weight_1, experts_weight_3)
+        else:
+            raise NotImplementedError(
+                "Unexpected shape of weights_scaling_factor. The quantization algorithm is not"
+                " supported."
+            )
+    if experts_weight_1.weights_scaling_factor_2 is not None:
+        if experts_weight_1.weights_scaling_factor_2.dim() == 2:
+            experts_weight_1.weights_scaling_factor_2 = _max_stack_scaling_factor(
+                experts_weight_3.weights_scaling_factor_2, experts_weight_1.weights_scaling_factor_2
+            )
+        else:
+            raise NotImplementedError(
+                "Unexpected shape of weights_scaling_factor_2. The quantization algorithm is not"
+                " supported."
+            )
+    return experts_weight_1, experts_weight_2
+
+
+def build_moe_config(module: nn.Module, decoder_type, dtype: torch.dtype) -> MOEConfig:
+    """Builds the MOE config for the module."""
+    assert is_moe(module)
+    # We only support mixtral that use the llama decoder for now
+    assert decoder_type in ["llama"]
+
+    config = MOEConfig()
+
+    # Router: TRT-LLM uses fp32 for router to keep precision
+    config.router = build_linear_config(module.gate, LINEAR_ROW, torch.float32)
+
+    # Experts
+    experts = ExpertConfig()
+    experts.fc, experts.proj = build_stacked_experts(module.experts, dtype)
+    config.experts = experts
+
+    # activation for mixtral
+    config.hidden_act = "swiglu"
+
+    return config
+
+
+def build_decoder_config(
+    module: nn.Module,
+    model_metadata_config,
+    decoder_type: str,
+    dtype: torch.dtype,
+) -> DecoderLayerConfig:
+    """Builds the full decoder config from the module."""
+    quantization = _get_quantization(module)
+    config = DecoderLayerConfig(decoder_type=decoder_type, quantization=quantization)
+    for k in ["n_head", "num_attention_heads", "n_heads"]:
+        if k in model_metadata_config:
+            config.num_attention_heads = model_metadata_config[k]
+
+    for k in [
+        "num_key_value_heads",
+        "num_kv_heads",
+        "num_kv",
+        "n_head_kv",
+        "multi_query_group_num",
+        "num_query_groups",
+    ]:
+        if k in model_metadata_config:
+            config.num_kv_heads = model_metadata_config[k]
+
+    # Supporting different attention layer config in MCoreGPTModel. If per layer config
+    # exists, override the global config.
+    if hasattr(module, "self_attention"):
+        if hasattr(module.self_attention, "config"):
+            if hasattr(module.self_attention.config, "num_attention_heads"):
+                config.num_attention_heads = module.self_attention.config.num_attention_heads
+            if hasattr(module.self_attention.config, "kv_channels"):
+                config.attention_head_size = module.self_attention.config.kv_channels
+            if hasattr(module.self_attention.config, "num_query_groups"):
+                config.num_kv_heads = module.self_attention.config.num_query_groups
+
+    for k in ["n_positions", "max_position_embeddings"]:
+        if k in model_metadata_config:
+            config.max_position_embeddings = model_metadata_config[k]
+    # 2048 is the default TRT LLM max_position_embeddings
+    if config.max_position_embeddings == 0:
+        config.max_position_embeddings = 2048
+
+    for k in ["rotary_percentage", "rotary_percent", "rotary_pct"]:
+        if k in model_metadata_config:
+            config.rotary_pct = model_metadata_config[k]
+
+    for k in ["alibi"]:
+        if k in model_metadata_config:
+            config.use_alibi = model_metadata_config[k]
+
+    for k in ["alibi_bias_max"]:
+        if k in model_metadata_config:
+            config.alibi_bias_max = model_metadata_config[k]
+
+    for k in ["new_decoder_architecture"]:
+        if k in model_metadata_config:
+            config.new_decoder_architecture = model_metadata_config[k]
+
+    for k in ["apply_residual_connection_post_layernorm"]:
+        if k in model_metadata_config:
+            config.apply_residual_connection_post_layernorm = model_metadata_config[k]
+
+    for k in ["use_cache"]:
+        if k in model_metadata_config:
+            config.use_cache = model_metadata_config[k]
+
+    for k in ["_name_or_path"]:
+        if k in model_metadata_config:
+            model_path_name = model_metadata_config[k]
+            model_version = ""
+            model_scale = ""
+            for n in ["chatglm3", "chatglm2", "chatglm"]:
+                if n in model_path_name:
+                    model_version = n
+                    break
+            for n in ["6b-32k", "6b-base", "6b"]:
+                if n in model_path_name:
+                    model_scale = n.replace("-", "_")
+                    break
+            config.model_name = model_version + "_" + model_scale
+            if "chatglm" in config.model_name:
+                assert config.model_name in (
+                    [
+                        "chatglm3_6b",
+                        "chatglm3_6b_32k",
+                        "chatglm3_6b_base",
+                        "chatglm2_6b",
+                        "chatglm2_6b_32k",
+                    ]
+                ), f"{config.model_name} is not supported now."
+
+    # For chatglm
+    for k in ["rope_ratio"]:
+        if k in model_metadata_config:
+            config.rope_ratio = model_metadata_config[k]
+
+    # For Falcon variants do not provide new_decoder_architecture, but we can infer it from the model_type
+    if "model_type" in model_metadata_config:
+        if model_metadata_config["model_type"] == "RefinedWeb":
+            # Case 1. Falcon-40B / Falcon-40B-instruct
+            # https://huggingface.co/tiiuae/falcon-40b/blob/main/config.json
+            config.new_decoder_architecture = True
+        elif model_metadata_config["model_type"] == "RefinedWebModel":
+            # Case 2. Falcon-7B / Falcon-7B-instruct
+            # https://huggingface.co/tiiuae/falcon-7b/blob/main/config.json
+            config.new_decoder_architecture = False
+    for k in ["parallel_attn"]:
+        if k in model_metadata_config:
+            config.parallel_attention = model_metadata_config[k]
+
+    # For Falcon variants, they might not specify the number of kv heads with MQA models, e.g., 7b
+    if (
+        not config.new_decoder_architecture
+        and "multi_query" in model_metadata_config
+        and model_metadata_config["multi_query"]
+    ):
+        config.num_kv_heads = 1
+
+    # For Qwen
+    for k in ["seq_length"]:
+        if k in model_metadata_config:
+            config.seq_length = model_metadata_config[k]
+
+    # For Qwen rotary_emb_base
+    # CodeLlama using different rotary_base in comparison to LLaMA v1/v2 models
+    for k in ["rotary_emb_base", "rope_theta", "rotary_base"]:
+        if k in model_metadata_config:
+            config.rotary_base = model_metadata_config[k]
+
+    # For Phi
+    config.partial_rotary_factor = model_metadata_config.get("partial_rotary_factor", 0)
+
+    # Mixture of Experts
+    for k in ["num_local_experts"]:
+        if k in model_metadata_config:
+            config.moe_num_experts = model_metadata_config[k]
+    for k in ["num_experts_per_tok"]:
+        if k in model_metadata_config:
+            config.moe_top_k = model_metadata_config[k]
+
+    # Mixture of Experts
+    for k in ["num_local_experts"]:
+        if k in model_metadata_config:
+            config.moe_num_experts = model_metadata_config[k]
+    for k in ["num_experts_per_tok"]:
+        if k in model_metadata_config:
+            config.moe_top_k = model_metadata_config[k]
+
+    for name, layer in module.named_children():
+        # We assume input_layernorm should be before the post_layernorm in decoder block layout,
+        # and residual_layernorm could be after post_layernorm
+        if is_layernorm(layer):
+            layernorm_config = build_layernorm_config(layer, dtype)
+            if name in ["ln_mlp"]:
+                config.mlp_layernorm = layernorm_config
+            elif config.input_layernorm is None:
+                config.input_layernorm = layernorm_config
+            elif config.post_layernorm is None:
+                config.post_layernorm = layernorm_config
+            else:
+                assert model_metadata_config[
+                    "parallel_attn_mlp_res"
+                ], "Unexpected layernorm in a layer"
+                config.residual_layernorm = layernorm_config
+
+        elif is_attention(layer):
+            if decoder_type in ["bloom", "falcon"]:
+                model_metadata_config["head_is_first_dim"] = True
+            config.attention = build_attention_config(layer, model_metadata_config, dtype, config)
+
+        elif is_moe(layer):
+            config.mlp = build_moe_config(layer, decoder_type, dtype)
+
+        # We assume MoE layer should be before the residual MLP layer in decoder block layout,
+        # so MLP layer after a MoE layer is treated as a residual MLP layer
+        elif is_mlp(layer):
+            if config.mlp is None:
+                config.mlp = build_mlp_config(layer, decoder_type, dtype)
+            else:
+                assert model_metadata_config["parallel_attn_mlp_res"], "Unexpected mlp in a layer"
+                config.residual_mlp = build_mlp_config(layer, decoder_type, dtype)
+
+    return config
+
+
+def _split_fused_qkv_weight_and_scaling(
+    weight: torch.Tensor,
+    hidden_size: int,
+    num_heads: int,
+    num_kv_heads: Optional[int] = None,
+    training_tp: int = 1,
+    is_scaling_factor: bool = False,
+    keep_channel_order: bool = False,
+):
+    """Reorder the qkv weight for spliting QKV weights.
+
+    The shape of the fused QKV weights in HF is different from the shape that
+    TRT-LLM requires. In particular, the weight of HF consists of interleaved
+    q, k, v head weights, while that of TRT-LLM is contigous.
+        HF     : [q1, k1, v1, ..., qh, kh, vh]
+        TRT-LLM: [q1, ..., qh, k1, ..., kh, v1, vh]
+    where qi, vi, ki are weight vectors corresponding to attention head i.
+    It's similar to multi/grouped query attention cases.
+    if keep_channel_order
+        HF     : [q1, ..., qh, k1, ..., kh, v1, ..., vh]
+        TRT-LLM: [q1, ..., qh, k1, ..., kh, v1, ..., vh]
+    """
+    # Query types and expected kv heads.
+    #  - Conventional MHA: num_heads = num_kv_heads
+    #  - Multi-Query Attention: num_kv_heads = 1
+    #  - Grouped-Query Attention: num_heads % num_kv_heads = 0
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    assert (
+        num_heads % num_kv_heads == 0
+    ), f"num_heads({num_heads}) must be divisible by num_kv_heads({num_kv_heads}))."
+
+    # The number of attention heads per group: N q head + 1 k head + 1 v head.
+    num_group_heads = num_heads // num_kv_heads + 2
+    num_kv_heads_single_tp = max(num_kv_heads // training_tp, 1)
+    size_per_head = weight.shape[0] // num_kv_heads_single_tp // num_group_heads
+
+    if size_per_head != hidden_size // num_heads:
+        print("Warning: qkv have different hidden size than the input.")
+
+    if is_scaling_factor:
+        # For AWQ, weight scaling facotrs will be a 2D array (out_feat, in_feat/group_size)
+        # Int8_sq can be regarded as a specific instance where the group_size is equal to in_feat.
+        output_ch = num_kv_heads_single_tp * num_group_heads * size_per_head
+        weight_size = weight.numel()
+
+        qkv_in = weight_size // output_ch
+    else:
+        qkv_in = hidden_size
+
+    if keep_channel_order:
+        kv_size = num_kv_heads * size_per_head
+        q_w = weight[: -2 * kv_size, ...]
+        k_w = weight[-2 * kv_size : -1 * kv_size, ...]
+        v_w = weight[-1 * kv_size :, ...]
+
+        if is_scaling_factor:
+            q_w = q_w.reshape(-1)
+            k_w = k_w.reshape(-1)
+            v_w = v_w.reshape(-1)
+    else:
+        # Split Q/K/V weights
+        weight = weight.reshape(num_kv_heads_single_tp, num_group_heads, size_per_head, qkv_in)
+        q_w = weight[:, :-2, ...]  # (nKV, num_heads // nKV, size_per_head, qkv_in)
+        k_w = weight[:, -2:-1, ...]  # (nKV, 1, size_per_head, qkv_in)
+        v_w = weight[:, -1:, ...]  # (nKV, 1, size_per_head, qkv_in)
+
+        q_w = q_w.reshape(-1, qkv_in)
+        k_w = k_w.reshape(-1, qkv_in)
+        v_w = v_w.reshape(-1, qkv_in)
+
+        if not is_scaling_factor:
+            q_w = q_w.reshape(-1, qkv_in)
+            k_w = k_w.reshape(-1, qkv_in)
+            v_w = v_w.reshape(-1, qkv_in)
+        else:
+            q_w = q_w.reshape(-1)
+            k_w = k_w.reshape(-1)
+            v_w = v_w.reshape(-1)
+
+    return q_w, k_w, v_w
+
+
+def _get_quantization(decoder_layer) -> str:
+    """Gets the quantization string."""
+    for _, decoder_sub_module in decoder_layer.named_children():
+        if is_attention(decoder_sub_module):
+            for _, layer in decoder_sub_module.named_children():
+                if is_linear(layer):
+                    if not hasattr(layer, "weight_quantizer"):
+                        return QUANTIZATION_NONE
+                    w_quantizer = layer.weight_quantizer
+                    if isinstance(w_quantizer, SequentialQuantizer):
+                        assert (
+                            len(w_quantizer) == 2
+                            and w_quantizer[0].num_bits == 4
+                            and w_quantizer[1].num_bits == (4, 3)
+                        ), "Unsupported quantizer"
+                        assert (
+                            w_quantizer[0].block_sizes
+                            and len(w_quantizer[0].block_sizes) > 0
+                            and w_quantizer[0].block_sizes[-1] > 0
+                        ), "Invalid block_sizes"
+                        return QUANTIZATION_W4A8_AWQ
+                    if w_quantizer.num_bits == 4:
+                        assert (
+                            len(w_quantizer.block_sizes) > 0 and w_quantizer.block_sizes[-1] > 0
+                        ), "Invalid block_sizes"
+                        return QUANTIZATION_INT4_AWQ
+                    elif w_quantizer.num_bits == 8:
+                        return QUANTIZATION_INT8_SQ
+                    elif w_quantizer.num_bits == (4, 3):
+                        return QUANTIZATION_FP8
+                    else:
+                        raise NotImplementedError(
+                            f"Unsupported quantizer with num_bits: {w_quantizer.num_bits}"
+                        )
+
+    return QUANTIZATION_NONE
```

## modelopt/torch/export/model_config.py

 * *Ordering differences only*

```diff
@@ -1,448 +1,448 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""This module defines the model_config format.
-
-This format can be converted from huggingface, nemo or modelopt-quantized model.
-And we will build tensorrt_llm engine from the context saved with this format.
-"""
-
-import math
-from dataclasses import dataclass, field
-from typing import List, Union
-
-import torch
-
-DECODER_GPT2 = "gpt2"
-DECODER_GPTJ = "gptj"
-DECODER_LLAMA = "llama"
-DECODER_GPTNEXT = "gptnext"
-DECODER_FALCON = "falcon"
-DECODER_BAICHUAN = "baichuan"
-DECODER_MPT = "mpt"
-DECODER_BLOOM = "bloom"
-DECODER_CHATGLM = "chatglm"
-DECODER_QWEN = "qwen"
-
-QUANTIZATION_NONE = ""
-QUANTIZATION_FP8 = "fp8"
-QUANTIZATION_INT8_SQ = "int8_sq"
-QUANTIZATION_INT4_AWQ = "int4_awq"
-QUANTIZATION_W4A8_AWQ = "w4a8_awq"
-
-KV_CACHE_FP8 = "FP8"
-KV_CACHE_INT8 = "INT8"
-
-LINEAR_COLUMN = "column"
-LINEAR_ROW = "row"
-
-LAYERNORM_DEFAULT = ""
-LAYERNORM_RMS = "rms"
-
-CURRENT_VERSION = 1.8
-
-
-@dataclass
-class EmbeddingConfig:
-    """The embedding layer config."""
-
-    weight: torch.Tensor = None
-
-    @property
-    def local_vocab_size(self):
-        """Infers the vocab_size from the embedding layer weights shape."""
-        return self.weight.shape[0]
-
-    @property
-    def hidden_size(self):
-        """Infers the hidden_size from the embedding layer weights shape."""
-        return self.weight.shape[1]
-
-
-@dataclass
-class LayernormConfig:
-    """The layernorm layer config."""
-
-    weight: torch.Tensor = None
-    bias: torch.Tensor = None
-    layernorm_type: str = LAYERNORM_DEFAULT
-    eps: float = 1e-5
-
-
-@dataclass
-class LinearConfig:
-    """The linear layer config."""
-
-    linear_type: str = LINEAR_COLUMN
-    weight: torch.Tensor = None
-    bias: torch.Tensor = None
-    activation_scaling_factor: torch.Tensor = None
-    weights_scaling_factor: torch.Tensor = None
-
-    # For methods like W4A8 AWQ, we have two quantizers for weights
-    # For W4A8, the first quantizer is for INT4 quantization and the second quantizer is for FP8 quantization
-    # `weight_scaling_factor_2` is the scaling factor the the second FP8 quantizer
-    weights_scaling_factor_2: torch.Tensor = None
-
-    prequant_scaling_factor: torch.Tensor = None
-    awq_block_size: int = 0
-
-
-@dataclass
-class QKVConfig:
-    """The QKV layer config."""
-
-    q: LinearConfig = None
-    k: LinearConfig = None
-    v: LinearConfig = None
-
-    @property
-    def weight(self):
-        """The generated linear layer weight.
-
-        The Q, K, V weights are concat together to fit the TensorRT-LLM QKV linear layer.
-        """
-        return torch.cat((self.q.weight, self.k.weight, self.v.weight))
-
-    @property
-    def bias(self):
-        """The generated linear layer bias.
-
-        The Q, K, V bias are concat together to fit the TensorRT-LLM QKV linear layer.
-        """
-        if self.q.bias is None:
-            assert (
-                self.k.bias is None and self.v.bias is None
-            ), "K and V should have valid bias as Q"
-            return None
-        return torch.cat((self.q.bias, self.k.bias, self.v.bias))
-
-    @property
-    def activation_scaling_factor(self):
-        """Returns the merged activation_scaling_factor across Q, K and V.
-
-        The max of the Q, K, V activation scaling factors is returned.
-        """
-        if (
-            self.q.activation_scaling_factor is None
-            or self.k.activation_scaling_factor is None
-            or self.v.activation_scaling_factor is None
-        ):
-            return None
-
-        return (
-            torch.stack(
-                [
-                    self.q.activation_scaling_factor,
-                    self.k.activation_scaling_factor,
-                    self.v.activation_scaling_factor,
-                ]
-            )
-            .max(dim=0)
-            .values
-        )
-
-    @property
-    def weights_scaling_factor(self):
-        """Returns the merged weights_scaling_factor across Q, K and V.
-
-        If the quantization is FP8, the max of the Q, K, V weight scaling factors is returned.
-        If the quanitzation is INT8_SQ, the concat value is returned.
-        """
-        if (
-            self.q.weights_scaling_factor is None
-            or self.k.weights_scaling_factor is None
-            or self.v.weights_scaling_factor is None
-        ):
-            return None
-
-        if self.q.weights_scaling_factor.numel() != 1:
-            # for Int4 AWQ and Int8 SQ case, we concatenate the
-            # q_weight_scaling_factor, k_weight_scaling_factor, v_weight_scaling_factor
-            qkv_weights_scaling_factor = torch.cat(
-                (
-                    self.q.weights_scaling_factor,
-                    self.k.weights_scaling_factor,
-                    self.v.weights_scaling_factor,
-                )
-            )
-        else:
-            # for FP8 set qkv_weight_scaling_factor to the max of
-            # q_weight_scaling_factor, k_weight_scaling_factor, v_weight_scaling_factor
-            qkv_weights_scaling_factor = (
-                torch.stack(
-                    [
-                        self.q.weights_scaling_factor,
-                        self.k.weights_scaling_factor,
-                        self.v.weights_scaling_factor,
-                    ],
-                )
-                .max(dim=0)
-                .values
-            )
-        return qkv_weights_scaling_factor
-
-    @property
-    def weights_scaling_factor_2(self):
-        """Returns the merged weights_scaling_factor_2 across Q, K and V.
-
-        weight_scaling_factor_2 is needed for W4A8 AWQ.
-        """
-        if (
-            self.q.weights_scaling_factor_2 is None
-            or self.k.weights_scaling_factor_2 is None
-            or self.v.weights_scaling_factor_2 is None
-        ):
-            return None
-
-        # For W4A8 AWQ, weight_scaling_factor_2 corresponds to the per-tensor FP8 quantization.
-        # Hence weight_scaling_factor_2 should be a scalar.
-        assert self.q.weights_scaling_factor_2.numel() == 1
-
-        # set qkv_weight_scaling_factor_2 to the max of q,k,v weight_scaling_factor_2
-        qkv_weights_scaling_factor_2 = (
-            torch.stack(
-                [
-                    self.q.weights_scaling_factor_2,
-                    self.k.weights_scaling_factor_2,
-                    self.v.weights_scaling_factor_2,
-                ]
-            )
-            .max(dim=0)
-            .values
-        )
-
-        return qkv_weights_scaling_factor_2
-
-    @property
-    def prequant_scaling_factor(self):
-        """Returns the merged prequant_scaling_factor across Q, K and V.
-
-        Prequant scaling factors for Q, K, V should be the same. So just return one of them.
-        """
-        if (
-            self.q.prequant_scaling_factor is None
-            or self.k.prequant_scaling_factor is None
-            or self.v.prequant_scaling_factor is None
-        ):
-            return None
-
-        assert torch.equal(
-            self.q.prequant_scaling_factor, self.k.prequant_scaling_factor
-        ) and torch.equal(
-            self.k.prequant_scaling_factor, self.v.prequant_scaling_factor
-        ), "Prequant scaling factors of Q, K and V should be the same"
-        return self.q.prequant_scaling_factor
-
-    @property
-    def awq_block_size(self):
-        """Returns the awq_block_size of this QKV layer."""
-        assert (
-            self.q.awq_block_size == self.k.awq_block_size == self.v.awq_block_size
-        ), "awq_block_size of QKV should be the same."
-        return self.q.awq_block_size
-
-
-@dataclass
-class AttentionConfig:
-    """The attention layer config."""
-
-    # QKV can either be stored as splitted (for easier postprocessing)
-    # or merged (for TRT LLM export)
-    qkv: Union[QKVConfig, LinearConfig] = None
-    dense: LinearConfig = None
-    kv_cache_scaling_factor: torch.Tensor = None
-    kv_cache_dtype: str = None
-
-    rotary_dim: int = -math.inf
-    # MPT variants
-    clip_qkv: float = None
-
-
-@dataclass
-class MLPConfig:
-    """The MLP layer config."""
-
-    fc: LinearConfig = None
-    gate: LinearConfig = None
-    proj: LinearConfig = None
-    hidden_act: str = ""
-    # we could merge the fc1 gemm and gate gemm in the runtime
-    merged_fc1_gate: bool = False
-
-
-@dataclass
-class ExpertConfig:
-    """The Expert config."""
-
-    # Aligning the naming convesion with TRT-LLM
-    fc: LinearConfig = None  # stacked experts for concatenated w3 and w1
-    proj: LinearConfig = None  # stacked experts for w2
-
-
-@dataclass
-class MOEConfig:
-    """The Mixture of Expert layer config."""
-
-    router: LinearConfig = None
-    experts: ExpertConfig = None
-    hidden_act: str = ""
-
-    @property
-    def fc(self):
-        """Return the fc module from experts."""
-        return self.experts.fc
-
-
-@dataclass
-class DecoderLayerConfig:
-    """The decoder layer config."""
-
-    quantization: str = QUANTIZATION_NONE
-
-    decoder_type: str = ""
-    input_layernorm: LayernormConfig = None
-    mlp_layernorm: LayernormConfig = None
-    attention: AttentionConfig = None
-    post_layernorm: LayernormConfig = None
-    mlp: Union[MLPConfig, MOEConfig] = None
-
-    num_attention_heads: int = 0
-    # Supporting different attention_head_size per layer.
-    attention_head_size: int = None
-
-    num_kv_heads: int = 0
-    max_position_embeddings: int = 0
-    rotary_pct: float = 1.0
-
-    # Falcon and Baichuan variants
-    use_alibi: bool = False
-    new_decoder_architecture: bool = False
-    parallel_attention: bool = False
-
-    # chatglm variants
-    apply_residual_connection_post_layernorm: bool = False
-    use_cache: bool = True
-    model_name: str = ""
-    rope_ratio: float = 1.0
-
-    # Qwen config
-    seq_length: int = 0
-
-    # Qwen and CodeLlama
-    rotary_base: int = 0
-
-    # Phi
-    partial_rotary_factor: float = 0
-
-    # Mixture of Experts
-    moe_num_experts: int = 0
-    moe_top_k: int = 0
-    moe_tp_mode: int = 0
-    moe_renorm_mode: int = 0
-
-    # MPT
-    alibi_bias_max: int = 0
-
-    # Arctic variants
-    residual_layernorm: LayernormConfig = None
-    residual_mlp: MLPConfig = None
-
-    @property
-    def hidden_size(self):
-        """Returns the hidden size of the transformer model."""
-        if isinstance(self.mlp, MOEConfig):
-            # fc.weight for MOE is stacked
-            return self.mlp.fc.weight.shape[-1]
-        else:
-            return self.mlp.fc.weight.shape[1]
-
-    @property
-    def ffn_hidden_size_local(self):
-        """Returns the ffn hidden size of the transformer model."""
-        fc = self.mlp.fc
-        if isinstance(self.mlp, MOEConfig):
-            # fc in MoE merge fc and gate
-            k = fc.weight.shape[1] // 2
-        else:
-            k = fc.weight.shape[0]
-        if self.quantization not in [QUANTIZATION_INT4_AWQ, QUANTIZATION_W4A8_AWQ]:
-            return k
-        return k * 2
-
-
-@dataclass
-class ModelConfig:
-    """The full LLM model config that includes the full information needed for tensorrt_llm engine building.
-
-    This class includes all the fields that tensorrt_llm supports, but not all of the fields are required.
-    pipeline_parallel > 1 is only supported for TensorRT-LLM checkpoint.
-    """
-
-    version: float = 0.0
-
-    # Global metadata
-    quantization: str = QUANTIZATION_NONE
-    dtype: str = "float16"
-    vocab_size: int = 0
-
-    # Parallel metadata
-    rank: int = 0
-    tensor_parallel: int = 1
-    pipeline_parallel: int = 1
-
-    # Model structure and weights
-    vocab_embedding: EmbeddingConfig = None
-    position_embedding: EmbeddingConfig = None
-    ln_embed: LayernormConfig = None
-    layers: List[DecoderLayerConfig] = field(default_factory=list)
-
-    ln_f: LayernormConfig = None
-
-    lm_head: LinearConfig = None
-    share_embedding_table: bool = False
-
-    @property
-    def vocab_size_padded(self):
-        """Returns the padded vocab_size of the model rounds to the tensor_parallel."""
-
-        def _pad_vocab_size(vocab_size, tp_size):
-            return int(math.ceil(vocab_size / tp_size) * tp_size)
-
-        return _pad_vocab_size(self.vocab_size, self.tensor_parallel)
-
-    @property
-    def hidden_size(self):
-        """Returns the hidden_size of the model."""
-        return self.layers[0].hidden_size
-
-    @property
-    def max_position_embeddings(self):
-        """Returns the max_position_embedding of the model."""
-        return self.layers[0].max_position_embeddings
-
-    @property
-    def num_attention_heads(self):
-        """Returns the num_attention_heads of the model."""
-        return self.layers[0].num_attention_heads
-
-    @property
-    def num_kv_heads(self):
-        """Returns the num_key_value_heads of the model."""
-        return (
-            self.layers[0].num_kv_heads
-            if self.layers[0].num_kv_heads > 0
-            else self.num_attention_heads
-        )
-
-    @property
-    def hidden_act(self):
-        """Returns the hidden_act of the model."""
-        return self.layers[0].mlp.hidden_act
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""This module defines the model_config format.
+
+This format can be converted from huggingface, nemo or modelopt-quantized model.
+And we will build tensorrt_llm engine from the context saved with this format.
+"""
+
+import math
+from dataclasses import dataclass, field
+from typing import List, Union
+
+import torch
+
+DECODER_GPT2 = "gpt2"
+DECODER_GPTJ = "gptj"
+DECODER_LLAMA = "llama"
+DECODER_GPTNEXT = "gptnext"
+DECODER_FALCON = "falcon"
+DECODER_BAICHUAN = "baichuan"
+DECODER_MPT = "mpt"
+DECODER_BLOOM = "bloom"
+DECODER_CHATGLM = "chatglm"
+DECODER_QWEN = "qwen"
+
+QUANTIZATION_NONE = ""
+QUANTIZATION_FP8 = "fp8"
+QUANTIZATION_INT8_SQ = "int8_sq"
+QUANTIZATION_INT4_AWQ = "int4_awq"
+QUANTIZATION_W4A8_AWQ = "w4a8_awq"
+
+KV_CACHE_FP8 = "FP8"
+KV_CACHE_INT8 = "INT8"
+
+LINEAR_COLUMN = "column"
+LINEAR_ROW = "row"
+
+LAYERNORM_DEFAULT = ""
+LAYERNORM_RMS = "rms"
+
+CURRENT_VERSION = 1.8
+
+
+@dataclass
+class EmbeddingConfig:
+    """The embedding layer config."""
+
+    weight: torch.Tensor = None
+
+    @property
+    def local_vocab_size(self):
+        """Infers the vocab_size from the embedding layer weights shape."""
+        return self.weight.shape[0]
+
+    @property
+    def hidden_size(self):
+        """Infers the hidden_size from the embedding layer weights shape."""
+        return self.weight.shape[1]
+
+
+@dataclass
+class LayernormConfig:
+    """The layernorm layer config."""
+
+    weight: torch.Tensor = None
+    bias: torch.Tensor = None
+    layernorm_type: str = LAYERNORM_DEFAULT
+    eps: float = 1e-5
+
+
+@dataclass
+class LinearConfig:
+    """The linear layer config."""
+
+    linear_type: str = LINEAR_COLUMN
+    weight: torch.Tensor = None
+    bias: torch.Tensor = None
+    activation_scaling_factor: torch.Tensor = None
+    weights_scaling_factor: torch.Tensor = None
+
+    # For methods like W4A8 AWQ, we have two quantizers for weights
+    # For W4A8, the first quantizer is for INT4 quantization and the second quantizer is for FP8 quantization
+    # `weight_scaling_factor_2` is the scaling factor the the second FP8 quantizer
+    weights_scaling_factor_2: torch.Tensor = None
+
+    prequant_scaling_factor: torch.Tensor = None
+    awq_block_size: int = 0
+
+
+@dataclass
+class QKVConfig:
+    """The QKV layer config."""
+
+    q: LinearConfig = None
+    k: LinearConfig = None
+    v: LinearConfig = None
+
+    @property
+    def weight(self):
+        """The generated linear layer weight.
+
+        The Q, K, V weights are concat together to fit the TensorRT-LLM QKV linear layer.
+        """
+        return torch.cat((self.q.weight, self.k.weight, self.v.weight))
+
+    @property
+    def bias(self):
+        """The generated linear layer bias.
+
+        The Q, K, V bias are concat together to fit the TensorRT-LLM QKV linear layer.
+        """
+        if self.q.bias is None:
+            assert (
+                self.k.bias is None and self.v.bias is None
+            ), "K and V should have valid bias as Q"
+            return None
+        return torch.cat((self.q.bias, self.k.bias, self.v.bias))
+
+    @property
+    def activation_scaling_factor(self):
+        """Returns the merged activation_scaling_factor across Q, K and V.
+
+        The max of the Q, K, V activation scaling factors is returned.
+        """
+        if (
+            self.q.activation_scaling_factor is None
+            or self.k.activation_scaling_factor is None
+            or self.v.activation_scaling_factor is None
+        ):
+            return None
+
+        return (
+            torch.stack(
+                [
+                    self.q.activation_scaling_factor,
+                    self.k.activation_scaling_factor,
+                    self.v.activation_scaling_factor,
+                ]
+            )
+            .max(dim=0)
+            .values
+        )
+
+    @property
+    def weights_scaling_factor(self):
+        """Returns the merged weights_scaling_factor across Q, K and V.
+
+        If the quantization is FP8, the max of the Q, K, V weight scaling factors is returned.
+        If the quanitzation is INT8_SQ, the concat value is returned.
+        """
+        if (
+            self.q.weights_scaling_factor is None
+            or self.k.weights_scaling_factor is None
+            or self.v.weights_scaling_factor is None
+        ):
+            return None
+
+        if self.q.weights_scaling_factor.numel() != 1:
+            # for Int4 AWQ and Int8 SQ case, we concatenate the
+            # q_weight_scaling_factor, k_weight_scaling_factor, v_weight_scaling_factor
+            qkv_weights_scaling_factor = torch.cat(
+                (
+                    self.q.weights_scaling_factor,
+                    self.k.weights_scaling_factor,
+                    self.v.weights_scaling_factor,
+                )
+            )
+        else:
+            # for FP8 set qkv_weight_scaling_factor to the max of
+            # q_weight_scaling_factor, k_weight_scaling_factor, v_weight_scaling_factor
+            qkv_weights_scaling_factor = (
+                torch.stack(
+                    [
+                        self.q.weights_scaling_factor,
+                        self.k.weights_scaling_factor,
+                        self.v.weights_scaling_factor,
+                    ],
+                )
+                .max(dim=0)
+                .values
+            )
+        return qkv_weights_scaling_factor
+
+    @property
+    def weights_scaling_factor_2(self):
+        """Returns the merged weights_scaling_factor_2 across Q, K and V.
+
+        weight_scaling_factor_2 is needed for W4A8 AWQ.
+        """
+        if (
+            self.q.weights_scaling_factor_2 is None
+            or self.k.weights_scaling_factor_2 is None
+            or self.v.weights_scaling_factor_2 is None
+        ):
+            return None
+
+        # For W4A8 AWQ, weight_scaling_factor_2 corresponds to the per-tensor FP8 quantization.
+        # Hence weight_scaling_factor_2 should be a scalar.
+        assert self.q.weights_scaling_factor_2.numel() == 1
+
+        # set qkv_weight_scaling_factor_2 to the max of q,k,v weight_scaling_factor_2
+        qkv_weights_scaling_factor_2 = (
+            torch.stack(
+                [
+                    self.q.weights_scaling_factor_2,
+                    self.k.weights_scaling_factor_2,
+                    self.v.weights_scaling_factor_2,
+                ]
+            )
+            .max(dim=0)
+            .values
+        )
+
+        return qkv_weights_scaling_factor_2
+
+    @property
+    def prequant_scaling_factor(self):
+        """Returns the merged prequant_scaling_factor across Q, K and V.
+
+        Prequant scaling factors for Q, K, V should be the same. So just return one of them.
+        """
+        if (
+            self.q.prequant_scaling_factor is None
+            or self.k.prequant_scaling_factor is None
+            or self.v.prequant_scaling_factor is None
+        ):
+            return None
+
+        assert torch.equal(
+            self.q.prequant_scaling_factor, self.k.prequant_scaling_factor
+        ) and torch.equal(
+            self.k.prequant_scaling_factor, self.v.prequant_scaling_factor
+        ), "Prequant scaling factors of Q, K and V should be the same"
+        return self.q.prequant_scaling_factor
+
+    @property
+    def awq_block_size(self):
+        """Returns the awq_block_size of this QKV layer."""
+        assert (
+            self.q.awq_block_size == self.k.awq_block_size == self.v.awq_block_size
+        ), "awq_block_size of QKV should be the same."
+        return self.q.awq_block_size
+
+
+@dataclass
+class AttentionConfig:
+    """The attention layer config."""
+
+    # QKV can either be stored as splitted (for easier postprocessing)
+    # or merged (for TRT LLM export)
+    qkv: Union[QKVConfig, LinearConfig] = None
+    dense: LinearConfig = None
+    kv_cache_scaling_factor: torch.Tensor = None
+    kv_cache_dtype: str = None
+
+    rotary_dim: int = -math.inf
+    # MPT variants
+    clip_qkv: float = None
+
+
+@dataclass
+class MLPConfig:
+    """The MLP layer config."""
+
+    fc: LinearConfig = None
+    gate: LinearConfig = None
+    proj: LinearConfig = None
+    hidden_act: str = ""
+    # we could merge the fc1 gemm and gate gemm in the runtime
+    merged_fc1_gate: bool = False
+
+
+@dataclass
+class ExpertConfig:
+    """The Expert config."""
+
+    # Aligning the naming convesion with TRT-LLM
+    fc: LinearConfig = None  # stacked experts for concatenated w3 and w1
+    proj: LinearConfig = None  # stacked experts for w2
+
+
+@dataclass
+class MOEConfig:
+    """The Mixture of Expert layer config."""
+
+    router: LinearConfig = None
+    experts: ExpertConfig = None
+    hidden_act: str = ""
+
+    @property
+    def fc(self):
+        """Return the fc module from experts."""
+        return self.experts.fc
+
+
+@dataclass
+class DecoderLayerConfig:
+    """The decoder layer config."""
+
+    quantization: str = QUANTIZATION_NONE
+
+    decoder_type: str = ""
+    input_layernorm: LayernormConfig = None
+    mlp_layernorm: LayernormConfig = None
+    attention: AttentionConfig = None
+    post_layernorm: LayernormConfig = None
+    mlp: Union[MLPConfig, MOEConfig] = None
+
+    num_attention_heads: int = 0
+    # Supporting different attention_head_size per layer.
+    attention_head_size: int = None
+
+    num_kv_heads: int = 0
+    max_position_embeddings: int = 0
+    rotary_pct: float = 1.0
+
+    # Falcon and Baichuan variants
+    use_alibi: bool = False
+    new_decoder_architecture: bool = False
+    parallel_attention: bool = False
+
+    # chatglm variants
+    apply_residual_connection_post_layernorm: bool = False
+    use_cache: bool = True
+    model_name: str = ""
+    rope_ratio: float = 1.0
+
+    # Qwen config
+    seq_length: int = 0
+
+    # Qwen and CodeLlama
+    rotary_base: int = 0
+
+    # Phi
+    partial_rotary_factor: float = 0
+
+    # Mixture of Experts
+    moe_num_experts: int = 0
+    moe_top_k: int = 0
+    moe_tp_mode: int = 0
+    moe_renorm_mode: int = 0
+
+    # MPT
+    alibi_bias_max: int = 0
+
+    # Arctic variants
+    residual_layernorm: LayernormConfig = None
+    residual_mlp: MLPConfig = None
+
+    @property
+    def hidden_size(self):
+        """Returns the hidden size of the transformer model."""
+        if isinstance(self.mlp, MOEConfig):
+            # fc.weight for MOE is stacked
+            return self.mlp.fc.weight.shape[-1]
+        else:
+            return self.mlp.fc.weight.shape[1]
+
+    @property
+    def ffn_hidden_size_local(self):
+        """Returns the ffn hidden size of the transformer model."""
+        fc = self.mlp.fc
+        if isinstance(self.mlp, MOEConfig):
+            # fc in MoE merge fc and gate
+            k = fc.weight.shape[1] // 2
+        else:
+            k = fc.weight.shape[0]
+        if self.quantization not in [QUANTIZATION_INT4_AWQ, QUANTIZATION_W4A8_AWQ]:
+            return k
+        return k * 2
+
+
+@dataclass
+class ModelConfig:
+    """The full LLM model config that includes the full information needed for tensorrt_llm engine building.
+
+    This class includes all the fields that tensorrt_llm supports, but not all of the fields are required.
+    pipeline_parallel > 1 is only supported for TensorRT-LLM checkpoint.
+    """
+
+    version: float = 0.0
+
+    # Global metadata
+    quantization: str = QUANTIZATION_NONE
+    dtype: str = "float16"
+    vocab_size: int = 0
+
+    # Parallel metadata
+    rank: int = 0
+    tensor_parallel: int = 1
+    pipeline_parallel: int = 1
+
+    # Model structure and weights
+    vocab_embedding: EmbeddingConfig = None
+    position_embedding: EmbeddingConfig = None
+    ln_embed: LayernormConfig = None
+    layers: List[DecoderLayerConfig] = field(default_factory=list)
+
+    ln_f: LayernormConfig = None
+
+    lm_head: LinearConfig = None
+    share_embedding_table: bool = False
+
+    @property
+    def vocab_size_padded(self):
+        """Returns the padded vocab_size of the model rounds to the tensor_parallel."""
+
+        def _pad_vocab_size(vocab_size, tp_size):
+            return int(math.ceil(vocab_size / tp_size) * tp_size)
+
+        return _pad_vocab_size(self.vocab_size, self.tensor_parallel)
+
+    @property
+    def hidden_size(self):
+        """Returns the hidden_size of the model."""
+        return self.layers[0].hidden_size
+
+    @property
+    def max_position_embeddings(self):
+        """Returns the max_position_embedding of the model."""
+        return self.layers[0].max_position_embeddings
+
+    @property
+    def num_attention_heads(self):
+        """Returns the num_attention_heads of the model."""
+        return self.layers[0].num_attention_heads
+
+    @property
+    def num_kv_heads(self):
+        """Returns the num_key_value_heads of the model."""
+        return (
+            self.layers[0].num_kv_heads
+            if self.layers[0].num_kv_heads > 0
+            else self.num_attention_heads
+        )
+
+    @property
+    def hidden_act(self):
+        """Returns the hidden_act of the model."""
+        return self.layers[0].mlp.hidden_act
```

## modelopt/torch/export/model_config_export.py

 * *Ordering differences only*

```diff
@@ -1,395 +1,395 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Code that export optimized models to the TensorRT-LLM checkpoint."""
-
-import copy
-import json
-import math
-import tempfile
-import traceback
-from pathlib import Path
-from typing import Any, Dict, Iterator, Optional, Tuple, Union
-
-import torch
-import torch.nn as nn
-from safetensors.torch import save_file
-
-try:
-    from megatron.core.models.gpt import GPTModel as MCoreGPTModel
-    from megatron.core.transformer.module import MegatronModule
-
-    has_mcore = True
-except ImportError:
-    has_mcore = False
-
-from . import QUANTIZATION_INT4_AWQ, QUANTIZATION_W4A8_AWQ
-from .distribute import get_rank, get_world_size
-from .layer_utils import (
-    build_decoder_config,
-    build_embedding_config,
-    build_layernorm_config,
-    build_linear_config,
-    check_model_compatibility,
-    get_transformer_layers,
-    is_decoder_list,
-    is_embedding,
-    is_layernorm,
-    is_linear,
-)
-from .model_config import CURRENT_VERSION, ModelConfig
-from .model_config_utils import (
-    merge_fc1_gate,
-    merge_qkv,
-    model_config_to_dict,
-    naive_quantization,
-    pack_linear_weights,
-    split_config_and_weights,
-)
-from .postprocess import (
-    check_weight_shape_valid,
-    pad_embedding_lm_head,
-    postprocess_model_config,
-    postprocess_tensors,
-)
-from .tensorrt_llm_utils import (
-    convert_to_tensorrt_llm_config,
-    is_tensorrt_llm_0_8_or_9,
-    weights_to_npz,
-)
-
-
-def torch_to_tensorrt_llm_checkpoint(
-    model: nn.Module,
-    decoder_type: str,
-    dtype: torch.dtype = torch.float16,
-    inference_tensor_parallel: int = 0,
-    inference_pipeline_parallel: int = 1,
-    export_npz: bool = False,
-    naive_fp8_quantization: bool = False,
-    workspace_path: Optional[Union[Path, str]] = None,
-) -> Iterator[Tuple[Dict[str, Any], Dict[str, torch.Tensor]]]:
-    """Converts the torch model to the TensorRT-LLM checkpoint per GPU rank.
-
-    TensorRT-LLM checkpoint is the LLM model format that can be used by the TensorRT-LLM build API.
-    for the engine building process.
-    https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/checkpoint.md
-
-    Args:
-        model: the torch model.
-        decoder_type: the type of the decoder, e.g. gpt2, gptj, llama or gptnext.
-        dtype: the weights data type to export the unquantized layers.
-        inference_tensor_parallel: The target inference time tensor parallel.
-            We will merge or split the calibration tensor parallelism to inference.
-            Default is 0, meaning using the calibration without manual config merge or split.
-        inference_pipeline_parallel: The target inference time pipeline parallel.
-            We will merge or split the calibration pipeline parallelism to inference.
-            Default is 1, meaning no pipeline parallelism.
-        export_npz: Whether or not to export the model_config to the old NPZ format for backward
-            compatibility.
-        naive_fp8_quantization: Quantize the model naively to FP8 without calibration.
-            All scaling factors are set to 1.
-        workspace_path: the path to the NFS directory for postprocess cross rank communication.
-
-    Yields:
-        A tuple of
-            tensorrt_llm_config: A dict that maps to the ``PretrainedConfig`` in TensorRT-LLM.
-            https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/models/modeling_utils.py
-            weights: A dict that stores all model weights and scaling factors for each rank.
-    """
-    if export_npz:
-        print("Warning: export_npz is going to be deprecated soon and replaced by safetensors.")
-
-    if dtype not in [torch.float16, torch.bfloat16]:
-        print(
-            f"Warning: dtype {dtype} not fully compatible with TensorRT-LLM optimizations, Default to float16."
-        )
-        dtype = torch.float16
-
-    if has_mcore and isinstance(model, MegatronModule):
-        if not isinstance(model, MCoreGPTModel):
-            raise ValueError("Only megatron.core.models.gpt.GPTModel is supported!")
-        # MCoreGPTModel.config has type TransformerConfig
-        #
-        # We choose to deepcopy here since TransformerConfig deserialization is sensitive to
-        # additional attributes.
-        model_metadata_config = copy.deepcopy(model.config.__dict__)
-        vocab_size = model.vocab_size
-        model_metadata_config["max_position_embeddings"] = model.max_position_embeddings
-        model_metadata_config["rotary_percent"] = model.rotary_percent
-    elif hasattr(model, "config"):
-        # Huggingface models
-        model_metadata_config = model.config.__dict__
-        vocab_size = model.config.vocab_size
-
-        # For Baichuan 13B, we check if alibi is used with the alibi_mask property.
-        if hasattr(model, "model") and hasattr(model.model, "alibi_mask"):
-            model_metadata_config["alibi"] = True
-
-        # For MPT
-        if "attn_config" in model_metadata_config:
-            model_metadata_config.update(model_metadata_config["attn_config"])
-
-    elif hasattr(model, "cfg"):
-        # NeMo Legacy MegatronGPTModel
-        model_metadata_config = dict(model.cfg)
-        vocab_size = model.tokenizer.vocab_size
-    else:
-        raise ValueError("Cannot find valid model metadata config in model")
-
-    if "multi_query_group_num" in model_metadata_config.keys():
-        if model_metadata_config["multi_query_group_num"] % inference_tensor_parallel != 0:
-            raise ValueError(
-                "Cannot divide {} kv_heads into {} gpus".format(
-                    model_metadata_config["multi_query_group_num"], inference_tensor_parallel
-                )
-            )
-
-    training_pipeline_parallel = model_metadata_config.get("pipeline_model_parallel_size", 1)
-    training_tensor_parallel = get_world_size() // training_pipeline_parallel
-    model_metadata_config["training_pipeline_parallel"] = training_pipeline_parallel
-    model_metadata_config["training_tensor_parallel"] = training_tensor_parallel
-
-    if "make_vocab_size_divisible_by" in model_metadata_config:
-        # For some nemo models, the vocab_size is pre-padded.
-        # We calculate the pre-padded vocab_size with this config: make_vocab_size_divisible_by.
-        make_vocab_size_divisible_by = model_metadata_config["make_vocab_size_divisible_by"]
-        make_vocab_size_divisible_by_with_tp = (
-            make_vocab_size_divisible_by * training_tensor_parallel
-        )
-        vocab_size = int(
-            math.ceil(vocab_size / make_vocab_size_divisible_by_with_tp)
-            * make_vocab_size_divisible_by_with_tp
-        )
-        print(
-            f"the new vocab_size is updated: {vocab_size}, make_vocab_size_divisible_by"
-            f" {make_vocab_size_divisible_by}, training_tensor_parallel"
-            f" {training_tensor_parallel}."
-        )
-
-    transformer_layers = get_transformer_layers(model)
-    if training_pipeline_parallel == 1:
-        compatible, has_position_embedding, has_embedding_layernorm = check_model_compatibility(
-            transformer_layers
-        )
-    else:
-        # For Megatron models with more than one PP,
-        # we skip the compatibility check as not all ranks have the full model.
-        # For Megatron Core GPTModel, both gptnext and llama do not have position embedding
-        # nor embedding layernorm.
-        compatible = len(transformer_layers) > 0
-        has_position_embedding = False
-        has_embedding_layernorm = False
-    assert compatible, "The model is not supported"
-
-    config = ModelConfig(
-        version=CURRENT_VERSION,
-        dtype=str(dtype).split(".")[1],
-        rank=get_rank(),
-        tensor_parallel=training_tensor_parallel,
-        pipeline_parallel=training_pipeline_parallel,
-        vocab_size=vocab_size,
-    )
-    # Build the full model_config dict layer by layer.
-    for module in transformer_layers:
-        if is_embedding(module):
-            if config.vocab_embedding is None:
-                # We assume the first embedding in the list the vocab_embedding.
-
-                normalization_constant = 1
-                # Normalize vocab embedding for gemma.
-                if decoder_type == "gemma" and is_tensorrt_llm_0_8_or_9():
-                    normalization_constant = model_metadata_config["hidden_size"] ** 0.5
-
-                config.vocab_embedding = build_embedding_config(
-                    module, dtype, normalization_constant=normalization_constant
-                )
-            elif has_position_embedding:
-                config.position_embedding = build_embedding_config(module, dtype)
-        elif is_decoder_list(module):
-            layers = []
-            for layer in module.children():
-                layers.append(
-                    build_decoder_config(layer, model_metadata_config, decoder_type, dtype)
-                )
-            config.layers = layers
-        elif is_layernorm(module):
-            if has_embedding_layernorm and config.ln_embed is None:
-                # Assume embedding_layernorm is placed before the ln_f.
-                config.ln_embed = build_layernorm_config(module, dtype)
-            else:
-                config.ln_f = build_layernorm_config(module, dtype)
-        elif is_linear(module):
-            # TRT LLM forces the embedding table to be shared for the following models.
-            force_share_embedding_table = decoder_type in ["gemma"]
-            if force_share_embedding_table and torch.equal(
-                module.weight.to(dtype), config.vocab_embedding.weight
-            ):
-                config.share_embedding_table = True
-            else:
-                config.lm_head = build_linear_config(module, "column", dtype)
-
-    # For the training time PP, not all ranks will have the lm_head layer.
-    if config.lm_head is None and training_pipeline_parallel == 1:
-        # Models that share weights for lm_head and vocab_embedding
-        assert decoder_type in [
-            "mpt",
-            "gpt2",
-            "gemma",
-        ], f"lm_head not available for decoder {decoder_type}"
-        config.share_embedding_table = True
-
-    config.quantization = config.layers[0].quantization
-    if config.quantization in [QUANTIZATION_INT4_AWQ, QUANTIZATION_W4A8_AWQ]:
-        if config.vocab_size % 64 != 0:
-            # TODO: Check if this works for Mixtral
-            assert training_tensor_parallel == 1, "We do not support padding for training time TP"
-            print("Padding vocab_embedding and lm_head for AWQ weights export")
-            pad_embedding_lm_head(config)
-
-    check_weight_shape_valid(
-        config,
-        inference_tensor_parallel,
-        training_tensor_parallel,
-    )
-
-    # Set this value to export unsharded model config. This is only for some models like phi,
-    # so we don't split model config and will overwrite mapping in config.json.
-    # TODO: check PP support for phi
-    tp_size_overwrite = None
-    if decoder_type in ["phi"]:
-        tp_size_overwrite = inference_tensor_parallel
-
-    # If inference_tensor_parallel or inference_pipeline_parallel is different from world_size,
-    # we try to merge or split the model configs based on the rank selected.
-    if (
-        inference_tensor_parallel > 0
-        or inference_pipeline_parallel > 0
-        or training_pipeline_parallel > 1
-    ):
-        model_configs = postprocess_model_config(
-            config,
-            1 if tp_size_overwrite else inference_tensor_parallel,
-            inference_pipeline_parallel,
-            training_pipeline_parallel=training_pipeline_parallel,
-            workspace_path=workspace_path,
-        )
-    else:
-        model_configs = [config]
-
-    for model_config in model_configs:
-        assert model_config.rank >= 0, "Invalid model_config, postprocess_model_config fails."
-
-        if not model_config.quantization and naive_fp8_quantization:
-            naive_quantization(model_config)
-
-        if export_npz:
-            # The npz format is not compatible with modelopt.deploy.llm for AWQ.
-            model_config.version = 0.8
-        else:
-            merge_qkv(model_config)
-            merge_fc1_gate(model_config)
-            pack_linear_weights(model_config)
-            # Postprocess the tensors in the model_config.
-            # Exporting the safetensors also allows the tensor to be a view.
-            postprocess_tensors(
-                model_config, force_cpu=True, force_contiguous=True, force_non_view=False
-            )
-
-        weights = {}
-        model_config_dict = model_config_to_dict(model_config)
-        # We split the weights from model_config and save them separately as two files.
-        split_config_and_weights(model_config_dict, weights)
-
-        # We only export the json once across ranks as all jsons should be the same except for the rank.
-        tensorrt_llm_config = convert_to_tensorrt_llm_config(model_config, tp_size_overwrite)
-
-        yield tensorrt_llm_config, weights
-
-
-def export_tensorrt_llm_checkpoint(
-    model: nn.Module,
-    decoder_type: str,
-    dtype: torch.dtype = torch.float16,
-    export_dir: Union[Path, str] = tempfile.gettempdir(),
-    inference_tensor_parallel: int = 0,
-    inference_pipeline_parallel: int = 1,
-    export_npz: bool = False,
-    naive_fp8_quantization: bool = False,
-    use_nfs_workspace: bool = False,
-):
-    """Exports the torch model to the TensorRT-LLM checkpoint and save to the export_dir.
-
-    Args:
-        model: the torch model.
-        decoder_type: the type of the decoder, e.g. gpt2, gptj, llama or gptnext.
-        dtype: the weights data type to export the unquantized layers.
-        export_dir: the target export path.
-        inference_tensor_parallel: The target inference time tensor parallel.
-            We will merge or split the calibration tensor parallelism to inference.
-            Default is 0, meaning using the calibration without manual config merge or split.
-        inference_pipeline_parallel: The target inference time pipeline parallel.
-            We will merge or split the calibration pipeline parallelism to inference.
-            Default is 1, meaning no pipeline parallelism.
-        inference_pipeline_parallel: The target inference time pipeline parallel.
-        export_npz: Whether or not to export the model_config to the old NPZ format for backward
-            compatibility.
-        naive_fp8_quantization: Quantize the model naively to FP8 without calibration.
-            All scaling factors are set to 1.
-        use_nfs_workspace: if True, the an NFS workspace will be created under the export_dir and
-            used as a shared memory for cross process/node communication.
-
-    For tensorrt_llm deployment, save the representation under ``export_dir``.
-    We will save the model_config as two files:
-
-        * ``.json``: The nested dict that maps to the ``PretrainedConfig`` in TensorRT-LLM.
-            https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/models/modeling_utils.py.
-        * ``.safetensors``: The file for the list of weights as safetensors. Unique for each rank.
-    """
-    export_dir = Path(export_dir)
-    export_dir.mkdir(parents=True, exist_ok=True)
-    # Create a NFS workspace under the export folder which is also assumed to be NFS.
-    workspace_path = None
-    if use_nfs_workspace:
-        workspace_path = export_dir.joinpath("workspace")
-        workspace_path.mkdir(parents=True, exist_ok=True)
-    try:
-        for tensorrt_llm_config, weights in torch_to_tensorrt_llm_checkpoint(
-            model=model,
-            decoder_type=decoder_type,
-            dtype=dtype,
-            inference_tensor_parallel=inference_tensor_parallel,
-            inference_pipeline_parallel=inference_pipeline_parallel,
-            export_npz=export_npz,
-            naive_fp8_quantization=naive_fp8_quantization,
-            workspace_path=workspace_path,
-        ):
-            rank = tensorrt_llm_config["rank"]
-            if rank == 0:
-                # We only export the json once across ranks as all jsons should be the same except for the rank.
-                with open(export_dir / "config.json", "w") as f:
-                    json.dump(tensorrt_llm_config, f, indent=4)
-
-            if export_npz:
-                weights_to_npz(weights, tensorrt_llm_config, export_dir)
-            else:
-                weights_path = export_dir / f"rank{rank}.safetensors"
-                save_file(weights, weights_path)
-
-    except Exception as e:
-        fallback_model_path = export_dir / f"modelopt_model.{get_rank()}.pth"
-        torch.save(model.state_dict(), fallback_model_path)
-        print(
-            "Cannot export model to the model_config. The modelopt-optimized model state_dict"
-            f" (including the quantization factors) is saved to {fallback_model_path} using"
-            " torch.save for further inspection."
-        )
-        print(f"Detailed export error: {e}")
-        traceback.print_exc()
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Code that export optimized models to the TensorRT-LLM checkpoint."""
+
+import copy
+import json
+import math
+import tempfile
+import traceback
+from pathlib import Path
+from typing import Any, Dict, Iterator, Optional, Tuple, Union
+
+import torch
+import torch.nn as nn
+from safetensors.torch import save_file
+
+try:
+    from megatron.core.models.gpt import GPTModel as MCoreGPTModel
+    from megatron.core.transformer.module import MegatronModule
+
+    has_mcore = True
+except ImportError:
+    has_mcore = False
+
+from . import QUANTIZATION_INT4_AWQ, QUANTIZATION_W4A8_AWQ
+from .distribute import get_rank, get_world_size
+from .layer_utils import (
+    build_decoder_config,
+    build_embedding_config,
+    build_layernorm_config,
+    build_linear_config,
+    check_model_compatibility,
+    get_transformer_layers,
+    is_decoder_list,
+    is_embedding,
+    is_layernorm,
+    is_linear,
+)
+from .model_config import CURRENT_VERSION, ModelConfig
+from .model_config_utils import (
+    merge_fc1_gate,
+    merge_qkv,
+    model_config_to_dict,
+    naive_quantization,
+    pack_linear_weights,
+    split_config_and_weights,
+)
+from .postprocess import (
+    check_weight_shape_valid,
+    pad_embedding_lm_head,
+    postprocess_model_config,
+    postprocess_tensors,
+)
+from .tensorrt_llm_utils import (
+    convert_to_tensorrt_llm_config,
+    is_tensorrt_llm_0_8_or_9,
+    weights_to_npz,
+)
+
+
+def torch_to_tensorrt_llm_checkpoint(
+    model: nn.Module,
+    decoder_type: str,
+    dtype: torch.dtype = torch.float16,
+    inference_tensor_parallel: int = 0,
+    inference_pipeline_parallel: int = 1,
+    export_npz: bool = False,
+    naive_fp8_quantization: bool = False,
+    workspace_path: Optional[Union[Path, str]] = None,
+) -> Iterator[Tuple[Dict[str, Any], Dict[str, torch.Tensor]]]:
+    """Converts the torch model to the TensorRT-LLM checkpoint per GPU rank.
+
+    TensorRT-LLM checkpoint is the LLM model format that can be used by the TensorRT-LLM build API.
+    for the engine building process.
+    https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/checkpoint.md
+
+    Args:
+        model: the torch model.
+        decoder_type: the type of the decoder, e.g. gpt2, gptj, llama or gptnext.
+        dtype: the weights data type to export the unquantized layers.
+        inference_tensor_parallel: The target inference time tensor parallel.
+            We will merge or split the calibration tensor parallelism to inference.
+            Default is 0, meaning using the calibration without manual config merge or split.
+        inference_pipeline_parallel: The target inference time pipeline parallel.
+            We will merge or split the calibration pipeline parallelism to inference.
+            Default is 1, meaning no pipeline parallelism.
+        export_npz: Whether or not to export the model_config to the old NPZ format for backward
+            compatibility.
+        naive_fp8_quantization: Quantize the model naively to FP8 without calibration.
+            All scaling factors are set to 1.
+        workspace_path: the path to the NFS directory for postprocess cross rank communication.
+
+    Yields:
+        A tuple of
+            tensorrt_llm_config: A dict that maps to the ``PretrainedConfig`` in TensorRT-LLM.
+            https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/models/modeling_utils.py
+            weights: A dict that stores all model weights and scaling factors for each rank.
+    """
+    if export_npz:
+        print("Warning: export_npz is going to be deprecated soon and replaced by safetensors.")
+
+    if dtype not in [torch.float16, torch.bfloat16]:
+        print(
+            f"Warning: dtype {dtype} not fully compatible with TensorRT-LLM optimizations, Default to float16."
+        )
+        dtype = torch.float16
+
+    if has_mcore and isinstance(model, MegatronModule):
+        if not isinstance(model, MCoreGPTModel):
+            raise ValueError("Only megatron.core.models.gpt.GPTModel is supported!")
+        # MCoreGPTModel.config has type TransformerConfig
+        #
+        # We choose to deepcopy here since TransformerConfig deserialization is sensitive to
+        # additional attributes.
+        model_metadata_config = copy.deepcopy(model.config.__dict__)
+        vocab_size = model.vocab_size
+        model_metadata_config["max_position_embeddings"] = model.max_position_embeddings
+        model_metadata_config["rotary_percent"] = model.rotary_percent
+    elif hasattr(model, "config"):
+        # Huggingface models
+        model_metadata_config = model.config.__dict__
+        vocab_size = model.config.vocab_size
+
+        # For Baichuan 13B, we check if alibi is used with the alibi_mask property.
+        if hasattr(model, "model") and hasattr(model.model, "alibi_mask"):
+            model_metadata_config["alibi"] = True
+
+        # For MPT
+        if "attn_config" in model_metadata_config:
+            model_metadata_config.update(model_metadata_config["attn_config"])
+
+    elif hasattr(model, "cfg"):
+        # NeMo Legacy MegatronGPTModel
+        model_metadata_config = dict(model.cfg)
+        vocab_size = model.tokenizer.vocab_size
+    else:
+        raise ValueError("Cannot find valid model metadata config in model")
+
+    if "multi_query_group_num" in model_metadata_config.keys():
+        if model_metadata_config["multi_query_group_num"] % inference_tensor_parallel != 0:
+            raise ValueError(
+                "Cannot divide {} kv_heads into {} gpus".format(
+                    model_metadata_config["multi_query_group_num"], inference_tensor_parallel
+                )
+            )
+
+    training_pipeline_parallel = model_metadata_config.get("pipeline_model_parallel_size", 1)
+    training_tensor_parallel = get_world_size() // training_pipeline_parallel
+    model_metadata_config["training_pipeline_parallel"] = training_pipeline_parallel
+    model_metadata_config["training_tensor_parallel"] = training_tensor_parallel
+
+    if "make_vocab_size_divisible_by" in model_metadata_config:
+        # For some nemo models, the vocab_size is pre-padded.
+        # We calculate the pre-padded vocab_size with this config: make_vocab_size_divisible_by.
+        make_vocab_size_divisible_by = model_metadata_config["make_vocab_size_divisible_by"]
+        make_vocab_size_divisible_by_with_tp = (
+            make_vocab_size_divisible_by * training_tensor_parallel
+        )
+        vocab_size = int(
+            math.ceil(vocab_size / make_vocab_size_divisible_by_with_tp)
+            * make_vocab_size_divisible_by_with_tp
+        )
+        print(
+            f"the new vocab_size is updated: {vocab_size}, make_vocab_size_divisible_by"
+            f" {make_vocab_size_divisible_by}, training_tensor_parallel"
+            f" {training_tensor_parallel}."
+        )
+
+    transformer_layers = get_transformer_layers(model)
+    if training_pipeline_parallel == 1:
+        compatible, has_position_embedding, has_embedding_layernorm = check_model_compatibility(
+            transformer_layers
+        )
+    else:
+        # For Megatron models with more than one PP,
+        # we skip the compatibility check as not all ranks have the full model.
+        # For Megatron Core GPTModel, both gptnext and llama do not have position embedding
+        # nor embedding layernorm.
+        compatible = len(transformer_layers) > 0
+        has_position_embedding = False
+        has_embedding_layernorm = False
+    assert compatible, "The model is not supported"
+
+    config = ModelConfig(
+        version=CURRENT_VERSION,
+        dtype=str(dtype).split(".")[1],
+        rank=get_rank(),
+        tensor_parallel=training_tensor_parallel,
+        pipeline_parallel=training_pipeline_parallel,
+        vocab_size=vocab_size,
+    )
+    # Build the full model_config dict layer by layer.
+    for module in transformer_layers:
+        if is_embedding(module):
+            if config.vocab_embedding is None:
+                # We assume the first embedding in the list the vocab_embedding.
+
+                normalization_constant = 1
+                # Normalize vocab embedding for gemma.
+                if decoder_type == "gemma" and is_tensorrt_llm_0_8_or_9():
+                    normalization_constant = model_metadata_config["hidden_size"] ** 0.5
+
+                config.vocab_embedding = build_embedding_config(
+                    module, dtype, normalization_constant=normalization_constant
+                )
+            elif has_position_embedding:
+                config.position_embedding = build_embedding_config(module, dtype)
+        elif is_decoder_list(module):
+            layers = []
+            for layer in module.children():
+                layers.append(
+                    build_decoder_config(layer, model_metadata_config, decoder_type, dtype)
+                )
+            config.layers = layers
+        elif is_layernorm(module):
+            if has_embedding_layernorm and config.ln_embed is None:
+                # Assume embedding_layernorm is placed before the ln_f.
+                config.ln_embed = build_layernorm_config(module, dtype)
+            else:
+                config.ln_f = build_layernorm_config(module, dtype)
+        elif is_linear(module):
+            # TRT LLM forces the embedding table to be shared for the following models.
+            force_share_embedding_table = decoder_type in ["gemma"]
+            if force_share_embedding_table and torch.equal(
+                module.weight.to(dtype), config.vocab_embedding.weight
+            ):
+                config.share_embedding_table = True
+            else:
+                config.lm_head = build_linear_config(module, "column", dtype)
+
+    # For the training time PP, not all ranks will have the lm_head layer.
+    if config.lm_head is None and training_pipeline_parallel == 1:
+        # Models that share weights for lm_head and vocab_embedding
+        assert decoder_type in [
+            "mpt",
+            "gpt2",
+            "gemma",
+        ], f"lm_head not available for decoder {decoder_type}"
+        config.share_embedding_table = True
+
+    config.quantization = config.layers[0].quantization
+    if config.quantization in [QUANTIZATION_INT4_AWQ, QUANTIZATION_W4A8_AWQ]:
+        if config.vocab_size % 64 != 0:
+            # TODO: Check if this works for Mixtral
+            assert training_tensor_parallel == 1, "We do not support padding for training time TP"
+            print("Padding vocab_embedding and lm_head for AWQ weights export")
+            pad_embedding_lm_head(config)
+
+    check_weight_shape_valid(
+        config,
+        inference_tensor_parallel,
+        training_tensor_parallel,
+    )
+
+    # Set this value to export unsharded model config. This is only for some models like phi,
+    # so we don't split model config and will overwrite mapping in config.json.
+    # TODO: check PP support for phi
+    tp_size_overwrite = None
+    if decoder_type in ["phi"]:
+        tp_size_overwrite = inference_tensor_parallel
+
+    # If inference_tensor_parallel or inference_pipeline_parallel is different from world_size,
+    # we try to merge or split the model configs based on the rank selected.
+    if (
+        inference_tensor_parallel > 0
+        or inference_pipeline_parallel > 0
+        or training_pipeline_parallel > 1
+    ):
+        model_configs = postprocess_model_config(
+            config,
+            1 if tp_size_overwrite else inference_tensor_parallel,
+            inference_pipeline_parallel,
+            training_pipeline_parallel=training_pipeline_parallel,
+            workspace_path=workspace_path,
+        )
+    else:
+        model_configs = [config]
+
+    for model_config in model_configs:
+        assert model_config.rank >= 0, "Invalid model_config, postprocess_model_config fails."
+
+        if not model_config.quantization and naive_fp8_quantization:
+            naive_quantization(model_config)
+
+        if export_npz:
+            # The npz format is not compatible with modelopt.deploy.llm for AWQ.
+            model_config.version = 0.8
+        else:
+            merge_qkv(model_config)
+            merge_fc1_gate(model_config)
+            pack_linear_weights(model_config)
+            # Postprocess the tensors in the model_config.
+            # Exporting the safetensors also allows the tensor to be a view.
+            postprocess_tensors(
+                model_config, force_cpu=True, force_contiguous=True, force_non_view=False
+            )
+
+        weights = {}
+        model_config_dict = model_config_to_dict(model_config)
+        # We split the weights from model_config and save them separately as two files.
+        split_config_and_weights(model_config_dict, weights)
+
+        # We only export the json once across ranks as all jsons should be the same except for the rank.
+        tensorrt_llm_config = convert_to_tensorrt_llm_config(model_config, tp_size_overwrite)
+
+        yield tensorrt_llm_config, weights
+
+
+def export_tensorrt_llm_checkpoint(
+    model: nn.Module,
+    decoder_type: str,
+    dtype: torch.dtype = torch.float16,
+    export_dir: Union[Path, str] = tempfile.gettempdir(),
+    inference_tensor_parallel: int = 0,
+    inference_pipeline_parallel: int = 1,
+    export_npz: bool = False,
+    naive_fp8_quantization: bool = False,
+    use_nfs_workspace: bool = False,
+):
+    """Exports the torch model to the TensorRT-LLM checkpoint and save to the export_dir.
+
+    Args:
+        model: the torch model.
+        decoder_type: the type of the decoder, e.g. gpt2, gptj, llama or gptnext.
+        dtype: the weights data type to export the unquantized layers.
+        export_dir: the target export path.
+        inference_tensor_parallel: The target inference time tensor parallel.
+            We will merge or split the calibration tensor parallelism to inference.
+            Default is 0, meaning using the calibration without manual config merge or split.
+        inference_pipeline_parallel: The target inference time pipeline parallel.
+            We will merge or split the calibration pipeline parallelism to inference.
+            Default is 1, meaning no pipeline parallelism.
+        inference_pipeline_parallel: The target inference time pipeline parallel.
+        export_npz: Whether or not to export the model_config to the old NPZ format for backward
+            compatibility.
+        naive_fp8_quantization: Quantize the model naively to FP8 without calibration.
+            All scaling factors are set to 1.
+        use_nfs_workspace: if True, the an NFS workspace will be created under the export_dir and
+            used as a shared memory for cross process/node communication.
+
+    For tensorrt_llm deployment, save the representation under ``export_dir``.
+    We will save the model_config as two files:
+
+        * ``.json``: The nested dict that maps to the ``PretrainedConfig`` in TensorRT-LLM.
+            https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/models/modeling_utils.py.
+        * ``.safetensors``: The file for the list of weights as safetensors. Unique for each rank.
+    """
+    export_dir = Path(export_dir)
+    export_dir.mkdir(parents=True, exist_ok=True)
+    # Create a NFS workspace under the export folder which is also assumed to be NFS.
+    workspace_path = None
+    if use_nfs_workspace:
+        workspace_path = export_dir.joinpath("workspace")
+        workspace_path.mkdir(parents=True, exist_ok=True)
+    try:
+        for tensorrt_llm_config, weights in torch_to_tensorrt_llm_checkpoint(
+            model=model,
+            decoder_type=decoder_type,
+            dtype=dtype,
+            inference_tensor_parallel=inference_tensor_parallel,
+            inference_pipeline_parallel=inference_pipeline_parallel,
+            export_npz=export_npz,
+            naive_fp8_quantization=naive_fp8_quantization,
+            workspace_path=workspace_path,
+        ):
+            rank = tensorrt_llm_config["rank"]
+            if rank == 0:
+                # We only export the json once across ranks as all jsons should be the same except for the rank.
+                with open(export_dir / "config.json", "w") as f:
+                    json.dump(tensorrt_llm_config, f, indent=4)
+
+            if export_npz:
+                weights_to_npz(weights, tensorrt_llm_config, export_dir)
+            else:
+                weights_path = export_dir / f"rank{rank}.safetensors"
+                save_file(weights, weights_path)
+
+    except Exception as e:
+        fallback_model_path = export_dir / f"modelopt_model.{get_rank()}.pth"
+        torch.save(model.state_dict(), fallback_model_path)
+        print(
+            "Cannot export model to the model_config. The modelopt-optimized model state_dict"
+            f" (including the quantization factors) is saved to {fallback_model_path} using"
+            " torch.save for further inspection."
+        )
+        print(f"Detailed export error: {e}")
+        traceback.print_exc()
```

## modelopt/torch/export/model_config_utils.py

 * *Ordering differences only*

```diff
@@ -1,449 +1,449 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Common utils for the ModelConfig."""
-
-import dataclasses
-import math
-from typing import Dict, Union, get_args, get_origin
-
-import numpy as np
-import torch
-
-from .model_config import (
-    KV_CACHE_FP8,
-    QUANTIZATION_FP8,
-    QUANTIZATION_INT4_AWQ,
-    QUANTIZATION_INT8_SQ,
-    QUANTIZATION_W4A8_AWQ,
-    DecoderLayerConfig,
-    LayernormConfig,
-    LinearConfig,
-    MLPConfig,
-    ModelConfig,
-    MOEConfig,
-    QKVConfig,
-)
-
-# numpy doesn't know bfloat16, define abstract binary type instead
-np_bfloat16 = np.dtype("V2", metadata={"dtype": "bfloat16"})
-
-
-def _numpy_to_torch(x):
-    """Convert numpy array to torch tensor."""
-    if isinstance(x, torch.Tensor):
-        return x
-
-    if x.dtype != np_bfloat16:
-        return torch.tensor(x)
-    return torch.tensor(x.view(np.int16)).view(torch.bfloat16)
-
-
-def model_config_to_dict(model_config: ModelConfig) -> dict:
-    """Converts the instance to a python dict."""
-    assert model_config is not None, "model_config is None"
-    return dataclasses.asdict(model_config)
-
-
-def split_config_and_weights(config, weights: Dict[str, torch.tensor], prefix: str = "transformer"):
-    """Util function to split the weights or any torch.Tensor in nested config to weights.
-
-    A weight id starts with transformers or lm_head will also be generated to link the original key to the weights dict.
-    The weights in the weights dict are contiguous.
-    """
-    if isinstance(config, dict):
-        for k, v in config.items():
-            if k == "lm_head":
-                # lm_head is not part of the transformer.
-                array_key = k
-            elif k == "experts":
-                # Omit the 'experts' key that is not in the model name
-                array_key = prefix
-            else:
-                array_key = f"{prefix}.{k}"
-            if isinstance(v, torch.Tensor):
-                weights[array_key] = v
-                config[k] = f"{array_key}"
-            else:
-                split_config_and_weights(v, weights, array_key)
-    elif isinstance(config, list):
-        for i, v in enumerate(config):
-            array_key = f"{prefix}.{i}"
-            if isinstance(v, torch.Tensor):
-                weights[array_key] = v
-                config[i] = f"{array_key}"
-            else:
-                split_config_and_weights(v, weights, array_key)
-
-
-def _unified_weights_key(k: str) -> str:
-    """Try to unify the weights dict key between old npz and the new safetensors format."""
-    prefixes = ["transformer.", "_np:"]
-    for prefix in prefixes:
-        if k.startswith(prefix):
-            k = k[len(prefix) :]
-
-    k = k.replace("final_layernorm", "ln_f")
-
-    return k.replace(":", ".")
-
-
-def _restore_model_config(model_config, weights: Dict[str, Union[np.ndarray, torch.Tensor]]):
-    def _is_tensor_key(k):
-        return isinstance(k, str) and _unified_weights_key(k) in weights
-
-    if isinstance(model_config, dict):
-        for k, v in model_config.items():
-            if _is_tensor_key(v):
-                model_config[k] = _numpy_to_torch(weights[_unified_weights_key(v)])
-            else:
-                _restore_model_config(v, weights)
-    if isinstance(model_config, list):
-        for i, v in enumerate(model_config):
-            if _is_tensor_key(v):
-                model_config[i] = _numpy_to_torch(weights[_unified_weights_key(v)])
-            else:
-                _restore_model_config(v, weights)
-
-
-def restore_model_config(model_config, weights: Dict[str, Union[np.ndarray, torch.Tensor]]):
-    """Recursively restores the model_config from json and loads np.ndarray or torch.Tensor weights from weights."""
-    unified_key_weights = {}
-    for k, v in weights.items():
-        unified_key_weights[_unified_weights_key(k)] = v
-
-    _restore_model_config(model_config, unified_key_weights)
-
-
-def _from_dict(class_type, data):
-    """Helper function to load the data as a class_type. class_type must be a dataclass."""
-    if data is None:
-        return None
-
-    if get_origin(class_type) == Union:
-        # Handle QKV
-        if all([key in data for key in ["q", "k", "v"]]):
-            # splitted qkv case
-            class_type = QKVConfig
-        elif all([key in data for key in ["router", "experts"]]):
-            # moe
-            class_type = MOEConfig
-        elif all([key in data for key in ["fc", "gate", "proj"]]):
-            # mlp
-            class_type = MLPConfig
-        else:
-            # merged qkv case
-            assert "linear_type" in data, f"{data} is not a valid LinearConfig"
-            class_type = LinearConfig
-
-    if dataclasses.is_dataclass(class_type):
-        fieldtypes = {f.name: f.type for f in dataclasses.fields(class_type)}
-        fields_map = {}
-        for k, v in data.items():
-            if k in fieldtypes:
-                # We only handle keys available in the fields.
-                # Deprecated fields in the checkpoint will be ignored.
-                fields_map[k] = _from_dict(fieldtypes[k], v)
-        return class_type(**fields_map)
-    elif get_origin(class_type) == list and dataclasses.is_dataclass(get_args(class_type)[0]):
-        list_value = []
-        for child in data:
-            child_class_type = get_args(class_type)[0]
-            list_value.append(_from_dict(child_class_type, child))
-        return list_value
-    else:
-        return data
-
-
-def model_config_from_dict(d: dict) -> ModelConfig:
-    """Load a dict to a `ModelConfig` instance."""
-    config_type = ModelConfig
-
-    config_type_map = {}
-    for t in [ModelConfig, DecoderLayerConfig, LayernormConfig, LinearConfig]:
-        config_type_map[t.__name__] = t
-
-    if "__name__" in d:
-        config_name = d.pop("__name__")
-        try:
-            config_type = config_type_map[config_name]
-        except Exception as e:
-            raise NotImplementedError(f"{config_name} not supported") from e
-
-    return _from_dict(config_type, d)
-
-
-def pad_weights(weights, tp_size):
-    """Returns the padded weights to tp_size."""
-    assert len(weights.shape) > 1
-
-    def _pad_size(original_size, tp_size):
-        return int(math.ceil(original_size / tp_size) * tp_size)
-
-    original_size = weights.shape[0]
-    padded_size = _pad_size(original_size, tp_size)
-
-    if original_size != padded_size:
-        pad_width = padded_size - original_size
-        return torch.nn.functional.pad(weights, (0, 0, 0, pad_width), "constant", value=0)
-    return weights
-
-
-def merge_qkv(model_config):
-    """Merges the qkv fields in model_config from QKVConfig to a single LinearConfig."""
-    for decoder_config in model_config.layers:
-        if isinstance(decoder_config.attention.qkv, QKVConfig):
-            splitted_qkv = decoder_config.attention.qkv
-            decoder_config.attention.qkv = LinearConfig()
-            decoder_config.attention.qkv.weight = splitted_qkv.weight
-            decoder_config.attention.qkv.bias = splitted_qkv.bias
-            decoder_config.attention.qkv.activation_scaling_factor = (
-                splitted_qkv.activation_scaling_factor
-            )
-            decoder_config.attention.qkv.weights_scaling_factor = (
-                splitted_qkv.weights_scaling_factor
-            )
-            decoder_config.attention.qkv.weights_scaling_factor_2 = (
-                splitted_qkv.weights_scaling_factor_2
-            )
-            decoder_config.attention.qkv.prequant_scaling_factor = (
-                splitted_qkv.prequant_scaling_factor
-            )
-            decoder_config.attention.qkv.awq_block_size = splitted_qkv.awq_block_size
-
-
-def merge_fc1_gate(model_config):
-    """Merges the qkv fields in model_config from QKVConfig to a single LinearConfig."""
-    for decoder_config in model_config.layers:
-        if isinstance(decoder_config.mlp, MLPConfig):
-            if decoder_config.mlp.merged_fc1_gate:
-                fc_weight = decoder_config.mlp.fc.weight
-                gate_weight = decoder_config.mlp.gate.weight
-
-                fc_bias = decoder_config.mlp.fc.bias
-                gate_bias = decoder_config.mlp.gate.bias
-
-                merged_weight = torch.cat([fc_weight, gate_weight], dim=0)
-                decoder_config.mlp.fc.weight = merged_weight
-
-                assert (fc_bias is not None and gate_bias is not None) or (
-                    fc_bias is None and gate_bias is None
-                )
-
-                if fc_bias is not None:
-                    decoder_config.mlp.fc.bias = torch.cat([fc_bias, gate_bias], dim=0)
-
-                assert (fc_bias is not None and gate_bias is not None) or (
-                    fc_bias is None and gate_bias is None
-                )
-
-                assert decoder_config.mlp.fc.linear_type == decoder_config.mlp.gate.linear_type
-
-                assert (
-                    decoder_config.mlp.fc.weights_scaling_factor_2 is None
-                    and decoder_config.mlp.gate.weights_scaling_factor_2 is None
-                ) or torch.equal(
-                    decoder_config.mlp.fc.weights_scaling_factor_2,
-                    decoder_config.mlp.gate.weights_scaling_factor_2,
-                )
-
-                assert (
-                    decoder_config.mlp.fc.activation_scaling_factor is None
-                    and decoder_config.mlp.gate.activation_scaling_factor is None
-                ) or torch.equal(
-                    decoder_config.mlp.fc.activation_scaling_factor,
-                    decoder_config.mlp.gate.activation_scaling_factor,
-                )
-
-                assert (
-                    decoder_config.mlp.fc.prequant_scaling_factor is None
-                    and decoder_config.mlp.gate.prequant_scaling_factor is None
-                ) or torch.equal(
-                    decoder_config.mlp.fc.prequant_scaling_factor,
-                    decoder_config.mlp.gate.prequant_scaling_factor,
-                )
-
-                assert (
-                    decoder_config.mlp.fc.awq_block_size == decoder_config.mlp.gate.awq_block_size
-                )
-
-                weight_scaling_factor = decoder_config.mlp.fc.weights_scaling_factor
-                if weight_scaling_factor is not None:
-                    if weight_scaling_factor.numel() != 1:
-                        assert (
-                            decoder_config.mlp.fc.weights_scaling_factor.shape
-                            == decoder_config.mlp.gate.weights_scaling_factor.shape
-                        )
-                        merged_weight_scaling_factors = torch.cat(
-                            [
-                                decoder_config.mlp.fc.weights_scaling_factor,
-                                decoder_config.mlp.gate.weights_scaling_factor,
-                            ],
-                            dim=0,
-                        )
-                        decoder_config.mlp.fc.weights_scaling_factor = merged_weight_scaling_factors
-                    else:
-                        assert torch.equal(
-                            decoder_config.mlp.fc.weights_scaling_factor,
-                            decoder_config.mlp.gate.weights_scaling_factor,
-                        )
-                decoder_config.mlp.gate = None
-
-
-def to_quantized_weight(
-    weight: torch.Tensor, weights_scaling_factor: torch.Tensor, quantization: str
-):
-    """Converts the weight to the quantized (packed) format."""
-    # Convert the tensor to CPU to avoid potential GPU OOM.
-    weight = weight.cpu()
-    weights_scaling_factor = weights_scaling_factor.cpu()
-
-    if quantization == QUANTIZATION_FP8:
-        # safe tensors does not support fp8 yet. So we pack the tensors as int8
-        if weight.dim() == 3:
-            # for MOE stacked weights
-            return (
-                (weight / weights_scaling_factor.unsqueeze(-1))
-                .to(torch.float8_e4m3fn)
-                .view(torch.int8)
-            )
-        return (weight / weights_scaling_factor).to(torch.float8_e4m3fn).view(torch.int8)
-
-    if quantization == QUANTIZATION_INT8_SQ:
-        return (weight / weights_scaling_factor[:, None]).round().clamp(-128, 127).to(torch.int8)
-
-    if quantization in [QUANTIZATION_INT4_AWQ, QUANTIZATION_W4A8_AWQ]:
-        out_dim = weight.shape[-2]
-        assert (
-            out_dim % 2 == 0
-        ), f"Cannot pack weight. Out dimension {out_dim} is not an even number."
-        in_dim = weight.shape[-1]
-        block_size = weight.shape[-1] // weights_scaling_factor.shape[-1]
-        int8_tensor = (
-            (weight / weights_scaling_factor[..., :, torch.arange(in_dim) // block_size])
-            .round()
-            .clamp(-8, 7)
-            .to(torch.int8)
-        )
-
-        if int8_tensor.dim() == 3:
-            # Case of MoE, where weights are stacked
-            transpose = int8_tensor.permute(0, 2, 1)  # (experts, in_dim, out_dim)
-            int8_tensor = transpose.reshape(
-                -1,
-                in_dim,
-                out_dim // 2,
-                2,
-            )
-            int4x2_tensor = (int8_tensor[..., 0] & 0x0F) | (int8_tensor[..., 1] << 4)
-            # The shape of the returned weight is (experts, out_dim // 2, in_dim)
-            return int4x2_tensor.permute(0, 2, 1).contiguous()
-
-        int8_tensor = int8_tensor.T.reshape(in_dim, out_dim // 2, 2)  # (in_dim, out_dim)
-        int4x2_tensor = (int8_tensor[..., 0] & 0x0F) | (int8_tensor[..., 1] << 4)
-        # The shape of the returned weight is (out_dim // 2, in_dim)
-        return int4x2_tensor.T.contiguous()
-
-    raise NotImplementedError(f"quantization format {quantization} not supported")
-
-
-def from_quantized_weight(
-    weight: torch.Tensor, weights_scaling_factor: torch.Tensor, quantization: str, torch_dtype
-):
-    """Converts the quantized weight to the target torch_dtype format."""
-    if weight.element_size() >= 2 or weights_scaling_factor is None or not quantization:
-        # No need to unquantize the weight.
-        return weight.to(torch_dtype)
-
-    if quantization == QUANTIZATION_FP8:
-        # safe tensors does not support fp8 yet. So we pack the tensors as int8
-        return weight.view(torch.float8_e4m3fn).to(torch_dtype) * weights_scaling_factor.to(
-            torch_dtype
-        )
-
-    if quantization == QUANTIZATION_INT8_SQ:
-        return weight.to(torch_dtype) * weights_scaling_factor[:, None].to(torch_dtype)
-
-    raise NotImplementedError(f"quantization format {quantization} not supported")
-
-
-def pack_linear_weights(model_config: ModelConfig):
-    """Packs the quantized linear weights in the model_config to the quantized format."""
-    if not model_config.quantization:
-        return
-
-    for decoder_config in model_config.layers:
-        linear_layers = [
-            decoder_config.attention.qkv,
-            decoder_config.attention.dense,
-        ]
-        if isinstance(decoder_config.mlp, MOEConfig):
-            if model_config.quantization not in [QUANTIZATION_FP8, QUANTIZATION_INT4_AWQ]:
-                raise NotImplementedError(
-                    f"MOE quantization for {model_config.quantization} is not supported yet."
-                )
-            else:
-                linear_layers.append(decoder_config.mlp.experts.fc)
-                linear_layers.append(decoder_config.mlp.experts.proj)
-        else:
-            linear_layers.append(decoder_config.mlp.fc)
-            linear_layers.append(decoder_config.mlp.proj)
-            linear_layers.append(decoder_config.mlp.gate)
-
-        for linear_layer in linear_layers:
-            if isinstance(linear_layer, LinearConfig):
-                if linear_layer.weights_scaling_factor is not None:
-                    linear_layer.weight = to_quantized_weight(
-                        linear_layer.weight,
-                        linear_layer.weights_scaling_factor,
-                        model_config.quantization,
-                    )
-
-
-def naive_quantization(config: ModelConfig):
-    """Generates a constant scaling factor (1) with target quantization.
-
-    This is for debugging and performance measurement only.
-    """
-    config.quantization = QUANTIZATION_FP8
-    default_scaling_factor = torch.tensor([1], dtype=torch.float32)
-
-    for layer in config.layers:
-        linear_layers = [
-            layer.attention.dense,
-        ]
-        if isinstance(layer.mlp, MOEConfig):
-            linear_layers.append(layer.mlp.experts.fc)
-            linear_layers.append(layer.mlp.experts.proj)
-        else:
-            linear_layers.append(layer.mlp.fc)
-            linear_layers.append(layer.mlp.proj)
-            linear_layers.append(layer.mlp.gate)
-
-        if isinstance(layer.attention.qkv, QKVConfig):
-            linear_layers += [
-                layer.attention.qkv.q,
-                layer.attention.qkv.k,
-                layer.attention.qkv.v,
-            ]
-        elif isinstance(layer.attention.qkv, LinearConfig):
-            linear_layers += [layer.attention.qkv]
-
-        for linear_layer in linear_layers:
-            if linear_layer:
-                linear_layer.activation_scaling_factor = default_scaling_factor
-                linear_layer.weights_scaling_factor = default_scaling_factor
-
-        layer.attention.kv_cache_dtype = KV_CACHE_FP8
-        layer.attention.kv_cache_scaling_factor = torch.tensor([1.0], dtype=torch.float)
-
-    if config.lm_head is not None:
-        config.lm_head.activation_scaling_factor = default_scaling_factor
-        config.lm_head.weights_scaling_factor = default_scaling_factor
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Common utils for the ModelConfig."""
+
+import dataclasses
+import math
+from typing import Dict, Union, get_args, get_origin
+
+import numpy as np
+import torch
+
+from .model_config import (
+    KV_CACHE_FP8,
+    QUANTIZATION_FP8,
+    QUANTIZATION_INT4_AWQ,
+    QUANTIZATION_INT8_SQ,
+    QUANTIZATION_W4A8_AWQ,
+    DecoderLayerConfig,
+    LayernormConfig,
+    LinearConfig,
+    MLPConfig,
+    ModelConfig,
+    MOEConfig,
+    QKVConfig,
+)
+
+# numpy doesn't know bfloat16, define abstract binary type instead
+np_bfloat16 = np.dtype("V2", metadata={"dtype": "bfloat16"})
+
+
+def _numpy_to_torch(x):
+    """Convert numpy array to torch tensor."""
+    if isinstance(x, torch.Tensor):
+        return x
+
+    if x.dtype != np_bfloat16:
+        return torch.tensor(x)
+    return torch.tensor(x.view(np.int16)).view(torch.bfloat16)
+
+
+def model_config_to_dict(model_config: ModelConfig) -> dict:
+    """Converts the instance to a python dict."""
+    assert model_config is not None, "model_config is None"
+    return dataclasses.asdict(model_config)
+
+
+def split_config_and_weights(config, weights: Dict[str, torch.tensor], prefix: str = "transformer"):
+    """Util function to split the weights or any torch.Tensor in nested config to weights.
+
+    A weight id starts with transformers or lm_head will also be generated to link the original key to the weights dict.
+    The weights in the weights dict are contiguous.
+    """
+    if isinstance(config, dict):
+        for k, v in config.items():
+            if k == "lm_head":
+                # lm_head is not part of the transformer.
+                array_key = k
+            elif k == "experts":
+                # Omit the 'experts' key that is not in the model name
+                array_key = prefix
+            else:
+                array_key = f"{prefix}.{k}"
+            if isinstance(v, torch.Tensor):
+                weights[array_key] = v
+                config[k] = f"{array_key}"
+            else:
+                split_config_and_weights(v, weights, array_key)
+    elif isinstance(config, list):
+        for i, v in enumerate(config):
+            array_key = f"{prefix}.{i}"
+            if isinstance(v, torch.Tensor):
+                weights[array_key] = v
+                config[i] = f"{array_key}"
+            else:
+                split_config_and_weights(v, weights, array_key)
+
+
+def _unified_weights_key(k: str) -> str:
+    """Try to unify the weights dict key between old npz and the new safetensors format."""
+    prefixes = ["transformer.", "_np:"]
+    for prefix in prefixes:
+        if k.startswith(prefix):
+            k = k[len(prefix) :]
+
+    k = k.replace("final_layernorm", "ln_f")
+
+    return k.replace(":", ".")
+
+
+def _restore_model_config(model_config, weights: Dict[str, Union[np.ndarray, torch.Tensor]]):
+    def _is_tensor_key(k):
+        return isinstance(k, str) and _unified_weights_key(k) in weights
+
+    if isinstance(model_config, dict):
+        for k, v in model_config.items():
+            if _is_tensor_key(v):
+                model_config[k] = _numpy_to_torch(weights[_unified_weights_key(v)])
+            else:
+                _restore_model_config(v, weights)
+    if isinstance(model_config, list):
+        for i, v in enumerate(model_config):
+            if _is_tensor_key(v):
+                model_config[i] = _numpy_to_torch(weights[_unified_weights_key(v)])
+            else:
+                _restore_model_config(v, weights)
+
+
+def restore_model_config(model_config, weights: Dict[str, Union[np.ndarray, torch.Tensor]]):
+    """Recursively restores the model_config from json and loads np.ndarray or torch.Tensor weights from weights."""
+    unified_key_weights = {}
+    for k, v in weights.items():
+        unified_key_weights[_unified_weights_key(k)] = v
+
+    _restore_model_config(model_config, unified_key_weights)
+
+
+def _from_dict(class_type, data):
+    """Helper function to load the data as a class_type. class_type must be a dataclass."""
+    if data is None:
+        return None
+
+    if get_origin(class_type) == Union:
+        # Handle QKV
+        if all([key in data for key in ["q", "k", "v"]]):
+            # splitted qkv case
+            class_type = QKVConfig
+        elif all([key in data for key in ["router", "experts"]]):
+            # moe
+            class_type = MOEConfig
+        elif all([key in data for key in ["fc", "gate", "proj"]]):
+            # mlp
+            class_type = MLPConfig
+        else:
+            # merged qkv case
+            assert "linear_type" in data, f"{data} is not a valid LinearConfig"
+            class_type = LinearConfig
+
+    if dataclasses.is_dataclass(class_type):
+        fieldtypes = {f.name: f.type for f in dataclasses.fields(class_type)}
+        fields_map = {}
+        for k, v in data.items():
+            if k in fieldtypes:
+                # We only handle keys available in the fields.
+                # Deprecated fields in the checkpoint will be ignored.
+                fields_map[k] = _from_dict(fieldtypes[k], v)
+        return class_type(**fields_map)
+    elif get_origin(class_type) == list and dataclasses.is_dataclass(get_args(class_type)[0]):
+        list_value = []
+        for child in data:
+            child_class_type = get_args(class_type)[0]
+            list_value.append(_from_dict(child_class_type, child))
+        return list_value
+    else:
+        return data
+
+
+def model_config_from_dict(d: dict) -> ModelConfig:
+    """Load a dict to a `ModelConfig` instance."""
+    config_type = ModelConfig
+
+    config_type_map = {}
+    for t in [ModelConfig, DecoderLayerConfig, LayernormConfig, LinearConfig]:
+        config_type_map[t.__name__] = t
+
+    if "__name__" in d:
+        config_name = d.pop("__name__")
+        try:
+            config_type = config_type_map[config_name]
+        except Exception as e:
+            raise NotImplementedError(f"{config_name} not supported") from e
+
+    return _from_dict(config_type, d)
+
+
+def pad_weights(weights, tp_size):
+    """Returns the padded weights to tp_size."""
+    assert len(weights.shape) > 1
+
+    def _pad_size(original_size, tp_size):
+        return int(math.ceil(original_size / tp_size) * tp_size)
+
+    original_size = weights.shape[0]
+    padded_size = _pad_size(original_size, tp_size)
+
+    if original_size != padded_size:
+        pad_width = padded_size - original_size
+        return torch.nn.functional.pad(weights, (0, 0, 0, pad_width), "constant", value=0)
+    return weights
+
+
+def merge_qkv(model_config):
+    """Merges the qkv fields in model_config from QKVConfig to a single LinearConfig."""
+    for decoder_config in model_config.layers:
+        if isinstance(decoder_config.attention.qkv, QKVConfig):
+            splitted_qkv = decoder_config.attention.qkv
+            decoder_config.attention.qkv = LinearConfig()
+            decoder_config.attention.qkv.weight = splitted_qkv.weight
+            decoder_config.attention.qkv.bias = splitted_qkv.bias
+            decoder_config.attention.qkv.activation_scaling_factor = (
+                splitted_qkv.activation_scaling_factor
+            )
+            decoder_config.attention.qkv.weights_scaling_factor = (
+                splitted_qkv.weights_scaling_factor
+            )
+            decoder_config.attention.qkv.weights_scaling_factor_2 = (
+                splitted_qkv.weights_scaling_factor_2
+            )
+            decoder_config.attention.qkv.prequant_scaling_factor = (
+                splitted_qkv.prequant_scaling_factor
+            )
+            decoder_config.attention.qkv.awq_block_size = splitted_qkv.awq_block_size
+
+
+def merge_fc1_gate(model_config):
+    """Merges the qkv fields in model_config from QKVConfig to a single LinearConfig."""
+    for decoder_config in model_config.layers:
+        if isinstance(decoder_config.mlp, MLPConfig):
+            if decoder_config.mlp.merged_fc1_gate:
+                fc_weight = decoder_config.mlp.fc.weight
+                gate_weight = decoder_config.mlp.gate.weight
+
+                fc_bias = decoder_config.mlp.fc.bias
+                gate_bias = decoder_config.mlp.gate.bias
+
+                merged_weight = torch.cat([fc_weight, gate_weight], dim=0)
+                decoder_config.mlp.fc.weight = merged_weight
+
+                assert (fc_bias is not None and gate_bias is not None) or (
+                    fc_bias is None and gate_bias is None
+                )
+
+                if fc_bias is not None:
+                    decoder_config.mlp.fc.bias = torch.cat([fc_bias, gate_bias], dim=0)
+
+                assert (fc_bias is not None and gate_bias is not None) or (
+                    fc_bias is None and gate_bias is None
+                )
+
+                assert decoder_config.mlp.fc.linear_type == decoder_config.mlp.gate.linear_type
+
+                assert (
+                    decoder_config.mlp.fc.weights_scaling_factor_2 is None
+                    and decoder_config.mlp.gate.weights_scaling_factor_2 is None
+                ) or torch.equal(
+                    decoder_config.mlp.fc.weights_scaling_factor_2,
+                    decoder_config.mlp.gate.weights_scaling_factor_2,
+                )
+
+                assert (
+                    decoder_config.mlp.fc.activation_scaling_factor is None
+                    and decoder_config.mlp.gate.activation_scaling_factor is None
+                ) or torch.equal(
+                    decoder_config.mlp.fc.activation_scaling_factor,
+                    decoder_config.mlp.gate.activation_scaling_factor,
+                )
+
+                assert (
+                    decoder_config.mlp.fc.prequant_scaling_factor is None
+                    and decoder_config.mlp.gate.prequant_scaling_factor is None
+                ) or torch.equal(
+                    decoder_config.mlp.fc.prequant_scaling_factor,
+                    decoder_config.mlp.gate.prequant_scaling_factor,
+                )
+
+                assert (
+                    decoder_config.mlp.fc.awq_block_size == decoder_config.mlp.gate.awq_block_size
+                )
+
+                weight_scaling_factor = decoder_config.mlp.fc.weights_scaling_factor
+                if weight_scaling_factor is not None:
+                    if weight_scaling_factor.numel() != 1:
+                        assert (
+                            decoder_config.mlp.fc.weights_scaling_factor.shape
+                            == decoder_config.mlp.gate.weights_scaling_factor.shape
+                        )
+                        merged_weight_scaling_factors = torch.cat(
+                            [
+                                decoder_config.mlp.fc.weights_scaling_factor,
+                                decoder_config.mlp.gate.weights_scaling_factor,
+                            ],
+                            dim=0,
+                        )
+                        decoder_config.mlp.fc.weights_scaling_factor = merged_weight_scaling_factors
+                    else:
+                        assert torch.equal(
+                            decoder_config.mlp.fc.weights_scaling_factor,
+                            decoder_config.mlp.gate.weights_scaling_factor,
+                        )
+                decoder_config.mlp.gate = None
+
+
+def to_quantized_weight(
+    weight: torch.Tensor, weights_scaling_factor: torch.Tensor, quantization: str
+):
+    """Converts the weight to the quantized (packed) format."""
+    # Convert the tensor to CPU to avoid potential GPU OOM.
+    weight = weight.cpu()
+    weights_scaling_factor = weights_scaling_factor.cpu()
+
+    if quantization == QUANTIZATION_FP8:
+        # safe tensors does not support fp8 yet. So we pack the tensors as int8
+        if weight.dim() == 3:
+            # for MOE stacked weights
+            return (
+                (weight / weights_scaling_factor.unsqueeze(-1))
+                .to(torch.float8_e4m3fn)
+                .view(torch.int8)
+            )
+        return (weight / weights_scaling_factor).to(torch.float8_e4m3fn).view(torch.int8)
+
+    if quantization == QUANTIZATION_INT8_SQ:
+        return (weight / weights_scaling_factor[:, None]).round().clamp(-128, 127).to(torch.int8)
+
+    if quantization in [QUANTIZATION_INT4_AWQ, QUANTIZATION_W4A8_AWQ]:
+        out_dim = weight.shape[-2]
+        assert (
+            out_dim % 2 == 0
+        ), f"Cannot pack weight. Out dimension {out_dim} is not an even number."
+        in_dim = weight.shape[-1]
+        block_size = weight.shape[-1] // weights_scaling_factor.shape[-1]
+        int8_tensor = (
+            (weight / weights_scaling_factor[..., :, torch.arange(in_dim) // block_size])
+            .round()
+            .clamp(-8, 7)
+            .to(torch.int8)
+        )
+
+        if int8_tensor.dim() == 3:
+            # Case of MoE, where weights are stacked
+            transpose = int8_tensor.permute(0, 2, 1)  # (experts, in_dim, out_dim)
+            int8_tensor = transpose.reshape(
+                -1,
+                in_dim,
+                out_dim // 2,
+                2,
+            )
+            int4x2_tensor = (int8_tensor[..., 0] & 0x0F) | (int8_tensor[..., 1] << 4)
+            # The shape of the returned weight is (experts, out_dim // 2, in_dim)
+            return int4x2_tensor.permute(0, 2, 1).contiguous()
+
+        int8_tensor = int8_tensor.T.reshape(in_dim, out_dim // 2, 2)  # (in_dim, out_dim)
+        int4x2_tensor = (int8_tensor[..., 0] & 0x0F) | (int8_tensor[..., 1] << 4)
+        # The shape of the returned weight is (out_dim // 2, in_dim)
+        return int4x2_tensor.T.contiguous()
+
+    raise NotImplementedError(f"quantization format {quantization} not supported")
+
+
+def from_quantized_weight(
+    weight: torch.Tensor, weights_scaling_factor: torch.Tensor, quantization: str, torch_dtype
+):
+    """Converts the quantized weight to the target torch_dtype format."""
+    if weight.element_size() >= 2 or weights_scaling_factor is None or not quantization:
+        # No need to unquantize the weight.
+        return weight.to(torch_dtype)
+
+    if quantization == QUANTIZATION_FP8:
+        # safe tensors does not support fp8 yet. So we pack the tensors as int8
+        return weight.view(torch.float8_e4m3fn).to(torch_dtype) * weights_scaling_factor.to(
+            torch_dtype
+        )
+
+    if quantization == QUANTIZATION_INT8_SQ:
+        return weight.to(torch_dtype) * weights_scaling_factor[:, None].to(torch_dtype)
+
+    raise NotImplementedError(f"quantization format {quantization} not supported")
+
+
+def pack_linear_weights(model_config: ModelConfig):
+    """Packs the quantized linear weights in the model_config to the quantized format."""
+    if not model_config.quantization:
+        return
+
+    for decoder_config in model_config.layers:
+        linear_layers = [
+            decoder_config.attention.qkv,
+            decoder_config.attention.dense,
+        ]
+        if isinstance(decoder_config.mlp, MOEConfig):
+            if model_config.quantization not in [QUANTIZATION_FP8, QUANTIZATION_INT4_AWQ]:
+                raise NotImplementedError(
+                    f"MOE quantization for {model_config.quantization} is not supported yet."
+                )
+            else:
+                linear_layers.append(decoder_config.mlp.experts.fc)
+                linear_layers.append(decoder_config.mlp.experts.proj)
+        else:
+            linear_layers.append(decoder_config.mlp.fc)
+            linear_layers.append(decoder_config.mlp.proj)
+            linear_layers.append(decoder_config.mlp.gate)
+
+        for linear_layer in linear_layers:
+            if isinstance(linear_layer, LinearConfig):
+                if linear_layer.weights_scaling_factor is not None:
+                    linear_layer.weight = to_quantized_weight(
+                        linear_layer.weight,
+                        linear_layer.weights_scaling_factor,
+                        model_config.quantization,
+                    )
+
+
+def naive_quantization(config: ModelConfig):
+    """Generates a constant scaling factor (1) with target quantization.
+
+    This is for debugging and performance measurement only.
+    """
+    config.quantization = QUANTIZATION_FP8
+    default_scaling_factor = torch.tensor([1], dtype=torch.float32)
+
+    for layer in config.layers:
+        linear_layers = [
+            layer.attention.dense,
+        ]
+        if isinstance(layer.mlp, MOEConfig):
+            linear_layers.append(layer.mlp.experts.fc)
+            linear_layers.append(layer.mlp.experts.proj)
+        else:
+            linear_layers.append(layer.mlp.fc)
+            linear_layers.append(layer.mlp.proj)
+            linear_layers.append(layer.mlp.gate)
+
+        if isinstance(layer.attention.qkv, QKVConfig):
+            linear_layers += [
+                layer.attention.qkv.q,
+                layer.attention.qkv.k,
+                layer.attention.qkv.v,
+            ]
+        elif isinstance(layer.attention.qkv, LinearConfig):
+            linear_layers += [layer.attention.qkv]
+
+        for linear_layer in linear_layers:
+            if linear_layer:
+                linear_layer.activation_scaling_factor = default_scaling_factor
+                linear_layer.weights_scaling_factor = default_scaling_factor
+
+        layer.attention.kv_cache_dtype = KV_CACHE_FP8
+        layer.attention.kv_cache_scaling_factor = torch.tensor([1.0], dtype=torch.float)
+
+    if config.lm_head is not None:
+        config.lm_head.activation_scaling_factor = default_scaling_factor
+        config.lm_head.weights_scaling_factor = default_scaling_factor
```

## modelopt/torch/export/postprocess.py

 * *Ordering differences only*

```diff
@@ -1,692 +1,692 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utils to load and process model_config."""
-
-
-import copy
-from dataclasses import fields, is_dataclass
-from pathlib import Path
-from typing import List, Optional, Union
-
-import torch
-
-from .distribute import (
-    barrier,
-    get_configs_parallel,
-    get_group,
-    get_rank,
-    get_tensors_parallel,
-    get_world_size,
-)
-from .model_config import (
-    LINEAR_COLUMN,
-    EmbeddingConfig,
-    ExpertConfig,
-    LinearConfig,
-    ModelConfig,
-    MOEConfig,
-)
-from .model_config_utils import pad_weights
-from .scaling_factor_utils import get_weights_scaling_factor, resmooth_and_get_scale
-
-
-def _same_tensor(tensors: List[torch.Tensor]):
-    return all(t is None for t in tensors) or all(torch.equal(tensors[0], t) for t in tensors[1:])
-
-
-def _split_model_config_for_tp(merged_config, split_factor):
-    """This method splits the tensor fields for linear config so the config can be used with more GPUs.
-
-    The implementation is recursive.
-    """
-    configs = [copy.copy(merged_config) for _ in range(split_factor)]
-
-    if isinstance(merged_config, EmbeddingConfig):
-        weights = torch.chunk(pad_weights(merged_config.weight, split_factor), split_factor, dim=0)
-        for i, config in enumerate(configs):
-            config.weight = weights[i]
-
-    elif isinstance(merged_config, MOEConfig):
-        split_expert_configs = _split_model_config_for_tp(
-            merged_config.experts,
-            split_factor,
-        )
-        # TP for rounter of MoE is skipped for better performance
-        # See https://github.com/NVIDIA/TensorRT-LLM/pull/1091 for details
-        for i in range(split_factor):
-            configs[i].experts = split_expert_configs[i]
-            configs[i].router = merged_config.router
-
-    elif isinstance(merged_config, ExpertConfig):
-        assert merged_config.proj.linear_type != LINEAR_COLUMN  # row
-        assert merged_config.fc.linear_type == LINEAR_COLUMN  # column
-        configs = [copy.deepcopy(merged_config) for _ in range(split_factor)]
-        weights = torch.chunk(
-            pad_weights(merged_config.proj.weight, split_factor), split_factor, dim=2
-        )
-        for i, config in enumerate(configs):
-            config.proj.weight = weights[i]
-
-        # Split w3 and w1 weight tensors separately and then concat for each TP
-        merged_w3, merged_w1 = torch.chunk(merged_config.fc.weight, 2, dim=1)
-        merged_w3_weights = torch.chunk(merged_w3, split_factor, dim=1)
-        merged_w1_weights = torch.chunk(merged_w1, split_factor, dim=1)
-        for i, config in enumerate(configs):
-            config.fc.weight = torch.concat([merged_w3_weights[i], merged_w1_weights[i]], dim=1)
-
-        # TODO: check if this works with int8_sq
-        # Split scaling factors for int4_awq
-        if (
-            config.fc.weights_scaling_factor is not None
-            and config.fc.weights_scaling_factor.dim() == 3
-        ):  # column linear
-            merged_scaling_factor_w3, merged_scaling_factor_w1 = torch.chunk(
-                merged_config.fc.weights_scaling_factor, 2, dim=1
-            )
-            scaling_factor_w3 = torch.chunk(merged_scaling_factor_w3, split_factor, dim=1)
-            scaling_factor_w1 = torch.chunk(merged_scaling_factor_w1, split_factor, dim=1)
-            for i, config in enumerate(configs):
-                config.fc.weights_scaling_factor = torch.concat(
-                    [scaling_factor_w3[i], scaling_factor_w1[i]], dim=1
-                )
-        if (
-            config.proj.weights_scaling_factor is not None
-            and config.proj.weights_scaling_factor.dim() == 3
-        ):  # row linear
-            scaling_factor_proj = torch.chunk(
-                merged_config.proj.weights_scaling_factor, split_factor, dim=2
-            )
-            prequant_scaling_factors = torch.chunk(
-                merged_config.proj.prequant_scaling_factor, split_factor, dim=1
-            )
-            for i, config in enumerate(configs):
-                config.proj.weights_scaling_factor = scaling_factor_proj[i]
-                config.proj.prequant_scaling_factor = prequant_scaling_factors[i]
-
-    elif isinstance(merged_config, LinearConfig):
-        # The scaling factors split rule is summarized as below:
-
-        # S: all ranks should have the same scaling factor.
-        # P0: Split the scaling factors on dim 0.
-        # P1: Split the scaling factors on dim 1.
-        # NA: Not valid / present
-
-        # ws: weight scaling factor
-        # as: activation scaling factor
-        # ps: prequant scaling factor
-
-        # C: Colum Linear
-        # R: Row Linear
-
-        # F: FP8
-        # I8: INT8 SQ
-        # I4: INT4 AWQ
-
-        # Split Rules:
-        #      ws  as  ps
-        # FC   S   S   NA
-        # FR   S   S   NA
-        # I8C  P0  S   S
-        # I8R  S   S   P0
-        # I4C  P0  S   S
-        # I4R  P1  S   P0
-
-        # For INT4 AWQ reference implemention: please check examples/llama/weight.py in the tekit repo
-        # For normal linear layers, we split the column linear on the dim 0 and row on the dim 1
-        split_axis = 0 if merged_config.linear_type == LINEAR_COLUMN else 1
-        if merged_config.linear_type == LINEAR_COLUMN:
-            merged_config.weight = pad_weights(merged_config.weight, split_factor)
-        weights = torch.chunk(merged_config.weight, split_factor, dim=split_axis)
-        for i, config in enumerate(configs):
-            config.weight = weights[i]
-
-        # Only split the bias for column linear.
-        if merged_config.linear_type == LINEAR_COLUMN and merged_config.bias is not None:
-            biases = torch.chunk(merged_config.bias, split_factor, dim=0)
-            for i, config in enumerate(configs):
-                config.bias = biases[i]
-
-        if merged_config.linear_type == LINEAR_COLUMN:
-            if (
-                merged_config.weights_scaling_factor is not None
-                and merged_config.weights_scaling_factor.numel() != 1
-            ):
-                # INT4 AWQ and INT8 sq all linear cases
-                weights_scaling_factors = torch.chunk(
-                    merged_config.weights_scaling_factor, split_factor, dim=0
-                )
-                for i, config in enumerate(configs):
-                    config.weights_scaling_factor = weights_scaling_factors[i]
-        else:
-            if (
-                merged_config.weights_scaling_factor is not None
-                and merged_config.awq_block_size != 0
-            ):
-                weights_scaling_factors = torch.chunk(
-                    merged_config.weights_scaling_factor, split_factor, dim=1
-                )
-                for i, config in enumerate(configs):
-                    config.weights_scaling_factor = weights_scaling_factors[i]
-            if merged_config.prequant_scaling_factor is not None:
-                prequant_scaling_factors = torch.chunk(
-                    merged_config.prequant_scaling_factor, split_factor, dim=0
-                )
-                for i, config in enumerate(configs):
-                    config.prequant_scaling_factor = prequant_scaling_factors[i]
-
-    elif is_dataclass(merged_config):
-        for field in fields(merged_config):
-            field_configs = _split_model_config_for_tp(
-                getattr(merged_config, field.name),
-                split_factor,
-            )
-            for r in range(split_factor):
-                setattr(configs[r], field.name, field_configs[r])
-    elif isinstance(merged_config, list):
-        for i in range(len(merged_config)):
-            field_configs = _split_model_config_for_tp(
-                merged_config[i],
-                split_factor,
-            )
-            for r in range(split_factor):
-                configs[r][i] = field_configs[r]
-
-    return configs
-
-
-def _split_model_config_for_pp(merged_config, split_factor):
-    """This method splits ModelConfig for inference pipeline parallel."""
-    num_layers = len(merged_config.layers)
-    assert num_layers % split_factor == 0
-    layers_per_pp = num_layers // split_factor
-
-    configs = [copy.copy(merged_config) for _ in range(split_factor)]
-    for i, config in enumerate(configs):
-        if i > 0:
-            config.vocab_embedding = None
-            config.position_embedding = None
-            config.ln_embed = None
-        if i < split_factor - 1:
-            config.ln_f = None
-            config.lm_head = None
-        config.layers = config.layers[i * layers_per_pp : (i + 1) * layers_per_pp]
-
-    return configs
-
-
-def _merge_model_configs_to_first_tp(config, ranks: List[int], group=None):
-    """This method merges the tensor fields for linear config so the config can be used with fewer GPUs.
-
-    The implementation is recursive.
-    """
-    if isinstance(config, EmbeddingConfig):
-        assert config.weight is not None
-        with get_tensors_parallel(config.weight, ranks, group) as weights:
-            if weights:
-                config.weight = torch.cat(weights, dim=0)
-
-    elif isinstance(config, LinearConfig):
-        # The scaling factors merge rule is summarized as below:
-
-        # S: all ranks should have the same scaling factor.
-        # M: Pick elementwise max among the ranks. Merged shape same as single rank.
-        # C0: Concat the scaling factors on dim 0. Merged shape == tensor_parallel * original shape.
-        # C1: Concat the scaling factors on dim 1. Merged shape == original shape * tensor_parallel.
-        # NA: Not valid / present
-
-        # ws: weight scaling factor
-        # as: activation scaling factor
-        # ps: prequant scaling factor
-
-        # C: Colum Linear
-        # R: Row Linear
-
-        # F: FP8
-        # I8: INT8 SQ
-        # I4: INT4 AWQ
-
-        # Merge Rules:
-        #      ws  as  ps
-        # FC   M   M   NA
-        # FR   M   M   NA
-        # I8C  C0  M   S
-        # I8R  M   M   C0
-        # I4C  C0  M   S
-        # I4R  C1  M   C0
-
-        # Handling constants
-        for field_name in [
-            "activation_scaling_factor",
-            "weights_scaling_factor",
-            "weights_scaling_factor_2",
-        ]:
-            field_value = getattr(config, field_name)
-            if field_value is not None and field_value.numel() == 1:
-                with get_tensors_parallel(field_value, ranks, group) as scaling_factors:
-                    if scaling_factors:
-                        # Scaling factor is a scalar.
-                        setattr(
-                            config,
-                            field_name,
-                            torch.stack(scaling_factors).max(dim=0).values,
-                        )
-
-        # We merge column linear on the dim 0 and row on the dim 1
-        merge_axis = 0 if config.linear_type == LINEAR_COLUMN else 1
-
-        assert config.weight is not None
-        with get_tensors_parallel(config.weight, ranks, group) as weights:
-            if weights:
-                config.weight = torch.cat(weights, dim=merge_axis)
-
-        # Only cat the bias for column linear.
-        if config.linear_type == LINEAR_COLUMN and config.bias is not None:
-            with get_tensors_parallel(config.bias, ranks, group) as biases:
-                if biases:
-                    config.bias = torch.cat(biases, dim=0)
-
-        if config.linear_type == LINEAR_COLUMN:
-            if (
-                config.weights_scaling_factor is not None
-                and config.weights_scaling_factor.numel() != 1
-            ):
-                # INT8 sq
-                with get_tensors_parallel(
-                    config.weights_scaling_factor, ranks, group
-                ) as w_scaling_factors:
-                    if w_scaling_factors:
-                        config.weights_scaling_factor = torch.cat(w_scaling_factors, dim=0)
-            if config.prequant_scaling_factor is not None:
-                with get_tensors_parallel(
-                    config.prequant_scaling_factor, ranks, group
-                ) as p_scaling_factors:
-                    if p_scaling_factors:
-                        # INT4 AWQ, desmooth and de-smooth and re-smooth across all ranks
-                        if config.awq_block_size != 0:
-                            (
-                                config.weight,
-                                config.weights_scaling_factor,
-                                config.prequant_scaling_factor,
-                            ) = resmooth_and_get_scale(
-                                config.weight, p_scaling_factors, len(ranks), config.awq_block_size
-                            )
-                        else:
-                            assert _same_tensor(
-                                p_scaling_factors
-                            ), f"Failed to merge config {config} with others"
-        else:
-            if config.weights_scaling_factor is not None:
-                with get_tensors_parallel(
-                    config.weights_scaling_factor, ranks, group
-                ) as w_scaling_factors:
-                    if w_scaling_factors:
-                        if config.awq_block_size != 0:
-                            # INT4 AWQ
-                            if w_scaling_factors[0].ndim == 2:
-                                scaling_factors_total_size = 0
-                                for _, w_scaling_factor in enumerate(w_scaling_factors):
-                                    scaling_factors_total_size += w_scaling_factor.numel()
-                                if scaling_factors_total_size != config.weight.numel():
-                                    # The weights from each rank are padded to a multiple of group_size in this case.
-                                    # We need to merge the weights and recalculate the scaling factors.
-                                    config.weights_scaling_factor = get_weights_scaling_factor(
-                                        config.weight, config.awq_block_size
-                                    )
-                                else:
-                                    config.weights_scaling_factor = torch.cat(
-                                        w_scaling_factors, dim=1
-                                    )
-                            else:
-                                raise NotImplementedError(
-                                    "Unexpected dimensions for scaling factors."
-                                )
-                        else:
-                            # INT8 SQ
-                            config.weights_scaling_factor = (
-                                torch.stack(w_scaling_factors).max(dim=0).values
-                            )
-            if config.prequant_scaling_factor is not None:
-                with get_tensors_parallel(
-                    config.prequant_scaling_factor, ranks, group
-                ) as p_scaling_factors:
-                    if p_scaling_factors:
-                        config.prequant_scaling_factor = torch.cat(p_scaling_factors, dim=0)
-
-    elif is_dataclass(config):
-        for field in fields(config):
-            _merge_model_configs_to_first_tp(getattr(config, field.name), ranks, group)
-    elif isinstance(config, list):
-        for i in range(len(config)):
-            _merge_model_configs_to_first_tp(config[i], ranks, group)
-
-
-def _model_model_configs_to_first_pp(
-    model_config: ModelConfig, ranks: List[int], workspace_path: Optional[Union[Path, str]] = None
-):
-    """Merges the mode_config from each rank to the first pp rank.
-
-    Args:
-        model_config: model or module config
-        ranks: a list of ranks in the same pipeline parallel group
-        workspace_path: the path to the NFS directory for postprocess cross rank communication.
-    """
-    # TODO: There is an NCCL error if we try group sync based on the pp_ranks.
-    # So we just ask all groups to sync together for now.
-    group = None
-
-    # Merge decoder layers.
-    decoder_layers = []
-    for layer in model_config.layers:
-        with get_configs_parallel(
-            layer, ranks, group, workspace_path=workspace_path
-        ) as layer_configs:
-            layer_configs_copy = []
-            if layer_configs:
-                layer_configs_copy.append(layer_configs[0])
-                for config in layer_configs[1:]:
-                    if config:
-                        # Have to copy the config from the other pps as the shm will be releases after
-                        layer_configs_copy.append(copy.deepcopy(config))
-                decoder_layers.append(layer_configs_copy)
-
-    # If we take a 80-layer TP8/PP4 export, then locally each pp rank has 20 layers.
-    # If we list the global layer id (0-base) in decoder.layers, then
-    #
-    # decoder_layers = [
-    #   [decoder.layers.0  (owned by rank 0-7), 20 (rank 8-15), 40 (rank 16-23), 60 (rank 24-41)],
-    #   [decoder.layers.1  (owned by rank 0-7), 21 (rank 8-15), 41 (rank 16-23), 61 (rank 24-41)],
-    #   ...
-    #   [decoder.layers.19 (owned by rank 0-7), 39 (rank 8-15), 59 (rank 16-23), 79 (rank 24-41)],
-    # ]
-    #
-    # To merge into PP1, we need to gather and shuffle back to the order of
-    #
-    # shuffled_config_layers = [
-    #   decoder.layers.0  (owned by rank 0-7),
-    #   decoder.layers.1  (owned by rank 0-7),
-    #   ...
-    #   decoder.layers.19 (owned by rank 0-7),
-    #   decoder.layers.20 (owned by rank 0-7),
-    #   decoder.layers.21 (owned by rank 0-7),
-    #   ...
-    #   decoder.layers.78 (owned by rank 0-7),
-    #   decoder.layers.79 (owned by rank 0-7),
-    # ]
-    model_config.layers = [
-        layer for shuffled_layers in zip(*decoder_layers) for layer in shuffled_layers
-    ]
-
-    # Get the ln_f from the last PP rank
-    with get_configs_parallel(
-        model_config.ln_f, ranks, group, workspace_path=workspace_path
-    ) as configs:
-        if configs and configs[-1] is not None:
-            model_config.ln_f = configs[-1]
-
-    # Get the lm_head from the last PP rank
-    with get_configs_parallel(
-        model_config.lm_head, ranks, group, workspace_path=workspace_path
-    ) as configs:
-        if configs and configs[-1] is not None:
-            model_config.lm_head = configs[-1]
-
-
-def postprocess_model_config(
-    model_config,
-    inference_tensor_parallel: int = 1,
-    inference_pipeline_parallel: int = 1,
-    training_pipeline_parallel: int = 1,
-    workspace_path: Optional[Union[Path, str]] = None,
-) -> List[ModelConfig]:
-    """Postprocesses the model configs with trained tensor parallel to target inference tensor parallel.
-
-    If the training_pipeline_parallel > 1, the model configs across PP will be merged to one.
-
-    Returns:
-        The processed model config as a list.
-            For the merging case:
-                The merged rank will return the merged model_config as an single item list.
-                The other ranks will return an empty list as we no longer export them.
-            For the split case:
-                The splitted model config list is returned.
-    """
-    rank = get_rank()
-
-    # We assume the ranks ardistributed in training as [PP size, TP size].
-    training_tensor_parallel = get_world_size() // training_pipeline_parallel
-    tp_rank = rank % training_tensor_parallel
-    pp_rank = rank // training_tensor_parallel
-
-    print(f"current rank: {rank}, tp rank: {tp_rank}, pp rank: {pp_rank}")
-    barrier()
-
-    # Merge PP ranks to the first
-    if training_pipeline_parallel > 1:
-        # The pp_ranks for the same tp is [tp_rank, tp_rank + tp, tp_rank + tp * 2, ...]
-        pp_ranks = torch.arange(
-            tp_rank, get_world_size(), training_tensor_parallel, dtype=int
-        ).tolist()
-
-        print(f"PP: Current rank {rank}, merge to {pp_ranks[0]}. Merge group {pp_ranks}")
-        barrier()
-        _model_model_configs_to_first_pp(
-            model_config,
-            pp_ranks,
-            workspace_path=workspace_path,
-        )
-
-    # Returns the empty model_config on other PP ranks.
-    if pp_rank != 0:
-        model_config.rank = -1
-        return []
-
-    # Now we are with PP == 1
-    model_config.pipeline_parallel = 1
-    model_config.rank = tp_rank
-
-    tp_world_size = get_world_size() // training_pipeline_parallel
-    if inference_tensor_parallel < tp_world_size:
-        # Merge the model_configs to target inference tensor parallel.
-        assert (
-            tp_world_size % inference_tensor_parallel == 0
-        ), f"Cannot merge {tp_world_size} configs to {inference_tensor_parallel}"
-
-        num_configs_per_group = tp_world_size // inference_tensor_parallel
-        local_tp_group_id = tp_rank // num_configs_per_group
-        tp_ranks = list(
-            range(
-                local_tp_group_id * num_configs_per_group,
-                (local_tp_group_id + 1) * num_configs_per_group,
-            )
-        )
-
-        print(f"TP: Current rank {rank}, merge to {tp_ranks[0]}. Merge group {tp_ranks}.")
-        # We sync on all TP ranks (and pp_rank = 0)
-        group = get_group(list(range(training_tensor_parallel)))
-        _merge_model_configs_to_first_tp(model_config, tp_ranks, group)
-        model_config.tensor_parallel = inference_tensor_parallel
-        if rank == tp_ranks[0]:
-            model_config.rank = local_tp_group_id
-            splitted_model_configs = [model_config]
-        else:
-            # Mark this config to be invalid and return it as invalid.
-            model_config.rank = -1
-            return []
-
-    elif inference_tensor_parallel > tp_world_size:
-        assert (
-            tp_world_size == 1
-        ), "We only support splitting a single model config to multiple GPUs"
-        split_factor = inference_tensor_parallel // tp_world_size
-        splitted_model_configs = _split_model_config_for_tp(
-            model_config,
-            split_factor,
-        )
-        for i, config in enumerate(splitted_model_configs):
-            config.rank = i
-            config.tensor_parallel = inference_tensor_parallel
-
-    else:
-        splitted_model_configs = [model_config]
-
-    if inference_pipeline_parallel > 1:
-        splitted_model_configs_tp_pp = []
-        for i, model_config_tp in enumerate(splitted_model_configs):
-            splitted_model_configs_pp = _split_model_config_for_pp(
-                model_config_tp, inference_pipeline_parallel
-            )
-            for j, config in enumerate(splitted_model_configs_pp):
-                config.rank = i + j * inference_tensor_parallel
-                config.tensor_parallel = inference_tensor_parallel
-                config.pipeline_parallel = inference_pipeline_parallel
-            splitted_model_configs_tp_pp.extend(splitted_model_configs_pp)
-        return splitted_model_configs_tp_pp
-    else:
-        return splitted_model_configs
-
-
-def pad_embedding_lm_head(model_config: ModelConfig, padding_factor: int = 64):
-    """Pad lm_head and embedding as multiples of 64 for AWQ quantization."""
-    vocab_size = model_config.vocab_size
-    # Pad the lm_head and vocab_embedding only if the lm_head is quantized with AWQ.
-    if vocab_size % padding_factor == 0:
-        return
-
-    pad_vocab_size = int((vocab_size + padding_factor - 1) / padding_factor) * padding_factor
-    model_config.vocab_size = pad_vocab_size
-
-    if hasattr(model_config, "vocab_embedding"):
-        embedding_config = model_config.vocab_embedding
-        original_weight = embedding_config.weight
-        pad_size = (0, 0, 0, pad_vocab_size - original_weight.shape[0])
-        embedding_config.weight = torch.nn.functional.pad(
-            original_weight, pad_size, mode="constant", value=0
-        )
-
-    if hasattr(model_config, "lm_head"):
-        lm_head_config = model_config.lm_head
-        original_weight = lm_head_config.weight
-        original_bias = lm_head_config.bias
-        original_wsf = lm_head_config.weights_scaling_factor
-
-        lm_head_config.weight = torch.nn.functional.pad(
-            original_weight,
-            (0, 0, 0, pad_vocab_size - original_weight.shape[0]),
-            mode="constant",
-            value=0,
-        )
-        if original_bias is not None:
-            lm_head_config.bias = torch.nn.functional.pad(
-                original_bias,
-                (0, pad_vocab_size - original_bias.shape[0]),
-                mode="constant",
-                value=0,
-            )
-
-        if original_wsf is not None:
-            assert len(original_wsf.shape) == 2, "AWQ weight scaling factor should be 2D."
-            pad_weights_scaling_factor = (
-                torch.ones(
-                    (pad_vocab_size, original_wsf.shape[1]),
-                    dtype=original_wsf.dtype,
-                )
-                / 7.0  # int4: maxbound = 7.0
-            )
-
-            pad_weights_scaling_factor[:vocab_size, :] = original_wsf
-            lm_head_config.weights_scaling_factor = pad_weights_scaling_factor
-
-
-def check_weight_shape_valid(config, inference_tensor_parallel=1, training_tensor_parallel=1):
-    """Check if weight shape are valid with inference TP.
-
-    This function is recurisve.
-    """
-
-    def _check_merged_weight(merged_k):
-        assert (
-            merged_k % inference_tensor_parallel == 0
-        ), f"Weights cannot be split into {inference_tensor_parallel} ranks."
-
-    def _check_merged_weight_scaling_factor(merged_k, awq_block_size):
-        if awq_block_size > 0 and (merged_k // inference_tensor_parallel) % awq_block_size != 0:
-            raise NotImplementedError(
-                "Weight shape is not divisible for block size for block quantization."
-            )
-
-    def _check_merged_channel_is_valid(merged_k, awq_block_size):
-        _check_merged_weight(merged_k=merged_k)
-        _check_merged_weight_scaling_factor(merged_k=merged_k, awq_block_size=awq_block_size)
-
-    if isinstance(config, LinearConfig):
-        # check weight shape
-        if config.linear_type == LINEAR_COLUMN:
-            _, k = config.weight.shape
-            merged_k = k * training_tensor_parallel
-            _check_merged_channel_is_valid(merged_k, config.awq_block_size)
-        else:
-            k, m = config.weight.shape
-            merged_k = k * training_tensor_parallel
-            merged_m = m * training_tensor_parallel
-            # For int4_awq, weight scaling factors will be split as (k, (merged_m // TP) // block_size)
-            _check_merged_weight(merged_k=merged_k)
-            _check_merged_weight_scaling_factor(merged_m, config.awq_block_size)
-
-        return
-
-    if isinstance(config, ExpertConfig):
-        _, _, k = config.fc.weight.shape
-        merged_k = k * training_tensor_parallel
-        _check_merged_channel_is_valid(merged_k, config.fc.awq_block_size)
-        return
-
-    if is_dataclass(config):
-        for field in fields(config):
-            check_weight_shape_valid(
-                getattr(config, field.name),
-                inference_tensor_parallel,
-                training_tensor_parallel,
-            )
-    elif isinstance(config, list):
-        for config_i in config:
-            check_weight_shape_valid(config_i, inference_tensor_parallel, training_tensor_parallel)
-
-
-def postprocess_tensors(
-    model_config: ModelConfig,
-    force_cpu: bool = True,
-    force_contiguous: bool = True,
-    force_non_view: bool = True,
-):
-    """Make all tensors in the model_config are on CPU, contiguous and own the memory."""
-
-    def _postprocess_tensor(tensor):
-        if force_cpu:
-            tensor = tensor.cpu()
-        if force_contiguous:
-            tensor = tensor.contiguous()
-        if force_non_view and tensor._is_view():
-            tensor = tensor.clone()
-        return tensor
-
-    for field in fields(model_config):
-        field_value = getattr(model_config, field.name)
-
-        if isinstance(field_value, torch.Tensor):
-            setattr(model_config, field.name, _postprocess_tensor(field_value))
-        elif isinstance(field_value, list):
-            for i, v in enumerate(field_value):
-                if isinstance(v, torch.Tensor):
-                    field_value[i] = _postprocess_tensor(v)
-                elif is_dataclass(v):
-                    postprocess_tensors(v)
-        elif is_dataclass(field_value):
-            postprocess_tensors(field_value)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utils to load and process model_config."""
+
+
+import copy
+from dataclasses import fields, is_dataclass
+from pathlib import Path
+from typing import List, Optional, Union
+
+import torch
+
+from .distribute import (
+    barrier,
+    get_configs_parallel,
+    get_group,
+    get_rank,
+    get_tensors_parallel,
+    get_world_size,
+)
+from .model_config import (
+    LINEAR_COLUMN,
+    EmbeddingConfig,
+    ExpertConfig,
+    LinearConfig,
+    ModelConfig,
+    MOEConfig,
+)
+from .model_config_utils import pad_weights
+from .scaling_factor_utils import get_weights_scaling_factor, resmooth_and_get_scale
+
+
+def _same_tensor(tensors: List[torch.Tensor]):
+    return all(t is None for t in tensors) or all(torch.equal(tensors[0], t) for t in tensors[1:])
+
+
+def _split_model_config_for_tp(merged_config, split_factor):
+    """This method splits the tensor fields for linear config so the config can be used with more GPUs.
+
+    The implementation is recursive.
+    """
+    configs = [copy.copy(merged_config) for _ in range(split_factor)]
+
+    if isinstance(merged_config, EmbeddingConfig):
+        weights = torch.chunk(pad_weights(merged_config.weight, split_factor), split_factor, dim=0)
+        for i, config in enumerate(configs):
+            config.weight = weights[i]
+
+    elif isinstance(merged_config, MOEConfig):
+        split_expert_configs = _split_model_config_for_tp(
+            merged_config.experts,
+            split_factor,
+        )
+        # TP for rounter of MoE is skipped for better performance
+        # See https://github.com/NVIDIA/TensorRT-LLM/pull/1091 for details
+        for i in range(split_factor):
+            configs[i].experts = split_expert_configs[i]
+            configs[i].router = merged_config.router
+
+    elif isinstance(merged_config, ExpertConfig):
+        assert merged_config.proj.linear_type != LINEAR_COLUMN  # row
+        assert merged_config.fc.linear_type == LINEAR_COLUMN  # column
+        configs = [copy.deepcopy(merged_config) for _ in range(split_factor)]
+        weights = torch.chunk(
+            pad_weights(merged_config.proj.weight, split_factor), split_factor, dim=2
+        )
+        for i, config in enumerate(configs):
+            config.proj.weight = weights[i]
+
+        # Split w3 and w1 weight tensors separately and then concat for each TP
+        merged_w3, merged_w1 = torch.chunk(merged_config.fc.weight, 2, dim=1)
+        merged_w3_weights = torch.chunk(merged_w3, split_factor, dim=1)
+        merged_w1_weights = torch.chunk(merged_w1, split_factor, dim=1)
+        for i, config in enumerate(configs):
+            config.fc.weight = torch.concat([merged_w3_weights[i], merged_w1_weights[i]], dim=1)
+
+        # TODO: check if this works with int8_sq
+        # Split scaling factors for int4_awq
+        if (
+            config.fc.weights_scaling_factor is not None
+            and config.fc.weights_scaling_factor.dim() == 3
+        ):  # column linear
+            merged_scaling_factor_w3, merged_scaling_factor_w1 = torch.chunk(
+                merged_config.fc.weights_scaling_factor, 2, dim=1
+            )
+            scaling_factor_w3 = torch.chunk(merged_scaling_factor_w3, split_factor, dim=1)
+            scaling_factor_w1 = torch.chunk(merged_scaling_factor_w1, split_factor, dim=1)
+            for i, config in enumerate(configs):
+                config.fc.weights_scaling_factor = torch.concat(
+                    [scaling_factor_w3[i], scaling_factor_w1[i]], dim=1
+                )
+        if (
+            config.proj.weights_scaling_factor is not None
+            and config.proj.weights_scaling_factor.dim() == 3
+        ):  # row linear
+            scaling_factor_proj = torch.chunk(
+                merged_config.proj.weights_scaling_factor, split_factor, dim=2
+            )
+            prequant_scaling_factors = torch.chunk(
+                merged_config.proj.prequant_scaling_factor, split_factor, dim=1
+            )
+            for i, config in enumerate(configs):
+                config.proj.weights_scaling_factor = scaling_factor_proj[i]
+                config.proj.prequant_scaling_factor = prequant_scaling_factors[i]
+
+    elif isinstance(merged_config, LinearConfig):
+        # The scaling factors split rule is summarized as below:
+
+        # S: all ranks should have the same scaling factor.
+        # P0: Split the scaling factors on dim 0.
+        # P1: Split the scaling factors on dim 1.
+        # NA: Not valid / present
+
+        # ws: weight scaling factor
+        # as: activation scaling factor
+        # ps: prequant scaling factor
+
+        # C: Colum Linear
+        # R: Row Linear
+
+        # F: FP8
+        # I8: INT8 SQ
+        # I4: INT4 AWQ
+
+        # Split Rules:
+        #      ws  as  ps
+        # FC   S   S   NA
+        # FR   S   S   NA
+        # I8C  P0  S   S
+        # I8R  S   S   P0
+        # I4C  P0  S   S
+        # I4R  P1  S   P0
+
+        # For INT4 AWQ reference implemention: please check examples/llama/weight.py in the tekit repo
+        # For normal linear layers, we split the column linear on the dim 0 and row on the dim 1
+        split_axis = 0 if merged_config.linear_type == LINEAR_COLUMN else 1
+        if merged_config.linear_type == LINEAR_COLUMN:
+            merged_config.weight = pad_weights(merged_config.weight, split_factor)
+        weights = torch.chunk(merged_config.weight, split_factor, dim=split_axis)
+        for i, config in enumerate(configs):
+            config.weight = weights[i]
+
+        # Only split the bias for column linear.
+        if merged_config.linear_type == LINEAR_COLUMN and merged_config.bias is not None:
+            biases = torch.chunk(merged_config.bias, split_factor, dim=0)
+            for i, config in enumerate(configs):
+                config.bias = biases[i]
+
+        if merged_config.linear_type == LINEAR_COLUMN:
+            if (
+                merged_config.weights_scaling_factor is not None
+                and merged_config.weights_scaling_factor.numel() != 1
+            ):
+                # INT4 AWQ and INT8 sq all linear cases
+                weights_scaling_factors = torch.chunk(
+                    merged_config.weights_scaling_factor, split_factor, dim=0
+                )
+                for i, config in enumerate(configs):
+                    config.weights_scaling_factor = weights_scaling_factors[i]
+        else:
+            if (
+                merged_config.weights_scaling_factor is not None
+                and merged_config.awq_block_size != 0
+            ):
+                weights_scaling_factors = torch.chunk(
+                    merged_config.weights_scaling_factor, split_factor, dim=1
+                )
+                for i, config in enumerate(configs):
+                    config.weights_scaling_factor = weights_scaling_factors[i]
+            if merged_config.prequant_scaling_factor is not None:
+                prequant_scaling_factors = torch.chunk(
+                    merged_config.prequant_scaling_factor, split_factor, dim=0
+                )
+                for i, config in enumerate(configs):
+                    config.prequant_scaling_factor = prequant_scaling_factors[i]
+
+    elif is_dataclass(merged_config):
+        for field in fields(merged_config):
+            field_configs = _split_model_config_for_tp(
+                getattr(merged_config, field.name),
+                split_factor,
+            )
+            for r in range(split_factor):
+                setattr(configs[r], field.name, field_configs[r])
+    elif isinstance(merged_config, list):
+        for i in range(len(merged_config)):
+            field_configs = _split_model_config_for_tp(
+                merged_config[i],
+                split_factor,
+            )
+            for r in range(split_factor):
+                configs[r][i] = field_configs[r]
+
+    return configs
+
+
+def _split_model_config_for_pp(merged_config, split_factor):
+    """This method splits ModelConfig for inference pipeline parallel."""
+    num_layers = len(merged_config.layers)
+    assert num_layers % split_factor == 0
+    layers_per_pp = num_layers // split_factor
+
+    configs = [copy.copy(merged_config) for _ in range(split_factor)]
+    for i, config in enumerate(configs):
+        if i > 0:
+            config.vocab_embedding = None
+            config.position_embedding = None
+            config.ln_embed = None
+        if i < split_factor - 1:
+            config.ln_f = None
+            config.lm_head = None
+        config.layers = config.layers[i * layers_per_pp : (i + 1) * layers_per_pp]
+
+    return configs
+
+
+def _merge_model_configs_to_first_tp(config, ranks: List[int], group=None):
+    """This method merges the tensor fields for linear config so the config can be used with fewer GPUs.
+
+    The implementation is recursive.
+    """
+    if isinstance(config, EmbeddingConfig):
+        assert config.weight is not None
+        with get_tensors_parallel(config.weight, ranks, group) as weights:
+            if weights:
+                config.weight = torch.cat(weights, dim=0)
+
+    elif isinstance(config, LinearConfig):
+        # The scaling factors merge rule is summarized as below:
+
+        # S: all ranks should have the same scaling factor.
+        # M: Pick elementwise max among the ranks. Merged shape same as single rank.
+        # C0: Concat the scaling factors on dim 0. Merged shape == tensor_parallel * original shape.
+        # C1: Concat the scaling factors on dim 1. Merged shape == original shape * tensor_parallel.
+        # NA: Not valid / present
+
+        # ws: weight scaling factor
+        # as: activation scaling factor
+        # ps: prequant scaling factor
+
+        # C: Colum Linear
+        # R: Row Linear
+
+        # F: FP8
+        # I8: INT8 SQ
+        # I4: INT4 AWQ
+
+        # Merge Rules:
+        #      ws  as  ps
+        # FC   M   M   NA
+        # FR   M   M   NA
+        # I8C  C0  M   S
+        # I8R  M   M   C0
+        # I4C  C0  M   S
+        # I4R  C1  M   C0
+
+        # Handling constants
+        for field_name in [
+            "activation_scaling_factor",
+            "weights_scaling_factor",
+            "weights_scaling_factor_2",
+        ]:
+            field_value = getattr(config, field_name)
+            if field_value is not None and field_value.numel() == 1:
+                with get_tensors_parallel(field_value, ranks, group) as scaling_factors:
+                    if scaling_factors:
+                        # Scaling factor is a scalar.
+                        setattr(
+                            config,
+                            field_name,
+                            torch.stack(scaling_factors).max(dim=0).values,
+                        )
+
+        # We merge column linear on the dim 0 and row on the dim 1
+        merge_axis = 0 if config.linear_type == LINEAR_COLUMN else 1
+
+        assert config.weight is not None
+        with get_tensors_parallel(config.weight, ranks, group) as weights:
+            if weights:
+                config.weight = torch.cat(weights, dim=merge_axis)
+
+        # Only cat the bias for column linear.
+        if config.linear_type == LINEAR_COLUMN and config.bias is not None:
+            with get_tensors_parallel(config.bias, ranks, group) as biases:
+                if biases:
+                    config.bias = torch.cat(biases, dim=0)
+
+        if config.linear_type == LINEAR_COLUMN:
+            if (
+                config.weights_scaling_factor is not None
+                and config.weights_scaling_factor.numel() != 1
+            ):
+                # INT8 sq
+                with get_tensors_parallel(
+                    config.weights_scaling_factor, ranks, group
+                ) as w_scaling_factors:
+                    if w_scaling_factors:
+                        config.weights_scaling_factor = torch.cat(w_scaling_factors, dim=0)
+            if config.prequant_scaling_factor is not None:
+                with get_tensors_parallel(
+                    config.prequant_scaling_factor, ranks, group
+                ) as p_scaling_factors:
+                    if p_scaling_factors:
+                        # INT4 AWQ, desmooth and de-smooth and re-smooth across all ranks
+                        if config.awq_block_size != 0:
+                            (
+                                config.weight,
+                                config.weights_scaling_factor,
+                                config.prequant_scaling_factor,
+                            ) = resmooth_and_get_scale(
+                                config.weight, p_scaling_factors, len(ranks), config.awq_block_size
+                            )
+                        else:
+                            assert _same_tensor(
+                                p_scaling_factors
+                            ), f"Failed to merge config {config} with others"
+        else:
+            if config.weights_scaling_factor is not None:
+                with get_tensors_parallel(
+                    config.weights_scaling_factor, ranks, group
+                ) as w_scaling_factors:
+                    if w_scaling_factors:
+                        if config.awq_block_size != 0:
+                            # INT4 AWQ
+                            if w_scaling_factors[0].ndim == 2:
+                                scaling_factors_total_size = 0
+                                for _, w_scaling_factor in enumerate(w_scaling_factors):
+                                    scaling_factors_total_size += w_scaling_factor.numel()
+                                if scaling_factors_total_size != config.weight.numel():
+                                    # The weights from each rank are padded to a multiple of group_size in this case.
+                                    # We need to merge the weights and recalculate the scaling factors.
+                                    config.weights_scaling_factor = get_weights_scaling_factor(
+                                        config.weight, config.awq_block_size
+                                    )
+                                else:
+                                    config.weights_scaling_factor = torch.cat(
+                                        w_scaling_factors, dim=1
+                                    )
+                            else:
+                                raise NotImplementedError(
+                                    "Unexpected dimensions for scaling factors."
+                                )
+                        else:
+                            # INT8 SQ
+                            config.weights_scaling_factor = (
+                                torch.stack(w_scaling_factors).max(dim=0).values
+                            )
+            if config.prequant_scaling_factor is not None:
+                with get_tensors_parallel(
+                    config.prequant_scaling_factor, ranks, group
+                ) as p_scaling_factors:
+                    if p_scaling_factors:
+                        config.prequant_scaling_factor = torch.cat(p_scaling_factors, dim=0)
+
+    elif is_dataclass(config):
+        for field in fields(config):
+            _merge_model_configs_to_first_tp(getattr(config, field.name), ranks, group)
+    elif isinstance(config, list):
+        for i in range(len(config)):
+            _merge_model_configs_to_first_tp(config[i], ranks, group)
+
+
+def _model_model_configs_to_first_pp(
+    model_config: ModelConfig, ranks: List[int], workspace_path: Optional[Union[Path, str]] = None
+):
+    """Merges the mode_config from each rank to the first pp rank.
+
+    Args:
+        model_config: model or module config
+        ranks: a list of ranks in the same pipeline parallel group
+        workspace_path: the path to the NFS directory for postprocess cross rank communication.
+    """
+    # TODO: There is an NCCL error if we try group sync based on the pp_ranks.
+    # So we just ask all groups to sync together for now.
+    group = None
+
+    # Merge decoder layers.
+    decoder_layers = []
+    for layer in model_config.layers:
+        with get_configs_parallel(
+            layer, ranks, group, workspace_path=workspace_path
+        ) as layer_configs:
+            layer_configs_copy = []
+            if layer_configs:
+                layer_configs_copy.append(layer_configs[0])
+                for config in layer_configs[1:]:
+                    if config:
+                        # Have to copy the config from the other pps as the shm will be releases after
+                        layer_configs_copy.append(copy.deepcopy(config))
+                decoder_layers.append(layer_configs_copy)
+
+    # If we take a 80-layer TP8/PP4 export, then locally each pp rank has 20 layers.
+    # If we list the global layer id (0-base) in decoder.layers, then
+    #
+    # decoder_layers = [
+    #   [decoder.layers.0  (owned by rank 0-7), 20 (rank 8-15), 40 (rank 16-23), 60 (rank 24-41)],
+    #   [decoder.layers.1  (owned by rank 0-7), 21 (rank 8-15), 41 (rank 16-23), 61 (rank 24-41)],
+    #   ...
+    #   [decoder.layers.19 (owned by rank 0-7), 39 (rank 8-15), 59 (rank 16-23), 79 (rank 24-41)],
+    # ]
+    #
+    # To merge into PP1, we need to gather and shuffle back to the order of
+    #
+    # shuffled_config_layers = [
+    #   decoder.layers.0  (owned by rank 0-7),
+    #   decoder.layers.1  (owned by rank 0-7),
+    #   ...
+    #   decoder.layers.19 (owned by rank 0-7),
+    #   decoder.layers.20 (owned by rank 0-7),
+    #   decoder.layers.21 (owned by rank 0-7),
+    #   ...
+    #   decoder.layers.78 (owned by rank 0-7),
+    #   decoder.layers.79 (owned by rank 0-7),
+    # ]
+    model_config.layers = [
+        layer for shuffled_layers in zip(*decoder_layers) for layer in shuffled_layers
+    ]
+
+    # Get the ln_f from the last PP rank
+    with get_configs_parallel(
+        model_config.ln_f, ranks, group, workspace_path=workspace_path
+    ) as configs:
+        if configs and configs[-1] is not None:
+            model_config.ln_f = configs[-1]
+
+    # Get the lm_head from the last PP rank
+    with get_configs_parallel(
+        model_config.lm_head, ranks, group, workspace_path=workspace_path
+    ) as configs:
+        if configs and configs[-1] is not None:
+            model_config.lm_head = configs[-1]
+
+
+def postprocess_model_config(
+    model_config,
+    inference_tensor_parallel: int = 1,
+    inference_pipeline_parallel: int = 1,
+    training_pipeline_parallel: int = 1,
+    workspace_path: Optional[Union[Path, str]] = None,
+) -> List[ModelConfig]:
+    """Postprocesses the model configs with trained tensor parallel to target inference tensor parallel.
+
+    If the training_pipeline_parallel > 1, the model configs across PP will be merged to one.
+
+    Returns:
+        The processed model config as a list.
+            For the merging case:
+                The merged rank will return the merged model_config as an single item list.
+                The other ranks will return an empty list as we no longer export them.
+            For the split case:
+                The splitted model config list is returned.
+    """
+    rank = get_rank()
+
+    # We assume the ranks ardistributed in training as [PP size, TP size].
+    training_tensor_parallel = get_world_size() // training_pipeline_parallel
+    tp_rank = rank % training_tensor_parallel
+    pp_rank = rank // training_tensor_parallel
+
+    print(f"current rank: {rank}, tp rank: {tp_rank}, pp rank: {pp_rank}")
+    barrier()
+
+    # Merge PP ranks to the first
+    if training_pipeline_parallel > 1:
+        # The pp_ranks for the same tp is [tp_rank, tp_rank + tp, tp_rank + tp * 2, ...]
+        pp_ranks = torch.arange(
+            tp_rank, get_world_size(), training_tensor_parallel, dtype=int
+        ).tolist()
+
+        print(f"PP: Current rank {rank}, merge to {pp_ranks[0]}. Merge group {pp_ranks}")
+        barrier()
+        _model_model_configs_to_first_pp(
+            model_config,
+            pp_ranks,
+            workspace_path=workspace_path,
+        )
+
+    # Returns the empty model_config on other PP ranks.
+    if pp_rank != 0:
+        model_config.rank = -1
+        return []
+
+    # Now we are with PP == 1
+    model_config.pipeline_parallel = 1
+    model_config.rank = tp_rank
+
+    tp_world_size = get_world_size() // training_pipeline_parallel
+    if inference_tensor_parallel < tp_world_size:
+        # Merge the model_configs to target inference tensor parallel.
+        assert (
+            tp_world_size % inference_tensor_parallel == 0
+        ), f"Cannot merge {tp_world_size} configs to {inference_tensor_parallel}"
+
+        num_configs_per_group = tp_world_size // inference_tensor_parallel
+        local_tp_group_id = tp_rank // num_configs_per_group
+        tp_ranks = list(
+            range(
+                local_tp_group_id * num_configs_per_group,
+                (local_tp_group_id + 1) * num_configs_per_group,
+            )
+        )
+
+        print(f"TP: Current rank {rank}, merge to {tp_ranks[0]}. Merge group {tp_ranks}.")
+        # We sync on all TP ranks (and pp_rank = 0)
+        group = get_group(list(range(training_tensor_parallel)))
+        _merge_model_configs_to_first_tp(model_config, tp_ranks, group)
+        model_config.tensor_parallel = inference_tensor_parallel
+        if rank == tp_ranks[0]:
+            model_config.rank = local_tp_group_id
+            splitted_model_configs = [model_config]
+        else:
+            # Mark this config to be invalid and return it as invalid.
+            model_config.rank = -1
+            return []
+
+    elif inference_tensor_parallel > tp_world_size:
+        assert (
+            tp_world_size == 1
+        ), "We only support splitting a single model config to multiple GPUs"
+        split_factor = inference_tensor_parallel // tp_world_size
+        splitted_model_configs = _split_model_config_for_tp(
+            model_config,
+            split_factor,
+        )
+        for i, config in enumerate(splitted_model_configs):
+            config.rank = i
+            config.tensor_parallel = inference_tensor_parallel
+
+    else:
+        splitted_model_configs = [model_config]
+
+    if inference_pipeline_parallel > 1:
+        splitted_model_configs_tp_pp = []
+        for i, model_config_tp in enumerate(splitted_model_configs):
+            splitted_model_configs_pp = _split_model_config_for_pp(
+                model_config_tp, inference_pipeline_parallel
+            )
+            for j, config in enumerate(splitted_model_configs_pp):
+                config.rank = i + j * inference_tensor_parallel
+                config.tensor_parallel = inference_tensor_parallel
+                config.pipeline_parallel = inference_pipeline_parallel
+            splitted_model_configs_tp_pp.extend(splitted_model_configs_pp)
+        return splitted_model_configs_tp_pp
+    else:
+        return splitted_model_configs
+
+
+def pad_embedding_lm_head(model_config: ModelConfig, padding_factor: int = 64):
+    """Pad lm_head and embedding as multiples of 64 for AWQ quantization."""
+    vocab_size = model_config.vocab_size
+    # Pad the lm_head and vocab_embedding only if the lm_head is quantized with AWQ.
+    if vocab_size % padding_factor == 0:
+        return
+
+    pad_vocab_size = int((vocab_size + padding_factor - 1) / padding_factor) * padding_factor
+    model_config.vocab_size = pad_vocab_size
+
+    if hasattr(model_config, "vocab_embedding"):
+        embedding_config = model_config.vocab_embedding
+        original_weight = embedding_config.weight
+        pad_size = (0, 0, 0, pad_vocab_size - original_weight.shape[0])
+        embedding_config.weight = torch.nn.functional.pad(
+            original_weight, pad_size, mode="constant", value=0
+        )
+
+    if hasattr(model_config, "lm_head"):
+        lm_head_config = model_config.lm_head
+        original_weight = lm_head_config.weight
+        original_bias = lm_head_config.bias
+        original_wsf = lm_head_config.weights_scaling_factor
+
+        lm_head_config.weight = torch.nn.functional.pad(
+            original_weight,
+            (0, 0, 0, pad_vocab_size - original_weight.shape[0]),
+            mode="constant",
+            value=0,
+        )
+        if original_bias is not None:
+            lm_head_config.bias = torch.nn.functional.pad(
+                original_bias,
+                (0, pad_vocab_size - original_bias.shape[0]),
+                mode="constant",
+                value=0,
+            )
+
+        if original_wsf is not None:
+            assert len(original_wsf.shape) == 2, "AWQ weight scaling factor should be 2D."
+            pad_weights_scaling_factor = (
+                torch.ones(
+                    (pad_vocab_size, original_wsf.shape[1]),
+                    dtype=original_wsf.dtype,
+                )
+                / 7.0  # int4: maxbound = 7.0
+            )
+
+            pad_weights_scaling_factor[:vocab_size, :] = original_wsf
+            lm_head_config.weights_scaling_factor = pad_weights_scaling_factor
+
+
+def check_weight_shape_valid(config, inference_tensor_parallel=1, training_tensor_parallel=1):
+    """Check if weight shape are valid with inference TP.
+
+    This function is recurisve.
+    """
+
+    def _check_merged_weight(merged_k):
+        assert (
+            merged_k % inference_tensor_parallel == 0
+        ), f"Weights cannot be split into {inference_tensor_parallel} ranks."
+
+    def _check_merged_weight_scaling_factor(merged_k, awq_block_size):
+        if awq_block_size > 0 and (merged_k // inference_tensor_parallel) % awq_block_size != 0:
+            raise NotImplementedError(
+                "Weight shape is not divisible for block size for block quantization."
+            )
+
+    def _check_merged_channel_is_valid(merged_k, awq_block_size):
+        _check_merged_weight(merged_k=merged_k)
+        _check_merged_weight_scaling_factor(merged_k=merged_k, awq_block_size=awq_block_size)
+
+    if isinstance(config, LinearConfig):
+        # check weight shape
+        if config.linear_type == LINEAR_COLUMN:
+            _, k = config.weight.shape
+            merged_k = k * training_tensor_parallel
+            _check_merged_channel_is_valid(merged_k, config.awq_block_size)
+        else:
+            k, m = config.weight.shape
+            merged_k = k * training_tensor_parallel
+            merged_m = m * training_tensor_parallel
+            # For int4_awq, weight scaling factors will be split as (k, (merged_m // TP) // block_size)
+            _check_merged_weight(merged_k=merged_k)
+            _check_merged_weight_scaling_factor(merged_m, config.awq_block_size)
+
+        return
+
+    if isinstance(config, ExpertConfig):
+        _, _, k = config.fc.weight.shape
+        merged_k = k * training_tensor_parallel
+        _check_merged_channel_is_valid(merged_k, config.fc.awq_block_size)
+        return
+
+    if is_dataclass(config):
+        for field in fields(config):
+            check_weight_shape_valid(
+                getattr(config, field.name),
+                inference_tensor_parallel,
+                training_tensor_parallel,
+            )
+    elif isinstance(config, list):
+        for config_i in config:
+            check_weight_shape_valid(config_i, inference_tensor_parallel, training_tensor_parallel)
+
+
+def postprocess_tensors(
+    model_config: ModelConfig,
+    force_cpu: bool = True,
+    force_contiguous: bool = True,
+    force_non_view: bool = True,
+):
+    """Make all tensors in the model_config are on CPU, contiguous and own the memory."""
+
+    def _postprocess_tensor(tensor):
+        if force_cpu:
+            tensor = tensor.cpu()
+        if force_contiguous:
+            tensor = tensor.contiguous()
+        if force_non_view and tensor._is_view():
+            tensor = tensor.clone()
+        return tensor
+
+    for field in fields(model_config):
+        field_value = getattr(model_config, field.name)
+
+        if isinstance(field_value, torch.Tensor):
+            setattr(model_config, field.name, _postprocess_tensor(field_value))
+        elif isinstance(field_value, list):
+            for i, v in enumerate(field_value):
+                if isinstance(v, torch.Tensor):
+                    field_value[i] = _postprocess_tensor(v)
+                elif is_dataclass(v):
+                    postprocess_tensors(v)
+        elif is_dataclass(field_value):
+            postprocess_tensors(field_value)
```

## modelopt/torch/export/scaling_factor_utils.py

 * *Ordering differences only*

```diff
@@ -1,88 +1,88 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utils for scaling factors adjustments."""
-
-
-from typing import List
-
-import torch
-
-
-def get_weights_scaling_factor(weight, group_size):
-    """Calculate the weight scaling facotrs for a given group size."""
-    [n, k] = weight.shape
-
-    if group_size != 0:
-        # int4_awq
-        if k % group_size != 0:
-            raise NotImplementedError(
-                "Weight shape is not divisible for block size for block quantiation."
-            )
-        weight = weight.reshape(n, k // group_size, group_size)
-        maxbound = 7.0
-    else:
-        # int8_sq
-        maxbound = 127.0
-    amax = weight.abs().max(dim=-1)[0].float()
-
-    weights_scaling_factor = amax / maxbound
-
-    # Let's filter the zeros in the scaling factor if the weights are zero
-    # to avoid the divided-by-zero error..
-    weights_scaling_factor[weights_scaling_factor == 0] = 1.0
-
-    return weights_scaling_factor
-
-
-def resmooth_and_get_scale(
-    merged_weights: torch.Tensor,
-    pre_quant_scales: List[torch.Tensor],
-    ranks: int,
-    group_size: int,
-    avg_pre_quant_scale: torch.Tensor = None,
-):
-    """Resmooths weights from a single or multiple ranks.
-
-    Args:
-        merged_weights: Merged weights from ranks.
-        pre_quant_scales: List of pre-quantization scales for each rank.
-        ranks: Number of ranks.
-        group_size: Group size of the quantization block.
-        avg_pre_quant_scale (optional): If not provided, weights will be resmoothed using
-            the average of pre_quant_scales.
-
-    Returns:
-        weights: Resmoothed weights.
-        weight_scaling_factors: Resmoothed scaling factors.
-        avg_pre_quant_scale: Calculated average of the quantization scale.
-    """
-    if avg_pre_quant_scale is None:
-        avg_pre_quant_scale = torch.stack(pre_quant_scales).mean(dim=0)
-
-    assert (
-        len(pre_quant_scales) > 0 and avg_pre_quant_scale.numel() == merged_weights.shape[1]
-    ), "Shape of pre_quant_scales and weights do not match."
-    weights = torch.chunk(merged_weights, ranks, dim=0)
-
-    scales = []
-    new_weights = []
-    for i, p_scaling_factor in enumerate(pre_quant_scales):
-        # De smooth & Re smooth
-        weight = (
-            weights[i]
-            * p_scaling_factor.type(weights[i].dtype)
-            / avg_pre_quant_scale.type(weights[i].dtype)
-        )
-        new_weights.append(weight)
-        scale = get_weights_scaling_factor(weight, group_size)
-        scales.append(scale)
-
-    return torch.cat(new_weights, dim=0), torch.cat(scales, dim=0), avg_pre_quant_scale
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utils for scaling factors adjustments."""
+
+
+from typing import List
+
+import torch
+
+
+def get_weights_scaling_factor(weight, group_size):
+    """Calculate the weight scaling facotrs for a given group size."""
+    [n, k] = weight.shape
+
+    if group_size != 0:
+        # int4_awq
+        if k % group_size != 0:
+            raise NotImplementedError(
+                "Weight shape is not divisible for block size for block quantiation."
+            )
+        weight = weight.reshape(n, k // group_size, group_size)
+        maxbound = 7.0
+    else:
+        # int8_sq
+        maxbound = 127.0
+    amax = weight.abs().max(dim=-1)[0].float()
+
+    weights_scaling_factor = amax / maxbound
+
+    # Let's filter the zeros in the scaling factor if the weights are zero
+    # to avoid the divided-by-zero error..
+    weights_scaling_factor[weights_scaling_factor == 0] = 1.0
+
+    return weights_scaling_factor
+
+
+def resmooth_and_get_scale(
+    merged_weights: torch.Tensor,
+    pre_quant_scales: List[torch.Tensor],
+    ranks: int,
+    group_size: int,
+    avg_pre_quant_scale: torch.Tensor = None,
+):
+    """Resmooths weights from a single or multiple ranks.
+
+    Args:
+        merged_weights: Merged weights from ranks.
+        pre_quant_scales: List of pre-quantization scales for each rank.
+        ranks: Number of ranks.
+        group_size: Group size of the quantization block.
+        avg_pre_quant_scale (optional): If not provided, weights will be resmoothed using
+            the average of pre_quant_scales.
+
+    Returns:
+        weights: Resmoothed weights.
+        weight_scaling_factors: Resmoothed scaling factors.
+        avg_pre_quant_scale: Calculated average of the quantization scale.
+    """
+    if avg_pre_quant_scale is None:
+        avg_pre_quant_scale = torch.stack(pre_quant_scales).mean(dim=0)
+
+    assert (
+        len(pre_quant_scales) > 0 and avg_pre_quant_scale.numel() == merged_weights.shape[1]
+    ), "Shape of pre_quant_scales and weights do not match."
+    weights = torch.chunk(merged_weights, ranks, dim=0)
+
+    scales = []
+    new_weights = []
+    for i, p_scaling_factor in enumerate(pre_quant_scales):
+        # De smooth & Re smooth
+        weight = (
+            weights[i]
+            * p_scaling_factor.type(weights[i].dtype)
+            / avg_pre_quant_scale.type(weights[i].dtype)
+        )
+        new_weights.append(weight)
+        scale = get_weights_scaling_factor(weight, group_size)
+        scales.append(scale)
+
+    return torch.cat(new_weights, dim=0), torch.cat(scales, dim=0), avg_pre_quant_scale
```

## modelopt/torch/export/tensorrt_llm_utils.py

 * *Ordering differences only*

```diff
@@ -1,290 +1,290 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utils for TensorRT-LLM checkpoint export.
-
-Some of the logics in this file are empirical and needs constant update if exceptions occur.
-"""
-
-from pathlib import Path
-from typing import Any, Dict, Optional
-
-import numpy as np
-import torch
-
-from modelopt import __version__
-
-from .model_config import (
-    LAYERNORM_DEFAULT,
-    LAYERNORM_RMS,
-    QUANTIZATION_NONE,
-    ModelConfig,
-)
-
-MODEL_NAME_TO_HF_ARCH_MAP = {
-    "bloom": "BloomForCausalLM",
-    "baichuan": "BaichuanForCausalLM",
-    "chatglm": "ChatGLMForCausalLM",
-    "falcon": "FalconForCausalLM",
-    "gptj": "GPTJForCausalLM",
-    "llama": "LlamaForCausalLM",
-    "mpt": "MPTForCausalLM",
-    "qwen": "QWenForCausalLM",
-    "gemma": "GemmaForCausalLM",
-    "phi": "PhiForCausalLM",
-    "gpt2": "GPTForCausalLM",
-    "gptnext": "GPTForCausalLM",
-}
-
-
-def is_tensorrt_llm_0_8_or_9():
-    """Returns true if tensorrt_llm version is 0.8 or 0.9."""
-    try:
-        import tensorrt_llm
-
-        return tensorrt_llm.__version__.startswith(("0.8", "0.9"))
-    except Exception:
-        return False
-
-
-def _find_layernorm_type(model_config: ModelConfig):
-    if model_config.ln_f:
-        return model_config.ln_f.layernorm_type
-    for layer in model_config.layers:
-        if layer.input_layernorm:
-            return layer.input_layernorm.layernorm_type
-        if layer.post_layernorm:
-            return layer.post_layernorm.layernorm_type
-    return LAYERNORM_DEFAULT
-
-
-def convert_to_tensorrt_llm_config(
-    model_config: ModelConfig, tp_size_overwrite: Optional[int] = None
-):
-    """Convert to TensorRT-LLM checkpoint config.
-
-    `tp_size_overwrite` overwrites the tp_size in config.mapping, set only only for phi with TP.
-    This is because the TRT-LLM builder expects its checkpoint to be unsharded.
-    """
-    decoder_type = model_config.layers[0].decoder_type
-    tp_size = model_config.tensor_parallel
-    pp_size = model_config.pipeline_parallel
-    config = {
-        "producer": {
-            "name": "modelopt",
-            "version": __version__,
-        },
-        "architecture": MODEL_NAME_TO_HF_ARCH_MAP[decoder_type],
-        "dtype": model_config.dtype,
-        "num_hidden_layers": len(model_config.layers) * pp_size,
-        "num_attention_heads": model_config.num_attention_heads,
-        "num_key_value_heads": model_config.num_kv_heads,
-        "hidden_size": model_config.hidden_size,
-        "norm_epsilon": model_config.layers[0].input_layernorm.eps,
-        "vocab_size": model_config.vocab_size,
-        "max_position_embeddings": model_config.max_position_embeddings,
-        "hidden_act": model_config.hidden_act,
-        "use_parallel_embedding": True,
-        "embedding_sharding_dim": 0,
-        "quantization": {"quant_algo": None, "kv_cache_quant_algo": None},
-        "mapping": {
-            "world_size": tp_size_overwrite * pp_size if tp_size_overwrite else tp_size * pp_size,
-            "tp_size": tp_size_overwrite if tp_size_overwrite else tp_size,
-            "pp_size": pp_size,
-        },
-        "head_size": model_config.layers[0].attention_head_size,
-        "intermediate_size": model_config.layers[0].ffn_hidden_size_local * tp_size,
-        "position_embedding_type": "alibi" if model_config.layers[0].use_alibi else "rope_gpt_neox",
-        "share_embedding_table": True if (model_config.lm_head is None and pp_size == 1) else False,
-        "residual_mlp": model_config.layers[0].residual_mlp is not None,
-        # Model Optimizer customized fields
-        "bias": model_config.layers[0].attention.dense.bias is not None,
-        "rotary_pct": model_config.layers[0].rotary_pct,
-        "rank": model_config.rank,
-        "decoder": model_config.layers[0].decoder_type,
-        "rmsnorm": _find_layernorm_type(model_config) == LAYERNORM_RMS,
-        "lm_head_bias": model_config.lm_head is not None and model_config.lm_head.bias is not None,
-    }
-
-    if model_config.quantization == "fp8":
-        config["quantization"].update({"quant_algo": "FP8"})
-    elif model_config.quantization == "int4_awq":
-        config["quantization"].update(
-            {
-                "quant_algo": "W4A16_AWQ",
-                "group_size": model_config.layers[0].attention.qkv.awq_block_size,
-                "has_zero_point": False,
-                "pre_quant_scale": True,
-                "exclude_modules": ["lm_head"],
-            }
-        )
-    elif model_config.quantization == "w4a8_awq":
-        config["quantization"].update(
-            {
-                "quant_algo": "W4A8_AWQ",
-                "group_size": model_config.layers[0].attention.qkv.awq_block_size,
-                "has_zero_point": False,
-                "pre_quant_scale": True,
-                "exclude_modules": ["lm_head"],
-            }
-        )
-    elif model_config.quantization == "int8_sq":
-        config["quantization"].update(
-            {
-                "quant_algo": "W8A8_SQ_PER_CHANNEL",
-            }
-        )
-    elif model_config.quantization == QUANTIZATION_NONE:
-        config["quantization"].update(
-            {
-                "quant_algo": None,
-            }
-        )
-    else:
-        config["quantization"].update(
-            {
-                "quant_algo": model_config.quantization,
-            }
-        )
-
-    if model_config.layers[0].attention.kv_cache_dtype is not None:
-        config["quantization"].update(
-            {
-                "kv_cache_quant_algo": model_config.layers[0].attention.kv_cache_dtype,
-            }
-        )
-
-    if decoder_type == "gpt2":
-        config["position_embedding_type"] = "learned_absolute"
-    elif decoder_type == "chatglm":
-        config.update(
-            {
-                "position_embedding_type": "rope_gptj",
-                "intermediate_size": model_config.layers[0].ffn_hidden_size_local * tp_size // 2,
-                "max_position_embeddings": model_config.layers[0].seq_length,  # 32768
-                "chatglm_version": model_config.layers[0].model_name.split("_")[0],
-                "add_bias_linear": model_config.layers[0].attention.dense.bias is not None,  # False
-                "add_qkv_bias": model_config.layers[0].attention.qkv.bias is not None,  # True
-                "apply_query_key_layer_scaling": False,
-                "apply_residual_connection_post_layernorm": model_config.layers[
-                    0
-                ].apply_residual_connection_post_layernorm,  # False
-                "rope_ratio": model_config.layers[0].rope_ratio,
-            }
-        )
-    elif decoder_type == "falcon":
-        config.update(
-            {
-                "position_embedding_type": (
-                    "alibi_with_scale" if model_config.layers[0].use_alibi else "rope_gpt_neox"
-                ),
-                "parallel_attention": model_config.layers[0].parallel_attention,
-                "new_decoder_architecture": model_config.layers[0].new_decoder_architecture,
-            }
-        )
-    elif decoder_type == "gptj":
-        config.update(
-            {
-                "position_embedding_type": "rope_gptj",
-                "rotary_dim": model_config.layers[0].attention.rotary_dim,
-            }
-        )
-    elif (
-        decoder_type == "llama" and model_config.layers[0].moe_num_experts
-    ):  # For Mixtral and Arctic
-        config.update(
-            {
-                "moe_num_experts": model_config.layers[0].moe_num_experts,
-                "moe_top_k": model_config.layers[0].moe_top_k,
-            }
-        )
-    elif decoder_type == "mpt":
-        config.update(
-            {
-                "clip_qkv": model_config.layers[0].attention.clip_qkv,
-                "alibi_bias_max": model_config.layers[0].alibi_bias_max,
-            }
-        )
-    elif decoder_type == "qwen":
-        config.update(
-            {
-                "intermediate_size": model_config.layers[0].ffn_hidden_size_local * 2 * tp_size,
-                "seq_length": model_config.layers[0].seq_length,
-            }
-        )
-    elif decoder_type == "phi":
-        config["partial_rotary_factor"] = model_config.layers[0].partial_rotary_factor
-    elif model_config.layers[0].rotary_base:
-        config.update(
-            {
-                "rotary_base": model_config.layers[0].rotary_base,
-            }
-        )
-
-    return config
-
-
-def weights_to_npz(
-    weights: Dict[str, np.ndarray], tensorrt_llm_config: Dict[str, Any], export_dir: Path
-):
-    """Export the model_config and the weights in the backward-compatible npz forward."""
-    print("Warning: this is an old NPZ format and will be deprecated soon.")
-
-    # step 1: rename key
-    def get_npz_key(k):
-        key_mapping = {
-            "transformer.position_embedding": "_np:position_embedding:weight",
-            "transformer.vocab_embedding": "_np:vocab_embedding:weight",
-            "transformer.ln_f.weight": "_np:final_layernorm:weight",
-            "transformer.ln_f.bias": "_np:final_layernorm:bias",
-        }
-        if k in key_mapping:
-            return key_mapping[k]
-
-        if "lm_head" in k:
-            # src: lm_head.weight
-            # dst: _np:lm_head:weight
-            ns = k.split(".")
-            return ":".join(["_np"] + ns)
-        else:
-            # src: transformers.layers.0.attention.q.weight
-            # dst: _np:layers:20:attention:qkv:q:weight
-            ns = k.split(".")
-            return ":".join(["_np"] + ns[1:])
-
-    # numpy doesn't know bfloat16, define abstract binary type instead
-
-    np_bfloat16 = np.dtype("V2", metadata={"dtype": "bfloat16"})
-
-    def _torch_to_numpy(x):
-        if x.dtype != torch.bfloat16:
-            return x.detach().cpu().numpy()
-        return x.detach().view(torch.int16).cpu().numpy().view(np_bfloat16)
-
-    np_weights = {}
-    for k in list(weights):
-        np_weights[get_npz_key(k)] = _torch_to_numpy(weights.pop(k))
-    weights = np_weights
-
-    # step 2: awq post process
-    if "AWQ" in tensorrt_llm_config.get("quantization", {}).get("quant_algo", ""):
-        for k in list(weights):
-            if k.endswith("weights_scaling_factor"):
-                if "qkv" in k:
-                    weights[k] = np.transpose(weights[k])
-                else:
-                    weights[k] = weights[k].flatten()
-
-    decoder = tensorrt_llm_config["decoder"]
-    tp_size = tensorrt_llm_config["mapping"]["tp_size"]
-    pp_size = tensorrt_llm_config["mapping"]["pp_size"]
-
-    weights_path = export_dir / f"{decoder}_tp{tp_size}_rank{pp_size}.npz"
-    np.savez(weights_path, **weights)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utils for TensorRT-LLM checkpoint export.
+
+Some of the logics in this file are empirical and needs constant update if exceptions occur.
+"""
+
+from pathlib import Path
+from typing import Any, Dict, Optional
+
+import numpy as np
+import torch
+
+from modelopt import __version__
+
+from .model_config import (
+    LAYERNORM_DEFAULT,
+    LAYERNORM_RMS,
+    QUANTIZATION_NONE,
+    ModelConfig,
+)
+
+MODEL_NAME_TO_HF_ARCH_MAP = {
+    "bloom": "BloomForCausalLM",
+    "baichuan": "BaichuanForCausalLM",
+    "chatglm": "ChatGLMForCausalLM",
+    "falcon": "FalconForCausalLM",
+    "gptj": "GPTJForCausalLM",
+    "llama": "LlamaForCausalLM",
+    "mpt": "MPTForCausalLM",
+    "qwen": "QWenForCausalLM",
+    "gemma": "GemmaForCausalLM",
+    "phi": "PhiForCausalLM",
+    "gpt2": "GPTForCausalLM",
+    "gptnext": "GPTForCausalLM",
+}
+
+
+def is_tensorrt_llm_0_8_or_9():
+    """Returns true if tensorrt_llm version is 0.8 or 0.9."""
+    try:
+        import tensorrt_llm
+
+        return tensorrt_llm.__version__.startswith(("0.8", "0.9"))
+    except Exception:
+        return False
+
+
+def _find_layernorm_type(model_config: ModelConfig):
+    if model_config.ln_f:
+        return model_config.ln_f.layernorm_type
+    for layer in model_config.layers:
+        if layer.input_layernorm:
+            return layer.input_layernorm.layernorm_type
+        if layer.post_layernorm:
+            return layer.post_layernorm.layernorm_type
+    return LAYERNORM_DEFAULT
+
+
+def convert_to_tensorrt_llm_config(
+    model_config: ModelConfig, tp_size_overwrite: Optional[int] = None
+):
+    """Convert to TensorRT-LLM checkpoint config.
+
+    `tp_size_overwrite` overwrites the tp_size in config.mapping, set only only for phi with TP.
+    This is because the TRT-LLM builder expects its checkpoint to be unsharded.
+    """
+    decoder_type = model_config.layers[0].decoder_type
+    tp_size = model_config.tensor_parallel
+    pp_size = model_config.pipeline_parallel
+    config = {
+        "producer": {
+            "name": "modelopt",
+            "version": __version__,
+        },
+        "architecture": MODEL_NAME_TO_HF_ARCH_MAP[decoder_type],
+        "dtype": model_config.dtype,
+        "num_hidden_layers": len(model_config.layers) * pp_size,
+        "num_attention_heads": model_config.num_attention_heads,
+        "num_key_value_heads": model_config.num_kv_heads,
+        "hidden_size": model_config.hidden_size,
+        "norm_epsilon": model_config.layers[0].input_layernorm.eps,
+        "vocab_size": model_config.vocab_size,
+        "max_position_embeddings": model_config.max_position_embeddings,
+        "hidden_act": model_config.hidden_act,
+        "use_parallel_embedding": True,
+        "embedding_sharding_dim": 0,
+        "quantization": {"quant_algo": None, "kv_cache_quant_algo": None},
+        "mapping": {
+            "world_size": tp_size_overwrite * pp_size if tp_size_overwrite else tp_size * pp_size,
+            "tp_size": tp_size_overwrite if tp_size_overwrite else tp_size,
+            "pp_size": pp_size,
+        },
+        "head_size": model_config.layers[0].attention_head_size,
+        "intermediate_size": model_config.layers[0].ffn_hidden_size_local * tp_size,
+        "position_embedding_type": "alibi" if model_config.layers[0].use_alibi else "rope_gpt_neox",
+        "share_embedding_table": True if (model_config.lm_head is None and pp_size == 1) else False,
+        "residual_mlp": model_config.layers[0].residual_mlp is not None,
+        # Model Optimizer customized fields
+        "bias": model_config.layers[0].attention.dense.bias is not None,
+        "rotary_pct": model_config.layers[0].rotary_pct,
+        "rank": model_config.rank,
+        "decoder": model_config.layers[0].decoder_type,
+        "rmsnorm": _find_layernorm_type(model_config) == LAYERNORM_RMS,
+        "lm_head_bias": model_config.lm_head is not None and model_config.lm_head.bias is not None,
+    }
+
+    if model_config.quantization == "fp8":
+        config["quantization"].update({"quant_algo": "FP8"})
+    elif model_config.quantization == "int4_awq":
+        config["quantization"].update(
+            {
+                "quant_algo": "W4A16_AWQ",
+                "group_size": model_config.layers[0].attention.qkv.awq_block_size,
+                "has_zero_point": False,
+                "pre_quant_scale": True,
+                "exclude_modules": ["lm_head"],
+            }
+        )
+    elif model_config.quantization == "w4a8_awq":
+        config["quantization"].update(
+            {
+                "quant_algo": "W4A8_AWQ",
+                "group_size": model_config.layers[0].attention.qkv.awq_block_size,
+                "has_zero_point": False,
+                "pre_quant_scale": True,
+                "exclude_modules": ["lm_head"],
+            }
+        )
+    elif model_config.quantization == "int8_sq":
+        config["quantization"].update(
+            {
+                "quant_algo": "W8A8_SQ_PER_CHANNEL",
+            }
+        )
+    elif model_config.quantization == QUANTIZATION_NONE:
+        config["quantization"].update(
+            {
+                "quant_algo": None,
+            }
+        )
+    else:
+        config["quantization"].update(
+            {
+                "quant_algo": model_config.quantization,
+            }
+        )
+
+    if model_config.layers[0].attention.kv_cache_dtype is not None:
+        config["quantization"].update(
+            {
+                "kv_cache_quant_algo": model_config.layers[0].attention.kv_cache_dtype,
+            }
+        )
+
+    if decoder_type == "gpt2":
+        config["position_embedding_type"] = "learned_absolute"
+    elif decoder_type == "chatglm":
+        config.update(
+            {
+                "position_embedding_type": "rope_gptj",
+                "intermediate_size": model_config.layers[0].ffn_hidden_size_local * tp_size // 2,
+                "max_position_embeddings": model_config.layers[0].seq_length,  # 32768
+                "chatglm_version": model_config.layers[0].model_name.split("_")[0],
+                "add_bias_linear": model_config.layers[0].attention.dense.bias is not None,  # False
+                "add_qkv_bias": model_config.layers[0].attention.qkv.bias is not None,  # True
+                "apply_query_key_layer_scaling": False,
+                "apply_residual_connection_post_layernorm": model_config.layers[
+                    0
+                ].apply_residual_connection_post_layernorm,  # False
+                "rope_ratio": model_config.layers[0].rope_ratio,
+            }
+        )
+    elif decoder_type == "falcon":
+        config.update(
+            {
+                "position_embedding_type": (
+                    "alibi_with_scale" if model_config.layers[0].use_alibi else "rope_gpt_neox"
+                ),
+                "parallel_attention": model_config.layers[0].parallel_attention,
+                "new_decoder_architecture": model_config.layers[0].new_decoder_architecture,
+            }
+        )
+    elif decoder_type == "gptj":
+        config.update(
+            {
+                "position_embedding_type": "rope_gptj",
+                "rotary_dim": model_config.layers[0].attention.rotary_dim,
+            }
+        )
+    elif (
+        decoder_type == "llama" and model_config.layers[0].moe_num_experts
+    ):  # For Mixtral and Arctic
+        config.update(
+            {
+                "moe_num_experts": model_config.layers[0].moe_num_experts,
+                "moe_top_k": model_config.layers[0].moe_top_k,
+            }
+        )
+    elif decoder_type == "mpt":
+        config.update(
+            {
+                "clip_qkv": model_config.layers[0].attention.clip_qkv,
+                "alibi_bias_max": model_config.layers[0].alibi_bias_max,
+            }
+        )
+    elif decoder_type == "qwen":
+        config.update(
+            {
+                "intermediate_size": model_config.layers[0].ffn_hidden_size_local * 2 * tp_size,
+                "seq_length": model_config.layers[0].seq_length,
+            }
+        )
+    elif decoder_type == "phi":
+        config["partial_rotary_factor"] = model_config.layers[0].partial_rotary_factor
+    elif model_config.layers[0].rotary_base:
+        config.update(
+            {
+                "rotary_base": model_config.layers[0].rotary_base,
+            }
+        )
+
+    return config
+
+
+def weights_to_npz(
+    weights: Dict[str, np.ndarray], tensorrt_llm_config: Dict[str, Any], export_dir: Path
+):
+    """Export the model_config and the weights in the backward-compatible npz forward."""
+    print("Warning: this is an old NPZ format and will be deprecated soon.")
+
+    # step 1: rename key
+    def get_npz_key(k):
+        key_mapping = {
+            "transformer.position_embedding": "_np:position_embedding:weight",
+            "transformer.vocab_embedding": "_np:vocab_embedding:weight",
+            "transformer.ln_f.weight": "_np:final_layernorm:weight",
+            "transformer.ln_f.bias": "_np:final_layernorm:bias",
+        }
+        if k in key_mapping:
+            return key_mapping[k]
+
+        if "lm_head" in k:
+            # src: lm_head.weight
+            # dst: _np:lm_head:weight
+            ns = k.split(".")
+            return ":".join(["_np"] + ns)
+        else:
+            # src: transformers.layers.0.attention.q.weight
+            # dst: _np:layers:20:attention:qkv:q:weight
+            ns = k.split(".")
+            return ":".join(["_np"] + ns[1:])
+
+    # numpy doesn't know bfloat16, define abstract binary type instead
+
+    np_bfloat16 = np.dtype("V2", metadata={"dtype": "bfloat16"})
+
+    def _torch_to_numpy(x):
+        if x.dtype != torch.bfloat16:
+            return x.detach().cpu().numpy()
+        return x.detach().view(torch.int16).cpu().numpy().view(np_bfloat16)
+
+    np_weights = {}
+    for k in list(weights):
+        np_weights[get_npz_key(k)] = _torch_to_numpy(weights.pop(k))
+    weights = np_weights
+
+    # step 2: awq post process
+    if "AWQ" in tensorrt_llm_config.get("quantization", {}).get("quant_algo", ""):
+        for k in list(weights):
+            if k.endswith("weights_scaling_factor"):
+                if "qkv" in k:
+                    weights[k] = np.transpose(weights[k])
+                else:
+                    weights[k] = weights[k].flatten()
+
+    decoder = tensorrt_llm_config["decoder"]
+    tp_size = tensorrt_llm_config["mapping"]["tp_size"]
+    pp_size = tensorrt_llm_config["mapping"]["pp_size"]
+
+    weights_path = export_dir / f"{decoder}_tp{tp_size}_rank{pp_size}.npz"
+    np.savez(weights_path, **weights)
```

## modelopt/torch/export/transformer_engine.py

 * *Ordering differences only*

```diff
@@ -1,90 +1,90 @@
-# Adapted from https://github.com/huggingface/accelerate/blob/c2f193a25c5449c0e34408bbbffa61ce420b00f6/
-# src/accelerate/utils/transformer_engine.py
-
-# Copyright 2021 The HuggingFace Team. All rights reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-# Not a contribution
-# Changes made by NVIDIA CORPORATION & AFFILIATES or otherwise documented as
-# NVIDIA-proprietary are not a contribution and subject to the following terms and conditions:
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Convert the Model Optimizer quantized model to the transformer_engine."""
-
-import torch
-from torch import nn
-
-
-def _convert_model(model):
-    """Recursively converts the linear layers to their `transformers_engine` counterpart."""
-    import transformer_engine.pytorch as te
-
-    with te.fp8_model_init(enabled=True):
-        for name, module in model.named_children():
-            if type(module).__name__ == "QuantLinear":
-                # Return early if the linear layer weights are not multiples of 16
-                if any(p % 16 != 0 for p in module.weight.shape):
-                    return
-                has_bias = module.bias is not None
-                te_module = te.Linear(
-                    module.in_features,
-                    module.out_features,
-                    bias=has_bias,
-                    params_dtype=module.weight.dtype,
-                )
-                te_module.weight.copy_(module.weight)
-                if has_bias:
-                    te_module.bias.copy_(module.bias)
-
-                input_quantizer = module.input_quantizer
-                quant_max = input_quantizer.maxbound
-                input_scale_inv = input_quantizer.export_amax().float()[0] / quant_max
-                input_scale = 1 / input_scale_inv
-
-                weight_quantizer = module.weight_quantizer
-                quant_max = weight_quantizer.maxbound
-                weight_scale_inv = weight_quantizer.export_amax().float()[0] / quant_max
-                weight_scale = 1 / weight_scale_inv
-
-                # Update the TE layers with the Model Optimizer scaling factors
-                te_module.fp8_meta["scaling_fwd"].scale_inv[0].copy_(input_scale_inv)
-                te_module.fp8_meta["scaling_fwd"].scale_inv[1].copy_(weight_scale_inv)
-                te_module.fp8_meta["scaling_fwd"].scale[0].copy_(input_scale)
-                te_module.fp8_meta["scaling_fwd"].scale[1].copy_(weight_scale)
-
-                setattr(model, name, te_module)
-            else:
-                _convert_model(module)
-
-
-def convert_to_transformer_engine(model: nn.Module):
-    """Converts the `Model Optimizer` quantized model to the `transformers_engine`."""
-    import transformer_engine.common.recipe as te_recipe
-    from transformer_engine.pytorch import fp8_autocast
-
-    with torch.no_grad():
-        _convert_model(model)
-
-    fp8_recipe = te_recipe.DelayedScaling()
-    model.forward = fp8_autocast(enabled=True, fp8_recipe=fp8_recipe)(model.forward)
-
-    return model
+# Adapted from https://github.com/huggingface/accelerate/blob/c2f193a25c5449c0e34408bbbffa61ce420b00f6/
+# src/accelerate/utils/transformer_engine.py
+
+# Copyright 2021 The HuggingFace Team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Not a contribution
+# Changes made by NVIDIA CORPORATION & AFFILIATES or otherwise documented as
+# NVIDIA-proprietary are not a contribution and subject to the following terms and conditions:
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Convert the Model Optimizer quantized model to the transformer_engine."""
+
+import torch
+from torch import nn
+
+
+def _convert_model(model):
+    """Recursively converts the linear layers to their `transformers_engine` counterpart."""
+    import transformer_engine.pytorch as te
+
+    with te.fp8_model_init(enabled=True):
+        for name, module in model.named_children():
+            if type(module).__name__ == "QuantLinear":
+                # Return early if the linear layer weights are not multiples of 16
+                if any(p % 16 != 0 for p in module.weight.shape):
+                    return
+                has_bias = module.bias is not None
+                te_module = te.Linear(
+                    module.in_features,
+                    module.out_features,
+                    bias=has_bias,
+                    params_dtype=module.weight.dtype,
+                )
+                te_module.weight.copy_(module.weight)
+                if has_bias:
+                    te_module.bias.copy_(module.bias)
+
+                input_quantizer = module.input_quantizer
+                quant_max = input_quantizer.maxbound
+                input_scale_inv = input_quantizer.export_amax().float()[0] / quant_max
+                input_scale = 1 / input_scale_inv
+
+                weight_quantizer = module.weight_quantizer
+                quant_max = weight_quantizer.maxbound
+                weight_scale_inv = weight_quantizer.export_amax().float()[0] / quant_max
+                weight_scale = 1 / weight_scale_inv
+
+                # Update the TE layers with the Model Optimizer scaling factors
+                te_module.fp8_meta["scaling_fwd"].scale_inv[0].copy_(input_scale_inv)
+                te_module.fp8_meta["scaling_fwd"].scale_inv[1].copy_(weight_scale_inv)
+                te_module.fp8_meta["scaling_fwd"].scale[0].copy_(input_scale)
+                te_module.fp8_meta["scaling_fwd"].scale[1].copy_(weight_scale)
+
+                setattr(model, name, te_module)
+            else:
+                _convert_model(module)
+
+
+def convert_to_transformer_engine(model: nn.Module):
+    """Converts the `Model Optimizer` quantized model to the `transformers_engine`."""
+    import transformer_engine.common.recipe as te_recipe
+    from transformer_engine.pytorch import fp8_autocast
+
+    with torch.no_grad():
+        _convert_model(model)
+
+    fp8_recipe = te_recipe.DelayedScaling()
+    model.forward = fp8_autocast(enabled=True, fp8_recipe=fp8_recipe)(model.forward)
+
+    return model
```

## modelopt/torch/opt/__init__.py

```diff
@@ -1,34 +1,34 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Module for general-purpose model optimization infrastructure.
-
-``modelopt.torch.opt`` contains tooling to:
-
-* ingest a user-provided model and set it up for optimization;
-* define and implement search and optimization procedures;
-* export a model back to a regular pytorch model after optimization;
-* save, restore, and manage checkpoints from which the model modifications can be resumed.
-
-Please refer to each individual sub-module to learn more about the various concepts wihin
-``modelopt.torch.opt`` and how to use them to implement a model optimization algorithm.
-"""
-
-try:
-    # only for torch > 1.13
-    from . import _hooks
-except ImportError:
-    pass
-
-from . import utils
-from .config import *
-from .conversion import *
-from .mode import *
-from .searcher import *
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Module for general-purpose model optimization infrastructure.
+
+``modelopt.torch.opt`` contains tooling to:
+
+* ingest a user-provided model and set it up for optimization;
+* define and implement search and optimization procedures;
+* export a model back to a regular pytorch model after optimization;
+* save, restore, and manage checkpoints from which the model modifications can be resumed.
+
+Please refer to each individual sub-module to learn more about the various concepts wihin
+``modelopt.torch.opt`` and how to use them to implement a model optimization algorithm.
+"""
+
+try:
+    # only for torch > 1.13
+    from . import _hooks
+except ImportError:
+    pass
+
+from . import plugins, utils
+from .config import *
+from .conversion import *
+from .mode import *
+from .searcher import *
```

## modelopt/torch/opt/_hooks.py

 * *Ordering differences only*

```diff
@@ -1,63 +1,63 @@
-# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Hooks for Pytorch to ensure compatibility with dynamic modules."""
-
-from contextlib import ExitStack, nullcontext
-from typing import Union
-
-import torch
-import torch.nn as nn
-from packaging.version import Version
-
-from .dynamic import DynamicModule
-
-if Version(torch.__version__) >= Version("2.1"):
-    # Older versions have torch.distributed.fsdp.flat_param module but without `_safe_setattr_tensor_or_param`
-    from torch.distributed.fsdp import _flat_param
-    from torch.distributed.fsdp._flat_param import FlatParamHandle
-
-    def _safe_setattr_tensor_or_param_with_dm_check(
-        module: nn.Module, param_name: str, tensor_or_param: Union[torch.Tensor, nn.Parameter]
-    ):
-        """A batched version of _safe_setattr_tensor_or_param ensuring compatibility with DMs.
-
-        This function is called during the creation of the flat param in the FSDP module. We intercept
-        this call to ensure that dynamic attributes are correctly preserved without interfering with
-        FSDP's management, retrieval, and updates of parameters that might also be dynamic attributes.
-        """
-        with (
-            module.reset_dynamic_attributes()
-            if isinstance(module, DynamicModule)
-            else nullcontext()
-        ):
-            return _flat_param._safe_setattr_tensor_or_param_original(
-                module, param_name, tensor_or_param
-            )
-
-    # set the new function as the replacement
-    _flat_param._safe_setattr_tensor_or_param_original = _flat_param._safe_setattr_tensor_or_param
-    _flat_param._safe_setattr_tensor_or_param = _safe_setattr_tensor_or_param_with_dm_check
-
-    def _writeback_orig_param(self: FlatParamHandle):
-        """A batched version of FlatParamHandle._writeback_orig_params ensuring compatibility with DMs.
-
-        This function is called when we use_orig_params=True. We intercept this call to ensure that
-        dynamic attributes are correctly preserved without interfering with FSDP's management,
-        retrieval, and updates of parameters that might also be dynamic attributes.
-        """
-        with ExitStack() as stack:
-            for _, module, _ in self.flat_param._param_infos:
-                if isinstance(module, DynamicModule):
-                    stack.enter_context(module.reset_dynamic_attributes())
-            return self._writeback_orig_params_original()
-
-    FlatParamHandle._writeback_orig_params_original = FlatParamHandle._writeback_orig_params
-    FlatParamHandle._writeback_orig_params = _writeback_orig_param
+# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Hooks for Pytorch to ensure compatibility with dynamic modules."""
+
+from contextlib import ExitStack, nullcontext
+from typing import Union
+
+import torch
+import torch.nn as nn
+from packaging.version import Version
+
+from .dynamic import DynamicModule
+
+if Version(torch.__version__) >= Version("2.1"):
+    # Older versions have torch.distributed.fsdp.flat_param module but without `_safe_setattr_tensor_or_param`
+    from torch.distributed.fsdp import _flat_param
+    from torch.distributed.fsdp._flat_param import FlatParamHandle
+
+    def _safe_setattr_tensor_or_param_with_dm_check(
+        module: nn.Module, param_name: str, tensor_or_param: Union[torch.Tensor, nn.Parameter]
+    ):
+        """A batched version of _safe_setattr_tensor_or_param ensuring compatibility with DMs.
+
+        This function is called during the creation of the flat param in the FSDP module. We intercept
+        this call to ensure that dynamic attributes are correctly preserved without interfering with
+        FSDP's management, retrieval, and updates of parameters that might also be dynamic attributes.
+        """
+        with (
+            module.reset_dynamic_attributes()
+            if isinstance(module, DynamicModule)
+            else nullcontext()
+        ):
+            return _flat_param._safe_setattr_tensor_or_param_original(
+                module, param_name, tensor_or_param
+            )
+
+    # set the new function as the replacement
+    _flat_param._safe_setattr_tensor_or_param_original = _flat_param._safe_setattr_tensor_or_param
+    _flat_param._safe_setattr_tensor_or_param = _safe_setattr_tensor_or_param_with_dm_check
+
+    def _writeback_orig_param(self: FlatParamHandle):
+        """A batched version of FlatParamHandle._writeback_orig_params ensuring compatibility with DMs.
+
+        This function is called when we use_orig_params=True. We intercept this call to ensure that
+        dynamic attributes are correctly preserved without interfering with FSDP's management,
+        retrieval, and updates of parameters that might also be dynamic attributes.
+        """
+        with ExitStack() as stack:
+            for _, module, _ in self.flat_param._param_infos:
+                if isinstance(module, DynamicModule):
+                    stack.enter_context(module.reset_dynamic_attributes())
+            return self._writeback_orig_params_original()
+
+    FlatParamHandle._writeback_orig_params_original = FlatParamHandle._writeback_orig_params
+    FlatParamHandle._writeback_orig_params = _writeback_orig_param
```

## modelopt/torch/opt/config.py

 * *Ordering differences only*

```diff
@@ -1,393 +1,393 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Modelopt's pydantic BaseModel used for any type of configuration in algorithms and mode."""
-
-import fnmatch
-import json
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Dict,
-    ItemsView,
-    Iterator,
-    KeysView,
-    Optional,
-    Type,
-    Union,
-    ValuesView,
-)
-
-import pydantic
-from pydantic import (
-    BaseModel,
-    Field,
-    TypeAdapter,
-    ValidationError,
-    ValidationInfo,
-    field_validator,
-    model_validator,
-)
-from pydantic_core import PydanticUndefined
-
-if TYPE_CHECKING:  # Only executed during type checking, e.g. by mypy
-    # typing.TypeAlias is only available for python 3.10+.
-    from typing_extensions import TypeAlias
-
-# A simple type alias for a config dictionary that is used as input to initialize a ModeloptBaseConfig.
-ConfigDict = Dict[str, Any]  # config dict for one mode
-
-# These are a static type annotations for rules that can be used to annotate functions that accept
-# various rule dictionaries. Proper type checking is done by the respective rule config class, see
-# below.
-# NOTE: the ``WrappedRule`` and ``Rule`` types are further refined via
-#       ``ModeloptBaseRule.get_rule_type(wrapped_only=True)`` and
-#       ``ModeloptBaseRule.get_rule_type(wrapped_only=False)``, respectively.
-SimpleRule = Dict[str, Any]  # a simple rule type
-WrappedRule = Dict[str, Optional[SimpleRule]]  # a wrapped rule type with glob patterns
-Rule = Union[Optional[SimpleRule], WrappedRule]  # a rule type that can be simple, wrapped, or None
-RulesDict = Dict[str, Rule]  # a dictionaries containing rules for different types
-
-
-def ModeloptField(default: Any = PydanticUndefined, **kwargs):  # noqa: N802
-    """A pydantic.Field that enforces setting a default value."""
-    assert default is not PydanticUndefined, "A default value must be set for ModeloptField."
-    return Field(default=default, **kwargs)
-
-
-# TODO: expand config classes to searcher
-
-
-class ModeloptBaseConfig(BaseModel):
-    """Our config base class for mode configuration.
-
-    The base class extends the capabilities of pydantic's BaseModel to provide additional methods
-    and properties for easier access and manipulation of the configuration.
-    """
-
-    model_config = pydantic.ConfigDict(extra="forbid", validate_assignment=True)
-
-    def model_dump(self, **kwargs):
-        """Dump the config to a dictionary with aliases and no warnings by default."""
-        kwargs = {"by_alias": True, "warnings": False, **kwargs}
-        return super().model_dump(**kwargs)
-
-    def model_dump_json(self, **kwargs):
-        """Dump the config to a json with aliases and no warnings by default."""
-        kwargs = {"by_alias": True, "warnings": False, **kwargs}
-        return super().model_dump_json(**kwargs)
-
-    @property
-    def _iterable_model_extra(self) -> Dict[str, Any]:
-        """Return model_extra or empty dict if None so we can iterate over it."""
-        return self.model_extra or {}
-
-    def get_field_name_from_key(self, key: str) -> str:
-        """Get the field name from the given key (can be name or alias of field)."""
-        assert isinstance(key, str), f"key must be a string, got {type(key)}"
-
-        if key in self.model_fields or key in self._iterable_model_extra:
-            return key
-        else:
-            for name, field_info in self.model_fields.items():
-                if field_info.alias == key:
-                    return name
-            raise AttributeError(f"Key {key} not found in the config.")
-
-    def __contains__(self, key: str) -> bool:
-        """Check if the given key is present in the config by its actual name or alias."""
-        try:
-            self.get_field_name_from_key(key)
-            return True
-        except AttributeError:
-            return False
-
-    def __getitem__(self, key: str) -> Any:
-        """Get the value for the given key (can be name or alias of field)."""
-        return getattr(self, self.get_field_name_from_key(key))
-
-    def __setitem__(self, key: str, value: Any) -> None:
-        """Set the value for the given key (can be name or alias of field)."""
-        setattr(self, self.get_field_name_from_key(key), value)
-
-    def get(self, key: str, default: Any = None) -> Any:
-        """Get the value for the given key (can be name or alias) or default if not found."""
-        try:
-            return self[key]
-        except AttributeError:
-            return default
-
-    def __len__(self) -> int:
-        """Return the length of the config."""
-        return len(self.model_fields) + len(self._iterable_model_extra)
-
-    def __iter__(self) -> Iterator[str]:
-        """Iterate over aliases (or name if alias is not defined) of fields."""
-        for field_name, field_info in self.model_fields.items():
-            yield field_info.alias or field_name
-        for key in self._iterable_model_extra:
-            yield key
-
-    def _get_kv_dict(self) -> Dict[str, Any]:
-        """Return a dictionary with keys as aliases if possible."""
-        return {k: self[k] for k in self}
-
-    def keys(self) -> KeysView[str]:
-        """Return the keys (aliases prioritized over names) of the config."""
-        return self._get_kv_dict().keys()
-
-    def values(self) -> ValuesView[Any]:
-        """Return the values of the config."""
-        return self._get_kv_dict().values()
-
-    def items(self) -> ItemsView[str, Any]:
-        """Return the items of the config with keys as aliases if possible."""
-        return self._get_kv_dict().items()
-
-    def update(self, config: ConfigDict) -> None:
-        """Update the config with the given config dictionary."""
-        for key, value in config.items():
-            self[key] = value
-
-
-class ModeloptBaseRule(ModeloptBaseConfig):
-    """Our base config class for rule-based config classes.
-
-    Rules are what governs the configuration for modifying dynamic module classes.
-    """
-
-    @classmethod
-    def get_rule_type(cls, wrapped_only: bool = False) -> "TypeAlias":
-        """Get the rule type for the given ModeloptBaseConfig."""
-        optional_rule_type: "TypeAlias" = Optional[cls]  # type: ignore[valid-type]
-        rule_type = Dict[str, optional_rule_type]
-        if wrapped_only:
-            return rule_type
-        return Union[optional_rule_type, rule_type]
-
-    @classmethod
-    def validate_rule(cls, rule: Rule) -> WrappedRule:
-        """Validate a rule with the current cls rule.
-
-        We will check the full rule type (wrapped and unwrapped) and then return the wrapped type.
-        """
-        # validate the rule with full rule tupe
-        TypeAdapter(cls.get_rule_type()).validate_python(rule)
-
-        # we want to reduce it to the wrapped type
-        wrapped_rule: WrappedRule
-        if rule is None:
-            wrapped_rule = {"*": rule}
-        else:
-            try:
-                # if this call doesn't fail, it means that field is a valid rule_tpye and we
-                # need to wrap it.
-                cls.model_validate(rule)
-                wrapped_rule = {"*": rule}
-            except ValidationError:
-                wrapped_rule = rule
-
-        # now check wrapped type only
-        TypeAdapter(cls.get_rule_type(wrapped_only=True)).validate_python(wrapped_rule)
-
-        return wrapped_rule
-
-    @classmethod
-    def customize_rule(cls, rule: Rule, key: str) -> Optional[SimpleRule]:
-        """Construct custom rule according to the provided key which is matched."""
-        # validate rule first
-        rule = cls.validate_rule(rule)
-
-        # create custom rule now
-        rule_custom: Optional[SimpleRule] = None
-        for pattern, subrule in rule.items():
-            if fnmatch.fnmatch(key, pattern):
-                if rule_custom is None or subrule is None:
-                    rule_custom = subrule
-                else:
-                    rule_custom = {**rule_custom, **subrule}
-        return rule_custom
-
-
-class ModeloptBaseRuleConfig(ModeloptBaseConfig):
-    """Our config base class for mode configuration that are purely made from rules.
-
-    The base class extends the capabilities of pydantic's BaseModel to provide additional methods
-    and properties for easier access and manipulation of the configuration.
-    """
-
-    model_config = pydantic.ConfigDict(extra="allow")
-
-    @classmethod
-    def __init_subclass__(cls, *args, registry, **kwargs):
-        cls._registry = registry
-        extra_default: Dict[str, WrappedRule] = {}
-        cls._extra_default = extra_default
-        super().__init_subclass__(*args, **kwargs)
-
-    @classmethod
-    def register_default(cls, extra_default: Dict[str, WrappedRule]) -> None:
-        """Register a new default value for the given key."""
-        # we don't allow overwriting existing field defaults
-        all_keys = cls.model_fields.keys() | {v.alias or k for k, v in cls.model_fields.items()}
-        overlap = all_keys & extra_default.keys()
-        assert not overlap, f"Updating default values for regular fields not allowed: {overlap}"
-
-        # iterate through provided defaults
-        for k, v in extra_default.items():
-            # run the rule validator and store
-            cls._extra_default[k] = cls._registry.get_rule_class(k).validate_rule(v)
-
-    @classmethod
-    def unregister_default(cls, key: str) -> None:
-        """Unregister the default value for the given key."""
-        cls._extra_default.pop(key)
-
-    @model_validator(mode="after")
-    def _check_for_extra_fields(self):
-        """Check for extra fields and either apply their field validator or raise an error."""
-        _check_field = "_updating_extra_fields"
-        if hasattr(self, _check_field):
-            # avoid recursion
-            return self
-        setattr(self, _check_field, True)
-        for k, v in {**self._extra_default, **self._iterable_model_extra}.items():
-            # try validating and updating the field from registry
-            setattr(self, k, self._registry.get_rule_class(k).validate_rule(v))
-        delattr(self, _check_field)
-        return self
-
-
-def _get_default_description(
-    prefix: str, alias: str, rule_cls: Type[ModeloptBaseRule], default: Rule
-) -> str:
-    prefix = prefix.lower()
-
-    newline_with_indent = "\n" + " " * 4
-
-    default_config = newline_with_indent.join(json.dumps(default, indent=2).splitlines())
-    default_config = " " * 4 + default_config
-    return f"""Configuration for {prefix} {alias} module.
-
-If the ``"{alias}"`` key is not specified, the default configuration (shown in JSON) will be used:
-
-.. code-block:: json
-
-{default_config}
-
-To deactivate any {prefix} {alias} module, use ``None`` instead of providing a dictionary ``{{}}``.
-
-To specify layer-specific configurations, you can specify a config for each submodule with the key
-specifying a glob pattern that matches the submodule name. For example, to convert to a {prefix}
-module for all ``{alias}`` layers except for those in the ``"lm_head"`` submodule use:
-
-.. code-block:: python
-
-    {{
-        "*": {{...}},
-        "*lm_head*": None,
-    }}
-
-Note that glob expressions are processed sequentially in the order they are specified. Later keys in
-the config will overwrite earlier keys if they match the same submodule name.
-
-If you want to specify the same configuration for all submodules, you can provide an unnested
-dictionary as well:
-
-.. code-block:: python
-
-        {{...}}
-
-which is short for
-
-.. code-block:: python
-
-        {{
-            "*": {{...}},
-        }}
-"""
-
-
-def _get_field_validator(alias: str) -> Callable:
-    def _validate_rule_with_correct_signature(cls, field: Any, info: ValidationInfo) -> Any:
-        return cls._registry.get_rule_class(alias).validate_rule(field)
-
-    return field_validator(_get_field_name(alias))(_validate_rule_with_correct_signature)
-
-
-def _get_field_name(alias: str) -> str:
-    return alias.replace(".", "_").lower()
-
-
-# TODO: ideally we annotate the correct type for registry but this would require a refactor since
-# we don't wanna introduce the `modelopt.torch.dynamic` dependency here. Leaving this to future work.
-def get_kwargs_for_create_model_with_rules(
-    registry: Any, default_rules: RulesDict, doc: str
-) -> Dict[str, Any]:
-    """Generate the kwargs for ``pydantic.create_model`` to auto-generate a rule config class.
-
-    Args:
-        registry: The dynamic module registry that contains all relevant dynamic modules.
-        rule_fields: The fields that the rule-based config class should have.
-        doc: The docstring for the rule-based config class.
-
-    A rule-based config class is a config class that purely consists of fields that pertain to
-    rules. We can procedurally generate these rule config classes by using
-
-    .. code-block:: python
-
-        from pydantic import create_model
-
-        MyRuleConfigs = create_model(
-            "MyRuleConfigs", **get_create_model_kwargs_for_rule_model(registry, rule_fields)
-        )
-
-    For more info and example usage, you can take a look at
-    :meth:`SparseMagnitudeConfig<modelopt.torch.sparsity.config.SparseMagnitudeConfig>`.
-
-    .. note::
-
-        We have this convenience function in place since autodocs only get generated when
-        ``create_model`` is *explicitly* called in the respective config file. So this function is a
-        workaround to at least lower the burden of correctly calling ``create_model``.
-
-
-    """
-    # generate fields
-    field_specs = {}
-    for alias, default in default_rules.items():
-        rule_class: Type[ModeloptBaseRule] = registry.get_rule_class(alias)
-        default_validated = rule_class.validate_rule(default)
-        field_specs[_get_field_name(alias)] = (
-            rule_class.get_rule_type(),
-            ModeloptField(
-                alias=alias,
-                default=default_validated,
-                title=f"{alias} config",
-                description=_get_default_description(
-                    registry.prefix, alias, rule_class, default_validated
-                ),
-            ),
-        )
-
-    # generate validators
-    field_validators = {
-        f"_validate_for_{_get_field_name(alias)}": _get_field_validator(alias)
-        for alias in default_rules
-    }
-
-    return {
-        "__base__": ModeloptBaseRuleConfig,
-        "__validators__": field_validators,
-        "__doc__": doc,
-        "__cls_kwargs__": {"registry": registry},
-        **field_specs,
-    }
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Modelopt's pydantic BaseModel used for any type of configuration in algorithms and mode."""
+
+import fnmatch
+import json
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Callable,
+    Dict,
+    ItemsView,
+    Iterator,
+    KeysView,
+    Optional,
+    Type,
+    Union,
+    ValuesView,
+)
+
+import pydantic
+from pydantic import (
+    BaseModel,
+    Field,
+    TypeAdapter,
+    ValidationError,
+    ValidationInfo,
+    field_validator,
+    model_validator,
+)
+from pydantic_core import PydanticUndefined
+
+if TYPE_CHECKING:  # Only executed during type checking, e.g. by mypy
+    # typing.TypeAlias is only available for python 3.10+.
+    from typing_extensions import TypeAlias
+
+# A simple type alias for a config dictionary that is used as input to initialize a ModeloptBaseConfig.
+ConfigDict = Dict[str, Any]  # config dict for one mode
+
+# These are a static type annotations for rules that can be used to annotate functions that accept
+# various rule dictionaries. Proper type checking is done by the respective rule config class, see
+# below.
+# NOTE: the ``WrappedRule`` and ``Rule`` types are further refined via
+#       ``ModeloptBaseRule.get_rule_type(wrapped_only=True)`` and
+#       ``ModeloptBaseRule.get_rule_type(wrapped_only=False)``, respectively.
+SimpleRule = Dict[str, Any]  # a simple rule type
+WrappedRule = Dict[str, Optional[SimpleRule]]  # a wrapped rule type with glob patterns
+Rule = Union[Optional[SimpleRule], WrappedRule]  # a rule type that can be simple, wrapped, or None
+RulesDict = Dict[str, Rule]  # a dictionaries containing rules for different types
+
+
+def ModeloptField(default: Any = PydanticUndefined, **kwargs):  # noqa: N802
+    """A pydantic.Field that enforces setting a default value."""
+    assert default is not PydanticUndefined, "A default value must be set for ModeloptField."
+    return Field(default=default, **kwargs)
+
+
+# TODO: expand config classes to searcher
+
+
+class ModeloptBaseConfig(BaseModel):
+    """Our config base class for mode configuration.
+
+    The base class extends the capabilities of pydantic's BaseModel to provide additional methods
+    and properties for easier access and manipulation of the configuration.
+    """
+
+    model_config = pydantic.ConfigDict(extra="forbid", validate_assignment=True)
+
+    def model_dump(self, **kwargs):
+        """Dump the config to a dictionary with aliases and no warnings by default."""
+        kwargs = {"by_alias": True, "warnings": False, **kwargs}
+        return super().model_dump(**kwargs)
+
+    def model_dump_json(self, **kwargs):
+        """Dump the config to a json with aliases and no warnings by default."""
+        kwargs = {"by_alias": True, "warnings": False, **kwargs}
+        return super().model_dump_json(**kwargs)
+
+    @property
+    def _iterable_model_extra(self) -> Dict[str, Any]:
+        """Return model_extra or empty dict if None so we can iterate over it."""
+        return self.model_extra or {}
+
+    def get_field_name_from_key(self, key: str) -> str:
+        """Get the field name from the given key (can be name or alias of field)."""
+        assert isinstance(key, str), f"key must be a string, got {type(key)}"
+
+        if key in self.model_fields or key in self._iterable_model_extra:
+            return key
+        else:
+            for name, field_info in self.model_fields.items():
+                if field_info.alias == key:
+                    return name
+            raise AttributeError(f"Key {key} not found in the config.")
+
+    def __contains__(self, key: str) -> bool:
+        """Check if the given key is present in the config by its actual name or alias."""
+        try:
+            self.get_field_name_from_key(key)
+            return True
+        except AttributeError:
+            return False
+
+    def __getitem__(self, key: str) -> Any:
+        """Get the value for the given key (can be name or alias of field)."""
+        return getattr(self, self.get_field_name_from_key(key))
+
+    def __setitem__(self, key: str, value: Any) -> None:
+        """Set the value for the given key (can be name or alias of field)."""
+        setattr(self, self.get_field_name_from_key(key), value)
+
+    def get(self, key: str, default: Any = None) -> Any:
+        """Get the value for the given key (can be name or alias) or default if not found."""
+        try:
+            return self[key]
+        except AttributeError:
+            return default
+
+    def __len__(self) -> int:
+        """Return the length of the config."""
+        return len(self.model_fields) + len(self._iterable_model_extra)
+
+    def __iter__(self) -> Iterator[str]:
+        """Iterate over aliases (or name if alias is not defined) of fields."""
+        for field_name, field_info in self.model_fields.items():
+            yield field_info.alias or field_name
+        for key in self._iterable_model_extra:
+            yield key
+
+    def _get_kv_dict(self) -> Dict[str, Any]:
+        """Return a dictionary with keys as aliases if possible."""
+        return {k: self[k] for k in self}
+
+    def keys(self) -> KeysView[str]:
+        """Return the keys (aliases prioritized over names) of the config."""
+        return self._get_kv_dict().keys()
+
+    def values(self) -> ValuesView[Any]:
+        """Return the values of the config."""
+        return self._get_kv_dict().values()
+
+    def items(self) -> ItemsView[str, Any]:
+        """Return the items of the config with keys as aliases if possible."""
+        return self._get_kv_dict().items()
+
+    def update(self, config: ConfigDict) -> None:
+        """Update the config with the given config dictionary."""
+        for key, value in config.items():
+            self[key] = value
+
+
+class ModeloptBaseRule(ModeloptBaseConfig):
+    """Our base config class for rule-based config classes.
+
+    Rules are what governs the configuration for modifying dynamic module classes.
+    """
+
+    @classmethod
+    def get_rule_type(cls, wrapped_only: bool = False) -> "TypeAlias":
+        """Get the rule type for the given ModeloptBaseConfig."""
+        optional_rule_type: "TypeAlias" = Optional[cls]  # type: ignore[valid-type]
+        rule_type = Dict[str, optional_rule_type]
+        if wrapped_only:
+            return rule_type
+        return Union[optional_rule_type, rule_type]
+
+    @classmethod
+    def validate_rule(cls, rule: Rule) -> WrappedRule:
+        """Validate a rule with the current cls rule.
+
+        We will check the full rule type (wrapped and unwrapped) and then return the wrapped type.
+        """
+        # validate the rule with full rule tupe
+        TypeAdapter(cls.get_rule_type()).validate_python(rule)
+
+        # we want to reduce it to the wrapped type
+        wrapped_rule: WrappedRule
+        if rule is None:
+            wrapped_rule = {"*": rule}
+        else:
+            try:
+                # if this call doesn't fail, it means that field is a valid rule_tpye and we
+                # need to wrap it.
+                cls.model_validate(rule)
+                wrapped_rule = {"*": rule}
+            except ValidationError:
+                wrapped_rule = rule
+
+        # now check wrapped type only
+        TypeAdapter(cls.get_rule_type(wrapped_only=True)).validate_python(wrapped_rule)
+
+        return wrapped_rule
+
+    @classmethod
+    def customize_rule(cls, rule: Rule, key: str) -> Optional[SimpleRule]:
+        """Construct custom rule according to the provided key which is matched."""
+        # validate rule first
+        rule = cls.validate_rule(rule)
+
+        # create custom rule now
+        rule_custom: Optional[SimpleRule] = None
+        for pattern, subrule in rule.items():
+            if fnmatch.fnmatch(key, pattern):
+                if rule_custom is None or subrule is None:
+                    rule_custom = subrule
+                else:
+                    rule_custom = {**rule_custom, **subrule}
+        return rule_custom
+
+
+class ModeloptBaseRuleConfig(ModeloptBaseConfig):
+    """Our config base class for mode configuration that are purely made from rules.
+
+    The base class extends the capabilities of pydantic's BaseModel to provide additional methods
+    and properties for easier access and manipulation of the configuration.
+    """
+
+    model_config = pydantic.ConfigDict(extra="allow")
+
+    @classmethod
+    def __init_subclass__(cls, *args, registry, **kwargs):
+        cls._registry = registry
+        extra_default: Dict[str, WrappedRule] = {}
+        cls._extra_default = extra_default
+        super().__init_subclass__(*args, **kwargs)
+
+    @classmethod
+    def register_default(cls, extra_default: Dict[str, WrappedRule]) -> None:
+        """Register a new default value for the given key."""
+        # we don't allow overwriting existing field defaults
+        all_keys = cls.model_fields.keys() | {v.alias or k for k, v in cls.model_fields.items()}
+        overlap = all_keys & extra_default.keys()
+        assert not overlap, f"Updating default values for regular fields not allowed: {overlap}"
+
+        # iterate through provided defaults
+        for k, v in extra_default.items():
+            # run the rule validator and store
+            cls._extra_default[k] = cls._registry.get_rule_class(k).validate_rule(v)
+
+    @classmethod
+    def unregister_default(cls, key: str) -> None:
+        """Unregister the default value for the given key."""
+        cls._extra_default.pop(key)
+
+    @model_validator(mode="after")
+    def _check_for_extra_fields(self):
+        """Check for extra fields and either apply their field validator or raise an error."""
+        _check_field = "_updating_extra_fields"
+        if hasattr(self, _check_field):
+            # avoid recursion
+            return self
+        setattr(self, _check_field, True)
+        for k, v in {**self._extra_default, **self._iterable_model_extra}.items():
+            # try validating and updating the field from registry
+            setattr(self, k, self._registry.get_rule_class(k).validate_rule(v))
+        delattr(self, _check_field)
+        return self
+
+
+def _get_default_description(
+    prefix: str, alias: str, rule_cls: Type[ModeloptBaseRule], default: Rule
+) -> str:
+    prefix = prefix.lower()
+
+    newline_with_indent = "\n" + " " * 4
+
+    default_config = newline_with_indent.join(json.dumps(default, indent=2).splitlines())
+    default_config = " " * 4 + default_config
+    return f"""Configuration for {prefix} {alias} module.
+
+If the ``"{alias}"`` key is not specified, the default configuration (shown in JSON) will be used:
+
+.. code-block:: json
+
+{default_config}
+
+To deactivate any {prefix} {alias} module, use ``None`` instead of providing a dictionary ``{{}}``.
+
+To specify layer-specific configurations, you can specify a config for each submodule with the key
+specifying a glob pattern that matches the submodule name. For example, to convert to a {prefix}
+module for all ``{alias}`` layers except for those in the ``"lm_head"`` submodule use:
+
+.. code-block:: python
+
+    {{
+        "*": {{...}},
+        "*lm_head*": None,
+    }}
+
+Note that glob expressions are processed sequentially in the order they are specified. Later keys in
+the config will overwrite earlier keys if they match the same submodule name.
+
+If you want to specify the same configuration for all submodules, you can provide an unnested
+dictionary as well:
+
+.. code-block:: python
+
+        {{...}}
+
+which is short for
+
+.. code-block:: python
+
+        {{
+            "*": {{...}},
+        }}
+"""
+
+
+def _get_field_validator(alias: str) -> Callable:
+    def _validate_rule_with_correct_signature(cls, field: Any, info: ValidationInfo) -> Any:
+        return cls._registry.get_rule_class(alias).validate_rule(field)
+
+    return field_validator(_get_field_name(alias))(_validate_rule_with_correct_signature)
+
+
+def _get_field_name(alias: str) -> str:
+    return alias.replace(".", "_").lower()
+
+
+# TODO: ideally we annotate the correct type for registry but this would require a refactor since
+# we don't wanna introduce the `modelopt.torch.dynamic` dependency here. Leaving this to future work.
+def get_kwargs_for_create_model_with_rules(
+    registry: Any, default_rules: RulesDict, doc: str
+) -> Dict[str, Any]:
+    """Generate the kwargs for ``pydantic.create_model`` to auto-generate a rule config class.
+
+    Args:
+        registry: The dynamic module registry that contains all relevant dynamic modules.
+        rule_fields: The fields that the rule-based config class should have.
+        doc: The docstring for the rule-based config class.
+
+    A rule-based config class is a config class that purely consists of fields that pertain to
+    rules. We can procedurally generate these rule config classes by using
+
+    .. code-block:: python
+
+        from pydantic import create_model
+
+        MyRuleConfigs = create_model(
+            "MyRuleConfigs", **get_create_model_kwargs_for_rule_model(registry, rule_fields)
+        )
+
+    For more info and example usage, you can take a look at
+    :meth:`SparseMagnitudeConfig<modelopt.torch.sparsity.config.SparseMagnitudeConfig>`.
+
+    .. note::
+
+        We have this convenience function in place since autodocs only get generated when
+        ``create_model`` is *explicitly* called in the respective config file. So this function is a
+        workaround to at least lower the burden of correctly calling ``create_model``.
+
+
+    """
+    # generate fields
+    field_specs = {}
+    for alias, default in default_rules.items():
+        rule_class: Type[ModeloptBaseRule] = registry.get_rule_class(alias)
+        default_validated = rule_class.validate_rule(default)
+        field_specs[_get_field_name(alias)] = (
+            rule_class.get_rule_type(),
+            ModeloptField(
+                alias=alias,
+                default=default_validated,
+                title=f"{alias} config",
+                description=_get_default_description(
+                    registry.prefix, alias, rule_class, default_validated
+                ),
+            ),
+        )
+
+    # generate validators
+    field_validators = {
+        f"_validate_for_{_get_field_name(alias)}": _get_field_validator(alias)
+        for alias in default_rules
+    }
+
+    return {
+        "__base__": ModeloptBaseRuleConfig,
+        "__validators__": field_validators,
+        "__doc__": doc,
+        "__cls_kwargs__": {"registry": registry},
+        **field_specs,
+    }
```

## modelopt/torch/opt/conversion.py

 * *Ordering differences only*

```diff
@@ -1,564 +1,564 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Module to handle model converting and restoring for optimization methods.
-
-When applying a model optimization algorithm, we usually need to modify the model in each step
-(mode) of the algorithm. This module provides the state manager, which is a standardized interface
-(class) to record and store state information in the model.
-
-Op top of the state manager, this module provides utilities to save a history of these modifications
-("modelopt state dict") and restoring a unmodified model to the state indicated in the state dict.
-"""
-
-import copy
-import os
-import warnings
-from collections import deque
-from typing import Any, BinaryIO, Deque, Dict, Iterator, List, Optional, Tuple, Union
-
-import torch
-import torch.nn as nn
-
-from modelopt import __version__
-from modelopt.torch.utils import ModelLike, init_model_from_model_like, unwrap_model
-
-from .config import ConfigDict, ModeloptBaseConfig
-from .mode import (
-    MetadataDict,
-    ModeLike,
-    ModeState,
-    ModeType,
-    _ModeDescriptor,
-    _ModeRegistryCls,
-    get_mode_config,
-)
-
-__all__ = [
-    "ModeloptStateManager",
-    "apply_mode",
-    "modelopt_state",
-    "save",
-    "restore_from_modelopt_state",
-    "restore",
-]
-
-ModeloptStateList = List[Tuple[str, ModeState]]  # state data structure for multiple modes
-
-
-class ModeloptStateManager:
-    """A class to handle the modelopt state stored for each mode correspondig to a task/mode."""
-
-    _state_key = "_modelopt_state"
-
-    def __init__(self, model: Optional[nn.Module] = None, init_state: bool = False) -> None:
-        """Initialize state manager.
-
-        Args:
-            model: Module that has modelopt_state stored. If None, a fake module is created to store
-                any state that might be added with the manager.
-            init_state: Whether to initialize the modelopt state for the model if it does not exist.
-        """
-        # just assume fake module for easy implementation below
-        if not model:
-            model = nn.Module()
-            init_state = True  # always initialize fake module
-
-        # initialize modelopt state if desired. Note that by default we don't do that to avoid
-        # accidentally modifying a user-provided model.
-        if init_state:
-            assert not hasattr(model, self._state_key), "Model already has modelopt state!"
-            setattr(model, self._state_key, [])
-
-        # sanity check that root module has modelopt state now
-        assert self.is_converted(model, is_root=True), "Model must have modelopt state!"
-
-        # store reference to state
-        self._state: ModeloptStateList = getattr(model, self._state_key)
-
-    @property
-    def has_state(self) -> bool:
-        """Return whether the model has a non-trivial modelopt state."""
-        return bool(self._state)
-
-    @classmethod
-    def is_converted(cls, model: nn.Module, is_root: bool = False) -> bool:
-        """Check if model is converted.
-
-        Args:
-            model: A model to be checked for state/metadata from the convert process.
-            is_root: Additionally check whether the module with state is the root module.
-
-        Returns:
-            True if the model contains modelopt state indicating that it has been converted.
-
-        This method raises an assertion when multiple modelopt_states are detected or when is_root is
-        set to True but the module with state is not the root module.
-        """
-        # check for submodules with state
-        mods_with_state = [name for name, m in model.named_modules() if hasattr(m, cls._state_key)]
-        # check if there is multiple submodules with state
-        assert len(mods_with_state) <= 1, "Model has multiple modelopt states!"
-        is_converted = bool(mods_with_state)
-
-        # check if mod with state is root module if desired
-        if is_converted:
-            assert (
-                not is_root or mods_with_state[0] == ""
-            ), "Model has modelopt state but not the root!"
-
-        return is_converted
-
-    # TODO: consider renaming state_dict???
-    def state_dict(self) -> ModeloptStateList:
-        """Return the metadata of the model."""
-        return self._state
-
-    def load_state_dict(self, state_dict: ModeloptStateList) -> None:
-        """Load the provided ``state_dict`` to the modelopt_state."""
-        assert not self.has_state, "Cannot load state_dict if there is already one."
-
-        # make sure we operate on deepcopy
-        state_dict = copy.deepcopy(state_dict)
-        # add modes one-by-one
-        for m_str, m_state in state_dict:
-            # adds config and metadata with sanity checks
-            config = self.get_config_class(m_str, m_state["config"])
-            self.add_mode(m_str, config, m_state["metadata"])
-
-        # overwrite state manually afterwards to ensure exact consistency with provided state_dict
-        self._state.clear()
-        self._state.extend(state_dict)
-
-    @classmethod
-    def transfer_state_dict(cls, model_from: nn.Module, model_to: nn.Module) -> None:
-        """Transfer the state (same instance) from one model to another."""
-        manager_from = ModeloptStateManager(model_from, init_state=False)  # state must exist
-        manager_to = ModeloptStateManager(model_to, init_state=True)  # state must NOT exist
-
-        # transfer state_dict (this uses sanity checks + deepcopy)
-        manager_to.load_state_dict(manager_from.state_dict())
-
-        # manually set the state dict to be the exact same instance
-        setattr(model_to, cls._state_key, manager_from.state_dict())
-        manager_to = ModeloptStateManager(model_to, init_state=False)  # state must exist now
-
-        # remove state from model_from
-        delattr(model_from, cls._state_key)
-
-    def modes_with_states(
-        self,
-    ) -> Iterator[Tuple[_ModeDescriptor, ModeloptBaseConfig, MetadataDict]]:
-        """Yield the mode together with the full config and metadata from the state."""
-        for m_str, m_state in self._state:
-            config = self.get_config_class(m_str, m_state["config"])
-            yield _ModeRegistryCls.get_from_any(m_str), config, m_state["metadata"]
-
-    @property
-    def last_mode(self) -> Optional[_ModeDescriptor]:
-        """Return the last mode applied to the model (last stored mode)."""
-        return _ModeRegistryCls.get_from_any(self._state[-1][0]) if self._state else None
-
-    @property
-    def _last_metadata(self) -> MetadataDict:
-        """Return the metadata of the last mode applied to the model (must exist!)."""
-        return self._state[-1][1]["metadata"]
-
-    @property
-    def _last_config(self) -> ModeloptBaseConfig:
-        """Return the config of the last mode applied to the model (must exist!)."""
-        return self.get_config_class(self._state[-1][0], self._state[-1][1]["config"])
-
-    @_last_config.setter
-    def _last_config(self, config: ModeloptBaseConfig) -> None:
-        """Set the config of the last mode applied to the model (must exist!)."""
-        self._state[-1][1]["config"] = config.model_dump()
-
-    @property
-    def _export_stack(self) -> Deque[Tuple[str, str]]:
-        """Infer the stack of export modes that still must be applied from existing modes.
-
-        Returns:
-            A deque of tuples of the form ``(mode_str, export_mode_str)`` representing the mode
-            which requires an export mode and the export mode itself.
-        """
-        stack = deque()
-        for m, _, _ in self.modes_with_states():
-            if m.export_mode:
-                stack.append((str(m), m.export_mode))
-            elif m.is_export_mode:
-                assert str(m) == stack.pop()[1], "Inconsistent export stack!"
-        return stack
-
-    @staticmethod
-    def get_config_class(mode: ModeType, config: ConfigDict) -> ModeloptBaseConfig:
-        """Standardize the provided config to the corresponding config class."""
-        # validate and standardize to the config class and return
-        return _ModeRegistryCls.get_from_any(mode).config_class(**config)
-
-    def check_mode(self, mode: ModeType) -> None:
-        """Check if the proposed mode is compatible with the current state."""
-        # standardize mode to descriptor
-        mode_d = _ModeRegistryCls.get_from_any(mode)
-
-        # check for export mode compatibility
-        export_stack = self._export_stack
-        if mode_d.is_export_mode:
-            assert (
-                export_stack and str(mode_d) == export_stack[-1][1]
-            ), f"Cannot add {mode_d} according to the current export stack: {export_stack}."
-
-        # sanity checks for next mode incompatibilities according to the current last mode
-        last_mode = self.last_mode
-        if last_mode:
-            assert last_mode.next_modes is None or str(mode_d) in last_mode.next_modes, (
-                f"Cannot add {mode_d} after {last_mode}! Next modes of {last_mode} are"
-                f" {last_mode.next_modes}."
-            )
-
-        # sanity checks for next mode incompatible with last mode in the stack. These
-        # incompatibilities still apply since we did not apply corresponding export mode yet.
-        if export_stack:
-            # last_m_stack := last mode on the stack that has a corresponding export mode
-            last_m_stack = _ModeRegistryCls.get_from_any(export_stack[-1][0])
-            assert last_m_stack.next_modes is None or str(mode_d) in last_m_stack.next_modes, (
-                f"Cannot add {mode_d} after {last_m_stack}! Next modes of {last_m_stack} are"
-                f" {last_m_stack.next_modes}."
-            )
-
-    def add_mode(self, mode: ModeType, config: ModeloptBaseConfig, metadata: MetadataDict) -> None:
-        """Add mode and update state in-place.
-
-        Note that self._state is a list (preserves insertion order of keys) and we can therefore
-        recall the order of modes!
-        """
-        # standardize mode to descriptor
-        mode_d = _ModeRegistryCls.get_from_any(mode)
-
-        # sanity checks for mode incompatibilities
-        self.check_mode(mode_d)
-
-        # store mode information
-        m_state: ModeState = {"config": config.model_dump(), "metadata": metadata}
-
-        self._state.append((str(mode_d), m_state))
-
-    def update_last_state_before_new_mode(self, model: nn.Module) -> None:
-        """Update the metadata and config of the last mode applied to the model."""
-        last_mode = self.last_mode
-        if last_mode is not None:
-            last_config = self._last_config
-            last_mode.update_for_new_mode(model, last_config, self._last_metadata)
-            self._last_config = last_config
-
-    def update_last_state_before_save(self, model: nn.Module) -> None:
-        """Update the metadata and config of the last mode applied to the model."""
-        last_mode = self.last_mode
-        if last_mode is not None:
-            last_config = self._last_config
-            last_mode.update_for_save(model, last_config, self._last_metadata)
-            self._last_config = last_config
-
-
-class ApplyModeError(RuntimeError):
-    """Error raised when applying a mode to a model fails."""
-
-
-class ModelLikeModule(nn.Module):
-    """Just a temp module type to store the initialization recipe for the actual model."""
-
-    def __init__(self, modellike: ModelLike) -> None:
-        super().__init__()
-        assert not isinstance(modellike, nn.Module), "modellike should not be a nn.Module!"
-        self.modellike = modellike
-
-    def init_modellike(self) -> nn.Module:
-        """Initialize the modellike to be an actual model."""
-        model = init_model_from_model_like(self.modellike)
-        ModeloptStateManager.transfer_state_dict(self, model)
-        return model
-
-
-def _check_init_modellike(model: nn.Module, mode: _ModeDescriptor) -> nn.Module:
-    """Utility to initialize a ModelLikeModule if needed according to the mode."""
-    if mode.require_model_like:
-        assert isinstance(model, ModelLikeModule), "Model must be a ModelLikeModule!"
-    elif isinstance(model, ModelLikeModule):
-        model = model.init_modellike()
-    return model
-
-
-def apply_mode(
-    model: ModelLike,
-    mode: ModeLike,
-    registry: Optional[_ModeRegistryCls] = None,
-    init_state: Optional[bool] = None,
-) -> nn.Module:
-    """Apply the provided modes the model, record the changes, and return the model.
-
-    Args:
-        model: A model-like object. Can be an nn.Module, a model class type, or a tuple.
-            Tuple must be of the form ``(model_cls,)`` or ``(model_cls, args)`` or
-            ``(model_cls, args, kwargs)``. Model will be initialized as
-            ``model_cls(*args, **kwargs)``.
-        mode: A mode, a list of modes or a list of tuples containing the mode and its config. The
-            mode may be specified as a string or as the actual
-            :mod:`_ModeDescriptor<modelopt.torch.opt.mode._ModeDescriptor>` class such as
-            :mod:`QuantizeModeDescriptor<modelopt.torch.opt.quantization.QuantizeModeDescriptor>` class.
-        registry: An optional mode registry from which to retrieve the mode. If not provided, all
-            registries will be searched.
-        init_state: Flag indicating whether we should initialize the state manager for the model. If
-            not provided, it will be inferred from the model. This flag can be used to enforce a
-            certain behavior. For example, for ``init_state=True`` the state manager will raise an
-            error if the model already contains state.
-
-    Returns:
-        The converted model after applying the desired modes.
-    """
-    # initialize ModelLikeModule if needed.
-    model = model if isinstance(model, nn.Module) else ModelLikeModule(model)
-
-    # vanilla case (just initialize+return)
-    if not mode:
-        return model.init_modellike() if isinstance(model, ModelLikeModule) else model
-
-    # check if the model is in a wrapper
-    model = unwrap_model(model, raise_error=True)
-
-    # standardize mode to ModeConfigList
-    mode_and_config = get_mode_config(mode)
-
-    # get or initialize the state manager for the model
-    manager = ModeloptStateManager(
-        model,
-        init_state=(
-            not ModeloptStateManager.is_converted(model) if init_state is None else init_state
-        ),
-    )
-
-    # get mode function based on registry argument
-    get_mode = registry.__getitem__ if registry else _ModeRegistryCls.get_from_any
-
-    # update metadata of currently last mode before adding new modes
-    manager.update_last_state_before_new_mode(model)
-
-    # check whether a ModelLike should be initialized
-    model = _check_init_modellike(model, get_mode(mode_and_config[0][0]))
-
-    # loop through modes and call convert entrypoint for each mode and record data in manager.
-    for m, config in mode_and_config:
-        manager.check_mode(m)
-        config = manager.get_config_class(m, config)
-        model, metadata = get_mode(m).convert(model, config)
-        manager.add_mode(m, config, metadata)
-
-    # If the existing mode is empty, create an model instance from ModelLikeModule.
-    if not manager.has_state and isinstance(model, ModelLikeModule):
-        model = model.init_modellike()
-
-    # it cannot be a ModelLikeModule anymore at the end
-    assert not isinstance(model, ModelLikeModule), "Model must be a regular Module now!"
-
-    # return model with state recorded
-    return model
-
-
-def get_mode(model: nn.Module) -> Optional[_ModeDescriptor]:
-    """Get mode of converted network.
-
-    model: A model that contains modelopt_state
-
-    The mode of the model is defined as the last mode activated during the convert process.
-    """
-    if ModeloptStateManager.is_converted(model):
-        return ModeloptStateManager(model).last_mode
-    return None
-
-
-def modelopt_state(model: nn.Module) -> Dict[str, Any]:
-    """Return the modelopt state dict describing the modifications to the model.
-
-    Note that the returned ``modelopt_state`` does not contain the model parameters such as weights and biases.
-    ``modelopt_state`` is useful for saving and loading various modelopt optimization states separately from the
-    model parameters. For example:
-
-    .. code-block::
-
-        import modelopt.torch.opt as mto
-
-        # Save the modelopt state and model weights separately
-        torch.save(mto.modelopt_state(model), "modelopt_state.pt") # Save the modelopt state
-        torch.save(model.state_dict(), "model_weights.pt") # Save the model weights
-
-    If you want to save the model weights and the modelopt state together, please use
-    :meth:`mto.save()<modelopt.torch.opt.conversion.save>`.
-
-    Args:
-        model: the modelopt-modified model.
-
-    Returns:
-        An modelopt state dictionary describing the modifications to the model.
-    """
-    # unwrap model
-    model = unwrap_model(model, warn=True)
-
-    # retrieve state manager
-    manager = ModeloptStateManager(
-        model=model if ModeloptStateManager.is_converted(model) else None
-    )
-
-    # update metadata of current mode as needed
-    manager.update_last_state_before_save(model)
-
-    # construct state dict and return it
-    objs = {
-        "modelopt_state_dict": (
-            manager.state_dict()
-        ),  # empty state_dict is okay (saving regular models)
-        "modelopt_version": __version__,
-    }
-    return objs
-
-
-def save(model: nn.Module, f: Union[str, os.PathLike, BinaryIO], **kwargs) -> None:
-    """Save a model's state dict together with the modelopt state dict to restore its architecture.
-
-    Args:
-        model: Any model.
-        f: Target file location.
-        **kwargs: additional args for ``torch.save()``.
-
-    .. note::
-
-        If model is a wrapper such as DistributedDataParallel, it will be unwrapped for saving.
-    """
-    # unwrap model
-    model = unwrap_model(model, warn=True)
-
-    # store ckpt
-    ckpt_dict = {
-        "modelopt_state": modelopt_state(model),
-        "model_state_dict": model.state_dict(),
-    }
-
-    # store object
-    torch.save(ckpt_dict, f, **kwargs)
-
-
-def restore_from_modelopt_state(model: ModelLike, modelopt_state: Dict[str, Any]) -> nn.Module:
-    """Restore the model architecture from the modelopt state dictionary based on the user-provided model.
-
-    This method does not restore the model parameters such as weights and biases.
-    Please load the weights and biases with the original checkpoint loading method after restoring
-    modelopt states with `restore_from_modelopt_state`. For example:
-
-    .. code-block:: python
-
-        import modelopt.torch.opt as mto
-
-        model = ...  # Create the model-like object
-
-        # Restore the previously saved modelopt state followed by model weights
-        mto.restore_from_modelopt_state(model, torch.load("modelopt_state.pt"))  # Restore modelopt state
-        model.load_state_dict(torch.load("model_weights.pt"), ...)  # Load the model weights
-
-    If you want to restore the model weights and the modelopt state together, please use
-    :meth:`mto.restore()<modelopt.torch.opt.conversion.restore>`.
-
-    Args:
-        model: A model-like object. Can be an nn.Module, a model class type, or a tuple.
-            Tuple must be of the form ``(model_cls,)`` or ``(model_cls, args)`` or
-            ``(model_cls, args, kwargs)``. Model will be initialized as
-            ``model_cls(*args, **kwargs)``.
-        modelopt_state: The modelopt state dict describing the modelopt modifications to the model. The
-            ``modelopt_state`` can be generated via
-            :meth:`mto.modelopt_state()<modelopt.torch.opt.conversion.modelopt_state>`.
-
-    Returns:
-        A modified model architecture based on the restored modifications with the unmodified
-        weights as stored in the provided ``model`` argument.
-
-    .. note::
-
-        Note that wrappers such as DistributedDataParallel are `not` supported during the restore
-        process. Please wrap the model after the restore process.
-    """
-    # initialize ModelLikeModule if needed.
-    model = model if isinstance(model, nn.Module) else ModelLikeModule(model)
-
-    # Alert if the first 2 version numbers do not match, e.g., 0.3.2 vs 0.4.0.
-    version = modelopt_state["modelopt_version"]
-    if tuple(version.split(".")[:2]) != tuple(__version__.split(".")[:2]):
-        warnings.warn(
-            f"The checkpoint is stored with version {version}, but current version is"
-            f" {__version__}. Compatibility of checkpoint with current version is not guaranteed!"
-        )
-
-    # initialize state manager and load state
-    manager = ModeloptStateManager(model=model, init_state=True)
-    manager.load_state_dict(modelopt_state["modelopt_state_dict"])
-
-    # apply restore entrypoints for each of the modes
-    for i, (m, config, metadata) in enumerate(manager.modes_with_states()):
-        if i == 0:
-            model = _check_init_modellike(model, m)
-        model = m.restore(model, config, metadata)
-
-    # If the existing mode is empty, create an model instance from ModelLikeModule.
-    if not manager.has_state and isinstance(model, ModelLikeModule):
-        model = model.init_modellike()
-
-    # it cannot be a ModelLikeModule anymore at the end
-    assert not isinstance(model, ModelLikeModule), "Model must be a regular Module now!"
-
-    return model
-
-
-def restore(model: ModelLike, f: Union[str, os.PathLike, BinaryIO], **kwargs) -> nn.Module:
-    """Load the checkpoint, restore the modelopt model modifications, and load the model's weights.
-
-    Args:
-        model: A model-like object. Can be an nn.Module, a model class type, or a tuple.
-            Tuple must be of the form ``(model_cls,)`` or ``(model_cls, args)`` or ``(model_cls, args, kwargs)``.
-            Model will be initialized as ``model_cls(*args, **kwargs)``.
-        f: Target file location generated by :meth:`mto.save()<modelopt.torch.opt.conversion.save>`.
-        **kwargs: additional args for ``torch.load()``.
-
-    Returns:
-        The model with original weights and stored architecture.
-
-    .. note::
-
-        Note that wrappers such as DistributedDataParallel are `not` supported during the restore
-        process. Please wrap the model after the restore process.
-    """
-    # initialize ModelLikeModule if needed.
-    model = model if isinstance(model, nn.Module) else ModelLikeModule(model)
-
-    # load checkpoint
-    kwargs.setdefault("map_location", "cpu")
-    objs = torch.load(f, **kwargs)
-
-    # restore model architecture
-    model_restored = restore_from_modelopt_state(model, objs["modelopt_state"])
-
-    # load weights from checkpoint
-    model_restored.load_state_dict(objs["model_state_dict"])
-
-    # it cannot be a ModelLikeModule anymore at the end
-    assert not isinstance(model_restored, ModelLikeModule), "Model must be a regular Module now!"
-
-    return model_restored
-
-
-# TODO: add a generic export function that will apply all remaining export modes.
-def export(model: nn.Module) -> nn.Module:
-    """Fully export the model to a regular model and finalize any model modifications."""
-    raise NotImplementedError
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Module to handle model converting and restoring for optimization methods.
+
+When applying a model optimization algorithm, we usually need to modify the model in each step
+(mode) of the algorithm. This module provides the state manager, which is a standardized interface
+(class) to record and store state information in the model.
+
+Op top of the state manager, this module provides utilities to save a history of these modifications
+("modelopt state dict") and restoring a unmodified model to the state indicated in the state dict.
+"""
+
+import copy
+import os
+import warnings
+from collections import deque
+from typing import Any, BinaryIO, Deque, Dict, Iterator, List, Optional, Tuple, Union
+
+import torch
+import torch.nn as nn
+
+from modelopt import __version__
+from modelopt.torch.utils import ModelLike, init_model_from_model_like, unwrap_model
+
+from .config import ConfigDict, ModeloptBaseConfig
+from .mode import (
+    MetadataDict,
+    ModeLike,
+    ModeState,
+    ModeType,
+    _ModeDescriptor,
+    _ModeRegistryCls,
+    get_mode_config,
+)
+
+__all__ = [
+    "ModeloptStateManager",
+    "apply_mode",
+    "modelopt_state",
+    "save",
+    "restore_from_modelopt_state",
+    "restore",
+]
+
+ModeloptStateList = List[Tuple[str, ModeState]]  # state data structure for multiple modes
+
+
+class ModeloptStateManager:
+    """A class to handle the modelopt state stored for each mode correspondig to a task/mode."""
+
+    _state_key = "_modelopt_state"
+
+    def __init__(self, model: Optional[nn.Module] = None, init_state: bool = False) -> None:
+        """Initialize state manager.
+
+        Args:
+            model: Module that has modelopt_state stored. If None, a fake module is created to store
+                any state that might be added with the manager.
+            init_state: Whether to initialize the modelopt state for the model if it does not exist.
+        """
+        # just assume fake module for easy implementation below
+        if not model:
+            model = nn.Module()
+            init_state = True  # always initialize fake module
+
+        # initialize modelopt state if desired. Note that by default we don't do that to avoid
+        # accidentally modifying a user-provided model.
+        if init_state:
+            assert not hasattr(model, self._state_key), "Model already has modelopt state!"
+            setattr(model, self._state_key, [])
+
+        # sanity check that root module has modelopt state now
+        assert self.is_converted(model, is_root=True), "Model must have modelopt state!"
+
+        # store reference to state
+        self._state: ModeloptStateList = getattr(model, self._state_key)
+
+    @property
+    def has_state(self) -> bool:
+        """Return whether the model has a non-trivial modelopt state."""
+        return bool(self._state)
+
+    @classmethod
+    def is_converted(cls, model: nn.Module, is_root: bool = False) -> bool:
+        """Check if model is converted.
+
+        Args:
+            model: A model to be checked for state/metadata from the convert process.
+            is_root: Additionally check whether the module with state is the root module.
+
+        Returns:
+            True if the model contains modelopt state indicating that it has been converted.
+
+        This method raises an assertion when multiple modelopt_states are detected or when is_root is
+        set to True but the module with state is not the root module.
+        """
+        # check for submodules with state
+        mods_with_state = [name for name, m in model.named_modules() if hasattr(m, cls._state_key)]
+        # check if there is multiple submodules with state
+        assert len(mods_with_state) <= 1, "Model has multiple modelopt states!"
+        is_converted = bool(mods_with_state)
+
+        # check if mod with state is root module if desired
+        if is_converted:
+            assert (
+                not is_root or mods_with_state[0] == ""
+            ), "Model has modelopt state but not the root!"
+
+        return is_converted
+
+    # TODO: consider renaming state_dict???
+    def state_dict(self) -> ModeloptStateList:
+        """Return the metadata of the model."""
+        return self._state
+
+    def load_state_dict(self, state_dict: ModeloptStateList) -> None:
+        """Load the provided ``state_dict`` to the modelopt_state."""
+        assert not self.has_state, "Cannot load state_dict if there is already one."
+
+        # make sure we operate on deepcopy
+        state_dict = copy.deepcopy(state_dict)
+        # add modes one-by-one
+        for m_str, m_state in state_dict:
+            # adds config and metadata with sanity checks
+            config = self.get_config_class(m_str, m_state["config"])
+            self.add_mode(m_str, config, m_state["metadata"])
+
+        # overwrite state manually afterwards to ensure exact consistency with provided state_dict
+        self._state.clear()
+        self._state.extend(state_dict)
+
+    @classmethod
+    def transfer_state_dict(cls, model_from: nn.Module, model_to: nn.Module) -> None:
+        """Transfer the state (same instance) from one model to another."""
+        manager_from = ModeloptStateManager(model_from, init_state=False)  # state must exist
+        manager_to = ModeloptStateManager(model_to, init_state=True)  # state must NOT exist
+
+        # transfer state_dict (this uses sanity checks + deepcopy)
+        manager_to.load_state_dict(manager_from.state_dict())
+
+        # manually set the state dict to be the exact same instance
+        setattr(model_to, cls._state_key, manager_from.state_dict())
+        manager_to = ModeloptStateManager(model_to, init_state=False)  # state must exist now
+
+        # remove state from model_from
+        delattr(model_from, cls._state_key)
+
+    def modes_with_states(
+        self,
+    ) -> Iterator[Tuple[_ModeDescriptor, ModeloptBaseConfig, MetadataDict]]:
+        """Yield the mode together with the full config and metadata from the state."""
+        for m_str, m_state in self._state:
+            config = self.get_config_class(m_str, m_state["config"])
+            yield _ModeRegistryCls.get_from_any(m_str), config, m_state["metadata"]
+
+    @property
+    def last_mode(self) -> Optional[_ModeDescriptor]:
+        """Return the last mode applied to the model (last stored mode)."""
+        return _ModeRegistryCls.get_from_any(self._state[-1][0]) if self._state else None
+
+    @property
+    def _last_metadata(self) -> MetadataDict:
+        """Return the metadata of the last mode applied to the model (must exist!)."""
+        return self._state[-1][1]["metadata"]
+
+    @property
+    def _last_config(self) -> ModeloptBaseConfig:
+        """Return the config of the last mode applied to the model (must exist!)."""
+        return self.get_config_class(self._state[-1][0], self._state[-1][1]["config"])
+
+    @_last_config.setter
+    def _last_config(self, config: ModeloptBaseConfig) -> None:
+        """Set the config of the last mode applied to the model (must exist!)."""
+        self._state[-1][1]["config"] = config.model_dump()
+
+    @property
+    def _export_stack(self) -> Deque[Tuple[str, str]]:
+        """Infer the stack of export modes that still must be applied from existing modes.
+
+        Returns:
+            A deque of tuples of the form ``(mode_str, export_mode_str)`` representing the mode
+            which requires an export mode and the export mode itself.
+        """
+        stack = deque()
+        for m, _, _ in self.modes_with_states():
+            if m.export_mode:
+                stack.append((str(m), m.export_mode))
+            elif m.is_export_mode:
+                assert str(m) == stack.pop()[1], "Inconsistent export stack!"
+        return stack
+
+    @staticmethod
+    def get_config_class(mode: ModeType, config: ConfigDict) -> ModeloptBaseConfig:
+        """Standardize the provided config to the corresponding config class."""
+        # validate and standardize to the config class and return
+        return _ModeRegistryCls.get_from_any(mode).config_class(**config)
+
+    def check_mode(self, mode: ModeType) -> None:
+        """Check if the proposed mode is compatible with the current state."""
+        # standardize mode to descriptor
+        mode_d = _ModeRegistryCls.get_from_any(mode)
+
+        # check for export mode compatibility
+        export_stack = self._export_stack
+        if mode_d.is_export_mode:
+            assert (
+                export_stack and str(mode_d) == export_stack[-1][1]
+            ), f"Cannot add {mode_d} according to the current export stack: {export_stack}."
+
+        # sanity checks for next mode incompatibilities according to the current last mode
+        last_mode = self.last_mode
+        if last_mode:
+            assert last_mode.next_modes is None or str(mode_d) in last_mode.next_modes, (
+                f"Cannot add {mode_d} after {last_mode}! Next modes of {last_mode} are"
+                f" {last_mode.next_modes}."
+            )
+
+        # sanity checks for next mode incompatible with last mode in the stack. These
+        # incompatibilities still apply since we did not apply corresponding export mode yet.
+        if export_stack:
+            # last_m_stack := last mode on the stack that has a corresponding export mode
+            last_m_stack = _ModeRegistryCls.get_from_any(export_stack[-1][0])
+            assert last_m_stack.next_modes is None or str(mode_d) in last_m_stack.next_modes, (
+                f"Cannot add {mode_d} after {last_m_stack}! Next modes of {last_m_stack} are"
+                f" {last_m_stack.next_modes}."
+            )
+
+    def add_mode(self, mode: ModeType, config: ModeloptBaseConfig, metadata: MetadataDict) -> None:
+        """Add mode and update state in-place.
+
+        Note that self._state is a list (preserves insertion order of keys) and we can therefore
+        recall the order of modes!
+        """
+        # standardize mode to descriptor
+        mode_d = _ModeRegistryCls.get_from_any(mode)
+
+        # sanity checks for mode incompatibilities
+        self.check_mode(mode_d)
+
+        # store mode information
+        m_state: ModeState = {"config": config.model_dump(), "metadata": metadata}
+
+        self._state.append((str(mode_d), m_state))
+
+    def update_last_state_before_new_mode(self, model: nn.Module) -> None:
+        """Update the metadata and config of the last mode applied to the model."""
+        last_mode = self.last_mode
+        if last_mode is not None:
+            last_config = self._last_config
+            last_mode.update_for_new_mode(model, last_config, self._last_metadata)
+            self._last_config = last_config
+
+    def update_last_state_before_save(self, model: nn.Module) -> None:
+        """Update the metadata and config of the last mode applied to the model."""
+        last_mode = self.last_mode
+        if last_mode is not None:
+            last_config = self._last_config
+            last_mode.update_for_save(model, last_config, self._last_metadata)
+            self._last_config = last_config
+
+
+class ApplyModeError(RuntimeError):
+    """Error raised when applying a mode to a model fails."""
+
+
+class ModelLikeModule(nn.Module):
+    """Just a temp module type to store the initialization recipe for the actual model."""
+
+    def __init__(self, modellike: ModelLike) -> None:
+        super().__init__()
+        assert not isinstance(modellike, nn.Module), "modellike should not be a nn.Module!"
+        self.modellike = modellike
+
+    def init_modellike(self) -> nn.Module:
+        """Initialize the modellike to be an actual model."""
+        model = init_model_from_model_like(self.modellike)
+        ModeloptStateManager.transfer_state_dict(self, model)
+        return model
+
+
+def _check_init_modellike(model: nn.Module, mode: _ModeDescriptor) -> nn.Module:
+    """Utility to initialize a ModelLikeModule if needed according to the mode."""
+    if mode.require_model_like:
+        assert isinstance(model, ModelLikeModule), "Model must be a ModelLikeModule!"
+    elif isinstance(model, ModelLikeModule):
+        model = model.init_modellike()
+    return model
+
+
+def apply_mode(
+    model: ModelLike,
+    mode: ModeLike,
+    registry: Optional[_ModeRegistryCls] = None,
+    init_state: Optional[bool] = None,
+) -> nn.Module:
+    """Apply the provided modes the model, record the changes, and return the model.
+
+    Args:
+        model: A model-like object. Can be an nn.Module, a model class type, or a tuple.
+            Tuple must be of the form ``(model_cls,)`` or ``(model_cls, args)`` or
+            ``(model_cls, args, kwargs)``. Model will be initialized as
+            ``model_cls(*args, **kwargs)``.
+        mode: A mode, a list of modes or a list of tuples containing the mode and its config. The
+            mode may be specified as a string or as the actual
+            :mod:`_ModeDescriptor<modelopt.torch.opt.mode._ModeDescriptor>` class such as
+            :mod:`QuantizeModeDescriptor<modelopt.torch.opt.quantization.QuantizeModeDescriptor>` class.
+        registry: An optional mode registry from which to retrieve the mode. If not provided, all
+            registries will be searched.
+        init_state: Flag indicating whether we should initialize the state manager for the model. If
+            not provided, it will be inferred from the model. This flag can be used to enforce a
+            certain behavior. For example, for ``init_state=True`` the state manager will raise an
+            error if the model already contains state.
+
+    Returns:
+        The converted model after applying the desired modes.
+    """
+    # initialize ModelLikeModule if needed.
+    model = model if isinstance(model, nn.Module) else ModelLikeModule(model)
+
+    # vanilla case (just initialize+return)
+    if not mode:
+        return model.init_modellike() if isinstance(model, ModelLikeModule) else model
+
+    # check if the model is in a wrapper
+    model = unwrap_model(model, raise_error=True)
+
+    # standardize mode to ModeConfigList
+    mode_and_config = get_mode_config(mode)
+
+    # get or initialize the state manager for the model
+    manager = ModeloptStateManager(
+        model,
+        init_state=(
+            not ModeloptStateManager.is_converted(model) if init_state is None else init_state
+        ),
+    )
+
+    # get mode function based on registry argument
+    get_mode = registry.__getitem__ if registry else _ModeRegistryCls.get_from_any
+
+    # update metadata of currently last mode before adding new modes
+    manager.update_last_state_before_new_mode(model)
+
+    # check whether a ModelLike should be initialized
+    model = _check_init_modellike(model, get_mode(mode_and_config[0][0]))
+
+    # loop through modes and call convert entrypoint for each mode and record data in manager.
+    for m, config in mode_and_config:
+        manager.check_mode(m)
+        config = manager.get_config_class(m, config)
+        model, metadata = get_mode(m).convert(model, config)
+        manager.add_mode(m, config, metadata)
+
+    # If the existing mode is empty, create an model instance from ModelLikeModule.
+    if not manager.has_state and isinstance(model, ModelLikeModule):
+        model = model.init_modellike()
+
+    # it cannot be a ModelLikeModule anymore at the end
+    assert not isinstance(model, ModelLikeModule), "Model must be a regular Module now!"
+
+    # return model with state recorded
+    return model
+
+
+def get_mode(model: nn.Module) -> Optional[_ModeDescriptor]:
+    """Get mode of converted network.
+
+    model: A model that contains modelopt_state
+
+    The mode of the model is defined as the last mode activated during the convert process.
+    """
+    if ModeloptStateManager.is_converted(model):
+        return ModeloptStateManager(model).last_mode
+    return None
+
+
+def modelopt_state(model: nn.Module) -> Dict[str, Any]:
+    """Return the modelopt state dict describing the modifications to the model.
+
+    Note that the returned ``modelopt_state`` does not contain the model parameters such as weights and biases.
+    ``modelopt_state`` is useful for saving and loading various modelopt optimization states separately from the
+    model parameters. For example:
+
+    .. code-block::
+
+        import modelopt.torch.opt as mto
+
+        # Save the modelopt state and model weights separately
+        torch.save(mto.modelopt_state(model), "modelopt_state.pt") # Save the modelopt state
+        torch.save(model.state_dict(), "model_weights.pt") # Save the model weights
+
+    If you want to save the model weights and the modelopt state together, please use
+    :meth:`mto.save()<modelopt.torch.opt.conversion.save>`.
+
+    Args:
+        model: the modelopt-modified model.
+
+    Returns:
+        An modelopt state dictionary describing the modifications to the model.
+    """
+    # unwrap model
+    model = unwrap_model(model, warn=True)
+
+    # retrieve state manager
+    manager = ModeloptStateManager(
+        model=model if ModeloptStateManager.is_converted(model) else None
+    )
+
+    # update metadata of current mode as needed
+    manager.update_last_state_before_save(model)
+
+    # construct state dict and return it
+    objs = {
+        "modelopt_state_dict": (
+            manager.state_dict()
+        ),  # empty state_dict is okay (saving regular models)
+        "modelopt_version": __version__,
+    }
+    return objs
+
+
+def save(model: nn.Module, f: Union[str, os.PathLike, BinaryIO], **kwargs) -> None:
+    """Save a model's state dict together with the modelopt state dict to restore its architecture.
+
+    Args:
+        model: Any model.
+        f: Target file location.
+        **kwargs: additional args for ``torch.save()``.
+
+    .. note::
+
+        If model is a wrapper such as DistributedDataParallel, it will be unwrapped for saving.
+    """
+    # unwrap model
+    model = unwrap_model(model, warn=True)
+
+    # store ckpt
+    ckpt_dict = {
+        "modelopt_state": modelopt_state(model),
+        "model_state_dict": model.state_dict(),
+    }
+
+    # store object
+    torch.save(ckpt_dict, f, **kwargs)
+
+
+def restore_from_modelopt_state(model: ModelLike, modelopt_state: Dict[str, Any]) -> nn.Module:
+    """Restore the model architecture from the modelopt state dictionary based on the user-provided model.
+
+    This method does not restore the model parameters such as weights and biases.
+    Please load the weights and biases with the original checkpoint loading method after restoring
+    modelopt states with `restore_from_modelopt_state`. For example:
+
+    .. code-block:: python
+
+        import modelopt.torch.opt as mto
+
+        model = ...  # Create the model-like object
+
+        # Restore the previously saved modelopt state followed by model weights
+        mto.restore_from_modelopt_state(model, torch.load("modelopt_state.pt"))  # Restore modelopt state
+        model.load_state_dict(torch.load("model_weights.pt"), ...)  # Load the model weights
+
+    If you want to restore the model weights and the modelopt state together, please use
+    :meth:`mto.restore()<modelopt.torch.opt.conversion.restore>`.
+
+    Args:
+        model: A model-like object. Can be an nn.Module, a model class type, or a tuple.
+            Tuple must be of the form ``(model_cls,)`` or ``(model_cls, args)`` or
+            ``(model_cls, args, kwargs)``. Model will be initialized as
+            ``model_cls(*args, **kwargs)``.
+        modelopt_state: The modelopt state dict describing the modelopt modifications to the model. The
+            ``modelopt_state`` can be generated via
+            :meth:`mto.modelopt_state()<modelopt.torch.opt.conversion.modelopt_state>`.
+
+    Returns:
+        A modified model architecture based on the restored modifications with the unmodified
+        weights as stored in the provided ``model`` argument.
+
+    .. note::
+
+        Note that wrappers such as DistributedDataParallel are `not` supported during the restore
+        process. Please wrap the model after the restore process.
+    """
+    # initialize ModelLikeModule if needed.
+    model = model if isinstance(model, nn.Module) else ModelLikeModule(model)
+
+    # Alert if the first 2 version numbers do not match, e.g., 0.3.2 vs 0.4.0.
+    version = modelopt_state["modelopt_version"]
+    if tuple(version.split(".")[:2]) != tuple(__version__.split(".")[:2]):
+        warnings.warn(
+            f"The checkpoint is stored with version {version}, but current version is"
+            f" {__version__}. Compatibility of checkpoint with current version is not guaranteed!"
+        )
+
+    # initialize state manager and load state
+    manager = ModeloptStateManager(model=model, init_state=True)
+    manager.load_state_dict(modelopt_state["modelopt_state_dict"])
+
+    # apply restore entrypoints for each of the modes
+    for i, (m, config, metadata) in enumerate(manager.modes_with_states()):
+        if i == 0:
+            model = _check_init_modellike(model, m)
+        model = m.restore(model, config, metadata)
+
+    # If the existing mode is empty, create an model instance from ModelLikeModule.
+    if not manager.has_state and isinstance(model, ModelLikeModule):
+        model = model.init_modellike()
+
+    # it cannot be a ModelLikeModule anymore at the end
+    assert not isinstance(model, ModelLikeModule), "Model must be a regular Module now!"
+
+    return model
+
+
+def restore(model: ModelLike, f: Union[str, os.PathLike, BinaryIO], **kwargs) -> nn.Module:
+    """Load the checkpoint, restore the modelopt model modifications, and load the model's weights.
+
+    Args:
+        model: A model-like object. Can be an nn.Module, a model class type, or a tuple.
+            Tuple must be of the form ``(model_cls,)`` or ``(model_cls, args)`` or ``(model_cls, args, kwargs)``.
+            Model will be initialized as ``model_cls(*args, **kwargs)``.
+        f: Target file location generated by :meth:`mto.save()<modelopt.torch.opt.conversion.save>`.
+        **kwargs: additional args for ``torch.load()``.
+
+    Returns:
+        The model with original weights and stored architecture.
+
+    .. note::
+
+        Note that wrappers such as DistributedDataParallel are `not` supported during the restore
+        process. Please wrap the model after the restore process.
+    """
+    # initialize ModelLikeModule if needed.
+    model = model if isinstance(model, nn.Module) else ModelLikeModule(model)
+
+    # load checkpoint
+    kwargs.setdefault("map_location", "cpu")
+    objs = torch.load(f, **kwargs)
+
+    # restore model architecture
+    model_restored = restore_from_modelopt_state(model, objs["modelopt_state"])
+
+    # load weights from checkpoint
+    model_restored.load_state_dict(objs["model_state_dict"])
+
+    # it cannot be a ModelLikeModule anymore at the end
+    assert not isinstance(model_restored, ModelLikeModule), "Model must be a regular Module now!"
+
+    return model_restored
+
+
+# TODO: add a generic export function that will apply all remaining export modes.
+def export(model: nn.Module) -> nn.Module:
+    """Fully export the model to a regular model and finalize any model modifications."""
+    raise NotImplementedError
```

## modelopt/torch/opt/dynamic.py

 * *Ordering differences only*

```diff
@@ -1,1266 +1,1266 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Basic dynamic module class and hparam class."""
-
-# cython: annotation_typing = False
-
-import inspect
-from contextlib import contextmanager
-from itertools import chain
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Iterable,
-    Iterator,
-    List,
-    Optional,
-    Set,
-    Tuple,
-    Type,
-    TypeVar,
-    Union,
-)
-
-import torch
-import torch.nn as nn
-from pydantic import create_model
-from torch.fx.proxy import Proxy
-from torch.nn.parameter import Parameter
-
-from modelopt.torch.utils import is_channels_last, unwrap_model
-
-from .config import ModeloptBaseRule, RulesDict
-from .hparam import Hparam
-
-__all__ = ["DynamicModule", "_DMRegistryCls", "DynamicSpace"]
-
-_pytorch_managed = type("_pytorch_managed", (), {})  # pylint: disable=invalid-name
-_da_val_default = type("_da_val_default", (), {})  # pylint: disable=invalid-name
-
-
-DynamicAttributeCallback = Callable[["DynamicModule", Any], Any]
-SetAttrHook = Callable[["DynamicModule", str, Any], None]
-DelAttrHook = Callable[["DynamicModule", str], None]
-
-
-class _FoldedCallback:
-    """A callback functor that enables folding additional callbacks into an existing callback."""
-
-    def __init__(self, *callbacks: DynamicAttributeCallback):
-        self._callbacks: List[DynamicAttributeCallback] = []
-        self.extend(callbacks)
-
-    @property
-    def callback(self) -> DynamicAttributeCallback:
-        """Return the main callback."""
-        return self._callbacks[0]
-
-    @callback.setter
-    def callback(self, cb: DynamicAttributeCallback):
-        """Set the main callback."""
-        self._callbacks[0] = cb
-
-    def __iter__(self) -> Iterator[DynamicAttributeCallback]:
-        """Iterate over all callbacks."""
-        yield from self._callbacks
-
-    def __call__(self, self_module: "DynamicModule", val: Any) -> Any:
-        """Call the callback and all other callbacks."""
-        if isinstance(val, Proxy):
-            return val
-        for cb in self:
-            val = cb(self_module, val)
-        return val
-
-    def __len__(self) -> int:
-        """Return the number of callbacks."""
-        return len(self._callbacks)
-
-    def extend(self, cbs_other: Iterable[DynamicAttributeCallback]):
-        """Extend the list of other callbacks."""
-        self._callbacks.extend(cbs_other)
-
-
-class _DMAttributeManager:
-    """A class to manage the special attributes for dynamic modules in a hierarchical fashion.
-
-    Abbreviations:
-        - og_cls: original class
-        - hp: hparam
-        - da: dynamic attribute
-        - cb: callback
-        - attr: generic temporary attributes
-
-    This class handles all types of special attributes for dynamic modules. It is used to store
-    them, manage them, and provide access to them. Moreover, this class stores the hierarchy of
-    dynamic module conversions that were applied to the module and can associate the special
-    attributes with the correct level of inheritance.
-
-    Specifically, we handle the following types of special attributes:
-
-        1. **Original classes**:
-           We maintain a stack of original classes as we stack/convert dynamic modules. As we stack
-           dynamic modules on top or remove/export them, we maintain the stack.
-
-        2. **Hparams**:
-           We store hparams in a list of dictionaries. This way we can associate each hparam with
-           the corresponding level of hierarchy/inheritance. For hparams we do NOT allow adding or
-           overwriting hparams that exist in a different level of the stack, i.e., hparams must be
-           _unique_ across all levels of inheritance. Note that you can still access hparams from
-           any level of inheritance.
-
-        3. **Dynamic attributes**:
-           We store dynamic attributes in a dictionary. Additionally, a dynamic attribute can be
-           associated with a callback in an _arbitrary_ number of levels in the hierarchy. The
-           callbacks are stored in a list of dictionaries. This way we can associate each callback
-           with the corresponding level of hierarchy/inheritance and still allow for multiple
-           callbacks for the same dynamic attribute.
-
-        4. **Generic attributes**:
-           These are generic attributes that are not hparams or dynamic attributes. They are kept
-           track of so that we can remove them again upon the final export. They must be unique
-           and cannot correspond to an hparam or dynamic attribute.
-    """
-
-    def __init__(self):
-        self._og_cls_all: List[Type[nn.Module]] = []
-        self._hp_all: List[Dict[str, Hparam]] = []
-        self._da: Dict[str, Any] = {}
-        self._da_cb_all: List[Dict[str, _FoldedCallback]] = []
-        self._attr: Dict[str, Tuple[SetAttrHook, DelAttrHook]] = {}
-
-    def __bool__(self) -> bool:
-        all_data = [self._og_cls_all, self._da, self._da_cb_all, self._hp_all, self._attr]
-        has_state = [bool(x) for x in all_data]
-        if not has_state[0]:
-            # if we have no original class, we should have no other state either!
-            assert not any(has_state), "Inconsistent state for DM attributes!"
-        return has_state[0]
-
-    def _get_lookup(self, name: str, lookup_all: List[Dict[str, Any]]) -> Dict[str, Any]:
-        """Return the lookup for the given name."""
-        for lookup in lookup_all:
-            if name in lookup:
-                return lookup
-        raise KeyError(f"{name} not found!")
-
-    @property
-    def og_cls(self) -> Type[nn.Module]:
-        """Return the original class of the dynamic module."""
-        return self._og_cls_all[-1]
-
-    @property
-    def level(self) -> int:
-        """Return the current level of stacked inheritance."""
-        return len(self._og_cls_all)
-
-    def append_level(self, original_cls: Type[nn.Module]):
-        """Initialize a new level of special atttributes."""
-        self._og_cls_all.append(original_cls)
-        self._hp_all.append({})
-        self._da_cb_all.append({})
-
-    def pop_level(self) -> Type[nn.Module]:
-        """Remove the last level of special attributes and return the original class."""
-        # sanity checks on hparams
-        hp_lookup = self._hp_all[-1]
-        assert not hp_lookup, "Some hparams were not removed properly!"
-
-        # sanity checks on dynamic attributes and corresponding callbacks
-        cb_lookup = self._da_cb_all[-1]
-        assert not cb_lookup, "Some dynamic attribute callbacks were not removed properly!"
-
-        da_keys_remaining = set(chain(*self._da_cb_all[:-1]))
-        assert da_keys_remaining == self.da_keys(), "Dynamic attributes were not removed properly!"
-
-        # sanity checks on temporary attributes on last level
-        if self.level <= 1:
-            assert not self._attr, "Some attributes were not removed properly!"
-
-        # after clean-up, we can pop and return the original class
-        self._hp_all.pop()
-        self._da_cb_all.pop()
-        return self._og_cls_all.pop()
-
-    def hp_keys(self, all: bool = True) -> Set[str]:
-        """Return the keys of current or all hparams."""
-        idx_start = 0 if all else -1
-        return set(chain(*self._hp_all[idx_start:]))
-
-    def get_hp(self, name: str) -> Hparam:
-        """Return the hparam with the given name."""
-        hp_lookup = self._get_lookup(name, self._hp_all)
-        return hp_lookup[name]
-
-    def named_hps(self, all: bool = True) -> Iterator[Tuple[str, Hparam]]:
-        """Return the name and hparam for all or current hparams."""
-        idx_start = 0 if all else -1
-        for hp_lookup in self._hp_all[idx_start:]:
-            for name, hp in hp_lookup.items():
-                yield name, hp
-
-    def set_hp(self, name: str, hparam: Hparam):
-        """Store hparam by the provided name in the right location."""
-        # we do not allow the same hparam in different levels of inheritance
-        if name in self.hp_keys():
-            assert name in self.hp_keys(all=False), "Hparam already exists in the base cls!"
-        self._hp_all[-1][name] = hparam
-
-    def pop_hp(self, name: str) -> Any:
-        """Pop hparam by the provided name."""
-        hp_lookup = self._get_lookup(name, self._hp_all)
-        return hp_lookup.pop(name)
-
-    def da_keys(self) -> Set[str]:
-        """Return the keys of all dynamic attributes."""
-        return set(self._da)
-
-    def set_da(
-        self,
-        name: str,
-        val: Any = _da_val_default,
-        cb: Optional[DynamicAttributeCallback] = None,
-    ):
-        """Store a dynamic attribute together with its callback method."""
-        # sanity checks
-        if name in self.da_keys():
-            val = self.get_da_value(name) if val is _da_val_default else val
-        else:
-            assert val is not _da_val_default, "Value must be provided for new dynamic attribute!"
-            assert cb is not None, "Callback must be provided for new dynamic attribute!"
-
-        # store value
-        self._da[name] = val
-
-        # store wrapped callback
-        # NOTE: we only allow updating the original callback of this level of inheritance. Updating
-        # folded callbacks is not allowed!
-        if cb is not None:
-            if name in self._da_cb_all[-1]:
-                self._da_cb_all[-1][name].callback = cb
-            else:
-                self._da_cb_all[-1][name] = _FoldedCallback(cb)
-
-    def pop_da(self, name: str) -> Any:
-        """Pop dynamic attribute by the provided name."""
-        val = self._da.pop(name)
-        for cb_lookup in self._da_cb_all:
-            cb_lookup.pop(name, None)
-        return val
-
-    def get_da_value(self, name: str) -> Any:
-        """Return the raw value of the dynamic attribute."""
-        return self._da[name]
-
-    def get_da_cb(self, name: str) -> _FoldedCallback:
-        """Return the full callback for the dynamic attribute."""
-        cbs_all = list(chain.from_iterable(cb_d[name] for cb_d in self._da_cb_all if name in cb_d))
-        return _FoldedCallback(*cbs_all)
-
-    @contextmanager
-    def retain_cbs(self):
-        """Context manager to retain all callback information for dynamic attributes.
-
-        Any changes that are made to the callbacks within this context manager are not retained upon
-        exiting the context manager. However, changes to the value of dynamic attributes as well
-        as well deletions and additions of dynamic attributes are retained.
-        """
-        # retain original callbacks while exposing a list of empty dictionaries
-        da_cb_all = self._da_cb_all
-        da_keys_before = self.da_keys()
-        self._da_cb_all = [{} for _ in da_cb_all]
-        try:
-            yield None
-        finally:
-            # 1. We will restore callbacks to the original state for da's that still exist
-            # 2. We will maintain callbacks for newly added da's
-            # 3. We will throw away callbacks for da's that were removed
-            da_keys_removed = da_keys_before - self.da_keys()
-            da_keys_new = self.da_keys() - da_keys_before
-            da_keys_same = da_keys_before & self.da_keys()
-
-            da_cb_all_final = []
-            for cb_lookup_before, cb_lookup_now in zip(da_cb_all, self._da_cb_all):
-                cb_lookup_new = {k: v for k, v in cb_lookup_now.items() if k in da_keys_new}
-                cb_lookup_same = {k: v for k, v in cb_lookup_before.items() if k in da_keys_same}
-                cb_lookup_final = {**cb_lookup_new, **cb_lookup_same}
-                assert not (cb_lookup_final.keys() & da_keys_removed)  # final sanity check
-                da_cb_all_final.append({**cb_lookup_same, **cb_lookup_new})
-            self._da_cb_all = da_cb_all_final
-
-    def fold_cbs(self) -> Set[str]:
-        """Fold all callbacks that appear in lower levels into the corresponding lower level.
-
-        After this call, only dynamic attributes that appear solely in the top-level remain in the
-        top-level.
-
-        Returns:
-            The set of dynamic attributes that remain in the top-level.
-        """
-        cb_lookup = self._da_cb_all[-1]
-
-        # check if we can fold the callbacks into a lower level
-        for cb_lookup_other in reversed(self._da_cb_all[:-1]):
-            for k_other, cb_other in cb_lookup_other.items():
-                if k_other in cb_lookup:
-                    cb_other.extend(cb_lookup.pop(k_other))
-
-        # sanity check: at this point there should be no overlap between top-level and other levels
-        assert not any(k in cb_lookup for k in chain(*self._da_cb_all[:-1])), "Callback overlap!"
-
-        return set(cb_lookup)
-
-    def attr_keys(self) -> Set[str]:
-        """Return the keys of all or current other attributes."""
-        return set(self._attr)
-
-    def get_attr_set_hook(self, name: str) -> SetAttrHook:
-        """Return the setter for the given name."""
-        return self._attr[name][0]
-
-    def set_attr(self, name: str, set_hook: Optional[SetAttrHook], del_hook: Optional[DelAttrHook]):
-        """Store the name of an attribute in the other list."""
-        assert name not in self.attr_keys(), "Attribute already exists!"
-
-        set_hook = set_hook or (lambda m, n, v: None)
-        del_hook = del_hook or (lambda m, n: None)
-        self._attr[name] = (set_hook, del_hook)
-
-    def pop_attr(self, name: str) -> DelAttrHook:
-        """Pop attr by the provided name and return hook for delattr."""
-        return self._attr.pop(name)[1]
-
-
-class DynamicModule(nn.Module):
-    """Base class for dynamic modules.
-
-    Dynamic modules are usually extended from ``torch.nn.Module``'s. They
-    dynamically support a family of `torch.nn.Module`'s with different architectural
-    parameters, such as input/output channel numbers.
-
-    Dynamic modules can also be used to construct the basic searchable unit in a search space with
-    the option to select and sample a candidate unit.
-
-    Candidate units are usually described using ``Hparam`` objects and dynamic attributes. Each
-    hparam describes a basic searchable unit (e.g. number of output channels in ``DynamicConv2d``).
-    Dynamic attributes are callbacks to dynamically construct the attribute depending on the current
-    value of the hparam(s), e.g., the ``weight`` tensor in ``DynamicConv2d`` that depends on the
-    hparam ``out_channels``.
-
-    In addition, dynamic modules also support registering general attributes that are removed upon
-    export the module. This is useful for storing temporary attributes that are not hparams or
-    existing attributes that are converted to dynamic attributes.
-
-    For a ``DynamicModule`` class that contains other dynamic modules, the class implementation
-    should ensure only to expose ``hparams`` in the outermost class and handle other ``hparams``
-    internally including ``hparams`` of child modules that are exposed on their own usually
-    (e.g. block module implementations containing DynamicLinear).
-    """
-
-    # this is needed to store the special attributes for dynamic modules
-    _dm_attribute_manager: _DMAttributeManager
-
-    def __init__(self, *args, **kwargs):
-        """Initializing a dynamic module is not allowed!"""
-        raise RuntimeError("DynamicModule cannot be initialized directly; use convert instead!")
-
-    def _get_dm_attribute_manager(self, use_default: bool = False) -> _DMAttributeManager:
-        """Return the attribute manager or a default one if not available."""
-        if "_dm_attribute_manager" not in self.__dict__:
-            if use_default:
-                return _DMAttributeManager()
-            raise AttributeError("DynamicModule.convert() must be called before using the module!")
-        return self._dm_attribute_manager
-
-    def _register_hparam(self, name: str, hparam: Hparam):
-        """Register hparam by the provided name and do some sanity checks during registering.
-
-        Args:
-            name: The name of the hparam. Note that an hparam can be a new attribute or an existing
-                attribute that is not already registered as temporary attribute or dynamic
-                attribute. In addition, an hparam cannot be registered for tensor and therefore
-                neither for parameters nor buffers. Finally, an hparam cannot be registered when an
-                hparam under the same exists in one of the parent classes. However, one can re-
-                register an hparam of itself.
-            hparam: The hparam object to be registered.
-
-        An hparam is useful to configure the dynamic module on the fly. The collection of hparams
-        define the configurable space of the dynamic module.
-        """
-        # retrieve manager
-        manager = self._get_dm_attribute_manager()
-
-        # various sanity checks
-        if not isinstance(hparam, Hparam):
-            raise TypeError(f"Cannot assign {type(hparam)} to {name}. Hparam required!")
-        elif name in manager.attr_keys():
-            raise KeyError(f"Cannot use the reserved name {name} to assign an hparam!")
-        elif name in manager.da_keys():
-            raise RuntimeError(f"Cannot register attribute {name} that is dynamic as hparam.")
-        elif name in manager.hp_keys(all=True) and name not in manager.hp_keys(all=False):
-            raise RuntimeError(f"Cannot overwrite hparam {name} that is an hparam in the base cls.")
-        elif "." in name:
-            raise KeyError('hparam name can\'t contain "."')
-        elif name == "":
-            raise KeyError('hparam name can\'t be empty string ""')
-        elif name in ["_parameters", "_buffers", "_modules"]:
-            raise KeyError(f"Cannot use reserved name {name} to assign hparam.")
-        # delete name if it already exists
-        elif hasattr(self, name):
-            if isinstance(getattr(self, name), (torch.Tensor, torch.nn.Module)):
-                raise RuntimeError(f"Cannot register tensor/module attribute {name} as hparam.")
-            delattr(self, name)
-
-        # store new hparam
-        manager.set_hp(name, hparam)
-
-    def _register_dynamic_attribute(self, name: str, callback: DynamicAttributeCallback):
-        """Register a dynamic attribute together with its callback method.
-
-        Args:
-            name: The name of the dynamic attribute.
-            callback: The callback method that is executed when the dynamic attribute is accessed.
-                Generally, this callback can access the whole module (``self``) as well as expect
-                to have access to the original attribute without any modifications.
-
-        Note that a dynamic attribute should be an existing attribute that is dynamically affected
-        when the value of one or more hparams changes. It is not allowed to register a dynamic
-        attribute for a non-existing attribute. However, specifically tensors, parameters, and
-        buffers are supported. Moreover, a dynamic attribute can also be registered for an existing
-        dynamic attribute in the parent class. In this case, the ``callback`` will be executed
-        *after* the callback of the parent class.
-        """
-        # retrieve manager
-        manager = self._get_dm_attribute_manager()
-
-        # various sanity checks
-        if not hasattr(self, name):
-            raise AttributeError(f"{name} is not a valid attribute.")
-        elif name in manager.attr_keys():
-            raise KeyError(f"Cannot use the reserved name {name} to assign a dynamic attribute!")
-        elif name in manager.hp_keys():
-            raise RuntimeError(f"Cannot register attribute {name} that is an hparam as dynamic.")
-        elif "." in name:
-            raise KeyError('dynamic attribute name can\'t contain "."')
-        elif name == "":
-            raise KeyError('dynamic attribute name can\'t be empty string ""')
-
-        # however we have to handle regular attributes and params/buffers separately. Specifically,
-        # params/buffers won't appear in self.__dict__ since they are already managed by pytorch
-        # internally.
-        if name in self.__dict__["_parameters"] or name in self.__dict__["_buffers"]:
-            # here the attribute is already managed, so simply storing a fake reference.
-            value = _pytorch_managed
-        elif name in self.__dict__:
-            # here, we retrieve the value and delete the attribute from the regular __dict__ so we
-            # manage it.
-            value = getattr(self, name)
-            delattr(self, name)
-        elif name in manager.da_keys():
-            # here we previously registered the attribute as dynamic
-            value = manager.get_da_value(name)
-        else:
-            raise RuntimeError(f"The value of attribute {name} cannot be retrieved correctly!")
-
-        # store new dynamic attribute
-        manager.set_da(name, value, callback)
-
-    def _register_temp_attribute(
-        self,
-        name: str,
-        val: Any,
-        set_hook: Optional[SetAttrHook] = None,
-        del_hook: Optional[DelAttrHook] = None,
-    ):
-        """Register a temporary attribute to the instance that is deleted upon the final export.
-
-        Args:
-            name: The name of the attribute.
-            val: The value of the attribute.
-            set_hook: A hook that is executed before the attribute is set (Defaults to
-                ``lambda m, n, v: None``).
-            del_hook: A hook that is executed before the attribute is deleted (Defaults to
-                ``lambda m, n: None``).
-
-        Any attribute that is set like this will be removed upon the final export, i.e., only when
-        the module is not a dynamic module anymore after export. This is helpful if you want to
-        store extra attributes in the class that are not dynamic attributes or hparams but should
-        still be removed upon the final export, e.g., attributes or buffers that may access during
-        the dynamic attribute callbacks.
-
-        Unlike dynamic attributes, this can be used to set attributes that did not exist before and
-        *cannot* be used to overwrite existing attributes.
-        """
-        # retrieve manager
-        manager = self._get_dm_attribute_manager()
-
-        # sanity checks
-        if hasattr(self, name):
-            raise KeyError(f"Attribute {name} already exists!")
-        elif isinstance(val, Hparam):
-            raise RuntimeError("Please register hparam directly via _register_hparam()!")
-
-        # store in manager and set new attribute
-        manager.set_attr(name, set_hook, del_hook)
-        setattr(self, name, val)
-
-    @torch.no_grad()
-    def export(self) -> nn.Module:
-        """Export self (a dynamic module) **in-place** and return the exported module.
-
-        The export process will remove the top-level dynamic module and replace it with the original
-        module class. Note that the original class may be either another type of dynamic module or
-        a vanilla nn.Module. Consequently, any methods (including properties) that are implemented
-        in the child class will be removed. Hparams as well as dynamic and temporary attributes are
-        handled in a special fashion, see below.
-
-        In order to ensure that the exported module is still consistent there a several mechanisms
-        in place to handle hparams, dynamic attributes, and temporary attributes:
-
-        * **Hparams** of the current type are replaced with their currently active value.
-            Note that we do not need to explicitly handle hparams of the parent class as they are
-            mutually-exlusive, i.e., hparams are unique across all levels of inheritance.
-        * **Dynamic Attributes** are handled depending on whether they exist in a parent class:
-            1. The same dynamic attribute exists in a parent class. In this case, the callback is
-               folded into ("appended to") the callback for the same dyanmic attribute of the parent
-               class. This way we ensure that the final value of the attribute remains consistent.
-            2. The dynamic attribute does not exist in a parent class. In this case, the attribute
-               is not dynamic anymore as there are no more callbacks that could affect the value.
-               Therefore, we simply overwrite the underlying original object with the current value
-               and revert it to a regular attribute.
-        * **Temporary Attributes** are kept until the final export, i.e., until the resultign class
-          is not a dynamic module anymore. This is to ensure that folded callbacks that may need
-          access to these attributes can still access them.
-        """
-        manager = self._get_dm_attribute_manager()
-
-        # dissolve dynamic attributes when their corresponding callbacks cannot be folded into the
-        # parent class callbacks anymore.
-        da_removable = manager.fold_cbs()
-        for k in da_removable:
-            val = getattr(self, k)
-            if isinstance(val, torch.Tensor):
-                val = val.detach().clone()
-                if k in self._parameters:
-                    val = Parameter(val)
-            manager.pop_da(k)  # remove dynamic attribute from manager
-            setattr(self, k, val)  # now we set it as regular attribute/parameter/buffer
-
-        # replace hparams with active values and delete removable attributes in last level
-        with self._dict_with_special():  # this way we avoid recursion issues
-            for k in manager.hp_keys(all=False):
-                val = getattr(self, k)
-                manager.pop_hp(k)
-                setattr(self, k, val)
-
-            # remove attributes in the last level
-            if manager.level <= 1:
-                for k in manager.attr_keys():
-                    delattr(self, k)
-
-        # pop original class and the whole level of inheritance
-        self.__class__ = manager.pop_level()
-
-        # double-check that any remaining dynamic attributes still work
-        for k in manager.da_keys():
-            try:
-                getattr(self, k)
-            except Exception as e:
-                raise RuntimeError(f"Dynamic attribute {k} unretrievable after export!") from e
-
-        # remove manager if not needed anymore and check whether it should still be a DynamicModule
-        is_dynamic = isinstance(self, DynamicModule)
-        if manager:
-            assert is_dynamic, "Exported module should still be a DynamicModule!"
-        else:
-            assert not is_dynamic, "Exported module must not be a DynamicModule anymore!"
-            delattr(self, "_dm_attribute_manager")
-
-        return self
-
-    @torch.no_grad()
-    def force_assign(self):
-        """Force re-assign all dynamic attributes to their current values.
-
-        .. warning::
-
-            Note that this method overwrittes the actual buffers and parameters! Only use in
-            specific circumstances!!
-        """
-        # force-reassign all dynamic attributes
-        for name in self._get_dm_attribute_manager().da_keys():
-            val = getattr(self, name)
-            if isinstance(val, torch.Tensor):
-                val = val.detach().clone()
-            if name in self._parameters:
-                val = val if val is None else Parameter(val)
-                self.register_parameter(name, val)
-            elif name in self._buffers:
-                self.register_buffer(name, val)
-            else:
-                setattr(self, name, val)
-
-    @classmethod
-    @torch.no_grad()
-    def convert(cls, module: nn.Module) -> "DynamicModule":
-        """Converts a module in-place into its dynamic counterpart by patching its class.
-
-        Args:
-            module: The module to be converted into a dynamic module.
-
-        Returns:
-            The converted dynamic module.
-
-        This should generally be a *final* method and child classes should inherit ``_setup()``
-        instead to customize the conversion process.
-
-        Patching is achieved by updating the ``__class__`` attribute of the module to its dynamic
-        counterpart. The dynamic counterpart is a subclass of the original class, hence, we ensure
-        the module is fully compatible with the original class. Simultaneously, we can inject the
-        corresponding dynamic behavior in a standardized and rigoruos fashion.
-        """
-        # update class
-        original_cls = type(module)
-        module.__class__ = cls
-        assert isinstance(module, cls), f"Failed to convert {original_cls} to {cls}!"  # for mypy
-
-        # setup/update the attribute manager
-        if issubclass(original_cls, DynamicModule):
-            assert hasattr(module, "_dm_attribute_manager"), "Attribute manager not found!"
-        else:
-            assert not hasattr(module, "_dm_attribute_manager"), "Attribute manager found!"
-            module._dm_attribute_manager = _DMAttributeManager()
-        module._dm_attribute_manager.append_level(original_cls)
-
-        # setup new hparams and dynamic attributes
-        module._setup()
-
-        return module
-
-    def _setup(self):
-        """Setup dynamic attributes and hparams after the convert call.
-
-        This method should be overriden by the child class!
-        """
-        raise NotImplementedError("_setup() must be implemented by child class!")
-
-    def modify(self, *args, **kwargs):
-        """Modify the module's dynamic choices in a standardized & scalable fashion.
-
-        This method can be overriden by the child class! While users can also directly modify
-        the choices of individual hparams, this method should provide a way to modify a batch of
-        dynamic modules with the same arguments, e.g., ``out_features_ratio`` for ``DynamicLinear``.
-
-        Note that arguments of the modify method that are exposed to the user via the rule system
-        should be specified as **keyword-only arguments**. When they are exposed as keyword-only
-        arguments, the `_DMRegistryCls` can automatically generate the corresponding config class
-        on the fly that lets user provide configs and then they are automatically validated before
-        being passed to the ``modify`` method.
-
-        If possible, modify()'s keyword arguments should have default values that leave the hparams
-        intact if not provided, e.g., one might call ``some_dynamic_module.modify()`` without any
-        arguments and the module will remain unchanged.
-        """
-        pass
-
-    def freeze(self):
-        """Restrict the hparams of tbe dynamic module to the orginal choices.
-
-        This is useful to enforce the behavior of the parent class.
-
-        .. note::
-
-            After this call, the module's hparams can no longer be modified although the underlying
-            type is still a dynamic module.
-        """
-        for _, hp in self.named_hparams(configurable=True):
-            hp.active = hp.original
-            hp.choices = [hp.original]
-
-    @contextmanager
-    def reset_dynamic_attributes(self):
-        """Context manager to temporarily remove any dynamic attributes and re-register values.
-
-        This context manager is intended to be used when we want to access a dynamic attribute in
-        its original unmodified version, i.e., without this class interfering with its original
-        value and its corresponding getattr/setattr/delattr behavior.
-
-        Upon exiting the context manager, the dynamic attributes are re-registered and the same
-        callbacks are re-registered together with the new value.
-        """
-        manager = self._get_dm_attribute_manager()
-
-        with manager.retain_cbs():
-            # pop all dynamic attributes and make sure they are accessible in their original form
-            da_keys = manager.da_keys()
-            for k in da_keys:
-                val = manager.pop_da(k)
-                if not hasattr(self, k):
-                    setattr(self, k, val)
-            assert not manager.da_keys(), "Dynamic attributes were not removed properly!"
-            try:
-                yield None
-            finally:
-                # re-register dynamic attributes with current values + dummy callback (--> will be
-                # thrown away!)
-                # NOTE that we respect if attributes were deleted entirely.
-                for k in da_keys:
-                    if hasattr(self, k):
-                        self._register_dynamic_attribute(k, lambda _, v: v)
-
-    @contextmanager
-    def _dict_with_special(self):
-        """Context manager that checks that __dict__ contains _modules, _parameters, _buffers."""
-        # build up set of special keys that we temporarily add to __dict__
-        manager = self._get_dm_attribute_manager(use_default=True)
-        nn_special = set()
-        for key in ["_modules", "_parameters", "_buffers"]:
-            if key not in self.__dict__ and (key in manager.da_keys() or key in manager.hp_keys()):
-                nn_special.add(key)
-
-        # temporarily add to __dict__
-        for key in nn_special:
-            self.__dict__[key] = getattr(self, key)
-        try:
-            yield None
-        finally:
-            # remove from __dict__
-            for key in nn_special:
-                del self.__dict__[key]
-
-    def __setattr__(self, name: str, value: Any):
-        """Set attr and specifically handle hparams as well as dynamic & temporary attributes."""
-        # retrieve manager
-        manager = self._get_dm_attribute_manager(use_default=True)
-
-        if isinstance(value, Hparam):
-            self._register_hparam(name, value)
-        elif name in manager.hp_keys():
-            manager.get_hp(name).active = value
-        elif name in manager.da_keys() and manager.get_da_value(name) is not _pytorch_managed:
-            manager.set_da(name, value)
-        else:
-            if name in manager.attr_keys():
-                manager.get_attr_set_hook(name)(self, name, value)
-            with self._dict_with_special():
-                super().__setattr__(name, value)
-
-    def __getattr__(self, name: str) -> Union[torch.Tensor, torch.nn.Module]:
-        """Get attr and specifically handle hparams as well as dynamic & temporary attributes."""
-        # retrieve manager
-        manager = self._get_dm_attribute_manager(use_default=True)
-
-        # check if we can get value from our hparams
-        if name in manager.hp_keys():
-            return manager.get_hp(name).active
-
-        # check for dynamic attributes
-        if name in manager.da_keys():
-            value = manager.get_da_value(name)
-            # we might also need to grab value from super call (pytorch managed)
-            value = super().__getattr__(name) if value is _pytorch_managed else value
-            # apply all callbacks in order
-            return manager.get_da_cb(name)(self, value)
-
-        # regular case
-        with self._dict_with_special():
-            attr = super().__getattr__(name)
-        return attr
-
-    def __delattr__(self, name: str):
-        """Del an attr and specifically handle hparams as well as dynamic & temporary attributes."""
-        manager = self._get_dm_attribute_manager(use_default=True)
-        if name in manager.da_keys():
-            manager.pop_da(name)
-            # check if it still exists
-            if hasattr(self, name):
-                delattr(self, name)
-        elif name in manager.hp_keys():
-            manager.pop_hp(name)
-        else:
-            if name in manager.attr_keys():
-                del_hook = manager.pop_attr(name)
-                del_hook(self, name)
-            return super().__delattr__(name)
-
-    def get_hparam(self, target: str) -> Hparam:
-        """Look up and return hparam (like "torch.nn.Module.get_parameter()" but for hparam)."""
-        return self._get_dm_attribute_manager().get_hp(target)
-
-    def named_hparams(self, configurable: Optional[bool] = None) -> Iterator[Tuple[str, Hparam]]:
-        """Return an iterator over all hparams of the module.
-
-        Args:
-            configurable: Whether to include configurable hparams.
-
-        Yields:
-            (name, Hparam): Tuple containing the name and hparam.
-
-        Default behavior is to iterate over configurable and non-configurable hparams. Set
-        ``configurable`` accordingly to only iterate over either. If ``configurable`` is set to
-        ``True``, only configurable symbols are iterated over. If ``configurable`` is set to
-        ``False``, configurable symbols are skipped over (only non-configurable symbols).
-        """
-        for hp_name, hp in self._get_dm_attribute_manager().named_hps():
-            if configurable is None or hp.is_configurable == configurable:
-                yield hp_name, hp
-
-    def extra_repr(self):
-        """Generate extra_repr making sure all dynamic keys exist in self.__dict__.
-
-        Pytorch heavily uses self.__dict__ to generate extra_repr. However, we remove certain
-        attributes from self.__dict__ so we can manage them dynamically. Temporarily, adding them
-        back in here and removing them again afterwards.
-        """
-        added_keys = set()
-        manager = self._get_dm_attribute_manager()
-        for name in chain(manager.hp_keys(), manager.da_keys()):
-            val = getattr(self, name)
-            if name in self.__dict__ or val is None:
-                continue
-            added_keys.add(name)
-            self.__dict__[name] = getattr(self, name)
-
-        # make super call
-        extra_repr = super().extra_repr()
-
-        # remove keys again
-        for key in added_keys:
-            del self.__dict__[key]
-
-        return extra_repr
-
-    @property
-    def original_cls(self) -> Type[nn.Module]:
-        """Return the original class of the dynamic module."""
-        return self._get_dm_attribute_manager().og_cls
-
-
-class _DMRegistryCls:
-    """A registry to keep track of available dynamic modules.
-
-    The registry can also dynamically generate new entries when we have a class that inherits from
-    the registered nn.Module.
-    """
-
-    T = TypeVar("T", bound=DynamicModule)
-
-    def __init__(self, prefix: str):
-        super().__init__()
-
-        self._prefix = prefix  # global prefix to use to register dynamic classes
-
-        self._registry: Dict[Type[nn.Module], Type[DynamicModule]] = {}  # registered classes
-        self._key_registry: Dict[Type[nn.Module], str] = {}  # registered str-keys for classes
-
-        self._dynamic_classes: Dict[Type[nn.Module], Type[DynamicModule]] = {}  # generated DMs
-        # generated ModeloptBaseRule classes for the dynamic classes
-        self._rule_classes: Dict[Type[DynamicModule], Type[ModeloptBaseRule]] = {}
-
-    def _generate_rule_class(self, dm_cls: Type[DynamicModule]) -> Type[ModeloptBaseRule]:
-        """Generate a ModeloptBaseRule type for the given dynamic module class."""
-        spec = inspect.getfullargspec(dm_cls.modify)
-        if spec.kwonlyargs:
-            assert spec.kwonlydefaults is not None, "modify method should have default arguments!"
-            fields = {n: (spec.annotations[n], spec.kwonlydefaults[n]) for n in spec.kwonlyargs}
-        else:
-            fields = {}
-        return create_model(f"{dm_cls.__name__}Config", __base__=ModeloptBaseRule, **fields)
-
-    def _get_dynamic_class_name(self, nn_cls: Type[nn.Module]) -> str:
-        """Generate a name for the dynamic class."""
-
-        def _is_occupied(name: str) -> bool:
-            """Check if a name is already occupied by a dynamic class."""
-            return any(name == dm.__name__ for dm in self._dynamic_classes.values())
-
-        # first try: simply prepend self._prefix to the nn_cls name but only if not occupied already
-        name = f"{self._prefix}{nn_cls.__name__}"
-        if not _is_occupied(name):
-            return name
-
-        # 2nd try: add self._prefix and module name to the nn_cls name (should never be occupied!)
-        name = f"{nn_cls.__module__.replace('.', '_')}_{self._prefix}{nn_cls.__name__}"
-        assert not _is_occupied(name), f"Dynamic class name {name} should not be occupied!"
-        return name
-
-    def _get_registered_nn_class(self, nn_cls: Type[nn.Module]) -> Optional[Type[nn.Module]]:
-        """Optionally return the nn module class that should be used to register nn_cls.
-
-        This method loops through the registry to see if there are any nn_cls matches, i.e., any
-        subclass with a shared forward method.
-        """
-        for nn_cls_ in self._registry:
-            if issubclass(nn_cls, nn_cls_) and nn_cls.forward is nn_cls_.forward:
-                return nn_cls_
-        return None
-
-    def __contains__(self, item: Union[nn.Module, Type[nn.Module], str]):
-        if isinstance(item, str):
-            return item in self._key_registry.values()
-        nn_cls = type(item) if isinstance(item, nn.Module) else item
-        return self._get_registered_nn_class(nn_cls) is not None
-
-    def __getitem__(self, nn_cls: Union[Type[nn.Module], str]) -> Type[DynamicModule]:
-        """Return the dynamic module class for the given nn_cls type or registered string."""
-        dm_cls = self.get(nn_cls)
-        if dm_cls is None:
-            raise KeyError(f"{nn_cls} is not registered for a dynamic module!")
-        return dm_cls
-
-    def get(
-        self, nn_cls: Union[Type[nn.Module], str], default: Any = None
-    ) -> Optional[Type[DynamicModule]]:
-        """Return the dynamic module class for the given nn_cls type or registered string."""
-        # check string case first
-        if isinstance(nn_cls, str):
-            for nn_cls_, key in self._key_registry.items():
-                if key == nn_cls:
-                    return self.get(nn_cls_, default)
-            return default
-
-        assert issubclass(nn_cls, nn.Module), f"{nn_cls} is not a subclass of nn.Module!"
-
-        # see if dynamic class is already available and return
-        if nn_cls in self._dynamic_classes:
-            return self._dynamic_classes[nn_cls]
-
-        # see if we can generate a new dynamic class and then return
-        nn_cls_ = self._get_registered_nn_class(nn_cls)
-        if nn_cls_:
-            name = self._get_dynamic_class_name(nn_cls)
-            dm_base_class = self._registry[nn_cls_]
-            dm_class = type(name, (dm_base_class, nn_cls), {})
-            self._dynamic_classes[nn_cls] = dm_class
-            self._rule_classes[dm_class] = self._generate_rule_class(dm_class)
-            return self.get(nn_cls, default)
-
-        # default return
-        return default
-
-    def get_key_from_dm(self, dm_cls: Union[Type[DynamicModule], DynamicModule]) -> str:
-        """Retrieve the key that is registered for a given dynamic module class."""
-        dm_cls = type(dm_cls) if isinstance(dm_cls, DynamicModule) else dm_cls
-        for nn_cls, dm_cls_ in self._dynamic_classes.items():
-            if dm_cls == dm_cls_:
-                return self.get_key(nn_cls)
-        raise KeyError(f"{dm_cls} is not registered for a dynamic module!")
-
-    def get_key(self, nn_cls: Union[Type[nn.Module], str]) -> str:
-        """Retrieve the key that is registered for a given nn module class."""
-        # sanity check
-        assert nn_cls in self, f"{nn_cls} is not registered for a dynamic module!"
-
-        # handle string case
-        if isinstance(nn_cls, str):
-            return nn_cls
-
-        # handle module case
-        nn_cls_ = self._get_registered_nn_class(nn_cls)
-        assert nn_cls_ is not None
-        return self._key_registry[nn_cls_]
-
-    def get_rule_class(self, nn_cls: Union[Type[nn.Module], str]) -> Type[ModeloptBaseRule]:
-        """Retrieve the rule config class that is registered for a given nn module class."""
-        dm_cls = self.get(nn_cls)
-        if dm_cls is None:
-            raise KeyError(f"{nn_cls} is not registered for a dynamic module!")
-        return self._rule_classes[dm_cls]
-
-    def register(self, cls_to_key: Dict[Type[nn.Module], str]) -> Callable[[Type[T]], Type[T]]:
-        """Use this to register a new dynamic base module.
-
-        Usage:
-
-        .. code-block:: python
-
-            @DMRegistry.register({nn.Linear: "nn.Linear"})
-            class DynamicLinear(DynamicModule):
-                ...
-
-        .. note::
-
-            The dynamic base module must **NOT** inherit from the nn.Module that is registered.
-            Instead the registry will automatically generate a new class that inherits from both
-            the dynamic class (``DynamicLinear`` above) and the nn.Module (``nn.Linear`` above) for
-            an MRO that corresponds to ``class AutoGenerated(DynamicLinear, nn.Linear): pass``.
-        """
-
-        def decorator(dm_class: Type[_DMRegistryCls.T]) -> Type[_DMRegistryCls.T]:
-            """Register dnn_class with appropriate nn_class."""
-            for nn_cls_, key in cls_to_key.items():
-                assert nn_cls_ not in self._registry, f"{nn_cls_} already registered!"
-                self._registry[nn_cls_] = dm_class
-                self._key_registry[nn_cls_] = key
-            return dm_class
-
-        return decorator
-
-    def unregister(self, nn_cls: Union[Type[nn.Module], Type[T]]) -> None:
-        """Unregister a previously registered dynamic base module and all its inherited modules.
-
-        It throws a KeyError if the dynamic base module is not registered.
-        """
-        # 0. sanity check
-        if nn_cls not in self._registry:
-            raise KeyError(f"{nn_cls} is not registered!")
-
-        # 1. unregister any generated dynamic classes
-        for nn_cls_ in list(self._dynamic_classes):
-            if nn_cls == self._get_registered_nn_class(nn_cls_):
-                dm_cls = self._dynamic_classes.pop(nn_cls_)
-                self._rule_classes.pop(dm_cls)
-
-        # 2. unregister the base dynamic class
-        self._registry.pop(nn_cls)
-        self._key_registry.pop(nn_cls)
-
-    def convert(self, nn_mod: nn.Module) -> DynamicModule:
-        """Converts the module into a dynamic module if registered or raise KeyError."""
-        return self[type(nn_mod)].convert(nn_mod)
-
-    @property
-    def prefix(self) -> str:
-        """Return the prefix used for the dynamic classes."""
-        return self._prefix
-
-
-class DynamicSpace:
-    """A class to represent all dynamic model choices over a model with multiple submodules."""
-
-    def __init__(self, model: nn.Module) -> None:
-        """Initialize the dynamic space from the model."""
-        self.model = model
-
-    def _should_be_converted(self, mod: nn.Module) -> bool:
-        """Check if the module should be converted."""
-        return True
-
-    def convert_to_dynamic(
-        self, rules: Optional[RulesDict], dm_registry: _DMRegistryCls
-    ) -> Dict[str, nn.Module]:
-        """Convert the model to dynamic modules according to the rules and provided registry.
-
-        Args:
-            rules: A dictionary containing rules for the dynamic modules.
-            dm_registry: A registry containing the dynamic modules to be converted to.
-
-        Returns:
-            A dictionary containing the converted modules with submodule names as keys and the
-            converted dynamic modules as values.
-        """
-        # check if the model is DataParallel
-        unwrap_model(self.model, raise_error=True)
-
-        # check that it's not channels last
-        assert not is_channels_last(self.model)
-
-        # 1. patch the model with dynamic units
-        # NOTE: duplicate modules are handled automatically since we convert/patch modules in-place
-        mods_converted: Dict[str, nn.Module] = {}
-        for name, mod in self.model.named_modules():
-            if mod in dm_registry and self._should_be_converted(mod):
-                dm_registry.convert(mod)
-                mods_converted[name] = mod
-
-        # 2. modify search space if rules are provided
-        if rules is None:
-            return mods_converted
-
-        # change all keys to strings
-        rules = {dm_registry.get_key(key): rule for key, rule in rules.items()}
-
-        # iterator through all dynamic modules and modify them according to rules
-        for mod_name, mod in mods_converted.items():
-            # get the key for this module that is used in the rules
-            key = dm_registry.get_key_from_dm(mod)
-
-            # validate + construct custom rules using the rule class stored in the dm_registry
-            rule_custom = dm_registry.get_rule_class(key).customize_rule(rules.get(key), mod_name)
-
-            if rule_custom is None:
-                # freeze module if no rule is provided
-                mod.freeze()
-            else:
-                # modify module according to rule
-                mod.modify(**rule_custom)
-
-        return mods_converted
-
-    def named_dynamic_modules(self) -> Iterator[Tuple[str, DynamicModule]]:
-        """Recursively yield the name and instance of *all* DynamicModules.
-
-        Yields:
-            (name, DynamicModule): Tuple containing the name and module.
-        """
-        for name, module in self.model.named_modules():
-            if isinstance(module, DynamicModule):
-                yield name, module
-
-    def is_dynamic(self) -> bool:
-        """Check if any module is dynamic.
-
-        Returns:
-            True if the model contains DynamicModule(s).
-        """
-        return any(True for _ in self.named_dynamic_modules())
-
-    def named_hparams(self, configurable: Optional[bool] = None) -> Iterator[Tuple[str, Hparam]]:
-        """Recursively yield the name and instance of *all* hparams.
-
-        Args:
-            configurable: Whether to include configurable hparams.
-
-        Yields:
-            (name, Hparam): Tuple containing the name and hparam.
-
-        Default behavior is to iterate over all hparams. If ``configurable`` is set to ``True``,
-        only configurable, non-duplicate symbols are iterated over.
-        """
-        _memo = set()
-        assert configurable in [None, True], "Only all or configurable hparams are supported!"
-        for mod_name, mod in self.named_dynamic_modules():
-            for hp_name, hp in mod.named_hparams(configurable=configurable):
-                if configurable is None or hp not in _memo:
-                    yield mod_name + ("." if mod_name else "") + hp_name, hp
-                    _memo.add(hp)
-
-    def get_hparam(self, name: str) -> Hparam:
-        """Get the hparam with the given name."""
-        mod_name, _, hp_name = name.rpartition(".")
-        mod = self.model.get_submodule(mod_name)
-        assert isinstance(mod, DynamicModule), f"Module {mod} must be a DynamicModule!"
-        return mod.get_hparam(hp_name)
-
-    def is_configurable(self) -> bool:
-        """Check if the model has any configurable hyperparameters.
-
-        Args:
-            model: A model to be checked for DynamicModule(s) with configurable hyperparameters.
-
-        Returns:
-            True if the model contains DynamicModule(s) with configurable hyperparameters w/ more
-            than one choice.
-        """
-        return any(True for _ in self.named_hparams(configurable=True))
-
-    def size(self) -> int:
-        """Get the search space size of the model.
-
-        Returns:
-            A int representing the search space size of the model.
-        """
-        space_size = 1
-        for _, hp in self.named_hparams(configurable=True):
-            space_size *= len(hp.choices)
-        return space_size
-
-    def config(self, configurable: Optional[bool] = None) -> Dict[str, Any]:
-        """Return the config dict of all hyperparameters.
-
-        Args:
-            model: A model that contains DynamicModule(s).
-            configurable: None -> all hps, True -> configurable hps, False -> non-configurable hps
-
-        Returns:
-            A dict of ``(parameter_name, choice)`` that specifies an active subnet.
-        """
-        return {name: hp.active for name, hp in self.named_hparams(configurable)}
-
-    def select(self, config: Dict[str, Any], strict: bool = True) -> None:
-        """Select the subnet provided by config.
-
-        If `strict` is set, then `config` must contain the exact set of keys representing both the
-        configurable and non-configurable hparams.
-        """
-        # check if we have any overlap with dependent keys
-        check_non_configurable = any(
-            name in config and not hp.is_configurable for name, hp in self.named_hparams()
-        )
-
-        # go through config, select based on current config, and track key errors
-        missing_keys = []
-        inconsistent_keys = []
-        unexpected_keys = {k: True for k in config.keys()}
-
-        # assign free/searchable hparams from config
-        configurables = dict(self.named_hparams(configurable=True))
-        for name, hparam in configurables.items():
-            if name in config:
-                hparam.active = config[name]
-                unexpected_keys[name] = False
-            elif strict:
-                missing_keys.append(name)
-
-        # do a sanity check on the provided dynamic keys
-        if check_non_configurable and strict:
-            for name, hparam in self.named_hparams():
-                if name in configurables:
-                    continue
-                if name not in config:
-                    missing_keys.append(name)
-                    continue
-                unexpected_keys[name] = False
-                if hparam.active != config[name]:
-                    inconsistent_keys.append(
-                        f"{name}: active={hparam.active}, config={config[name]}"
-                    )
-
-        # raise error for missing and unexpected keys if strict
-        unexpected_keys = [k for k, val in unexpected_keys.items() if val]
-        error_msg = ""
-        if strict and len(missing_keys) > 0:
-            error_msg += "\n\t".join(["Missing keys in config for:"] + missing_keys)
-            error_msg += "\nMake sure all keys are present in config or set strict=False."
-            raise RuntimeError(error_msg)
-        if strict and len(unexpected_keys) > 0:
-            error_msg += "\n\t".join(["Unexpected keys in config for:"] + unexpected_keys)
-            error_msg += "\nMake sure only required keys are in config or set strict=False."
-        if strict and len(inconsistent_keys) > 0:
-            error_msg += "\n\t".join(["Inconsistent keys in config:"] + inconsistent_keys)
-            error_msg += "\nMake sure keys in config are consistent or set strict=False."
-        if error_msg:
-            raise RuntimeError(f"Error in selecting config:\n{error_msg}")
-
-    def export(self, dm_registry: _DMRegistryCls) -> nn.Module:
-        """Recursively export the module including self and return the result.
-
-        Args:
-            dm_registry: A dynamic module registry to check for dynamic modules that should be
-                exported.
-
-        Returns:
-            The model after exporting the dynamic modules found in the registry.
-        """
-
-        def _recursive_export(mod: nn.Module) -> None:
-            """Recursively export the module."""
-            for n, m in mod.named_children():
-                if isinstance(m, DynamicModule) and m.original_cls in dm_registry:
-                    setattr(mod, n, m.export())  # re-assigning needed for DynamicParallelKDModule
-                _recursive_export(m)
-
-        if isinstance(self.model, DynamicModule):
-            self.model = self.model.export()
-        _recursive_export(self.model)
-
-        return self.model
-
-    def __repr__(self):
-        lines = [f"{type(self).__name__}("]
-        spaces = "  "
-        attr_line = [f"{x}={getattr(self, x)()}" for x in ["is_dynamic", "is_configurable", "size"]]
-        lines.append(spaces + ", ".join(attr_line) + ",")
-        model_lines = str(self.model).split("\n")
-        lines.extend([spaces + ("" if i else "model=") + x for i, x in enumerate(model_lines)])
-        lines.append(")")
-        return "\n".join(lines)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Basic dynamic module class and hparam class."""
+
+# cython: annotation_typing = False
+
+import inspect
+from contextlib import contextmanager
+from itertools import chain
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Iterable,
+    Iterator,
+    List,
+    Optional,
+    Set,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
+
+import torch
+import torch.nn as nn
+from pydantic import create_model
+from torch.fx.proxy import Proxy
+from torch.nn.parameter import Parameter
+
+from modelopt.torch.utils import is_channels_last, unwrap_model
+
+from .config import ModeloptBaseRule, RulesDict
+from .hparam import Hparam
+
+__all__ = ["DynamicModule", "_DMRegistryCls", "DynamicSpace"]
+
+_pytorch_managed = type("_pytorch_managed", (), {})  # pylint: disable=invalid-name
+_da_val_default = type("_da_val_default", (), {})  # pylint: disable=invalid-name
+
+
+DynamicAttributeCallback = Callable[["DynamicModule", Any], Any]
+SetAttrHook = Callable[["DynamicModule", str, Any], None]
+DelAttrHook = Callable[["DynamicModule", str], None]
+
+
+class _FoldedCallback:
+    """A callback functor that enables folding additional callbacks into an existing callback."""
+
+    def __init__(self, *callbacks: DynamicAttributeCallback):
+        self._callbacks: List[DynamicAttributeCallback] = []
+        self.extend(callbacks)
+
+    @property
+    def callback(self) -> DynamicAttributeCallback:
+        """Return the main callback."""
+        return self._callbacks[0]
+
+    @callback.setter
+    def callback(self, cb: DynamicAttributeCallback):
+        """Set the main callback."""
+        self._callbacks[0] = cb
+
+    def __iter__(self) -> Iterator[DynamicAttributeCallback]:
+        """Iterate over all callbacks."""
+        yield from self._callbacks
+
+    def __call__(self, self_module: "DynamicModule", val: Any) -> Any:
+        """Call the callback and all other callbacks."""
+        if isinstance(val, Proxy):
+            return val
+        for cb in self:
+            val = cb(self_module, val)
+        return val
+
+    def __len__(self) -> int:
+        """Return the number of callbacks."""
+        return len(self._callbacks)
+
+    def extend(self, cbs_other: Iterable[DynamicAttributeCallback]):
+        """Extend the list of other callbacks."""
+        self._callbacks.extend(cbs_other)
+
+
+class _DMAttributeManager:
+    """A class to manage the special attributes for dynamic modules in a hierarchical fashion.
+
+    Abbreviations:
+        - og_cls: original class
+        - hp: hparam
+        - da: dynamic attribute
+        - cb: callback
+        - attr: generic temporary attributes
+
+    This class handles all types of special attributes for dynamic modules. It is used to store
+    them, manage them, and provide access to them. Moreover, this class stores the hierarchy of
+    dynamic module conversions that were applied to the module and can associate the special
+    attributes with the correct level of inheritance.
+
+    Specifically, we handle the following types of special attributes:
+
+        1. **Original classes**:
+           We maintain a stack of original classes as we stack/convert dynamic modules. As we stack
+           dynamic modules on top or remove/export them, we maintain the stack.
+
+        2. **Hparams**:
+           We store hparams in a list of dictionaries. This way we can associate each hparam with
+           the corresponding level of hierarchy/inheritance. For hparams we do NOT allow adding or
+           overwriting hparams that exist in a different level of the stack, i.e., hparams must be
+           _unique_ across all levels of inheritance. Note that you can still access hparams from
+           any level of inheritance.
+
+        3. **Dynamic attributes**:
+           We store dynamic attributes in a dictionary. Additionally, a dynamic attribute can be
+           associated with a callback in an _arbitrary_ number of levels in the hierarchy. The
+           callbacks are stored in a list of dictionaries. This way we can associate each callback
+           with the corresponding level of hierarchy/inheritance and still allow for multiple
+           callbacks for the same dynamic attribute.
+
+        4. **Generic attributes**:
+           These are generic attributes that are not hparams or dynamic attributes. They are kept
+           track of so that we can remove them again upon the final export. They must be unique
+           and cannot correspond to an hparam or dynamic attribute.
+    """
+
+    def __init__(self):
+        self._og_cls_all: List[Type[nn.Module]] = []
+        self._hp_all: List[Dict[str, Hparam]] = []
+        self._da: Dict[str, Any] = {}
+        self._da_cb_all: List[Dict[str, _FoldedCallback]] = []
+        self._attr: Dict[str, Tuple[SetAttrHook, DelAttrHook]] = {}
+
+    def __bool__(self) -> bool:
+        all_data = [self._og_cls_all, self._da, self._da_cb_all, self._hp_all, self._attr]
+        has_state = [bool(x) for x in all_data]
+        if not has_state[0]:
+            # if we have no original class, we should have no other state either!
+            assert not any(has_state), "Inconsistent state for DM attributes!"
+        return has_state[0]
+
+    def _get_lookup(self, name: str, lookup_all: List[Dict[str, Any]]) -> Dict[str, Any]:
+        """Return the lookup for the given name."""
+        for lookup in lookup_all:
+            if name in lookup:
+                return lookup
+        raise KeyError(f"{name} not found!")
+
+    @property
+    def og_cls(self) -> Type[nn.Module]:
+        """Return the original class of the dynamic module."""
+        return self._og_cls_all[-1]
+
+    @property
+    def level(self) -> int:
+        """Return the current level of stacked inheritance."""
+        return len(self._og_cls_all)
+
+    def append_level(self, original_cls: Type[nn.Module]):
+        """Initialize a new level of special atttributes."""
+        self._og_cls_all.append(original_cls)
+        self._hp_all.append({})
+        self._da_cb_all.append({})
+
+    def pop_level(self) -> Type[nn.Module]:
+        """Remove the last level of special attributes and return the original class."""
+        # sanity checks on hparams
+        hp_lookup = self._hp_all[-1]
+        assert not hp_lookup, "Some hparams were not removed properly!"
+
+        # sanity checks on dynamic attributes and corresponding callbacks
+        cb_lookup = self._da_cb_all[-1]
+        assert not cb_lookup, "Some dynamic attribute callbacks were not removed properly!"
+
+        da_keys_remaining = set(chain(*self._da_cb_all[:-1]))
+        assert da_keys_remaining == self.da_keys(), "Dynamic attributes were not removed properly!"
+
+        # sanity checks on temporary attributes on last level
+        if self.level <= 1:
+            assert not self._attr, "Some attributes were not removed properly!"
+
+        # after clean-up, we can pop and return the original class
+        self._hp_all.pop()
+        self._da_cb_all.pop()
+        return self._og_cls_all.pop()
+
+    def hp_keys(self, all: bool = True) -> Set[str]:
+        """Return the keys of current or all hparams."""
+        idx_start = 0 if all else -1
+        return set(chain(*self._hp_all[idx_start:]))
+
+    def get_hp(self, name: str) -> Hparam:
+        """Return the hparam with the given name."""
+        hp_lookup = self._get_lookup(name, self._hp_all)
+        return hp_lookup[name]
+
+    def named_hps(self, all: bool = True) -> Iterator[Tuple[str, Hparam]]:
+        """Return the name and hparam for all or current hparams."""
+        idx_start = 0 if all else -1
+        for hp_lookup in self._hp_all[idx_start:]:
+            for name, hp in hp_lookup.items():
+                yield name, hp
+
+    def set_hp(self, name: str, hparam: Hparam):
+        """Store hparam by the provided name in the right location."""
+        # we do not allow the same hparam in different levels of inheritance
+        if name in self.hp_keys():
+            assert name in self.hp_keys(all=False), "Hparam already exists in the base cls!"
+        self._hp_all[-1][name] = hparam
+
+    def pop_hp(self, name: str) -> Any:
+        """Pop hparam by the provided name."""
+        hp_lookup = self._get_lookup(name, self._hp_all)
+        return hp_lookup.pop(name)
+
+    def da_keys(self) -> Set[str]:
+        """Return the keys of all dynamic attributes."""
+        return set(self._da)
+
+    def set_da(
+        self,
+        name: str,
+        val: Any = _da_val_default,
+        cb: Optional[DynamicAttributeCallback] = None,
+    ):
+        """Store a dynamic attribute together with its callback method."""
+        # sanity checks
+        if name in self.da_keys():
+            val = self.get_da_value(name) if val is _da_val_default else val
+        else:
+            assert val is not _da_val_default, "Value must be provided for new dynamic attribute!"
+            assert cb is not None, "Callback must be provided for new dynamic attribute!"
+
+        # store value
+        self._da[name] = val
+
+        # store wrapped callback
+        # NOTE: we only allow updating the original callback of this level of inheritance. Updating
+        # folded callbacks is not allowed!
+        if cb is not None:
+            if name in self._da_cb_all[-1]:
+                self._da_cb_all[-1][name].callback = cb
+            else:
+                self._da_cb_all[-1][name] = _FoldedCallback(cb)
+
+    def pop_da(self, name: str) -> Any:
+        """Pop dynamic attribute by the provided name."""
+        val = self._da.pop(name)
+        for cb_lookup in self._da_cb_all:
+            cb_lookup.pop(name, None)
+        return val
+
+    def get_da_value(self, name: str) -> Any:
+        """Return the raw value of the dynamic attribute."""
+        return self._da[name]
+
+    def get_da_cb(self, name: str) -> _FoldedCallback:
+        """Return the full callback for the dynamic attribute."""
+        cbs_all = list(chain.from_iterable(cb_d[name] for cb_d in self._da_cb_all if name in cb_d))
+        return _FoldedCallback(*cbs_all)
+
+    @contextmanager
+    def retain_cbs(self):
+        """Context manager to retain all callback information for dynamic attributes.
+
+        Any changes that are made to the callbacks within this context manager are not retained upon
+        exiting the context manager. However, changes to the value of dynamic attributes as well
+        as well deletions and additions of dynamic attributes are retained.
+        """
+        # retain original callbacks while exposing a list of empty dictionaries
+        da_cb_all = self._da_cb_all
+        da_keys_before = self.da_keys()
+        self._da_cb_all = [{} for _ in da_cb_all]
+        try:
+            yield None
+        finally:
+            # 1. We will restore callbacks to the original state for da's that still exist
+            # 2. We will maintain callbacks for newly added da's
+            # 3. We will throw away callbacks for da's that were removed
+            da_keys_removed = da_keys_before - self.da_keys()
+            da_keys_new = self.da_keys() - da_keys_before
+            da_keys_same = da_keys_before & self.da_keys()
+
+            da_cb_all_final = []
+            for cb_lookup_before, cb_lookup_now in zip(da_cb_all, self._da_cb_all):
+                cb_lookup_new = {k: v for k, v in cb_lookup_now.items() if k in da_keys_new}
+                cb_lookup_same = {k: v for k, v in cb_lookup_before.items() if k in da_keys_same}
+                cb_lookup_final = {**cb_lookup_new, **cb_lookup_same}
+                assert not (cb_lookup_final.keys() & da_keys_removed)  # final sanity check
+                da_cb_all_final.append({**cb_lookup_same, **cb_lookup_new})
+            self._da_cb_all = da_cb_all_final
+
+    def fold_cbs(self) -> Set[str]:
+        """Fold all callbacks that appear in lower levels into the corresponding lower level.
+
+        After this call, only dynamic attributes that appear solely in the top-level remain in the
+        top-level.
+
+        Returns:
+            The set of dynamic attributes that remain in the top-level.
+        """
+        cb_lookup = self._da_cb_all[-1]
+
+        # check if we can fold the callbacks into a lower level
+        for cb_lookup_other in reversed(self._da_cb_all[:-1]):
+            for k_other, cb_other in cb_lookup_other.items():
+                if k_other in cb_lookup:
+                    cb_other.extend(cb_lookup.pop(k_other))
+
+        # sanity check: at this point there should be no overlap between top-level and other levels
+        assert not any(k in cb_lookup for k in chain(*self._da_cb_all[:-1])), "Callback overlap!"
+
+        return set(cb_lookup)
+
+    def attr_keys(self) -> Set[str]:
+        """Return the keys of all or current other attributes."""
+        return set(self._attr)
+
+    def get_attr_set_hook(self, name: str) -> SetAttrHook:
+        """Return the setter for the given name."""
+        return self._attr[name][0]
+
+    def set_attr(self, name: str, set_hook: Optional[SetAttrHook], del_hook: Optional[DelAttrHook]):
+        """Store the name of an attribute in the other list."""
+        assert name not in self.attr_keys(), "Attribute already exists!"
+
+        set_hook = set_hook or (lambda m, n, v: None)
+        del_hook = del_hook or (lambda m, n: None)
+        self._attr[name] = (set_hook, del_hook)
+
+    def pop_attr(self, name: str) -> DelAttrHook:
+        """Pop attr by the provided name and return hook for delattr."""
+        return self._attr.pop(name)[1]
+
+
+class DynamicModule(nn.Module):
+    """Base class for dynamic modules.
+
+    Dynamic modules are usually extended from ``torch.nn.Module``'s. They
+    dynamically support a family of `torch.nn.Module`'s with different architectural
+    parameters, such as input/output channel numbers.
+
+    Dynamic modules can also be used to construct the basic searchable unit in a search space with
+    the option to select and sample a candidate unit.
+
+    Candidate units are usually described using ``Hparam`` objects and dynamic attributes. Each
+    hparam describes a basic searchable unit (e.g. number of output channels in ``DynamicConv2d``).
+    Dynamic attributes are callbacks to dynamically construct the attribute depending on the current
+    value of the hparam(s), e.g., the ``weight`` tensor in ``DynamicConv2d`` that depends on the
+    hparam ``out_channels``.
+
+    In addition, dynamic modules also support registering general attributes that are removed upon
+    export the module. This is useful for storing temporary attributes that are not hparams or
+    existing attributes that are converted to dynamic attributes.
+
+    For a ``DynamicModule`` class that contains other dynamic modules, the class implementation
+    should ensure only to expose ``hparams`` in the outermost class and handle other ``hparams``
+    internally including ``hparams`` of child modules that are exposed on their own usually
+    (e.g. block module implementations containing DynamicLinear).
+    """
+
+    # this is needed to store the special attributes for dynamic modules
+    _dm_attribute_manager: _DMAttributeManager
+
+    def __init__(self, *args, **kwargs):
+        """Initializing a dynamic module is not allowed!"""
+        raise RuntimeError("DynamicModule cannot be initialized directly; use convert instead!")
+
+    def _get_dm_attribute_manager(self, use_default: bool = False) -> _DMAttributeManager:
+        """Return the attribute manager or a default one if not available."""
+        if "_dm_attribute_manager" not in self.__dict__:
+            if use_default:
+                return _DMAttributeManager()
+            raise AttributeError("DynamicModule.convert() must be called before using the module!")
+        return self._dm_attribute_manager
+
+    def _register_hparam(self, name: str, hparam: Hparam):
+        """Register hparam by the provided name and do some sanity checks during registering.
+
+        Args:
+            name: The name of the hparam. Note that an hparam can be a new attribute or an existing
+                attribute that is not already registered as temporary attribute or dynamic
+                attribute. In addition, an hparam cannot be registered for tensor and therefore
+                neither for parameters nor buffers. Finally, an hparam cannot be registered when an
+                hparam under the same exists in one of the parent classes. However, one can re-
+                register an hparam of itself.
+            hparam: The hparam object to be registered.
+
+        An hparam is useful to configure the dynamic module on the fly. The collection of hparams
+        define the configurable space of the dynamic module.
+        """
+        # retrieve manager
+        manager = self._get_dm_attribute_manager()
+
+        # various sanity checks
+        if not isinstance(hparam, Hparam):
+            raise TypeError(f"Cannot assign {type(hparam)} to {name}. Hparam required!")
+        elif name in manager.attr_keys():
+            raise KeyError(f"Cannot use the reserved name {name} to assign an hparam!")
+        elif name in manager.da_keys():
+            raise RuntimeError(f"Cannot register attribute {name} that is dynamic as hparam.")
+        elif name in manager.hp_keys(all=True) and name not in manager.hp_keys(all=False):
+            raise RuntimeError(f"Cannot overwrite hparam {name} that is an hparam in the base cls.")
+        elif "." in name:
+            raise KeyError('hparam name can\'t contain "."')
+        elif name == "":
+            raise KeyError('hparam name can\'t be empty string ""')
+        elif name in ["_parameters", "_buffers", "_modules"]:
+            raise KeyError(f"Cannot use reserved name {name} to assign hparam.")
+        # delete name if it already exists
+        elif hasattr(self, name):
+            if isinstance(getattr(self, name), (torch.Tensor, torch.nn.Module)):
+                raise RuntimeError(f"Cannot register tensor/module attribute {name} as hparam.")
+            delattr(self, name)
+
+        # store new hparam
+        manager.set_hp(name, hparam)
+
+    def _register_dynamic_attribute(self, name: str, callback: DynamicAttributeCallback):
+        """Register a dynamic attribute together with its callback method.
+
+        Args:
+            name: The name of the dynamic attribute.
+            callback: The callback method that is executed when the dynamic attribute is accessed.
+                Generally, this callback can access the whole module (``self``) as well as expect
+                to have access to the original attribute without any modifications.
+
+        Note that a dynamic attribute should be an existing attribute that is dynamically affected
+        when the value of one or more hparams changes. It is not allowed to register a dynamic
+        attribute for a non-existing attribute. However, specifically tensors, parameters, and
+        buffers are supported. Moreover, a dynamic attribute can also be registered for an existing
+        dynamic attribute in the parent class. In this case, the ``callback`` will be executed
+        *after* the callback of the parent class.
+        """
+        # retrieve manager
+        manager = self._get_dm_attribute_manager()
+
+        # various sanity checks
+        if not hasattr(self, name):
+            raise AttributeError(f"{name} is not a valid attribute.")
+        elif name in manager.attr_keys():
+            raise KeyError(f"Cannot use the reserved name {name} to assign a dynamic attribute!")
+        elif name in manager.hp_keys():
+            raise RuntimeError(f"Cannot register attribute {name} that is an hparam as dynamic.")
+        elif "." in name:
+            raise KeyError('dynamic attribute name can\'t contain "."')
+        elif name == "":
+            raise KeyError('dynamic attribute name can\'t be empty string ""')
+
+        # however we have to handle regular attributes and params/buffers separately. Specifically,
+        # params/buffers won't appear in self.__dict__ since they are already managed by pytorch
+        # internally.
+        if name in self.__dict__["_parameters"] or name in self.__dict__["_buffers"]:
+            # here the attribute is already managed, so simply storing a fake reference.
+            value = _pytorch_managed
+        elif name in self.__dict__:
+            # here, we retrieve the value and delete the attribute from the regular __dict__ so we
+            # manage it.
+            value = getattr(self, name)
+            delattr(self, name)
+        elif name in manager.da_keys():
+            # here we previously registered the attribute as dynamic
+            value = manager.get_da_value(name)
+        else:
+            raise RuntimeError(f"The value of attribute {name} cannot be retrieved correctly!")
+
+        # store new dynamic attribute
+        manager.set_da(name, value, callback)
+
+    def _register_temp_attribute(
+        self,
+        name: str,
+        val: Any,
+        set_hook: Optional[SetAttrHook] = None,
+        del_hook: Optional[DelAttrHook] = None,
+    ):
+        """Register a temporary attribute to the instance that is deleted upon the final export.
+
+        Args:
+            name: The name of the attribute.
+            val: The value of the attribute.
+            set_hook: A hook that is executed before the attribute is set (Defaults to
+                ``lambda m, n, v: None``).
+            del_hook: A hook that is executed before the attribute is deleted (Defaults to
+                ``lambda m, n: None``).
+
+        Any attribute that is set like this will be removed upon the final export, i.e., only when
+        the module is not a dynamic module anymore after export. This is helpful if you want to
+        store extra attributes in the class that are not dynamic attributes or hparams but should
+        still be removed upon the final export, e.g., attributes or buffers that may access during
+        the dynamic attribute callbacks.
+
+        Unlike dynamic attributes, this can be used to set attributes that did not exist before and
+        *cannot* be used to overwrite existing attributes.
+        """
+        # retrieve manager
+        manager = self._get_dm_attribute_manager()
+
+        # sanity checks
+        if hasattr(self, name):
+            raise KeyError(f"Attribute {name} already exists!")
+        elif isinstance(val, Hparam):
+            raise RuntimeError("Please register hparam directly via _register_hparam()!")
+
+        # store in manager and set new attribute
+        manager.set_attr(name, set_hook, del_hook)
+        setattr(self, name, val)
+
+    @torch.no_grad()
+    def export(self) -> nn.Module:
+        """Export self (a dynamic module) **in-place** and return the exported module.
+
+        The export process will remove the top-level dynamic module and replace it with the original
+        module class. Note that the original class may be either another type of dynamic module or
+        a vanilla nn.Module. Consequently, any methods (including properties) that are implemented
+        in the child class will be removed. Hparams as well as dynamic and temporary attributes are
+        handled in a special fashion, see below.
+
+        In order to ensure that the exported module is still consistent there a several mechanisms
+        in place to handle hparams, dynamic attributes, and temporary attributes:
+
+        * **Hparams** of the current type are replaced with their currently active value.
+            Note that we do not need to explicitly handle hparams of the parent class as they are
+            mutually-exlusive, i.e., hparams are unique across all levels of inheritance.
+        * **Dynamic Attributes** are handled depending on whether they exist in a parent class:
+            1. The same dynamic attribute exists in a parent class. In this case, the callback is
+               folded into ("appended to") the callback for the same dyanmic attribute of the parent
+               class. This way we ensure that the final value of the attribute remains consistent.
+            2. The dynamic attribute does not exist in a parent class. In this case, the attribute
+               is not dynamic anymore as there are no more callbacks that could affect the value.
+               Therefore, we simply overwrite the underlying original object with the current value
+               and revert it to a regular attribute.
+        * **Temporary Attributes** are kept until the final export, i.e., until the resultign class
+          is not a dynamic module anymore. This is to ensure that folded callbacks that may need
+          access to these attributes can still access them.
+        """
+        manager = self._get_dm_attribute_manager()
+
+        # dissolve dynamic attributes when their corresponding callbacks cannot be folded into the
+        # parent class callbacks anymore.
+        da_removable = manager.fold_cbs()
+        for k in da_removable:
+            val = getattr(self, k)
+            if isinstance(val, torch.Tensor):
+                val = val.detach().clone()
+                if k in self._parameters:
+                    val = Parameter(val)
+            manager.pop_da(k)  # remove dynamic attribute from manager
+            setattr(self, k, val)  # now we set it as regular attribute/parameter/buffer
+
+        # replace hparams with active values and delete removable attributes in last level
+        with self._dict_with_special():  # this way we avoid recursion issues
+            for k in manager.hp_keys(all=False):
+                val = getattr(self, k)
+                manager.pop_hp(k)
+                setattr(self, k, val)
+
+            # remove attributes in the last level
+            if manager.level <= 1:
+                for k in manager.attr_keys():
+                    delattr(self, k)
+
+        # pop original class and the whole level of inheritance
+        self.__class__ = manager.pop_level()
+
+        # double-check that any remaining dynamic attributes still work
+        for k in manager.da_keys():
+            try:
+                getattr(self, k)
+            except Exception as e:
+                raise RuntimeError(f"Dynamic attribute {k} unretrievable after export!") from e
+
+        # remove manager if not needed anymore and check whether it should still be a DynamicModule
+        is_dynamic = isinstance(self, DynamicModule)
+        if manager:
+            assert is_dynamic, "Exported module should still be a DynamicModule!"
+        else:
+            assert not is_dynamic, "Exported module must not be a DynamicModule anymore!"
+            delattr(self, "_dm_attribute_manager")
+
+        return self
+
+    @torch.no_grad()
+    def force_assign(self):
+        """Force re-assign all dynamic attributes to their current values.
+
+        .. warning::
+
+            Note that this method overwrittes the actual buffers and parameters! Only use in
+            specific circumstances!!
+        """
+        # force-reassign all dynamic attributes
+        for name in self._get_dm_attribute_manager().da_keys():
+            val = getattr(self, name)
+            if isinstance(val, torch.Tensor):
+                val = val.detach().clone()
+            if name in self._parameters:
+                val = val if val is None else Parameter(val)
+                self.register_parameter(name, val)
+            elif name in self._buffers:
+                self.register_buffer(name, val)
+            else:
+                setattr(self, name, val)
+
+    @classmethod
+    @torch.no_grad()
+    def convert(cls, module: nn.Module) -> "DynamicModule":
+        """Converts a module in-place into its dynamic counterpart by patching its class.
+
+        Args:
+            module: The module to be converted into a dynamic module.
+
+        Returns:
+            The converted dynamic module.
+
+        This should generally be a *final* method and child classes should inherit ``_setup()``
+        instead to customize the conversion process.
+
+        Patching is achieved by updating the ``__class__`` attribute of the module to its dynamic
+        counterpart. The dynamic counterpart is a subclass of the original class, hence, we ensure
+        the module is fully compatible with the original class. Simultaneously, we can inject the
+        corresponding dynamic behavior in a standardized and rigoruos fashion.
+        """
+        # update class
+        original_cls = type(module)
+        module.__class__ = cls
+        assert isinstance(module, cls), f"Failed to convert {original_cls} to {cls}!"  # for mypy
+
+        # setup/update the attribute manager
+        if issubclass(original_cls, DynamicModule):
+            assert hasattr(module, "_dm_attribute_manager"), "Attribute manager not found!"
+        else:
+            assert not hasattr(module, "_dm_attribute_manager"), "Attribute manager found!"
+            module._dm_attribute_manager = _DMAttributeManager()
+        module._dm_attribute_manager.append_level(original_cls)
+
+        # setup new hparams and dynamic attributes
+        module._setup()
+
+        return module
+
+    def _setup(self):
+        """Setup dynamic attributes and hparams after the convert call.
+
+        This method should be overriden by the child class!
+        """
+        raise NotImplementedError("_setup() must be implemented by child class!")
+
+    def modify(self, *args, **kwargs):
+        """Modify the module's dynamic choices in a standardized & scalable fashion.
+
+        This method can be overriden by the child class! While users can also directly modify
+        the choices of individual hparams, this method should provide a way to modify a batch of
+        dynamic modules with the same arguments, e.g., ``out_features_ratio`` for ``DynamicLinear``.
+
+        Note that arguments of the modify method that are exposed to the user via the rule system
+        should be specified as **keyword-only arguments**. When they are exposed as keyword-only
+        arguments, the `_DMRegistryCls` can automatically generate the corresponding config class
+        on the fly that lets user provide configs and then they are automatically validated before
+        being passed to the ``modify`` method.
+
+        If possible, modify()'s keyword arguments should have default values that leave the hparams
+        intact if not provided, e.g., one might call ``some_dynamic_module.modify()`` without any
+        arguments and the module will remain unchanged.
+        """
+        pass
+
+    def freeze(self):
+        """Restrict the hparams of tbe dynamic module to the orginal choices.
+
+        This is useful to enforce the behavior of the parent class.
+
+        .. note::
+
+            After this call, the module's hparams can no longer be modified although the underlying
+            type is still a dynamic module.
+        """
+        for _, hp in self.named_hparams(configurable=True):
+            hp.active = hp.original
+            hp.choices = [hp.original]
+
+    @contextmanager
+    def reset_dynamic_attributes(self):
+        """Context manager to temporarily remove any dynamic attributes and re-register values.
+
+        This context manager is intended to be used when we want to access a dynamic attribute in
+        its original unmodified version, i.e., without this class interfering with its original
+        value and its corresponding getattr/setattr/delattr behavior.
+
+        Upon exiting the context manager, the dynamic attributes are re-registered and the same
+        callbacks are re-registered together with the new value.
+        """
+        manager = self._get_dm_attribute_manager()
+
+        with manager.retain_cbs():
+            # pop all dynamic attributes and make sure they are accessible in their original form
+            da_keys = manager.da_keys()
+            for k in da_keys:
+                val = manager.pop_da(k)
+                if not hasattr(self, k):
+                    setattr(self, k, val)
+            assert not manager.da_keys(), "Dynamic attributes were not removed properly!"
+            try:
+                yield None
+            finally:
+                # re-register dynamic attributes with current values + dummy callback (--> will be
+                # thrown away!)
+                # NOTE that we respect if attributes were deleted entirely.
+                for k in da_keys:
+                    if hasattr(self, k):
+                        self._register_dynamic_attribute(k, lambda _, v: v)
+
+    @contextmanager
+    def _dict_with_special(self):
+        """Context manager that checks that __dict__ contains _modules, _parameters, _buffers."""
+        # build up set of special keys that we temporarily add to __dict__
+        manager = self._get_dm_attribute_manager(use_default=True)
+        nn_special = set()
+        for key in ["_modules", "_parameters", "_buffers"]:
+            if key not in self.__dict__ and (key in manager.da_keys() or key in manager.hp_keys()):
+                nn_special.add(key)
+
+        # temporarily add to __dict__
+        for key in nn_special:
+            self.__dict__[key] = getattr(self, key)
+        try:
+            yield None
+        finally:
+            # remove from __dict__
+            for key in nn_special:
+                del self.__dict__[key]
+
+    def __setattr__(self, name: str, value: Any):
+        """Set attr and specifically handle hparams as well as dynamic & temporary attributes."""
+        # retrieve manager
+        manager = self._get_dm_attribute_manager(use_default=True)
+
+        if isinstance(value, Hparam):
+            self._register_hparam(name, value)
+        elif name in manager.hp_keys():
+            manager.get_hp(name).active = value
+        elif name in manager.da_keys() and manager.get_da_value(name) is not _pytorch_managed:
+            manager.set_da(name, value)
+        else:
+            if name in manager.attr_keys():
+                manager.get_attr_set_hook(name)(self, name, value)
+            with self._dict_with_special():
+                super().__setattr__(name, value)
+
+    def __getattr__(self, name: str) -> Union[torch.Tensor, torch.nn.Module]:
+        """Get attr and specifically handle hparams as well as dynamic & temporary attributes."""
+        # retrieve manager
+        manager = self._get_dm_attribute_manager(use_default=True)
+
+        # check if we can get value from our hparams
+        if name in manager.hp_keys():
+            return manager.get_hp(name).active
+
+        # check for dynamic attributes
+        if name in manager.da_keys():
+            value = manager.get_da_value(name)
+            # we might also need to grab value from super call (pytorch managed)
+            value = super().__getattr__(name) if value is _pytorch_managed else value
+            # apply all callbacks in order
+            return manager.get_da_cb(name)(self, value)
+
+        # regular case
+        with self._dict_with_special():
+            attr = super().__getattr__(name)
+        return attr
+
+    def __delattr__(self, name: str):
+        """Del an attr and specifically handle hparams as well as dynamic & temporary attributes."""
+        manager = self._get_dm_attribute_manager(use_default=True)
+        if name in manager.da_keys():
+            manager.pop_da(name)
+            # check if it still exists
+            if hasattr(self, name):
+                delattr(self, name)
+        elif name in manager.hp_keys():
+            manager.pop_hp(name)
+        else:
+            if name in manager.attr_keys():
+                del_hook = manager.pop_attr(name)
+                del_hook(self, name)
+            return super().__delattr__(name)
+
+    def get_hparam(self, target: str) -> Hparam:
+        """Look up and return hparam (like "torch.nn.Module.get_parameter()" but for hparam)."""
+        return self._get_dm_attribute_manager().get_hp(target)
+
+    def named_hparams(self, configurable: Optional[bool] = None) -> Iterator[Tuple[str, Hparam]]:
+        """Return an iterator over all hparams of the module.
+
+        Args:
+            configurable: Whether to include configurable hparams.
+
+        Yields:
+            (name, Hparam): Tuple containing the name and hparam.
+
+        Default behavior is to iterate over configurable and non-configurable hparams. Set
+        ``configurable`` accordingly to only iterate over either. If ``configurable`` is set to
+        ``True``, only configurable symbols are iterated over. If ``configurable`` is set to
+        ``False``, configurable symbols are skipped over (only non-configurable symbols).
+        """
+        for hp_name, hp in self._get_dm_attribute_manager().named_hps():
+            if configurable is None or hp.is_configurable == configurable:
+                yield hp_name, hp
+
+    def extra_repr(self):
+        """Generate extra_repr making sure all dynamic keys exist in self.__dict__.
+
+        Pytorch heavily uses self.__dict__ to generate extra_repr. However, we remove certain
+        attributes from self.__dict__ so we can manage them dynamically. Temporarily, adding them
+        back in here and removing them again afterwards.
+        """
+        added_keys = set()
+        manager = self._get_dm_attribute_manager()
+        for name in chain(manager.hp_keys(), manager.da_keys()):
+            val = getattr(self, name)
+            if name in self.__dict__ or val is None:
+                continue
+            added_keys.add(name)
+            self.__dict__[name] = getattr(self, name)
+
+        # make super call
+        extra_repr = super().extra_repr()
+
+        # remove keys again
+        for key in added_keys:
+            del self.__dict__[key]
+
+        return extra_repr
+
+    @property
+    def original_cls(self) -> Type[nn.Module]:
+        """Return the original class of the dynamic module."""
+        return self._get_dm_attribute_manager().og_cls
+
+
+class _DMRegistryCls:
+    """A registry to keep track of available dynamic modules.
+
+    The registry can also dynamically generate new entries when we have a class that inherits from
+    the registered nn.Module.
+    """
+
+    T = TypeVar("T", bound=DynamicModule)
+
+    def __init__(self, prefix: str):
+        super().__init__()
+
+        self._prefix = prefix  # global prefix to use to register dynamic classes
+
+        self._registry: Dict[Type[nn.Module], Type[DynamicModule]] = {}  # registered classes
+        self._key_registry: Dict[Type[nn.Module], str] = {}  # registered str-keys for classes
+
+        self._dynamic_classes: Dict[Type[nn.Module], Type[DynamicModule]] = {}  # generated DMs
+        # generated ModeloptBaseRule classes for the dynamic classes
+        self._rule_classes: Dict[Type[DynamicModule], Type[ModeloptBaseRule]] = {}
+
+    def _generate_rule_class(self, dm_cls: Type[DynamicModule]) -> Type[ModeloptBaseRule]:
+        """Generate a ModeloptBaseRule type for the given dynamic module class."""
+        spec = inspect.getfullargspec(dm_cls.modify)
+        if spec.kwonlyargs:
+            assert spec.kwonlydefaults is not None, "modify method should have default arguments!"
+            fields = {n: (spec.annotations[n], spec.kwonlydefaults[n]) for n in spec.kwonlyargs}
+        else:
+            fields = {}
+        return create_model(f"{dm_cls.__name__}Config", __base__=ModeloptBaseRule, **fields)
+
+    def _get_dynamic_class_name(self, nn_cls: Type[nn.Module]) -> str:
+        """Generate a name for the dynamic class."""
+
+        def _is_occupied(name: str) -> bool:
+            """Check if a name is already occupied by a dynamic class."""
+            return any(name == dm.__name__ for dm in self._dynamic_classes.values())
+
+        # first try: simply prepend self._prefix to the nn_cls name but only if not occupied already
+        name = f"{self._prefix}{nn_cls.__name__}"
+        if not _is_occupied(name):
+            return name
+
+        # 2nd try: add self._prefix and module name to the nn_cls name (should never be occupied!)
+        name = f"{nn_cls.__module__.replace('.', '_')}_{self._prefix}{nn_cls.__name__}"
+        assert not _is_occupied(name), f"Dynamic class name {name} should not be occupied!"
+        return name
+
+    def _get_registered_nn_class(self, nn_cls: Type[nn.Module]) -> Optional[Type[nn.Module]]:
+        """Optionally return the nn module class that should be used to register nn_cls.
+
+        This method loops through the registry to see if there are any nn_cls matches, i.e., any
+        subclass with a shared forward method.
+        """
+        for nn_cls_ in self._registry:
+            if issubclass(nn_cls, nn_cls_) and nn_cls.forward is nn_cls_.forward:
+                return nn_cls_
+        return None
+
+    def __contains__(self, item: Union[nn.Module, Type[nn.Module], str]):
+        if isinstance(item, str):
+            return item in self._key_registry.values()
+        nn_cls = type(item) if isinstance(item, nn.Module) else item
+        return self._get_registered_nn_class(nn_cls) is not None
+
+    def __getitem__(self, nn_cls: Union[Type[nn.Module], str]) -> Type[DynamicModule]:
+        """Return the dynamic module class for the given nn_cls type or registered string."""
+        dm_cls = self.get(nn_cls)
+        if dm_cls is None:
+            raise KeyError(f"{nn_cls} is not registered for a dynamic module!")
+        return dm_cls
+
+    def get(
+        self, nn_cls: Union[Type[nn.Module], str], default: Any = None
+    ) -> Optional[Type[DynamicModule]]:
+        """Return the dynamic module class for the given nn_cls type or registered string."""
+        # check string case first
+        if isinstance(nn_cls, str):
+            for nn_cls_, key in self._key_registry.items():
+                if key == nn_cls:
+                    return self.get(nn_cls_, default)
+            return default
+
+        assert issubclass(nn_cls, nn.Module), f"{nn_cls} is not a subclass of nn.Module!"
+
+        # see if dynamic class is already available and return
+        if nn_cls in self._dynamic_classes:
+            return self._dynamic_classes[nn_cls]
+
+        # see if we can generate a new dynamic class and then return
+        nn_cls_ = self._get_registered_nn_class(nn_cls)
+        if nn_cls_:
+            name = self._get_dynamic_class_name(nn_cls)
+            dm_base_class = self._registry[nn_cls_]
+            dm_class = type(name, (dm_base_class, nn_cls), {})
+            self._dynamic_classes[nn_cls] = dm_class
+            self._rule_classes[dm_class] = self._generate_rule_class(dm_class)
+            return self.get(nn_cls, default)
+
+        # default return
+        return default
+
+    def get_key_from_dm(self, dm_cls: Union[Type[DynamicModule], DynamicModule]) -> str:
+        """Retrieve the key that is registered for a given dynamic module class."""
+        dm_cls = type(dm_cls) if isinstance(dm_cls, DynamicModule) else dm_cls
+        for nn_cls, dm_cls_ in self._dynamic_classes.items():
+            if dm_cls == dm_cls_:
+                return self.get_key(nn_cls)
+        raise KeyError(f"{dm_cls} is not registered for a dynamic module!")
+
+    def get_key(self, nn_cls: Union[Type[nn.Module], str]) -> str:
+        """Retrieve the key that is registered for a given nn module class."""
+        # sanity check
+        assert nn_cls in self, f"{nn_cls} is not registered for a dynamic module!"
+
+        # handle string case
+        if isinstance(nn_cls, str):
+            return nn_cls
+
+        # handle module case
+        nn_cls_ = self._get_registered_nn_class(nn_cls)
+        assert nn_cls_ is not None
+        return self._key_registry[nn_cls_]
+
+    def get_rule_class(self, nn_cls: Union[Type[nn.Module], str]) -> Type[ModeloptBaseRule]:
+        """Retrieve the rule config class that is registered for a given nn module class."""
+        dm_cls = self.get(nn_cls)
+        if dm_cls is None:
+            raise KeyError(f"{nn_cls} is not registered for a dynamic module!")
+        return self._rule_classes[dm_cls]
+
+    def register(self, cls_to_key: Dict[Type[nn.Module], str]) -> Callable[[Type[T]], Type[T]]:
+        """Use this to register a new dynamic base module.
+
+        Usage:
+
+        .. code-block:: python
+
+            @DMRegistry.register({nn.Linear: "nn.Linear"})
+            class DynamicLinear(DynamicModule):
+                ...
+
+        .. note::
+
+            The dynamic base module must **NOT** inherit from the nn.Module that is registered.
+            Instead the registry will automatically generate a new class that inherits from both
+            the dynamic class (``DynamicLinear`` above) and the nn.Module (``nn.Linear`` above) for
+            an MRO that corresponds to ``class AutoGenerated(DynamicLinear, nn.Linear): pass``.
+        """
+
+        def decorator(dm_class: Type[_DMRegistryCls.T]) -> Type[_DMRegistryCls.T]:
+            """Register dnn_class with appropriate nn_class."""
+            for nn_cls_, key in cls_to_key.items():
+                assert nn_cls_ not in self._registry, f"{nn_cls_} already registered!"
+                self._registry[nn_cls_] = dm_class
+                self._key_registry[nn_cls_] = key
+            return dm_class
+
+        return decorator
+
+    def unregister(self, nn_cls: Union[Type[nn.Module], Type[T]]) -> None:
+        """Unregister a previously registered dynamic base module and all its inherited modules.
+
+        It throws a KeyError if the dynamic base module is not registered.
+        """
+        # 0. sanity check
+        if nn_cls not in self._registry:
+            raise KeyError(f"{nn_cls} is not registered!")
+
+        # 1. unregister any generated dynamic classes
+        for nn_cls_ in list(self._dynamic_classes):
+            if nn_cls == self._get_registered_nn_class(nn_cls_):
+                dm_cls = self._dynamic_classes.pop(nn_cls_)
+                self._rule_classes.pop(dm_cls)
+
+        # 2. unregister the base dynamic class
+        self._registry.pop(nn_cls)
+        self._key_registry.pop(nn_cls)
+
+    def convert(self, nn_mod: nn.Module) -> DynamicModule:
+        """Converts the module into a dynamic module if registered or raise KeyError."""
+        return self[type(nn_mod)].convert(nn_mod)
+
+    @property
+    def prefix(self) -> str:
+        """Return the prefix used for the dynamic classes."""
+        return self._prefix
+
+
+class DynamicSpace:
+    """A class to represent all dynamic model choices over a model with multiple submodules."""
+
+    def __init__(self, model: nn.Module) -> None:
+        """Initialize the dynamic space from the model."""
+        self.model = model
+
+    def _should_be_converted(self, mod: nn.Module) -> bool:
+        """Check if the module should be converted."""
+        return True
+
+    def convert_to_dynamic(
+        self, rules: Optional[RulesDict], dm_registry: _DMRegistryCls
+    ) -> Dict[str, nn.Module]:
+        """Convert the model to dynamic modules according to the rules and provided registry.
+
+        Args:
+            rules: A dictionary containing rules for the dynamic modules.
+            dm_registry: A registry containing the dynamic modules to be converted to.
+
+        Returns:
+            A dictionary containing the converted modules with submodule names as keys and the
+            converted dynamic modules as values.
+        """
+        # check if the model is DataParallel
+        unwrap_model(self.model, raise_error=True)
+
+        # check that it's not channels last
+        assert not is_channels_last(self.model)
+
+        # 1. patch the model with dynamic units
+        # NOTE: duplicate modules are handled automatically since we convert/patch modules in-place
+        mods_converted: Dict[str, nn.Module] = {}
+        for name, mod in self.model.named_modules():
+            if mod in dm_registry and self._should_be_converted(mod):
+                dm_registry.convert(mod)
+                mods_converted[name] = mod
+
+        # 2. modify search space if rules are provided
+        if rules is None:
+            return mods_converted
+
+        # change all keys to strings
+        rules = {dm_registry.get_key(key): rule for key, rule in rules.items()}
+
+        # iterator through all dynamic modules and modify them according to rules
+        for mod_name, mod in mods_converted.items():
+            # get the key for this module that is used in the rules
+            key = dm_registry.get_key_from_dm(mod)
+
+            # validate + construct custom rules using the rule class stored in the dm_registry
+            rule_custom = dm_registry.get_rule_class(key).customize_rule(rules.get(key), mod_name)
+
+            if rule_custom is None:
+                # freeze module if no rule is provided
+                mod.freeze()
+            else:
+                # modify module according to rule
+                mod.modify(**rule_custom)
+
+        return mods_converted
+
+    def named_dynamic_modules(self) -> Iterator[Tuple[str, DynamicModule]]:
+        """Recursively yield the name and instance of *all* DynamicModules.
+
+        Yields:
+            (name, DynamicModule): Tuple containing the name and module.
+        """
+        for name, module in self.model.named_modules():
+            if isinstance(module, DynamicModule):
+                yield name, module
+
+    def is_dynamic(self) -> bool:
+        """Check if any module is dynamic.
+
+        Returns:
+            True if the model contains DynamicModule(s).
+        """
+        return any(True for _ in self.named_dynamic_modules())
+
+    def named_hparams(self, configurable: Optional[bool] = None) -> Iterator[Tuple[str, Hparam]]:
+        """Recursively yield the name and instance of *all* hparams.
+
+        Args:
+            configurable: Whether to include configurable hparams.
+
+        Yields:
+            (name, Hparam): Tuple containing the name and hparam.
+
+        Default behavior is to iterate over all hparams. If ``configurable`` is set to ``True``,
+        only configurable, non-duplicate symbols are iterated over.
+        """
+        _memo = set()
+        assert configurable in [None, True], "Only all or configurable hparams are supported!"
+        for mod_name, mod in self.named_dynamic_modules():
+            for hp_name, hp in mod.named_hparams(configurable=configurable):
+                if configurable is None or hp not in _memo:
+                    yield mod_name + ("." if mod_name else "") + hp_name, hp
+                    _memo.add(hp)
+
+    def get_hparam(self, name: str) -> Hparam:
+        """Get the hparam with the given name."""
+        mod_name, _, hp_name = name.rpartition(".")
+        mod = self.model.get_submodule(mod_name)
+        assert isinstance(mod, DynamicModule), f"Module {mod} must be a DynamicModule!"
+        return mod.get_hparam(hp_name)
+
+    def is_configurable(self) -> bool:
+        """Check if the model has any configurable hyperparameters.
+
+        Args:
+            model: A model to be checked for DynamicModule(s) with configurable hyperparameters.
+
+        Returns:
+            True if the model contains DynamicModule(s) with configurable hyperparameters w/ more
+            than one choice.
+        """
+        return any(True for _ in self.named_hparams(configurable=True))
+
+    def size(self) -> int:
+        """Get the search space size of the model.
+
+        Returns:
+            A int representing the search space size of the model.
+        """
+        space_size = 1
+        for _, hp in self.named_hparams(configurable=True):
+            space_size *= len(hp.choices)
+        return space_size
+
+    def config(self, configurable: Optional[bool] = None) -> Dict[str, Any]:
+        """Return the config dict of all hyperparameters.
+
+        Args:
+            model: A model that contains DynamicModule(s).
+            configurable: None -> all hps, True -> configurable hps, False -> non-configurable hps
+
+        Returns:
+            A dict of ``(parameter_name, choice)`` that specifies an active subnet.
+        """
+        return {name: hp.active for name, hp in self.named_hparams(configurable)}
+
+    def select(self, config: Dict[str, Any], strict: bool = True) -> None:
+        """Select the subnet provided by config.
+
+        If `strict` is set, then `config` must contain the exact set of keys representing both the
+        configurable and non-configurable hparams.
+        """
+        # check if we have any overlap with dependent keys
+        check_non_configurable = any(
+            name in config and not hp.is_configurable for name, hp in self.named_hparams()
+        )
+
+        # go through config, select based on current config, and track key errors
+        missing_keys = []
+        inconsistent_keys = []
+        unexpected_keys = {k: True for k in config.keys()}
+
+        # assign free/searchable hparams from config
+        configurables = dict(self.named_hparams(configurable=True))
+        for name, hparam in configurables.items():
+            if name in config:
+                hparam.active = config[name]
+                unexpected_keys[name] = False
+            elif strict:
+                missing_keys.append(name)
+
+        # do a sanity check on the provided dynamic keys
+        if check_non_configurable and strict:
+            for name, hparam in self.named_hparams():
+                if name in configurables:
+                    continue
+                if name not in config:
+                    missing_keys.append(name)
+                    continue
+                unexpected_keys[name] = False
+                if hparam.active != config[name]:
+                    inconsistent_keys.append(
+                        f"{name}: active={hparam.active}, config={config[name]}"
+                    )
+
+        # raise error for missing and unexpected keys if strict
+        unexpected_keys = [k for k, val in unexpected_keys.items() if val]
+        error_msg = ""
+        if strict and len(missing_keys) > 0:
+            error_msg += "\n\t".join(["Missing keys in config for:"] + missing_keys)
+            error_msg += "\nMake sure all keys are present in config or set strict=False."
+            raise RuntimeError(error_msg)
+        if strict and len(unexpected_keys) > 0:
+            error_msg += "\n\t".join(["Unexpected keys in config for:"] + unexpected_keys)
+            error_msg += "\nMake sure only required keys are in config or set strict=False."
+        if strict and len(inconsistent_keys) > 0:
+            error_msg += "\n\t".join(["Inconsistent keys in config:"] + inconsistent_keys)
+            error_msg += "\nMake sure keys in config are consistent or set strict=False."
+        if error_msg:
+            raise RuntimeError(f"Error in selecting config:\n{error_msg}")
+
+    def export(self, dm_registry: _DMRegistryCls) -> nn.Module:
+        """Recursively export the module including self and return the result.
+
+        Args:
+            dm_registry: A dynamic module registry to check for dynamic modules that should be
+                exported.
+
+        Returns:
+            The model after exporting the dynamic modules found in the registry.
+        """
+
+        def _recursive_export(mod: nn.Module) -> None:
+            """Recursively export the module."""
+            for n, m in mod.named_children():
+                if isinstance(m, DynamicModule) and m.original_cls in dm_registry:
+                    setattr(mod, n, m.export())  # re-assigning needed for DynamicParallelKDModule
+                _recursive_export(m)
+
+        if isinstance(self.model, DynamicModule):
+            self.model = self.model.export()
+        _recursive_export(self.model)
+
+        return self.model
+
+    def __repr__(self):
+        lines = [f"{type(self).__name__}("]
+        spaces = "  "
+        attr_line = [f"{x}={getattr(self, x)()}" for x in ["is_dynamic", "is_configurable", "size"]]
+        lines.append(spaces + ", ".join(attr_line) + ",")
+        model_lines = str(self.model).split("\n")
+        lines.extend([spaces + ("" if i else "model=") + x for i, x in enumerate(model_lines)])
+        lines.append(")")
+        return "\n".join(lines)
```

## modelopt/torch/opt/hparam.py

 * *Ordering differences only*

```diff
@@ -1,227 +1,227 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Standard hyperparameter class for regular symbol."""
-
-from contextlib import contextmanager
-from typing import Callable, Generator, List, Optional, Sequence, Tuple, Union
-
-import torch
-
-__all__ = ["Hparam", "HPType"]
-
-
-HPType = Union[Tuple[int, ...], int, float]
-
-
-class Hparam:
-    """A base hyperparameter of a DynamicModule.
-
-    An example of such a Hparam could be an hparam with identity dependencies.
-    """
-
-    Importance = Optional[torch.Tensor]
-    ImportanceEstimator = Callable[[], Importance]
-    ActiveSlice = Union[slice, torch.LongTensor]
-
-    def __init__(self, choices: Sequence[HPType], original: Optional[HPType] = None) -> None:
-        """Initializes Hparam with original value and choices."""
-        self._original = max(choices) if original is None else original
-        self._choices = sorted(set(choices) | {self.original})
-        self._active = self.original
-        self._is_configurable = True  # in case we want to manually overwritte configurability.
-
-        # Callback to compute hparam importance
-        self._importance_estimators: Optional[List[Hparam.ImportanceEstimator]] = [lambda: None]
-
-        # an optional order to enforce for active_slice
-        self._slice_order: Optional[torch.LongTensor] = None
-
-    def __iter__(self) -> Generator[HPType, None, None]:
-        """Iterate over choices."""
-        yield from self.choices
-
-    @property
-    def is_configurable(self):
-        """Return whether the hparam is configurable."""
-        return self._is_configurable and len(self.choices) > 1
-
-    @contextmanager
-    def _force_configurable(self):
-        """Context manager to temporarily set hparam to be configurable."""
-        original_value = self._is_configurable
-        self._is_configurable = True
-        yield
-        self._is_configurable = original_value
-
-    @property
-    def is_sortable(self):
-        """Return whether hparam in sortable."""
-        return self._importance_estimators is not None
-
-    @property
-    def active(self) -> HPType:
-        """Return the currently active value."""
-        return self._active
-
-    @active.setter
-    def active(self, val: Optional[HPType]):
-        """Set the active value with a sanity check for choices and dynamic hparams."""
-        val = self.original if val is None else val
-        assert val in self._choices, f"val = {val}, choices = {self.choices}"
-        if self.is_configurable:
-            self._active = val
-        else:
-            assert self._active == val
-
-    @property
-    def active_slice(self) -> ActiveSlice:
-        """Return the currently active sorted indices or slice corresponding to the active value."""
-        assert isinstance(self.active, int), "active_slice only supported for int hparams"
-        if self._slice_order is not None:
-            return self._slice_order[: self.active]
-        return slice(self.active)
-
-    @property
-    def choices(self) -> Sequence[HPType]:
-        """Return available choices."""
-        return self._choices
-
-    @choices.setter
-    def choices(self, val: Sequence[HPType]):
-        """Set available choices as subset of current choices (original must be preserved!)."""
-        # sanity checks
-        assert self.original in val, f"Original choice not in choices: {self.original} not in {val}"
-        assert self.active in val, f"Active choice not in choices: {self.active} not in {val}"
-        curr = set(self.choices)
-        val_set = set(val)
-
-        # update local choices
-        if self.is_configurable:
-            assert val_set <= curr, f"New choices must be a subset: current: {curr}, new: {val_set}"
-            self._choices = sorted(val_set)
-        else:
-            assert val_set == curr, f"Cannot update choices: current {curr}, new: {val_set}"
-
-    @property
-    def min(self) -> HPType:
-        """Return min value from among choices."""
-        return min(self.choices)
-
-    @property
-    def max(self) -> HPType:
-        """Return max value from among choices."""
-        return max(self.choices)
-
-    @property
-    def original(self) -> HPType:
-        """Return original value from among choices."""
-        return self._original
-
-    @property
-    @torch.no_grad()
-    def importance(self) -> Importance:
-        """Computes and returns the normalized importance among the features the hparam represents.
-
-        Note that the importance is represented as a 1d-tensor with the length equal to the max
-        choice (Hparam.max) of the hparam.
-
-        For example, if the hparam represents the number of in_channels to a Conv2d layer, the
-        importance should be a 1d-tensor of importance score with length equal to the number of
-        in_channels.
-
-        Note that each module should register appropriate importance callbacks to compute the
-        actual importance associated with the hparam choices. If there is no notion of importance
-        for the hparam, this function returns None.
-        """
-        # TODO: will we ever need a cycle detector here?
-        assert self.is_configurable
-        return self._get_importance() if self.is_sortable else None
-
-    # TODO: eventually importance should be registered by the search algorithm and _not_ by the
-    # module implementation. This is a temporary solution to get things working...
-    def register_importance(self, importance_estimator: ImportanceEstimator):
-        """Register importance estimator for the hparam.
-
-        This estimator does not take any arguments and should return a single argument (
-        optional 1d-tensor) representing the importance among features the hparam represents.
-        If the return argument is a tensor, the length of the tensor must be equal to the max choice
-        (Hparam.max) of the hparam.
-        """
-        if self._importance_estimators is None:  # use instead of self.is_sortable for mypy
-            raise RuntimeError("Cannot register importance for unsortable hparams.")
-        else:
-            self._importance_estimators.append(importance_estimator)
-
-    def _get_importance(self) -> Importance:
-        """Private method to retrieve importance without sanity checks."""
-        # iterate though all importance estimators
-        imps_all = []
-        for estimator in self._importance_estimators or []:
-            imp = estimator()
-            if imp is not None:
-                assert len(imp) == self.max, "Length of importance must be equal to max choice!"
-                imp = imp / (imp.max() + 1e-9)  # normalize importance
-                imps_all.append(imp)
-
-        return sum(imps_all) if imps_all else None
-
-    @torch.no_grad()
-    def enforce_order(self, order: Optional[torch.Tensor] = None) -> None:
-        """Store a reference to this order and enforce the order for active_slice.
-
-        This function enables the user to enforce an order how the active_slice is generated.
-
-        Example:
-            If the hparam has active value 16 and the max value is 32, the active_slice by default
-            will be ``slice(16)``, which is equivalent to ``range(16)`` (although faster).
-            When order is set active_slice will instead return ``self._order[:16]``.
-
-        TODO: will we ever need a cycle detector here?
-        """
-        if order is not None:
-            # convert to long (torch's index type)
-            order = order.long()
-
-            # check if the order is valid
-            assert torch.equal(
-                torch.arange(self.max, device=order.device), torch.sort(order)[0]
-            ), "order must be a permutation of range(self.max) to be valid!"
-
-        self._enforce_order(order)
-
-    def _enforce_order(self, order: Optional[torch.Tensor] = None) -> None:
-        """Private method to enforce order without sanity checks."""
-        self._slice_order = order
-
-    def __repr__(self) -> str:
-        """Return string representation with relevant properties of the class."""
-        attrs = ["choices", "active", "original"]
-        return f"{type(self).__name__}({', '.join(f'{x}={getattr(self, x)}' for x in attrs)})"
-
-    def __iand__(self, hp: "Hparam"):
-        """Merge another hparam into self."""
-        assert isinstance(hp, (Hparam)), f"Cannot merge {type(hp)} into {type(self)}!"
-
-        # merge choices
-        self.choices = sorted(set(self.choices) & set(hp.choices))
-
-        # remove slice order
-        self._slice_order = None
-
-        # merge importance estimators
-        if isinstance(self._importance_estimators, list) and isinstance(
-            hp._importance_estimators, list
-        ):
-            self._importance_estimators.extend(hp._importance_estimators)
-        else:
-            self._importance_estimators = None
-
-        return self
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Standard hyperparameter class for regular symbol."""
+
+from contextlib import contextmanager
+from typing import Callable, Generator, List, Optional, Sequence, Tuple, Union
+
+import torch
+
+__all__ = ["Hparam", "HPType"]
+
+
+HPType = Union[Tuple[int, ...], int, float]
+
+
+class Hparam:
+    """A base hyperparameter of a DynamicModule.
+
+    An example of such a Hparam could be an hparam with identity dependencies.
+    """
+
+    Importance = Optional[torch.Tensor]
+    ImportanceEstimator = Callable[[], Importance]
+    ActiveSlice = Union[slice, torch.LongTensor]
+
+    def __init__(self, choices: Sequence[HPType], original: Optional[HPType] = None) -> None:
+        """Initializes Hparam with original value and choices."""
+        self._original = max(choices) if original is None else original
+        self._choices = sorted(set(choices) | {self.original})
+        self._active = self.original
+        self._is_configurable = True  # in case we want to manually overwritte configurability.
+
+        # Callback to compute hparam importance
+        self._importance_estimators: Optional[List[Hparam.ImportanceEstimator]] = [lambda: None]
+
+        # an optional order to enforce for active_slice
+        self._slice_order: Optional[torch.LongTensor] = None
+
+    def __iter__(self) -> Generator[HPType, None, None]:
+        """Iterate over choices."""
+        yield from self.choices
+
+    @property
+    def is_configurable(self):
+        """Return whether the hparam is configurable."""
+        return self._is_configurable and len(self.choices) > 1
+
+    @contextmanager
+    def _force_configurable(self):
+        """Context manager to temporarily set hparam to be configurable."""
+        original_value = self._is_configurable
+        self._is_configurable = True
+        yield
+        self._is_configurable = original_value
+
+    @property
+    def is_sortable(self):
+        """Return whether hparam in sortable."""
+        return self._importance_estimators is not None
+
+    @property
+    def active(self) -> HPType:
+        """Return the currently active value."""
+        return self._active
+
+    @active.setter
+    def active(self, val: Optional[HPType]):
+        """Set the active value with a sanity check for choices and dynamic hparams."""
+        val = self.original if val is None else val
+        assert val in self._choices, f"val = {val}, choices = {self.choices}"
+        if self.is_configurable:
+            self._active = val
+        else:
+            assert self._active == val
+
+    @property
+    def active_slice(self) -> ActiveSlice:
+        """Return the currently active sorted indices or slice corresponding to the active value."""
+        assert isinstance(self.active, int), "active_slice only supported for int hparams"
+        if self._slice_order is not None:
+            return self._slice_order[: self.active]
+        return slice(self.active)
+
+    @property
+    def choices(self) -> Sequence[HPType]:
+        """Return available choices."""
+        return self._choices
+
+    @choices.setter
+    def choices(self, val: Sequence[HPType]):
+        """Set available choices as subset of current choices (original must be preserved!)."""
+        # sanity checks
+        assert self.original in val, f"Original choice not in choices: {self.original} not in {val}"
+        assert self.active in val, f"Active choice not in choices: {self.active} not in {val}"
+        curr = set(self.choices)
+        val_set = set(val)
+
+        # update local choices
+        if self.is_configurable:
+            assert val_set <= curr, f"New choices must be a subset: current: {curr}, new: {val_set}"
+            self._choices = sorted(val_set)
+        else:
+            assert val_set == curr, f"Cannot update choices: current {curr}, new: {val_set}"
+
+    @property
+    def min(self) -> HPType:
+        """Return min value from among choices."""
+        return min(self.choices)
+
+    @property
+    def max(self) -> HPType:
+        """Return max value from among choices."""
+        return max(self.choices)
+
+    @property
+    def original(self) -> HPType:
+        """Return original value from among choices."""
+        return self._original
+
+    @property
+    @torch.no_grad()
+    def importance(self) -> Importance:
+        """Computes and returns the normalized importance among the features the hparam represents.
+
+        Note that the importance is represented as a 1d-tensor with the length equal to the max
+        choice (Hparam.max) of the hparam.
+
+        For example, if the hparam represents the number of in_channels to a Conv2d layer, the
+        importance should be a 1d-tensor of importance score with length equal to the number of
+        in_channels.
+
+        Note that each module should register appropriate importance callbacks to compute the
+        actual importance associated with the hparam choices. If there is no notion of importance
+        for the hparam, this function returns None.
+        """
+        # TODO: will we ever need a cycle detector here?
+        assert self.is_configurable
+        return self._get_importance() if self.is_sortable else None
+
+    # TODO: eventually importance should be registered by the search algorithm and _not_ by the
+    # module implementation. This is a temporary solution to get things working...
+    def register_importance(self, importance_estimator: ImportanceEstimator):
+        """Register importance estimator for the hparam.
+
+        This estimator does not take any arguments and should return a single argument (
+        optional 1d-tensor) representing the importance among features the hparam represents.
+        If the return argument is a tensor, the length of the tensor must be equal to the max choice
+        (Hparam.max) of the hparam.
+        """
+        if self._importance_estimators is None:  # use instead of self.is_sortable for mypy
+            raise RuntimeError("Cannot register importance for unsortable hparams.")
+        else:
+            self._importance_estimators.append(importance_estimator)
+
+    def _get_importance(self) -> Importance:
+        """Private method to retrieve importance without sanity checks."""
+        # iterate though all importance estimators
+        imps_all = []
+        for estimator in self._importance_estimators or []:
+            imp = estimator()
+            if imp is not None:
+                assert len(imp) == self.max, "Length of importance must be equal to max choice!"
+                imp = imp / (imp.max() + 1e-9)  # normalize importance
+                imps_all.append(imp)
+
+        return sum(imps_all) if imps_all else None
+
+    @torch.no_grad()
+    def enforce_order(self, order: Optional[torch.Tensor] = None) -> None:
+        """Store a reference to this order and enforce the order for active_slice.
+
+        This function enables the user to enforce an order how the active_slice is generated.
+
+        Example:
+            If the hparam has active value 16 and the max value is 32, the active_slice by default
+            will be ``slice(16)``, which is equivalent to ``range(16)`` (although faster).
+            When order is set active_slice will instead return ``self._order[:16]``.
+
+        TODO: will we ever need a cycle detector here?
+        """
+        if order is not None:
+            # convert to long (torch's index type)
+            order = order.long()
+
+            # check if the order is valid
+            assert torch.equal(
+                torch.arange(self.max, device=order.device), torch.sort(order)[0]
+            ), "order must be a permutation of range(self.max) to be valid!"
+
+        self._enforce_order(order)
+
+    def _enforce_order(self, order: Optional[torch.Tensor] = None) -> None:
+        """Private method to enforce order without sanity checks."""
+        self._slice_order = order
+
+    def __repr__(self) -> str:
+        """Return string representation with relevant properties of the class."""
+        attrs = ["choices", "active", "original"]
+        return f"{type(self).__name__}({', '.join(f'{x}={getattr(self, x)}' for x in attrs)})"
+
+    def __iand__(self, hp: "Hparam"):
+        """Merge another hparam into self."""
+        assert isinstance(hp, (Hparam)), f"Cannot merge {type(hp)} into {type(self)}!"
+
+        # merge choices
+        self.choices = sorted(set(self.choices) & set(hp.choices))
+
+        # remove slice order
+        self._slice_order = None
+
+        # merge importance estimators
+        if isinstance(self._importance_estimators, list) and isinstance(
+            hp._importance_estimators, list
+        ):
+            self._importance_estimators.extend(hp._importance_estimators)
+        else:
+            self._importance_estimators = None
+
+        return self
```

## modelopt/torch/opt/mode.py

 * *Ordering differences only*

```diff
@@ -1,318 +1,318 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Interface and utilities for optimization modes/algorithms.
-
-A mode is a specific type or algorithms for model optimization, e.g., some type of algorithm for
-pruning or quantization. It can also specify a single step within an optimization algorithm instead
-of the whole algorithm. For example, a mode can prepare a model for pruning or export (i.e. fix the
-optimal model configuration) after pruning.
-
-Within ``modelopt``, a ``mode`` constitutes the unit for model optimization. We can define arbitrary
-modes, each mode gets recorded in the model's modelopt state dict, and we can define workflows as a
-sequence of modes.
-"""
-from abc import ABC, abstractmethod
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    List,
-    Optional,
-    Set,
-    Tuple,
-    Type,
-    TypeVar,
-    Union,
-)
-
-import torch.nn as nn
-
-from modelopt.torch.utils import val2list
-
-from .config import ConfigDict, ModeloptBaseConfig
-from .searcher import BaseSearcher
-
-MetadataDict = Dict[str, Any]  # metadata dict for one mode
-ModeConfigList = List[Tuple[str, ConfigDict]]  # config list for multiple modes
-ModeState = Dict[str, Union[ConfigDict, MetadataDict]]  # state dict for one mode
-
-ModeEntrypoint = Callable[
-    [nn.Module, ModeloptBaseConfig, MetadataDict], Tuple[nn.Module, MetadataDict]
-]
-ConvertReturnType = Tuple[nn.Module, MetadataDict]
-ConvertEntrypoint = Callable[[nn.Module, ModeloptBaseConfig], ConvertReturnType]
-RestoreEntrypoint = Callable[[nn.Module, ModeloptBaseConfig, MetadataDict], nn.Module]
-UpdateEntrypoint = Callable[[nn.Module, ModeloptBaseConfig, MetadataDict], None]
-
-__all__ = []
-
-
-class _ModeDescriptor(ABC):
-    """Abstract class to describe a mode."""
-
-    def __str__(self) -> str:
-        return str(self.name)
-
-    def __repr__(self) -> str:
-        return f"<{type(self).__name__}: '{self.name}'>"
-
-    def __hash__(self):
-        return hash(self.name)
-
-    @property
-    @abstractmethod
-    def name(self) -> str:
-        """Returns the string name of the mode."""
-
-    @property
-    @abstractmethod
-    def config_class(self) -> Type[ModeloptBaseConfig]:
-        """Specifies the config class for the mode."""
-
-    @property
-    def next_modes(self) -> Optional[Set[str]]:
-        """Modes that must immediately follow this mode.
-
-        Certain modes only makes sense if they are followed by certain other modes.
-
-        An empty set indicates that _no_ mode can follow this mode. A None value indicates that
-        there are no restrictions on the following mode.
-
-        Returns:
-            A set of mode names that must immediately follow this mode. Defaults to None.
-        """
-        return None
-
-    @property
-    def export_mode(self) -> Optional[str]:
-        """The mode that corresponds to the export mode of this mode.
-
-        Certain modes require a subsequent export step. For example, after pruning, we might want to
-        fine-tune the model and then export the model. This property specifies that mode if it
-        exists.
-
-        None indicates that there exists no such mode.
-
-        Returns:
-            The (optional) mode name that corresponds to the export mode of this mode. Defaults to
-            None.
-        """
-        return None
-
-    @property
-    def is_export_mode(self) -> bool:
-        """Whether the mode is an export mode.
-
-        Returns:
-            True if the mode is an export mode, False otherwise. Defaults to False.
-        """
-        return False
-
-    @property
-    def search_algorithm(self) -> Type[BaseSearcher]:
-        """Specifies the search algorithm to use for this mode (if any)."""
-        raise RuntimeError("Search is not supported for this mode.")
-
-    @property
-    @abstractmethod
-    def convert(self) -> ConvertEntrypoint:
-        """The mode's entrypoint for converting a model.
-
-        The function signature of the convert entrypoint is described below:
-
-        Args:
-            model: Model to be restored.
-            config: Config used for the model that was also used during convert.
-
-        Returns:
-            A tuple consisting of
-                1.  the in-place modified model. If the modification failed, the entrypoint can
-                    return None instead
-                2.  The config dict that can be used to call the restore entrypoint to instantly *restore*
-                    the modified model.
-                3.  The metatdata that can be used to call the restore entrypoint to instantly
-                    *restore* the modified model from the provided initial state, see below's
-                    description for the restore entrypoint to get more info about ``metadata``.
-
-        Raises:
-            :meth:`ApplyModeError<modelopt.torch.opt._conversion.ApplyModeError>` to indicate that the
-            conversion process failed. This error can be caught by user-facing APIs if they want to
-            enable a fall-back behavior.
-        """
-
-    @property
-    @abstractmethod
-    def restore(self) -> RestoreEntrypoint:
-        """The mode's entrypoint for restoring a model.
-
-        The function signature of the restore entrypoint is described below:
-
-        Args:
-            model: Model to be restored.
-            config: Config used for the model that was also used during convert.
-            metadata: The metadata is used during restoration of the model architecture to instantly
-                restore the modified model. The metadata is used on top of the config to ensure that
-                the model can be instantly restored/modified from the provided state. This is
-                helpful when the ``convert`` entrypoint contains non-deterministic operations whose
-                outcome can be stored in the metadata to ensure that the model can be restored
-                reliably. A few examples of potential non-deterministic operations are provided
-                below:
-                    * Latency measurements: if the conversion leverages latency measurements during
-                      conversion the conversion process may become non-deterministic.
-                    * Random operations: if the conversion leverages random operations during
-                      conversion, we should store the samples or random seed.
-                    * Module's train flag: the conversion process might be affected by the module's
-                      train flag (e.g. tracing is indirectly affected by train flag since the
-                      forward may be affected by the train flag). If so, we should store the train
-                      flag in the metadata and set the model into the correct mode.
-
-        Returns:
-            The in-place modified and restored model. If the modification failed, the entrypoint can
-                    return None instead
-
-        Raises:
-            :meth:`ApplyModeError<modelopt.torch.opt._conversion.ApplyModeError>` to indicate that the
-            conversion process failed. This error can be caught by user-facing APIs if they want to
-            enable a fall-back behavior.
-        """
-
-    @property
-    def update_for_save(self) -> UpdateEntrypoint:
-        """The mode's (optional) entrypoint for updating a model's config and metadata before saving.
-
-        This is useful if metadata or config needs to be updated for saving (and restoring) the mode.
-
-        The function signature of this update entrypoint is described below:
-
-        Args:
-            model: Model to be restored.
-            config: The config as described above. It should be modified IN-PLACE.
-            metadata: The metadata as described above. It should be modified IN-PLACE.
-
-        Returns:
-            None.
-        """
-        return lambda model, config, metadata: None
-
-    @property
-    def update_for_new_mode(self) -> UpdateEntrypoint:
-        """The mode's (optional) entrypoint for updating a model's config and metadata before a new mode.
-
-        This is useful if metadata or config needs to be updated before adding a new mode. For example, a
-        after adding a new mode, the current mode's restore might only need a subset of the metadata/config.
-
-        The function signature of this update entrypoint is described below:
-
-        Args:
-            model: Model to be restored.
-            config: The config as described above. It should be modified IN-PLACE.
-            metadata: The metadata as described above. It should be modified IN-PLACE.
-
-        Returns:
-            None.
-        """
-        return lambda model, config, metadata: None
-
-    @property
-    def require_model_like(self) -> bool:
-        """Whether the mode requires a ModelLike input?
-
-        Returns:
-            False
-        """
-        return False
-
-
-ModeType = Union[_ModeDescriptor, str]
-ModeLike = Union[ModeType, List[ModeType], ModeConfigList]
-
-
-class _ModeRegistryCls:
-    """A registry to keep track of available modes."""
-
-    T = TypeVar("T", bound=_ModeDescriptor)
-
-    # global list to keep track of all registries we initialize
-    _all_registries: List["_ModeRegistryCls"] = []
-
-    def __init__(self) -> None:
-        """Initialize the registry with the lookup dictionaries."""
-        self._name2descriptor: Dict[str, _ModeDescriptor] = {}
-        self._all_registries.append(self)
-
-    def register_mode(self, cls_descriptor: Type[T]) -> Type[T]:
-        """Register a new mode with the given descriptor."""
-        # initialize descriptor and get name
-        descriptor = cls_descriptor()
-        name = descriptor.name
-
-        # check if we have a descriptor instance already and use that instance instead.
-        if self.contained_in_any(name):
-            descriptor = self.get_from_any(name)
-
-        # check if mode_name/value is already taken
-        if name in self._name2descriptor:
-            raise ValueError(f"Mode {name} already registered: {self._name2descriptor}")
-
-        # register mode
-        self._name2descriptor[name] = descriptor
-        return cls_descriptor
-
-    def remove_mode(self, mode: ModeType) -> None:
-        """Remove a mode from the registry."""
-        # remove mode
-        del self._name2descriptor[str(mode)]
-
-    def get(self, mode: ModeType) -> Optional[_ModeDescriptor]:
-        """Get the mode by value or throw an error."""
-        return self._name2descriptor.get(str(mode))
-
-    def __getitem__(self, mode: ModeType) -> _ModeDescriptor:
-        """Get the mode by value or throw an error."""
-        return self._name2descriptor[str(mode)]
-
-    def __contains__(self, mode: ModeType) -> bool:
-        """Check if mode is registered in this registry."""
-        return str(mode) in self._name2descriptor
-
-    def __del__(self) -> None:
-        """Remove the registry from the global list."""
-        self._all_registries.remove(self)
-
-    @classmethod
-    def contained_in_any(cls, mode: ModeType) -> bool:
-        """Check if mode is registered in any registry."""
-        for registry in cls._all_registries:
-            if str(mode) in registry._name2descriptor:
-                return True
-        return False
-
-    @classmethod
-    def get_from_any(cls, mode: ModeType) -> _ModeDescriptor:
-        """Get the mode by value from any registry or throw a KeyError.
-
-        Adds a sanity check to ensure that the mode is not ambiguous, i.e., there is only one
-        instance.
-        """
-        mode_ds = [registry[mode] for registry in cls._all_registries if mode in registry]
-        if not mode_ds:
-            raise KeyError(f"Mode {mode} not found in any registry.")
-        assert all(mode_ds[0] == m_d for m_d in mode_ds), f"Mode {mode} is ambiguous."
-        return mode_ds[0]
-
-
-def get_mode_config(mode_like: ModeLike) -> ModeConfigList:
-    """Standardize mode to ModeConfigDict and return."""
-    mode_and_config = [
-        ((m, {}) if isinstance(m, str) else (m[0], m[1] or {})) for m in val2list(mode_like)
-    ]
-
-    return mode_and_config
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Interface and utilities for optimization modes/algorithms.
+
+A mode is a specific type or algorithms for model optimization, e.g., some type of algorithm for
+pruning or quantization. It can also specify a single step within an optimization algorithm instead
+of the whole algorithm. For example, a mode can prepare a model for pruning or export (i.e. fix the
+optimal model configuration) after pruning.
+
+Within ``modelopt``, a ``mode`` constitutes the unit for model optimization. We can define arbitrary
+modes, each mode gets recorded in the model's modelopt state dict, and we can define workflows as a
+sequence of modes.
+"""
+from abc import ABC, abstractmethod
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    List,
+    Optional,
+    Set,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
+
+import torch.nn as nn
+
+from modelopt.torch.utils import val2list
+
+from .config import ConfigDict, ModeloptBaseConfig
+from .searcher import BaseSearcher
+
+MetadataDict = Dict[str, Any]  # metadata dict for one mode
+ModeConfigList = List[Tuple[str, ConfigDict]]  # config list for multiple modes
+ModeState = Dict[str, Union[ConfigDict, MetadataDict]]  # state dict for one mode
+
+ModeEntrypoint = Callable[
+    [nn.Module, ModeloptBaseConfig, MetadataDict], Tuple[nn.Module, MetadataDict]
+]
+ConvertReturnType = Tuple[nn.Module, MetadataDict]
+ConvertEntrypoint = Callable[[nn.Module, ModeloptBaseConfig], ConvertReturnType]
+RestoreEntrypoint = Callable[[nn.Module, ModeloptBaseConfig, MetadataDict], nn.Module]
+UpdateEntrypoint = Callable[[nn.Module, ModeloptBaseConfig, MetadataDict], None]
+
+__all__ = []
+
+
+class _ModeDescriptor(ABC):
+    """Abstract class to describe a mode."""
+
+    def __str__(self) -> str:
+        return str(self.name)
+
+    def __repr__(self) -> str:
+        return f"<{type(self).__name__}: '{self.name}'>"
+
+    def __hash__(self):
+        return hash(self.name)
+
+    @property
+    @abstractmethod
+    def name(self) -> str:
+        """Returns the string name of the mode."""
+
+    @property
+    @abstractmethod
+    def config_class(self) -> Type[ModeloptBaseConfig]:
+        """Specifies the config class for the mode."""
+
+    @property
+    def next_modes(self) -> Optional[Set[str]]:
+        """Modes that must immediately follow this mode.
+
+        Certain modes only makes sense if they are followed by certain other modes.
+
+        An empty set indicates that _no_ mode can follow this mode. A None value indicates that
+        there are no restrictions on the following mode.
+
+        Returns:
+            A set of mode names that must immediately follow this mode. Defaults to None.
+        """
+        return None
+
+    @property
+    def export_mode(self) -> Optional[str]:
+        """The mode that corresponds to the export mode of this mode.
+
+        Certain modes require a subsequent export step. For example, after pruning, we might want to
+        fine-tune the model and then export the model. This property specifies that mode if it
+        exists.
+
+        None indicates that there exists no such mode.
+
+        Returns:
+            The (optional) mode name that corresponds to the export mode of this mode. Defaults to
+            None.
+        """
+        return None
+
+    @property
+    def is_export_mode(self) -> bool:
+        """Whether the mode is an export mode.
+
+        Returns:
+            True if the mode is an export mode, False otherwise. Defaults to False.
+        """
+        return False
+
+    @property
+    def search_algorithm(self) -> Type[BaseSearcher]:
+        """Specifies the search algorithm to use for this mode (if any)."""
+        raise RuntimeError("Search is not supported for this mode.")
+
+    @property
+    @abstractmethod
+    def convert(self) -> ConvertEntrypoint:
+        """The mode's entrypoint for converting a model.
+
+        The function signature of the convert entrypoint is described below:
+
+        Args:
+            model: Model to be restored.
+            config: Config used for the model that was also used during convert.
+
+        Returns:
+            A tuple consisting of
+                1.  the in-place modified model. If the modification failed, the entrypoint can
+                    return None instead
+                2.  The config dict that can be used to call the restore entrypoint to instantly *restore*
+                    the modified model.
+                3.  The metatdata that can be used to call the restore entrypoint to instantly
+                    *restore* the modified model from the provided initial state, see below's
+                    description for the restore entrypoint to get more info about ``metadata``.
+
+        Raises:
+            :meth:`ApplyModeError<modelopt.torch.opt._conversion.ApplyModeError>` to indicate that the
+            conversion process failed. This error can be caught by user-facing APIs if they want to
+            enable a fall-back behavior.
+        """
+
+    @property
+    @abstractmethod
+    def restore(self) -> RestoreEntrypoint:
+        """The mode's entrypoint for restoring a model.
+
+        The function signature of the restore entrypoint is described below:
+
+        Args:
+            model: Model to be restored.
+            config: Config used for the model that was also used during convert.
+            metadata: The metadata is used during restoration of the model architecture to instantly
+                restore the modified model. The metadata is used on top of the config to ensure that
+                the model can be instantly restored/modified from the provided state. This is
+                helpful when the ``convert`` entrypoint contains non-deterministic operations whose
+                outcome can be stored in the metadata to ensure that the model can be restored
+                reliably. A few examples of potential non-deterministic operations are provided
+                below:
+                    * Latency measurements: if the conversion leverages latency measurements during
+                      conversion the conversion process may become non-deterministic.
+                    * Random operations: if the conversion leverages random operations during
+                      conversion, we should store the samples or random seed.
+                    * Module's train flag: the conversion process might be affected by the module's
+                      train flag (e.g. tracing is indirectly affected by train flag since the
+                      forward may be affected by the train flag). If so, we should store the train
+                      flag in the metadata and set the model into the correct mode.
+
+        Returns:
+            The in-place modified and restored model. If the modification failed, the entrypoint can
+                    return None instead
+
+        Raises:
+            :meth:`ApplyModeError<modelopt.torch.opt._conversion.ApplyModeError>` to indicate that the
+            conversion process failed. This error can be caught by user-facing APIs if they want to
+            enable a fall-back behavior.
+        """
+
+    @property
+    def update_for_save(self) -> UpdateEntrypoint:
+        """The mode's (optional) entrypoint for updating a model's config and metadata before saving.
+
+        This is useful if metadata or config needs to be updated for saving (and restoring) the mode.
+
+        The function signature of this update entrypoint is described below:
+
+        Args:
+            model: Model to be restored.
+            config: The config as described above. It should be modified IN-PLACE.
+            metadata: The metadata as described above. It should be modified IN-PLACE.
+
+        Returns:
+            None.
+        """
+        return lambda model, config, metadata: None
+
+    @property
+    def update_for_new_mode(self) -> UpdateEntrypoint:
+        """The mode's (optional) entrypoint for updating a model's config and metadata before a new mode.
+
+        This is useful if metadata or config needs to be updated before adding a new mode. For example, a
+        after adding a new mode, the current mode's restore might only need a subset of the metadata/config.
+
+        The function signature of this update entrypoint is described below:
+
+        Args:
+            model: Model to be restored.
+            config: The config as described above. It should be modified IN-PLACE.
+            metadata: The metadata as described above. It should be modified IN-PLACE.
+
+        Returns:
+            None.
+        """
+        return lambda model, config, metadata: None
+
+    @property
+    def require_model_like(self) -> bool:
+        """Whether the mode requires a ModelLike input?
+
+        Returns:
+            False
+        """
+        return False
+
+
+ModeType = Union[_ModeDescriptor, str]
+ModeLike = Union[ModeType, List[ModeType], ModeConfigList]
+
+
+class _ModeRegistryCls:
+    """A registry to keep track of available modes."""
+
+    T = TypeVar("T", bound=_ModeDescriptor)
+
+    # global list to keep track of all registries we initialize
+    _all_registries: List["_ModeRegistryCls"] = []
+
+    def __init__(self) -> None:
+        """Initialize the registry with the lookup dictionaries."""
+        self._name2descriptor: Dict[str, _ModeDescriptor] = {}
+        self._all_registries.append(self)
+
+    def register_mode(self, cls_descriptor: Type[T]) -> Type[T]:
+        """Register a new mode with the given descriptor."""
+        # initialize descriptor and get name
+        descriptor = cls_descriptor()
+        name = descriptor.name
+
+        # check if we have a descriptor instance already and use that instance instead.
+        if self.contained_in_any(name):
+            descriptor = self.get_from_any(name)
+
+        # check if mode_name/value is already taken
+        if name in self._name2descriptor:
+            raise ValueError(f"Mode {name} already registered: {self._name2descriptor}")
+
+        # register mode
+        self._name2descriptor[name] = descriptor
+        return cls_descriptor
+
+    def remove_mode(self, mode: ModeType) -> None:
+        """Remove a mode from the registry."""
+        # remove mode
+        del self._name2descriptor[str(mode)]
+
+    def get(self, mode: ModeType) -> Optional[_ModeDescriptor]:
+        """Get the mode by value or throw an error."""
+        return self._name2descriptor.get(str(mode))
+
+    def __getitem__(self, mode: ModeType) -> _ModeDescriptor:
+        """Get the mode by value or throw an error."""
+        return self._name2descriptor[str(mode)]
+
+    def __contains__(self, mode: ModeType) -> bool:
+        """Check if mode is registered in this registry."""
+        return str(mode) in self._name2descriptor
+
+    def __del__(self) -> None:
+        """Remove the registry from the global list."""
+        self._all_registries.remove(self)
+
+    @classmethod
+    def contained_in_any(cls, mode: ModeType) -> bool:
+        """Check if mode is registered in any registry."""
+        for registry in cls._all_registries:
+            if str(mode) in registry._name2descriptor:
+                return True
+        return False
+
+    @classmethod
+    def get_from_any(cls, mode: ModeType) -> _ModeDescriptor:
+        """Get the mode by value from any registry or throw a KeyError.
+
+        Adds a sanity check to ensure that the mode is not ambiguous, i.e., there is only one
+        instance.
+        """
+        mode_ds = [registry[mode] for registry in cls._all_registries if mode in registry]
+        if not mode_ds:
+            raise KeyError(f"Mode {mode} not found in any registry.")
+        assert all(mode_ds[0] == m_d for m_d in mode_ds), f"Mode {mode} is ambiguous."
+        return mode_ds[0]
+
+
+def get_mode_config(mode_like: ModeLike) -> ModeConfigList:
+    """Standardize mode to ModeConfigDict and return."""
+    mode_and_config = [
+        ((m, {}) if isinstance(m, str) else (m[0], m[1] or {})) for m in val2list(mode_like)
+    ]
+
+    return mode_and_config
```

## modelopt/torch/opt/searcher.py

 * *Ordering differences only*

```diff
@@ -1,254 +1,254 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Standard interface to implement a searcher algorithm.
-
-A searcher is useful whenever we want to search/optimize over a set of hyperparameters in the model.
-Searchers are usually used in conjunction with a mode, which can define a search space via its
-entrypoints, i.e., convert the model into a search space. The searcher then optimizes over this
-search space.
-"""
-
-import copy
-import os
-from abc import ABC, abstractmethod
-from contextlib import nullcontext
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Optional,
-    Tuple,
-    Union,
-    final,
-)
-
-import torch
-import torch.nn as nn
-
-from modelopt.torch.utils import distributed as dist
-from modelopt.torch.utils import (
-    no_stdout,
-    run_forward_loop,
-)
-
-LimitsTuple = Tuple[float, float]
-ConstraintsDict = Dict[str, Union[str, float, None]]
-ForwardLoop = Callable[[nn.Module], None]
-ScoreFunc = Callable[[nn.Module], float]
-
-SearchConfig = Dict[str, Any]  # config dict for searcher
-SearchStateDict = Dict[str, Any]  # state dict for searcher
-
-__all__ = ["BaseSearcher"]
-
-
-class BaseSearcher(ABC):
-    """A basic search interface that can be used to search/optimize a model.
-
-    The base interface supports basic features like setting up a search, checkpointing, and
-    loading logic and defines a minimal workflow to follow.
-    """
-
-    model: nn.Module
-    config: SearchConfig
-    constraints: ConstraintsDict
-    dummy_input: Union[Any, Tuple]
-    forward_loop: Optional[ForwardLoop]
-
-    @final
-    def __init__(self) -> None:
-        """We don't allow to override __init__ method."""
-        super().__init__()
-
-    # TODO: see if we really want to keep all the config here.
-    @property
-    def default_search_config(self) -> SearchConfig:
-        """Get the default config for the searcher."""
-        return {
-            "checkpoint": None,
-            "verbose": dist.is_master(),
-            "forward_loop": None,
-            "data_loader": None,
-            "collect_func": None,
-            "max_iter_data_loader": None,
-            "score_func": None,
-            "loss_func": None,
-        }
-
-    @property
-    @abstractmethod
-    def default_state_dict(self) -> SearchStateDict:
-        """Return default state dict."""
-
-    def sanitize_search_config(self, config: Optional[SearchConfig]) -> SearchConfig:
-        """Sanitize the search config dict."""
-        # supply with defaults (for verbose we wanna make sure it's on master only)
-        config = {**self.default_search_config, **(config or {})}
-        config["verbose"] = config["verbose"] and self.default_search_config["verbose"]
-
-        # sanity checks
-        assert (
-            config.keys() == self.default_search_config.keys()
-        ), f"Unexpected config keys: {config.keys() - self.default_search_config.keys()}"
-        assert config["score_func"] is not None, "Please provide `score_func`!"
-
-        # return
-        return config
-
-    # TODO: double-check if we want all these args here.
-    @final
-    def search(
-        self,
-        model: nn.Module,
-        constraints: ConstraintsDict,
-        dummy_input: Union[Any, Tuple],
-        config: Optional[SearchConfig] = None,
-    ) -> SearchStateDict:
-        """Search a given prunable model for the best sub-net and return the search model.
-
-        The best sub-net maximizes the score given by ``score_func`` while satisfying the
-        ``constraints``.
-
-        Args:
-            model: The converted model to be searched.
-            constraints: The dictionary from constraint name to upper bound the searched model has
-                to satisfy.
-            dummy_input: Arguments of ``model.forward()``. This is used for exporting and
-                calculating inference-based metrics, such as latency/FLOPs. The format of
-                ``dummy_inputs`` follows the convention of the ``args`` argument in
-                `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_.
-            config: Additional optional arguments to configure the search.
-
-        Returns: A tuple (subnet, state_dict) where
-            subnet is the searched subnet (nn.Module), which can be used for subsequent tasks like
-            fine-tuning, state_dict contains the history and detailed stats of the search procedure.
-        """
-        # check model train state
-        is_training = model.training
-
-        # reset the search
-        self.reset_search()
-
-        # update and initialize searcher
-        self.model = model
-        self.config = self.sanitize_search_config(config)
-        self.constraints = constraints
-        self.dummy_input = dummy_input
-        self.forward_loop = self.construct_forward_loop(silent=not self.config["verbose"])
-
-        # load checkpoint if it exists
-        self.load_search_checkpoint()
-
-        # run initial step and sanity checks before the search
-        self.before_search()
-
-        # run actual search
-        self.run_search()
-
-        # run clean-up steps after search
-        self.after_search()
-
-        # make sure model is in original state
-        model.train(is_training)
-
-        # return the config for the best result
-        return self.best
-
-    def reset_search(self) -> None:
-        """Reset search at the beginning."""
-        # reset self.best where we store results
-        self.best: SearchStateDict = {}
-
-        # reset state dict (do it afterwards in case best is in state_dict)
-        for key, val in self.default_state_dict.items():
-            setattr(self, key, copy.deepcopy(val))
-
-    def before_search(self) -> None:
-        """Optional pre-processing steps before the search."""
-        pass
-
-    @abstractmethod
-    def run_search(self) -> None:
-        """Run actual search."""
-
-    def after_search(self) -> None:
-        """Optional post-processing steps after the search."""
-        pass
-
-    @property
-    def has_score(self) -> bool:
-        """Check if the model has a score function."""
-        return self.config["score_func"] is not None
-
-    def eval_score(self, silent=True) -> float:
-        """Optionally silent evaluation of the score function."""
-        assert self.has_score, "Please provide `score_func`!"
-        score_func: ScoreFunc = self.config["score_func"]
-        with no_stdout() if silent else nullcontext():
-            return float(score_func(self.model))
-
-    def construct_forward_loop(self, silent=True) -> Optional[ForwardLoop]:
-        """Get runnable forward loop on the model using the provided configs."""
-        # check config
-        data_loader = self.config["data_loader"]
-        forward_loop = self.config["forward_loop"]
-        assert None in [data_loader, forward_loop], "Only provide `data_loader` or `forward_loop`!"
-
-        # check trivial case
-        if not (data_loader or forward_loop):
-            return None
-
-        def forward_loop_with_silence_check(m: nn.Module) -> None:
-            with no_stdout() if silent else nullcontext():
-                if data_loader is not None:
-                    run_forward_loop(
-                        m,
-                        data_loader=data_loader,
-                        max_iters=self.config["max_iter_data_loader"],
-                        collect_func=self.config["collect_func"],
-                    )
-                elif forward_loop is not None:
-                    forward_loop(m)
-
-        return forward_loop_with_silence_check
-
-    @final
-    def state_dict(self) -> SearchStateDict:
-        """The state dictionary that can be stored/loaded."""
-        return {key: getattr(self, key) for key in self.default_state_dict}
-
-    def load_search_checkpoint(self) -> bool:
-        """Load function for search checkpoint returning indicator whether checkpoint was loaded."""
-        # check if checkpoint exists
-        checkpoint: Optional[str] = self.config["checkpoint"]
-        if checkpoint is None or not os.path.exists(checkpoint):
-            return False
-
-        # iterate through state dict and load keys
-        print(f"Loading searcher state from {checkpoint}...")
-        state_dict = torch.load(checkpoint)
-        assert state_dict.keys() == self.state_dict().keys(), "Keys in checkpoint don't match!"
-        for key, state in state_dict.items():
-            setattr(self, key, state)
-        return True
-
-    def save_search_checkpoint(self) -> None:
-        """Save function for search checkpoint."""
-        # check if save requirements are satisfied
-        checkpoint: Optional[str] = self.config["checkpoint"]
-        if checkpoint is None or not dist.is_master():
-            return
-
-        # save state dict
-        save_dirname, _ = os.path.split(checkpoint)
-        if save_dirname:
-            os.makedirs(save_dirname, exist_ok=True)
-        torch.save(self.state_dict(), checkpoint)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Standard interface to implement a searcher algorithm.
+
+A searcher is useful whenever we want to search/optimize over a set of hyperparameters in the model.
+Searchers are usually used in conjunction with a mode, which can define a search space via its
+entrypoints, i.e., convert the model into a search space. The searcher then optimizes over this
+search space.
+"""
+
+import copy
+import os
+from abc import ABC, abstractmethod
+from contextlib import nullcontext
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Optional,
+    Tuple,
+    Union,
+    final,
+)
+
+import torch
+import torch.nn as nn
+
+from modelopt.torch.utils import distributed as dist
+from modelopt.torch.utils import (
+    no_stdout,
+    run_forward_loop,
+)
+
+LimitsTuple = Tuple[float, float]
+ConstraintsDict = Dict[str, Union[str, float, None]]
+ForwardLoop = Callable[[nn.Module], None]
+ScoreFunc = Callable[[nn.Module], float]
+
+SearchConfig = Dict[str, Any]  # config dict for searcher
+SearchStateDict = Dict[str, Any]  # state dict for searcher
+
+__all__ = ["BaseSearcher"]
+
+
+class BaseSearcher(ABC):
+    """A basic search interface that can be used to search/optimize a model.
+
+    The base interface supports basic features like setting up a search, checkpointing, and
+    loading logic and defines a minimal workflow to follow.
+    """
+
+    model: nn.Module
+    config: SearchConfig
+    constraints: ConstraintsDict
+    dummy_input: Union[Any, Tuple]
+    forward_loop: Optional[ForwardLoop]
+
+    @final
+    def __init__(self) -> None:
+        """We don't allow to override __init__ method."""
+        super().__init__()
+
+    # TODO: see if we really want to keep all the config here.
+    @property
+    def default_search_config(self) -> SearchConfig:
+        """Get the default config for the searcher."""
+        return {
+            "checkpoint": None,
+            "verbose": dist.is_master(),
+            "forward_loop": None,
+            "data_loader": None,
+            "collect_func": None,
+            "max_iter_data_loader": None,
+            "score_func": None,
+            "loss_func": None,
+        }
+
+    @property
+    @abstractmethod
+    def default_state_dict(self) -> SearchStateDict:
+        """Return default state dict."""
+
+    def sanitize_search_config(self, config: Optional[SearchConfig]) -> SearchConfig:
+        """Sanitize the search config dict."""
+        # supply with defaults (for verbose we wanna make sure it's on master only)
+        config = {**self.default_search_config, **(config or {})}
+        config["verbose"] = config["verbose"] and self.default_search_config["verbose"]
+
+        # sanity checks
+        assert (
+            config.keys() == self.default_search_config.keys()
+        ), f"Unexpected config keys: {config.keys() - self.default_search_config.keys()}"
+        assert config["score_func"] is not None, "Please provide `score_func`!"
+
+        # return
+        return config
+
+    # TODO: double-check if we want all these args here.
+    @final
+    def search(
+        self,
+        model: nn.Module,
+        constraints: ConstraintsDict,
+        dummy_input: Union[Any, Tuple],
+        config: Optional[SearchConfig] = None,
+    ) -> SearchStateDict:
+        """Search a given prunable model for the best sub-net and return the search model.
+
+        The best sub-net maximizes the score given by ``score_func`` while satisfying the
+        ``constraints``.
+
+        Args:
+            model: The converted model to be searched.
+            constraints: The dictionary from constraint name to upper bound the searched model has
+                to satisfy.
+            dummy_input: Arguments of ``model.forward()``. This is used for exporting and
+                calculating inference-based metrics, such as latency/FLOPs. The format of
+                ``dummy_inputs`` follows the convention of the ``args`` argument in
+                `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_.
+            config: Additional optional arguments to configure the search.
+
+        Returns: A tuple (subnet, state_dict) where
+            subnet is the searched subnet (nn.Module), which can be used for subsequent tasks like
+            fine-tuning, state_dict contains the history and detailed stats of the search procedure.
+        """
+        # check model train state
+        is_training = model.training
+
+        # reset the search
+        self.reset_search()
+
+        # update and initialize searcher
+        self.model = model
+        self.config = self.sanitize_search_config(config)
+        self.constraints = constraints
+        self.dummy_input = dummy_input
+        self.forward_loop = self.construct_forward_loop(silent=not self.config["verbose"])
+
+        # load checkpoint if it exists
+        self.load_search_checkpoint()
+
+        # run initial step and sanity checks before the search
+        self.before_search()
+
+        # run actual search
+        self.run_search()
+
+        # run clean-up steps after search
+        self.after_search()
+
+        # make sure model is in original state
+        model.train(is_training)
+
+        # return the config for the best result
+        return self.best
+
+    def reset_search(self) -> None:
+        """Reset search at the beginning."""
+        # reset self.best where we store results
+        self.best: SearchStateDict = {}
+
+        # reset state dict (do it afterwards in case best is in state_dict)
+        for key, val in self.default_state_dict.items():
+            setattr(self, key, copy.deepcopy(val))
+
+    def before_search(self) -> None:
+        """Optional pre-processing steps before the search."""
+        pass
+
+    @abstractmethod
+    def run_search(self) -> None:
+        """Run actual search."""
+
+    def after_search(self) -> None:
+        """Optional post-processing steps after the search."""
+        pass
+
+    @property
+    def has_score(self) -> bool:
+        """Check if the model has a score function."""
+        return self.config["score_func"] is not None
+
+    def eval_score(self, silent=True) -> float:
+        """Optionally silent evaluation of the score function."""
+        assert self.has_score, "Please provide `score_func`!"
+        score_func: ScoreFunc = self.config["score_func"]
+        with no_stdout() if silent else nullcontext():
+            return float(score_func(self.model))
+
+    def construct_forward_loop(self, silent=True) -> Optional[ForwardLoop]:
+        """Get runnable forward loop on the model using the provided configs."""
+        # check config
+        data_loader = self.config["data_loader"]
+        forward_loop = self.config["forward_loop"]
+        assert None in [data_loader, forward_loop], "Only provide `data_loader` or `forward_loop`!"
+
+        # check trivial case
+        if not (data_loader or forward_loop):
+            return None
+
+        def forward_loop_with_silence_check(m: nn.Module) -> None:
+            with no_stdout() if silent else nullcontext():
+                if data_loader is not None:
+                    run_forward_loop(
+                        m,
+                        data_loader=data_loader,
+                        max_iters=self.config["max_iter_data_loader"],
+                        collect_func=self.config["collect_func"],
+                    )
+                elif forward_loop is not None:
+                    forward_loop(m)
+
+        return forward_loop_with_silence_check
+
+    @final
+    def state_dict(self) -> SearchStateDict:
+        """The state dictionary that can be stored/loaded."""
+        return {key: getattr(self, key) for key in self.default_state_dict}
+
+    def load_search_checkpoint(self) -> bool:
+        """Load function for search checkpoint returning indicator whether checkpoint was loaded."""
+        # check if checkpoint exists
+        checkpoint: Optional[str] = self.config["checkpoint"]
+        if checkpoint is None or not os.path.exists(checkpoint):
+            return False
+
+        # iterate through state dict and load keys
+        print(f"Loading searcher state from {checkpoint}...")
+        state_dict = torch.load(checkpoint)
+        assert state_dict.keys() == self.state_dict().keys(), "Keys in checkpoint don't match!"
+        for key, state in state_dict.items():
+            setattr(self, key, state)
+        return True
+
+    def save_search_checkpoint(self) -> None:
+        """Save function for search checkpoint."""
+        # check if save requirements are satisfied
+        checkpoint: Optional[str] = self.config["checkpoint"]
+        if checkpoint is None or not dist.is_master():
+            return
+
+        # save state dict
+        save_dirname, _ = os.path.split(checkpoint)
+        if save_dirname:
+            os.makedirs(save_dirname, exist_ok=True)
+        torch.save(self.state_dict(), checkpoint)
```

## modelopt/torch/opt/utils.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utilities for optimization."""
-
-from typing import Generator, Optional, Tuple
-
-import torch.nn as nn
-
-from modelopt.torch.utils import unwrap_model
-
-from .dynamic import DynamicSpace
-from .hparam import Hparam
-
-
-class _DynamicSpaceUnwrapped(DynamicSpace):
-    """A wrapper for the DynamicSpace class to handle unwrapping of model wrappers like DDP.
-
-    This is useful to ensure that configurations are valid for both vanilla models and wrapped
-    models, see :meth:`unwrap_models<modelopt.torch.utils.network.unwrap_model>` for supported wrappers.
-    """
-
-    def __init__(self, model: nn.Module) -> None:
-        super().__init__(unwrap_model(model))
-
-
-def is_configurable(model: nn.Module) -> bool:
-    """Check if the model is configurable."""
-    return _DynamicSpaceUnwrapped(model).is_configurable()
-
-
-def is_dynamic(model: nn.Module) -> bool:
-    """Check if the model is dynamic."""
-    return _DynamicSpaceUnwrapped(model).is_dynamic()
-
-
-def named_hparams(
-    model, configurable: Optional[bool] = None
-) -> Generator[Tuple[str, Hparam], None, None]:
-    """Recursively yield the name and instance of *all* hparams."""
-    yield from _DynamicSpaceUnwrapped(model).named_hparams(configurable)
-
-
-def search_space_size(model: nn.Module) -> int:
-    """Return the size of the search space."""
-    return _DynamicSpaceUnwrapped(model).size()
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utilities for optimization."""
+
+from typing import Generator, Optional, Tuple
+
+import torch.nn as nn
+
+from modelopt.torch.utils import unwrap_model
+
+from .dynamic import DynamicSpace
+from .hparam import Hparam
+
+
+class _DynamicSpaceUnwrapped(DynamicSpace):
+    """A wrapper for the DynamicSpace class to handle unwrapping of model wrappers like DDP.
+
+    This is useful to ensure that configurations are valid for both vanilla models and wrapped
+    models, see :meth:`unwrap_models<modelopt.torch.utils.network.unwrap_model>` for supported wrappers.
+    """
+
+    def __init__(self, model: nn.Module) -> None:
+        super().__init__(unwrap_model(model))
+
+
+def is_configurable(model: nn.Module) -> bool:
+    """Check if the model is configurable."""
+    return _DynamicSpaceUnwrapped(model).is_configurable()
+
+
+def is_dynamic(model: nn.Module) -> bool:
+    """Check if the model is dynamic."""
+    return _DynamicSpaceUnwrapped(model).is_dynamic()
+
+
+def named_hparams(
+    model, configurable: Optional[bool] = None
+) -> Generator[Tuple[str, Hparam], None, None]:
+    """Recursively yield the name and instance of *all* hparams."""
+    yield from _DynamicSpaceUnwrapped(model).named_hparams(configurable)
+
+
+def search_space_size(model: nn.Module) -> int:
+    """Return the size of the search space."""
+    return _DynamicSpaceUnwrapped(model).size()
```

## modelopt/torch/quantization/__init__.py

 * *Ordering differences only*

```diff
@@ -1,20 +1,20 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Quantization package."""
-# Initialize mode and plugins
-from . import mode, plugins
-
-# Add methods to mtq namespace
-from .config import *
-from .conversion import *
-from .model_calib import *
-from .model_quant import *
-from .nn.modules.quant_module import QuantModuleRegistry
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Quantization package."""
+# Initialize mode and plugins
+from . import mode, plugins
+
+# Add methods to mtq namespace
+from .config import *
+from .conversion import *
+from .model_calib import *
+from .model_quant import *
+from .nn.modules.quant_module import QuantModuleRegistry
```

## modelopt/torch/quantization/config.py

```diff
@@ -1,312 +1,279 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""This document lists the quantization formats supported by Model Optimizer and example quantization configs.
-
-.. _quantization-formats:
-
-Quantization Formats
-==========================================
-
-The following table lists the quantization formats supported by Model Optimizer, the corresponding quantization
-config and use-cases. See :ref:`Quantization Configs <example-quantization-configs>` for the
-specific quantization config definitions.
-
-.. note::
-
-    The recommended configs given below are for LLM models. For CNN models, only INT8 quantization
-    is supported. Please use quantization config ``INT8_DEFAULT_CFG`` for CNN models.
-
-=================================   =======================================================
-Quantization  Format                Details
-=================================   =======================================================
-INT8                                * Model Optimizer config: ``INT8_SMOOTHQUANT_CFG``
-                                    * 8-bit integer quantization with a variant of
-                                      `SmoothQuant <https://arxiv.org/pdf/2211.10438.pdf>`_
-                                      calibration.
-
-                                      - per-channel weight quantization, per-tensor activation
-                                        quantization.
-
-                                    * Compresses FP16/BF16 model to 50% of original size, faster
-                                      inference than FP16/BF16.
-                                    * Deploy using TensorRT, TRT-LLM. Supported on most GPUs.
-
-FP8                                 * Model Optimizer config: ``FP8_DEFAULT_CFG``
-                                    * FP8 per-tensor weight & activation quantization with min-max
-                                      calibration.
-                                    * Compresses FP16/BF16 model to 50% of original size.
-                                    * Least accuracy drop compared with other formats, faster
-                                      inference than FP16/BF16.
-                                    * Deploy via TensorRT, TRT-LLM. Supported GPUs: Ada, Hopper and
-                                      later.
-
-INT4 Weights only                   * Model Optimizer config: ``INT4_AWQ_CFG``
-                                    * 4-bit integer group-wise/block-wise weight only quantization
-                                      with `AWQ <https://arxiv.org/pdf/2306.00978.pdf>`_
-                                      calibration.
-                                    * Compresses FP16 model to 25% of original size.
-                                    * Faster inference for 'memory bound' applications than formats
-                                      like FP8, INT8. Memory bound applications typically have low
-                                      batch size.
-                                    * Quantized model accuracy typically better than INT8.
-                                    * Deploy via TRT-LLM. Supported GPUs: Ampere and later.
-
-INT4 Weights, FP8 Activations       * Model Optimizer config: ``W4A8_AWQ_BETA_CFG``
-                                    * 4-bit integer group-wise/block-wise weight quantization & FP8
-                                      per-tensor activation quantization.
-                                      `AWQ <https://arxiv.org/pdf/2306.00978.pdf>`_ calibration.
-                                    * Compresses FP16 model to 25% of original size.
-                                    * Typically faster inference than all other formats. More
-                                      effective for memory bound applications.
-                                    * Quantized model accuracy similar to INT4 weights only
-                                      quantization.
-                                    * TRT-LLM support in development; Supported GPUs: Ada, Hopper
-                                      and later.
-
-=================================   =======================================================
-
-.. _quantization-configs:
-
-Quantization Configs
-================================
-
-Quantization config is dictionary specifying the values for keys ``"quant_cfg"`` and
-``"algorithm"``. The ``"quant_cfg"`` key specifies the quantization configurations. The
-``"algorithm"`` key specifies the ``algorithm`` argument to
-:meth:`calibrate <modelopt.torch.quantization.model_calib.calibrate>`.
-
-Quantization configurations is a dictionary mapping wildcards or filter functions
-to its quantizer attributes. The wildcards or filter functions  are matched
-against the quantizer module names. The quantizer modules have names ending with
-``weight_quantizer`` and ``input_quantizer`` and they perform weight quantization and
-input quantization (or activation quantization) respectively. The quantizer modules are generally
-instances of
-:class:`TensorQuantizer <modelopt.torch.quantization.nn.modules.tensor_quantizer.TensorQuantizer>` and
-the specified quantizer attributes describe its quantization behavior. Quantizer attributes is a
-dictionary mapping quantizer attribute names to their values.
-
-Quantizer attributes can also be a list of dictionaries. In this case, the matched quantizer module
-is replaced with a
-:class:`SequentialQuantizer <modelopt.torch.quantization.nn.modules.tensor_quantizer.SequentialQuantizer>`
-module which is used to quantize a tensor in multiple formats sequentially. Each quantizer attribute
-dictionary in the list specifies the quantization formats for each quantization step of the
-sequential quantizer. For example, `SequentialQuantizer` is used in 'INT4 Weights, FP8 Activations'
-quantization in which the weights are quantized in INT4 followed by FP8.
-
-.. _example-quantization-configs:
-
-Here are examples quantization configs from Model Optimizer:
-
-.. code-block::
-
-    INT8_DEFAULT_CFG = {
-        "quant_cfg": {
-        "*weight_quantizer": {"num_bits": 8, "axis": 0},
-        "*input_quantizer": {"num_bits": 8, "axis": None},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "default": {"num_bits": 8, "axis": None},
-        },
-        "algorithm": "max",
-    }
-
-    INT8_SMOOTHQUANT_CFG = {
-        "quant_cfg": {
-        "*weight_quantizer": {"num_bits": 8, "axis": 0},
-        "*input_quantizer": {"num_bits": 8, "axis": -1},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "default": {"num_bits": 8, "axis": None},
-        },
-        "algorithm": "smoothquant",
-    }
-
-    FP8_DEFAULT_CFG = {
-        "quant_cfg": {
-        "*weight_quantizer": {"num_bits": (4, 3), "axis": None},
-        "*input_quantizer": {"num_bits": (4, 3), "axis": None},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "default": {"num_bits": (4, 3), "axis": None},
-        },
-        "algorithm": "max",
-    }
-
-    INT4_BLOCKWISE_WEIGHT_ONLY_CFG = {
-        "quant_cfg": {
-        "*weight_quantizer": {"num_bits": 4, "block_sizes": {-1: 128}, "enable": True},
-        "*input_quantizer": {"enable": False},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "default": {"enable": False},
-        },
-        "algorithm": "max",
-    }
-
-    INT4_AWQ_CFG = {
-        "quant_cfg": {
-        "*weight_quantizer": {"num_bits": 4, "block_sizes": {-1: 128}, "enable": True},
-        "*input_quantizer": {"enable": False},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "default": {"enable": False},
-        },
-        "algorithm": {"method": "awq_lite", "alpha_step": 0.1},
-        # "algorithm": {"method": "awq_full", "alpha_step": 0.1, "max_co_batch_size": 1024},
-        # "algorithm": {"method": "awq_clip", "max_co_batch_size": 2048},
-    }
-
-    W4A8_AWQ_BETA_CFG = {
-    "quant_cfg": {
-        "*weight_quantizer": [
-            {"num_bits": 4, "block_sizes": {-1: 128}, "enable": True},
-            {"num_bits": (4, 3), "axis": None, "enable": True},
-        ],
-        "*input_quantizer": {"num_bits": (4, 3), "axis": None, "enable": True},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "default": {"enable": False},
-    },
-    "algorithm": "awq_lite",
-    }
-
-These config can be accessed as attributes of ``modelopt.torch.quantization`` and can be given as
-input to :meth:`mtq.quantize() <modelopt.torch.quantization.model_quant.quantize>`. For example:
-
-.. code-block::
-
-    import modelopt.torch.quantization as mtq
-    model = mtq.quantize(model, mtq.INT8_DEFAULT_CFG, forward_loop)
-
-You can also create your own config by following these examples.
-For instance, if you want to quantize a model with int4 AWQ algorithm, but need to skip quantizing
-the layer named ``lm_head``,  you can create a custom config and quantize your model as following:
-
-.. code-block::
-
-    # Create custom config
-    CUSTOM_INT4_AWQ_CFG = copy.deepcopy(mtq.INT4_AWQ_CFG)
-    CUSTOM_INT4_AWQ_CFG["quant_cfg"]["*lm_head*"] = {"enable": False}
-
-    # quantize model
-    model = mtq.quantize(model, CUSTOM_INT4_AWQ_CFG, forward_loop)
-
-"""
-
-from typing import Any, Callable, Dict, Set, Union
-
-from modelopt.torch.opt.config import ModeloptBaseConfig, ModeloptField
-
-INT8_DEFAULT_CFG = {
-    "quant_cfg": {
-        "*weight_quantizer": {"num_bits": 8, "axis": 0},
-        "*input_quantizer": {"num_bits": 8, "axis": None},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "*output_layer*": {"enable": False},
-        "default": {"num_bits": 8, "axis": None},
-    },
-    "algorithm": "max",
-}
-
-INT8_SMOOTHQUANT_CFG = {
-    "quant_cfg": {
-        "*weight_quantizer": {"num_bits": 8, "axis": 0},
-        "*input_quantizer": {"num_bits": 8, "axis": -1},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "*output_layer*": {"enable": False},
-        "default": {"num_bits": 8, "axis": None},
-    },
-    "algorithm": "smoothquant",
-}
-
-FP8_DEFAULT_CFG = {
-    "quant_cfg": {
-        "*weight_quantizer": {"num_bits": (4, 3), "axis": None},
-        "*input_quantizer": {"num_bits": (4, 3), "axis": None},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "default": {"num_bits": (4, 3), "axis": None},
-    },
-    "algorithm": "max",
-}
-
-INT4_BLOCKWISE_WEIGHT_ONLY_CFG = {
-    "quant_cfg": {
-        "*weight_quantizer": {"num_bits": 4, "block_sizes": {-1: 128}, "enable": True},
-        "*input_quantizer": {"enable": False},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "*output_layer*": {"enable": False},
-        "default": {"enable": False},
-    },
-    "algorithm": "max",
-}
-
-INT4_AWQ_CFG = {
-    "quant_cfg": {
-        "*weight_quantizer": {
-            "num_bits": 4,
-            "block_sizes": {-1: 128, "type": "static"},
-            "enable": True,
-        },
-        "*input_quantizer": {"enable": False},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "*output_layer*": {"enable": False},
-        "default": {"enable": False},
-    },
-    "algorithm": {"method": "awq_lite", "alpha_step": 0.1},
-    # "algorithm": {"method": "awq_full", "alpha_step": 0.1, "max_co_batch_size": 1024},
-    # "algorithm": {"method": "awq_clip", "max_co_batch_size": 2048},
-}
-
-# W4A8 currently uses INT4 blockwise quantization (block size = 128) followed by FP8 quantization
-# for weights. This could change in the future
-W4A8_AWQ_BETA_CFG = {
-    "quant_cfg": {
-        "*weight_quantizer": [
-            {"num_bits": 4, "block_sizes": {-1: 128, "type": "static"}, "enable": True},
-            {"num_bits": (4, 3), "axis": None, "enable": True},
-        ],
-        "*input_quantizer": {"num_bits": (4, 3), "axis": -1, "enable": True},
-        "*lm_head*": {"enable": False},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "*output_layer*": {"enable": False},
-        "default": {"enable": False},
-    },
-    "algorithm": "awq_lite",
-}
-
-
-choices: Set[str] = {
-    "INT8_DEFAULT_CFG",
-    "INT8_SMOOTHQUANT_CFG",
-    "FP8_DEFAULT_CFG",
-    "INT4_BLOCKWISE_WEIGHT_ONLY_CFG",
-    "INT4_AWQ_CFG",
-    "W4A8_AWQ_BETA_CFG",
-}
-
-
-# TODO: [OMNIML-823] refine modes here.
-class QuantizeConfig(ModeloptBaseConfig):
-    """Default configuration for ``quantize`` mode."""
-
-    quant_cfg: Dict[Union[str, Callable], Any] = ModeloptField(
-        default={"default": {"num_bits": 8, "axis": None}}, title="Quantization configuration"
-    )
-    algorithm: Union[str, Dict[str, Any]] = ModeloptField(
-        default="max", title="Calibration algorithm"
-    )
-
-
-class _QuantizeExportConfig(ModeloptBaseConfig):
-    """An empty config."""
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""This document lists the quantization formats supported by Model Optimizer and example quantization configs.
+
+.. _quantization-formats:
+
+Quantization Formats
+==========================================
+
+The following table lists the quantization formats supported by Model Optimizer and the corresponding quantization
+config. See :ref:`Quantization Configs <example-quantization-configs>` for the
+specific quantization config definitions.
+
+Please see :doc:`choosing the right quantization formats <../../guides/_choosing_quant_methods>` to
+learn more about the formats and their use-cases.
+
+.. note::
+
+    The recommended configs given below are for LLM models. For CNN models, only INT8 quantization
+    is supported. Please use quantization config ``INT8_DEFAULT_CFG`` for CNN models.
+
+=================================   =======================================================
+Quantization  Format                Model Optimizer config
+=================================   =======================================================
+INT8                                ``INT8_SMOOTHQUANT_CFG``
+
+FP8                                 ``FP8_DEFAULT_CFG``
+
+INT4 Weights only AWQ (W4A16)       ``INT4_AWQ_CFG``
+
+INT4-FP8 AWQ (W4A8)                 ``W4A8_AWQ_BETA_CFG``
+
+=================================   =======================================================
+
+.. _quantization-configs:
+
+Quantization Configs
+================================
+
+Quantization config is dictionary specifying the values for keys ``"quant_cfg"`` and
+``"algorithm"``. The ``"quant_cfg"`` key specifies the quantization configurations. The
+``"algorithm"`` key specifies the ``algorithm`` argument to
+:meth:`calibrate <modelopt.torch.quantization.model_calib.calibrate>`.
+
+Quantization configurations is a dictionary mapping wildcards or filter functions
+to its quantizer attributes. The wildcards or filter functions  are matched
+against the quantizer module names. The quantizer modules have names ending with
+``weight_quantizer`` and ``input_quantizer`` and they perform weight quantization and
+input quantization (or activation quantization) respectively. The quantizer modules are generally
+instances of
+:class:`TensorQuantizer <modelopt.torch.quantization.nn.modules.tensor_quantizer.TensorQuantizer>` and
+the specified quantizer attributes describe its quantization behavior. Quantizer attributes is a
+dictionary mapping quantizer attribute names to their values.
+
+Quantizer attributes can also be a list of dictionaries. In this case, the matched quantizer module
+is replaced with a
+:class:`SequentialQuantizer <modelopt.torch.quantization.nn.modules.tensor_quantizer.SequentialQuantizer>`
+module which is used to quantize a tensor in multiple formats sequentially. Each quantizer attribute
+dictionary in the list specifies the quantization formats for each quantization step of the
+sequential quantizer. For example, `SequentialQuantizer` is used in 'INT4 Weights, FP8 Activations'
+quantization in which the weights are quantized in INT4 followed by FP8.
+
+.. _example-quantization-configs:
+
+Here are examples quantization configs from Model Optimizer:
+
+.. code-block::
+
+    INT8_DEFAULT_CFG = {
+        "quant_cfg": {
+        "*weight_quantizer": {"num_bits": 8, "axis": 0},
+        "*input_quantizer": {"num_bits": 8, "axis": None},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "default": {"num_bits": 8, "axis": None},
+        },
+        "algorithm": "max",
+    }
+
+    INT8_SMOOTHQUANT_CFG = {
+        "quant_cfg": {
+        "*weight_quantizer": {"num_bits": 8, "axis": 0},
+        "*input_quantizer": {"num_bits": 8, "axis": -1},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "default": {"num_bits": 8, "axis": None},
+        },
+        "algorithm": "smoothquant",
+    }
+
+    FP8_DEFAULT_CFG = {
+        "quant_cfg": {
+        "*weight_quantizer": {"num_bits": (4, 3), "axis": None},
+        "*input_quantizer": {"num_bits": (4, 3), "axis": None},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "default": {"num_bits": (4, 3), "axis": None},
+        },
+        "algorithm": "max",
+    }
+
+    INT4_BLOCKWISE_WEIGHT_ONLY_CFG = {
+        "quant_cfg": {
+        "*weight_quantizer": {"num_bits": 4, "block_sizes": {-1: 128}, "enable": True},
+        "*input_quantizer": {"enable": False},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "default": {"enable": False},
+        },
+        "algorithm": "max",
+    }
+
+    INT4_AWQ_CFG = {
+        "quant_cfg": {
+        "*weight_quantizer": {"num_bits": 4, "block_sizes": {-1: 128}, "enable": True},
+        "*input_quantizer": {"enable": False},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "default": {"enable": False},
+        },
+        "algorithm": {"method": "awq_lite", "alpha_step": 0.1},
+        # "algorithm": {"method": "awq_full", "alpha_step": 0.1, "max_co_batch_size": 1024},
+        # "algorithm": {"method": "awq_clip", "max_co_batch_size": 2048},
+    }
+
+    W4A8_AWQ_BETA_CFG = {
+    "quant_cfg": {
+        "*weight_quantizer": [
+            {"num_bits": 4, "block_sizes": {-1: 128}, "enable": True},
+            {"num_bits": (4, 3), "axis": None, "enable": True},
+        ],
+        "*input_quantizer": {"num_bits": (4, 3), "axis": None, "enable": True},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "default": {"enable": False},
+    },
+    "algorithm": "awq_lite",
+    }
+
+These config can be accessed as attributes of ``modelopt.torch.quantization`` and can be given as
+input to :meth:`mtq.quantize() <modelopt.torch.quantization.model_quant.quantize>`. For example:
+
+.. code-block::
+
+    import modelopt.torch.quantization as mtq
+    model = mtq.quantize(model, mtq.INT8_DEFAULT_CFG, forward_loop)
+
+You can also create your own config by following these examples.
+For instance, if you want to quantize a model with int4 AWQ algorithm, but need to skip quantizing
+the layer named ``lm_head``,  you can create a custom config and quantize your model as following:
+
+.. code-block::
+
+    # Create custom config
+    CUSTOM_INT4_AWQ_CFG = copy.deepcopy(mtq.INT4_AWQ_CFG)
+    CUSTOM_INT4_AWQ_CFG["quant_cfg"]["*lm_head*"] = {"enable": False}
+
+    # quantize model
+    model = mtq.quantize(model, CUSTOM_INT4_AWQ_CFG, forward_loop)
+
+"""
+
+from typing import Any, Callable, Dict, Set, Union
+
+from modelopt.torch.opt.config import ModeloptBaseConfig, ModeloptField
+
+INT8_DEFAULT_CFG = {
+    "quant_cfg": {
+        "*weight_quantizer": {"num_bits": 8, "axis": 0},
+        "*input_quantizer": {"num_bits": 8, "axis": None},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "*output_layer*": {"enable": False},
+        "default": {"num_bits": 8, "axis": None},
+    },
+    "algorithm": "max",
+}
+
+INT8_SMOOTHQUANT_CFG = {
+    "quant_cfg": {
+        "*weight_quantizer": {"num_bits": 8, "axis": 0},
+        "*input_quantizer": {"num_bits": 8, "axis": -1},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "*output_layer*": {"enable": False},
+        "default": {"num_bits": 8, "axis": None},
+    },
+    "algorithm": "smoothquant",
+}
+
+FP8_DEFAULT_CFG = {
+    "quant_cfg": {
+        "*weight_quantizer": {"num_bits": (4, 3), "axis": None},
+        "*input_quantizer": {"num_bits": (4, 3), "axis": None},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "default": {"num_bits": (4, 3), "axis": None},
+    },
+    "algorithm": "max",
+}
+
+INT4_BLOCKWISE_WEIGHT_ONLY_CFG = {
+    "quant_cfg": {
+        "*weight_quantizer": {"num_bits": 4, "block_sizes": {-1: 128}, "enable": True},
+        "*input_quantizer": {"enable": False},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "*output_layer*": {"enable": False},
+        "default": {"enable": False},
+    },
+    "algorithm": "max",
+}
+
+INT4_AWQ_CFG = {
+    "quant_cfg": {
+        "*weight_quantizer": {
+            "num_bits": 4,
+            "block_sizes": {-1: 128, "type": "static"},
+            "enable": True,
+        },
+        "*input_quantizer": {"enable": False},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "*output_layer*": {"enable": False},
+        "default": {"enable": False},
+    },
+    "algorithm": {"method": "awq_lite", "alpha_step": 0.1},
+    # "algorithm": {"method": "awq_full", "alpha_step": 0.1, "max_co_batch_size": 1024},
+    # "algorithm": {"method": "awq_clip", "max_co_batch_size": 2048},
+}
+
+# W4A8 currently uses INT4 blockwise quantization (block size = 128) followed by FP8 quantization
+# for weights. This could change in the future
+W4A8_AWQ_BETA_CFG = {
+    "quant_cfg": {
+        "*weight_quantizer": [
+            {"num_bits": 4, "block_sizes": {-1: 128, "type": "static"}, "enable": True},
+            {"num_bits": (4, 3), "axis": None, "enable": True},
+        ],
+        "*input_quantizer": {"num_bits": (4, 3), "axis": -1, "enable": True},
+        "*lm_head*": {"enable": False},
+        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
+        "*output_layer*": {"enable": False},
+        "default": {"enable": False},
+    },
+    "algorithm": "awq_lite",
+}
+
+
+choices: Set[str] = {
+    "INT8_DEFAULT_CFG",
+    "INT8_SMOOTHQUANT_CFG",
+    "FP8_DEFAULT_CFG",
+    "INT4_BLOCKWISE_WEIGHT_ONLY_CFG",
+    "INT4_AWQ_CFG",
+    "W4A8_AWQ_BETA_CFG",
+}
+
+
+# TODO: [OMNIML-823] refine modes here.
+class QuantizeConfig(ModeloptBaseConfig):
+    """Default configuration for ``quantize`` mode."""
+
+    quant_cfg: Dict[Union[str, Callable], Any] = ModeloptField(
+        default={"default": {"num_bits": 8, "axis": None}}, title="Quantization configuration"
+    )
+    algorithm: Union[str, Dict[str, Any]] = ModeloptField(
+        default="max", title="Calibration algorithm"
+    )
+
+
+class _QuantizeExportConfig(ModeloptBaseConfig):
+    """An empty config."""
```

## modelopt/torch/quantization/conversion.py

 * *Ordering differences only*

```diff
@@ -1,255 +1,255 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-"""Quantization conversion/restore utilities."""
-
-import fnmatch
-from typing import Any, Callable, Dict, Union
-
-import torch.nn as nn
-
-from modelopt.torch.opt.conversion import ApplyModeError, ModelLikeModule
-from modelopt.torch.opt.dynamic import DynamicModule
-from modelopt.torch.opt.mode import ConvertReturnType, MetadataDict
-
-from .config import QuantizeConfig, _QuantizeExportConfig
-from .nn import QuantLinearConvBase, QuantModuleRegistry, SequentialQuantizer, TensorQuantizer
-from .plugins.custom import register_custom_model_plugins_on_the_fly
-from .tensor_quant import QuantDescriptor
-from .utils import is_quantized, is_quantized_layer_with_weight
-
-__all__ = [
-    "replace_quant_module",
-    "set_quantizer_by_cfg",
-    "set_quantizer_attribute",
-    "register",
-    "unregister",
-]
-
-
-def convert_to_quantized_model(model: nn.Module, config: QuantizeConfig) -> ConvertReturnType:
-    """Convert the model to a quantized one as per `config`."""
-    # initialize the true module if necessary
-    model = model.init_modellike() if isinstance(model, ModelLikeModule) else model
-
-    replace_quant_module(model)
-    set_quantizer_by_cfg(model, config["quant_cfg"])
-
-    metadata = {"quantizer_state": quantizer_state(model)}
-
-    return model, metadata
-
-
-def restore_quantized_model(
-    model: nn.Module, config: QuantizeConfig, metadata: MetadataDict
-) -> nn.Module:
-    """Restores the quantizer states from the given state dict."""
-    # initialize the true module if necessary
-    model = model.init_modellike() if isinstance(model, ModelLikeModule) else model
-
-    assert not is_quantized(model), "Model must not be quantized!"
-
-    quantizer_state_dict = metadata["quantizer_state"]
-
-    replace_quant_module(model)
-    set_quantizer_by_cfg(model, config["quant_cfg"])
-
-    unmatched_keys = quantizer_state_dict.keys() - quantizer_state(model).keys()
-    extra_keys = quantizer_state(model).keys() - quantizer_state_dict.keys()
-
-    if unmatched_keys:
-        raise ApplyModeError(f"Unmatched keys in quantizer state_dict: {unmatched_keys}")
-    if extra_keys:
-        raise ApplyModeError(f"Extra keys in quantizer state_dict: {extra_keys}")
-
-    for name, module in model.named_modules():
-        if isinstance(module, TensorQuantizer):
-            module.set_from_modelopt_state(quantizer_state_dict[name], name)
-
-    for name, module in model.named_modules():
-        if is_quantized_layer_with_weight(module):
-            QuantLinearConvBase.initialize_quantizer_with_dummy_states(module)
-        if isinstance(module, TensorQuantizer):
-            module.clean_up_after_set_from_modelopt_state(name)
-
-    return model
-
-
-def update_quantize_metadata(
-    model: nn.Module, config: QuantizeConfig, metadata: MetadataDict
-) -> None:
-    """Update the quantizer state in the metadata dict."""
-    metadata["quantizer_state"] = quantizer_state(model)
-
-
-def quantizer_state(model: nn.Module) -> Dict[str, Any]:
-    """Returns the quantizer state dict describing the quantizer states in the model."""
-    return {
-        n: m.get_modelopt_state()
-        for n, m in model.named_modules()
-        if isinstance(m, (TensorQuantizer, SequentialQuantizer))
-    }
-
-
-def replace_quant_module(model: nn.Module):
-    """Recursively replace the module with quantized module."""
-    assert not is_quantized(model), "Model must not be quantized!"
-
-    register_custom_model_plugins_on_the_fly(model)
-
-    # If the model has a corresponding quantization module, replace it with it's quantized module
-    if type(model) in QuantModuleRegistry:
-        model = QuantModuleRegistry.convert(model)
-
-    def _replace_quant_module(model):
-        for name, module in model.named_children():
-            if type(module) in QuantModuleRegistry:
-                setattr(model, name, QuantModuleRegistry.convert(module))
-            # Continue replacing in case of nested quantization as well
-            _replace_quant_module(getattr(model, name))
-
-    _replace_quant_module(model)
-
-    replaced_modules = sum(isinstance(m, TensorQuantizer) for _, m in model.named_modules())
-    print(f"Inserted {replaced_modules} quantizers")
-
-
-def set_quantizer_by_cfg(quant_model: nn.Module, quant_cfg):
-    """Update the quantizer attributes based on the specified `quant_cfg`.
-
-    `quant_cfg` is a dictionary mapping wildcards or filter functions
-    to its quantizer attributes. The wildcards or filter functions  are matched
-    against the quantizer module names. The specified quantizer attributes of the
-    matched quantizer modules are set accordingly.
-
-    See :meth:`set_quantizer_attribute <modelopt.torch.quantization.conversion.set_quantizer_attribute>`
-    for more details.
-    """
-    quant_cfg = quant_cfg.copy()
-    if "default" in quant_cfg:
-        set_quantizer_attribute(quant_model, "*", quant_cfg["default"])
-        quant_cfg.pop("default")
-    for pattern, cfg in quant_cfg.items():
-        set_quantizer_attribute(quant_model, pattern, cfg)
-
-
-def set_quantizer_attribute(
-    quant_model: nn.Module,
-    wildcard_or_filter_func: Union[str, Callable],
-    attribute,
-):
-    """Finegrained adjustment of quantizer attribute by wildcard or filter function.
-
-    Args:
-        quant_model: A pytorch model
-        wildcard_or_filter_func: a wildcard string or a filter function. The wildcard string is matched
-            against the quantizer module names. The quantizer modules are
-            instances of
-            :class:`TensorQuantizer <modelopt.torch.quantization.nn.modules.tensor_quantizer.TensorQuantizer>`.
-            The filter function takes a quantized module name as input and returns ``True`` if the
-            quantizer should be adjusted and ``False`` otherwise.
-        attribute: a dict of quantizer attributes or a list of quantizer attribute dicts.
-            An example attribute dict is: ``{"num_bits": 8, "axis": 0, "enable": True}``.
-            If ``attribute`` is a list of dicts, the matched
-            :class:`TensorQuantizer <nn.modules.tensor_quantizer.TensorQuantizer>` modules will be replaced with
-            :class:`SequentialQuantizer <nn.modules.tensor_quantizer.SequentialQuantizer>` modules having one quantizer
-            for each attribute dict from the list.
-            See :meth:`set_from_attribute_dict <nn.modules.tensor_quantizer.TensorQuantizer.set_from_attribute_dict>`
-            for more details on the supported attributes and their types.
-
-    """
-    for name, module in quant_model.named_modules():
-        if isinstance(module, TensorQuantizer):
-            if isinstance(wildcard_or_filter_func, str):
-                if not fnmatch.fnmatch(name, wildcard_or_filter_func):
-                    continue
-            elif callable(wildcard_or_filter_func):
-                if not wildcard_or_filter_func(name):
-                    continue
-            else:
-                raise NotImplementedError(f"Unsupported type {type(wildcard_or_filter_func)}")
-
-            if isinstance(attribute, list):
-                parent_module = quant_model.get_submodule(name.rpartition(".")[0])
-                module = SequentialQuantizer(
-                    *(TensorQuantizer(QuantDescriptor()) for _ in range(len(attribute)))
-                )
-                setattr(parent_module, name.split(".")[-1], module)
-
-            module.set_from_attribute_dict(attribute)
-
-
-def register(original_cls: nn.Module, quantized_cls: nn.Module):
-    """Register a quantized class for the given un-quantized original class.
-
-    Args:
-        original_cls: The original un-quantized class.
-        quantized_cls: The quantized class. This class should have a `_setup` method which initializes
-            various quantizers called in the forward. The forward function of the quantized class should call the
-            quantizers at the correct location.
-
-    Here is an example of defining a quantized class and registering it:
-
-    .. code-block:: python
-
-        import modelopt.torch.quantization as mtq
-        from modelopt.torch.quantization.tensor_quant import TensorQuantizer, QuantDescriptor
-
-
-        class QuantLayerNorm(nn.LayerNorm):
-            def __init__(self, normalized_shape):
-                super().__init__(normalized_shape)
-                self._setup()
-
-            def _setup(self):
-                # Method to setup the quantizers
-                self.input_quantizer = TensorQuantizer(QuantDescriptor())
-                self.weight_quantizer = TensorQuantizer(QuantDescriptor())
-
-            def forward(self, input):
-                input = self.input_quantizer(input)
-                weight = self.weight_quantizer(self.weight)
-                return F.layer_norm(input, self.normalized_shape, weight, self.bias, self.eps)
-
-
-        # Register the custom quantized module
-        mtq.register(original_cls=nn.LayerNorm, quantized_cls=QuantLayerNorm)
-
-    """
-    assert hasattr(
-        quantized_cls, "_setup"
-    ), "Quantized class must have a _setup method which initializes various quantizers."
-
-    quantized_dm_cls = type(
-        quantized_cls.__name__, (quantized_cls, DynamicModule, original_cls), {}
-    )
-
-    QuantModuleRegistry.register({original_cls: original_cls.__name__})(quantized_dm_cls)
-
-
-def unregister(original_cls: nn.Module):
-    """Unregister the quantized class for the given un-quantized original class.
-
-    Args:
-        original_cls: The original un-quantized class.
-
-    """
-    QuantModuleRegistry.unregister(original_cls)
-
-
-def export_quantized_model(model: nn.Module, config: _QuantizeExportConfig) -> ConvertReturnType:
-    """Export the quantized model to a quantized model."""
-    raise NotImplementedError("Exporting a quantized model is not supported yet.")
-
-
-def restore_export_quantized_model(
-    model: nn.Module, config: _QuantizeExportConfig, metadata: MetadataDict
-) -> nn.Module:
-    """Restores the quantized model from the given state dict."""
-    raise NotImplementedError("Restoring a quantized & exported model is not supported yet.")
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+"""Quantization conversion/restore utilities."""
+
+import fnmatch
+from typing import Any, Callable, Dict, Union
+
+import torch.nn as nn
+
+from modelopt.torch.opt.conversion import ApplyModeError, ModelLikeModule
+from modelopt.torch.opt.dynamic import DynamicModule
+from modelopt.torch.opt.mode import ConvertReturnType, MetadataDict
+
+from .config import QuantizeConfig, _QuantizeExportConfig
+from .nn import QuantLinearConvBase, QuantModuleRegistry, SequentialQuantizer, TensorQuantizer
+from .plugins.custom import register_custom_model_plugins_on_the_fly
+from .tensor_quant import QuantDescriptor
+from .utils import is_quantized, is_quantized_layer_with_weight
+
+__all__ = [
+    "replace_quant_module",
+    "set_quantizer_by_cfg",
+    "set_quantizer_attribute",
+    "register",
+    "unregister",
+]
+
+
+def convert_to_quantized_model(model: nn.Module, config: QuantizeConfig) -> ConvertReturnType:
+    """Convert the model to a quantized one as per `config`."""
+    # initialize the true module if necessary
+    model = model.init_modellike() if isinstance(model, ModelLikeModule) else model
+
+    replace_quant_module(model)
+    set_quantizer_by_cfg(model, config["quant_cfg"])
+
+    metadata = {"quantizer_state": quantizer_state(model)}
+
+    return model, metadata
+
+
+def restore_quantized_model(
+    model: nn.Module, config: QuantizeConfig, metadata: MetadataDict
+) -> nn.Module:
+    """Restores the quantizer states from the given state dict."""
+    # initialize the true module if necessary
+    model = model.init_modellike() if isinstance(model, ModelLikeModule) else model
+
+    assert not is_quantized(model), "Model must not be quantized!"
+
+    quantizer_state_dict = metadata["quantizer_state"]
+
+    replace_quant_module(model)
+    set_quantizer_by_cfg(model, config["quant_cfg"])
+
+    unmatched_keys = quantizer_state_dict.keys() - quantizer_state(model).keys()
+    extra_keys = quantizer_state(model).keys() - quantizer_state_dict.keys()
+
+    if unmatched_keys:
+        raise ApplyModeError(f"Unmatched keys in quantizer state_dict: {unmatched_keys}")
+    if extra_keys:
+        raise ApplyModeError(f"Extra keys in quantizer state_dict: {extra_keys}")
+
+    for name, module in model.named_modules():
+        if isinstance(module, TensorQuantizer):
+            module.set_from_modelopt_state(quantizer_state_dict[name], name)
+
+    for name, module in model.named_modules():
+        if is_quantized_layer_with_weight(module):
+            QuantLinearConvBase.initialize_quantizer_with_dummy_states(module)
+        if isinstance(module, TensorQuantizer):
+            module.clean_up_after_set_from_modelopt_state(name)
+
+    return model
+
+
+def update_quantize_metadata(
+    model: nn.Module, config: QuantizeConfig, metadata: MetadataDict
+) -> None:
+    """Update the quantizer state in the metadata dict."""
+    metadata["quantizer_state"] = quantizer_state(model)
+
+
+def quantizer_state(model: nn.Module) -> Dict[str, Any]:
+    """Returns the quantizer state dict describing the quantizer states in the model."""
+    return {
+        n: m.get_modelopt_state()
+        for n, m in model.named_modules()
+        if isinstance(m, (TensorQuantizer, SequentialQuantizer))
+    }
+
+
+def replace_quant_module(model: nn.Module):
+    """Recursively replace the module with quantized module."""
+    assert not is_quantized(model), "Model must not be quantized!"
+
+    register_custom_model_plugins_on_the_fly(model)
+
+    # If the model has a corresponding quantization module, replace it with it's quantized module
+    if type(model) in QuantModuleRegistry:
+        model = QuantModuleRegistry.convert(model)
+
+    def _replace_quant_module(model):
+        for name, module in model.named_children():
+            if type(module) in QuantModuleRegistry:
+                setattr(model, name, QuantModuleRegistry.convert(module))
+            # Continue replacing in case of nested quantization as well
+            _replace_quant_module(getattr(model, name))
+
+    _replace_quant_module(model)
+
+    replaced_modules = sum(isinstance(m, TensorQuantizer) for _, m in model.named_modules())
+    print(f"Inserted {replaced_modules} quantizers")
+
+
+def set_quantizer_by_cfg(quant_model: nn.Module, quant_cfg):
+    """Update the quantizer attributes based on the specified `quant_cfg`.
+
+    `quant_cfg` is a dictionary mapping wildcards or filter functions
+    to its quantizer attributes. The wildcards or filter functions  are matched
+    against the quantizer module names. The specified quantizer attributes of the
+    matched quantizer modules are set accordingly.
+
+    See :meth:`set_quantizer_attribute <modelopt.torch.quantization.conversion.set_quantizer_attribute>`
+    for more details.
+    """
+    quant_cfg = quant_cfg.copy()
+    if "default" in quant_cfg:
+        set_quantizer_attribute(quant_model, "*", quant_cfg["default"])
+        quant_cfg.pop("default")
+    for pattern, cfg in quant_cfg.items():
+        set_quantizer_attribute(quant_model, pattern, cfg)
+
+
+def set_quantizer_attribute(
+    quant_model: nn.Module,
+    wildcard_or_filter_func: Union[str, Callable],
+    attribute,
+):
+    """Finegrained adjustment of quantizer attribute by wildcard or filter function.
+
+    Args:
+        quant_model: A pytorch model
+        wildcard_or_filter_func: a wildcard string or a filter function. The wildcard string is matched
+            against the quantizer module names. The quantizer modules are
+            instances of
+            :class:`TensorQuantizer <modelopt.torch.quantization.nn.modules.tensor_quantizer.TensorQuantizer>`.
+            The filter function takes a quantized module name as input and returns ``True`` if the
+            quantizer should be adjusted and ``False`` otherwise.
+        attribute: a dict of quantizer attributes or a list of quantizer attribute dicts.
+            An example attribute dict is: ``{"num_bits": 8, "axis": 0, "enable": True}``.
+            If ``attribute`` is a list of dicts, the matched
+            :class:`TensorQuantizer <nn.modules.tensor_quantizer.TensorQuantizer>` modules will be replaced with
+            :class:`SequentialQuantizer <nn.modules.tensor_quantizer.SequentialQuantizer>` modules having one quantizer
+            for each attribute dict from the list.
+            See :meth:`set_from_attribute_dict <nn.modules.tensor_quantizer.TensorQuantizer.set_from_attribute_dict>`
+            for more details on the supported attributes and their types.
+
+    """
+    for name, module in quant_model.named_modules():
+        if isinstance(module, TensorQuantizer):
+            if isinstance(wildcard_or_filter_func, str):
+                if not fnmatch.fnmatch(name, wildcard_or_filter_func):
+                    continue
+            elif callable(wildcard_or_filter_func):
+                if not wildcard_or_filter_func(name):
+                    continue
+            else:
+                raise NotImplementedError(f"Unsupported type {type(wildcard_or_filter_func)}")
+
+            if isinstance(attribute, list):
+                parent_module = quant_model.get_submodule(name.rpartition(".")[0])
+                module = SequentialQuantizer(
+                    *(TensorQuantizer(QuantDescriptor()) for _ in range(len(attribute)))
+                )
+                setattr(parent_module, name.split(".")[-1], module)
+
+            module.set_from_attribute_dict(attribute)
+
+
+def register(original_cls: nn.Module, quantized_cls: nn.Module):
+    """Register a quantized class for the given un-quantized original class.
+
+    Args:
+        original_cls: The original un-quantized class.
+        quantized_cls: The quantized class. This class should have a `_setup` method which initializes
+            various quantizers called in the forward. The forward function of the quantized class should call the
+            quantizers at the correct location.
+
+    Here is an example of defining a quantized class and registering it:
+
+    .. code-block:: python
+
+        import modelopt.torch.quantization as mtq
+        from modelopt.torch.quantization.tensor_quant import TensorQuantizer, QuantDescriptor
+
+
+        class QuantLayerNorm(nn.LayerNorm):
+            def __init__(self, normalized_shape):
+                super().__init__(normalized_shape)
+                self._setup()
+
+            def _setup(self):
+                # Method to setup the quantizers
+                self.input_quantizer = TensorQuantizer(QuantDescriptor())
+                self.weight_quantizer = TensorQuantizer(QuantDescriptor())
+
+            def forward(self, input):
+                input = self.input_quantizer(input)
+                weight = self.weight_quantizer(self.weight)
+                return F.layer_norm(input, self.normalized_shape, weight, self.bias, self.eps)
+
+
+        # Register the custom quantized module
+        mtq.register(original_cls=nn.LayerNorm, quantized_cls=QuantLayerNorm)
+
+    """
+    assert hasattr(
+        quantized_cls, "_setup"
+    ), "Quantized class must have a _setup method which initializes various quantizers."
+
+    quantized_dm_cls = type(
+        quantized_cls.__name__, (quantized_cls, DynamicModule, original_cls), {}
+    )
+
+    QuantModuleRegistry.register({original_cls: original_cls.__name__})(quantized_dm_cls)
+
+
+def unregister(original_cls: nn.Module):
+    """Unregister the quantized class for the given un-quantized original class.
+
+    Args:
+        original_cls: The original un-quantized class.
+
+    """
+    QuantModuleRegistry.unregister(original_cls)
+
+
+def export_quantized_model(model: nn.Module, config: _QuantizeExportConfig) -> ConvertReturnType:
+    """Export the quantized model to a quantized model."""
+    raise NotImplementedError("Exporting a quantized model is not supported yet.")
+
+
+def restore_export_quantized_model(
+    model: nn.Module, config: _QuantizeExportConfig, metadata: MetadataDict
+) -> nn.Module:
+    """Restores the quantized model from the given state dict."""
+    raise NotImplementedError("Restoring a quantized & exported model is not supported yet.")
```

## modelopt/torch/quantization/extensions.py

 * *Ordering differences only*

```diff
@@ -1,30 +1,30 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Module to load C++ / CUDA extensions."""
-from pathlib import Path
-
-from modelopt.torch.utils import load_cpp_extension
-
-path = Path(__file__).parent
-
-
-cuda_ext = load_cpp_extension(
-    name="modelopt_cuda_ext",
-    sources=[path / "src/tensor_quant.cpp", path / "src/tensor_quant_gpu.cu"],
-    cuda_version_specifiers=">=11",
-)
-
-
-cuda_ext_fp8 = load_cpp_extension(
-    name="modelopt_cuda_ext_fp8",
-    sources=[path / "src/tensor_quant_fp8.cpp", path / "src/tensor_quant_gpu_fp8.cu"],
-    cuda_version_specifiers=">=11.8",
-)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Module to load C++ / CUDA extensions."""
+from pathlib import Path
+
+from modelopt.torch.utils import load_cpp_extension
+
+path = Path(__file__).parent
+
+
+cuda_ext = load_cpp_extension(
+    name="modelopt_cuda_ext",
+    sources=[path / "src/tensor_quant.cpp", path / "src/tensor_quant_gpu.cu"],
+    cuda_version_specifiers=">=11",
+)
+
+
+cuda_ext_fp8 = load_cpp_extension(
+    name="modelopt_cuda_ext_fp8",
+    sources=[path / "src/tensor_quant_fp8.cpp", path / "src/tensor_quant_gpu_fp8.cu"],
+    cuda_version_specifiers=">=11.8",
+)
```

## modelopt/torch/quantization/mode.py

 * *Ordering differences only*

```diff
@@ -1,117 +1,117 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""This module contains the mode descriptor for the quantization mode."""
-
-from typing import Optional, Set, Type
-
-from modelopt.torch.opt.config import ModeloptBaseConfig
-from modelopt.torch.opt.mode import (
-    ConvertEntrypoint,
-    RestoreEntrypoint,
-    UpdateEntrypoint,
-    _ModeDescriptor,
-    _ModeRegistryCls,
-)
-
-from .config import QuantizeConfig, _QuantizeExportConfig
-from .conversion import (
-    convert_to_quantized_model,
-    export_quantized_model,
-    restore_export_quantized_model,
-    restore_quantized_model,
-    update_quantize_metadata,
-)
-
-QuantizeModeRegistry = _ModeRegistryCls()
-
-
-# TODO: OMNIML-717 Reuse search infra for quantization calibration algorithms
-@QuantizeModeRegistry.register_mode
-class QuantizeModeDescriptor(_ModeDescriptor):
-    """Class to describe the ``"quant"`` mode.
-
-    The properties of this mode can be inspected via the source code.
-    """
-
-    @property
-    def name(self) -> str:
-        """Returns the value (str representation) of the mode."""
-        return "quantize"
-
-    @property
-    def config_class(self) -> Type[ModeloptBaseConfig]:
-        """Specifies the config class for the mode."""
-        return QuantizeConfig
-
-    @property
-    def next_modes(self) -> Optional[Set[str]]:
-        """Modes that must immediately follow this mode."""
-        return {"kd_loss"}
-
-    @property
-    def export_mode(self) -> Optional[str]:
-        """The mode that corresponds to the export mode of this mode."""
-        return "export_quantize"
-
-    @property
-    def convert(self) -> ConvertEntrypoint:
-        """The mode's entrypoint for converting a model."""
-        return convert_to_quantized_model
-
-    @property
-    def restore(self) -> RestoreEntrypoint:
-        """The mode's entrypoint for restoring a model."""
-        return restore_quantized_model
-
-    @property
-    def update_for_save(self) -> UpdateEntrypoint:
-        """The mode's entrypoint for updating the models state before saving."""
-        return update_quantize_metadata
-
-    @property
-    def update_for_new_mode(self) -> UpdateEntrypoint:
-        """The mode's entrypoint for updating the models state before new mode."""
-        return update_quantize_metadata
-
-
-@QuantizeModeRegistry.register_mode
-class QuantizeExportModeDescriptor(_ModeDescriptor):
-    """Class to describe the export of quantization mode.
-
-    Note that this mode is just a placeholder to throw an error since we don't support exporting
-    quantized models right now. It is used to properly indicate that the ``quantize`` mode does
-    require an export mode if we ever wanted to do chaining/stacking of modes with it.
-    """
-
-    @property
-    def name(self) -> str:
-        """Returns the value (str representation) of the mode."""
-        return "quantize_export"
-
-    @property
-    def config_class(self) -> Type[ModeloptBaseConfig]:
-        """Specifies the config class for the mode."""
-        return _QuantizeExportConfig
-
-    @property
-    def is_export_mode(self) -> bool:
-        """Specifies whether the mode is an export mode."""
-        return True
-
-    @property
-    def convert(self) -> ConvertEntrypoint:
-        """The mode's entrypoint for converting a model."""
-        return export_quantized_model
-
-    @property
-    def restore(self) -> RestoreEntrypoint:
-        """The mode's entrypoint for restoring a model."""
-        return restore_export_quantized_model
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""This module contains the mode descriptor for the quantization mode."""
+
+from typing import Optional, Set, Type
+
+from modelopt.torch.opt.config import ModeloptBaseConfig
+from modelopt.torch.opt.mode import (
+    ConvertEntrypoint,
+    RestoreEntrypoint,
+    UpdateEntrypoint,
+    _ModeDescriptor,
+    _ModeRegistryCls,
+)
+
+from .config import QuantizeConfig, _QuantizeExportConfig
+from .conversion import (
+    convert_to_quantized_model,
+    export_quantized_model,
+    restore_export_quantized_model,
+    restore_quantized_model,
+    update_quantize_metadata,
+)
+
+QuantizeModeRegistry = _ModeRegistryCls()
+
+
+# TODO: OMNIML-717 Reuse search infra for quantization calibration algorithms
+@QuantizeModeRegistry.register_mode
+class QuantizeModeDescriptor(_ModeDescriptor):
+    """Class to describe the ``"quant"`` mode.
+
+    The properties of this mode can be inspected via the source code.
+    """
+
+    @property
+    def name(self) -> str:
+        """Returns the value (str representation) of the mode."""
+        return "quantize"
+
+    @property
+    def config_class(self) -> Type[ModeloptBaseConfig]:
+        """Specifies the config class for the mode."""
+        return QuantizeConfig
+
+    @property
+    def next_modes(self) -> Optional[Set[str]]:
+        """Modes that must immediately follow this mode."""
+        return {"kd_loss"}
+
+    @property
+    def export_mode(self) -> Optional[str]:
+        """The mode that corresponds to the export mode of this mode."""
+        return "export_quantize"
+
+    @property
+    def convert(self) -> ConvertEntrypoint:
+        """The mode's entrypoint for converting a model."""
+        return convert_to_quantized_model
+
+    @property
+    def restore(self) -> RestoreEntrypoint:
+        """The mode's entrypoint for restoring a model."""
+        return restore_quantized_model
+
+    @property
+    def update_for_save(self) -> UpdateEntrypoint:
+        """The mode's entrypoint for updating the models state before saving."""
+        return update_quantize_metadata
+
+    @property
+    def update_for_new_mode(self) -> UpdateEntrypoint:
+        """The mode's entrypoint for updating the models state before new mode."""
+        return update_quantize_metadata
+
+
+@QuantizeModeRegistry.register_mode
+class QuantizeExportModeDescriptor(_ModeDescriptor):
+    """Class to describe the export of quantization mode.
+
+    Note that this mode is just a placeholder to throw an error since we don't support exporting
+    quantized models right now. It is used to properly indicate that the ``quantize`` mode does
+    require an export mode if we ever wanted to do chaining/stacking of modes with it.
+    """
+
+    @property
+    def name(self) -> str:
+        """Returns the value (str representation) of the mode."""
+        return "quantize_export"
+
+    @property
+    def config_class(self) -> Type[ModeloptBaseConfig]:
+        """Specifies the config class for the mode."""
+        return _QuantizeExportConfig
+
+    @property
+    def is_export_mode(self) -> bool:
+        """Specifies whether the mode is an export mode."""
+        return True
+
+    @property
+    def convert(self) -> ConvertEntrypoint:
+        """The mode's entrypoint for converting a model."""
+        return export_quantized_model
+
+    @property
+    def restore(self) -> RestoreEntrypoint:
+        """The mode's entrypoint for restoring a model."""
+        return restore_export_quantized_model
```

## modelopt/torch/quantization/model_quant.py

 * *Ordering differences only*

```diff
@@ -1,149 +1,149 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""User-facing quantization API."""
-from typing import Any, Callable, Dict, Optional, Union
-
-import torch.nn as nn
-
-from modelopt.torch.opt import apply_mode
-from modelopt.torch.opt.searcher import ForwardLoop
-
-from .conversion import set_quantizer_attribute
-from .mode import QuantizeModeRegistry
-from .model_calib import calibrate
-from .nn import TensorQuantizer
-
-__all__ = [
-    "quantize",
-    "disable_quantizer",
-    "enable_quantizer",
-    "print_quant_summary",
-    "fold_weight",
-]
-
-
-def quantize(
-    model: nn.Module,
-    config: Dict[str, Any],
-    forward_loop: Optional[ForwardLoop] = None,
-) -> nn.Module:
-    """Quantizes and calibrates the model.
-
-    This method performs replacement of modules with their quantized counterparts and
-    performs calibration as specified by ``quant_cfg``.
-    ``forward_loop`` is used to forward data through the model and gather statistics for calibration.
-
-    Args:
-        model: A pytorch model
-        config: A dictionary specifying the values for keys ``"quant_cfg"`` and ``"algorithm"``.
-            The ``"quant_cfg"`` key specifies the quantization configurations.
-            The ``"algorithm"`` key specifies the ``algorithm`` argument to
-            :meth:`calibrate <modelopt.torch.quantization.model_calib.calibrate>`.
-
-            Quantization configurations is a dictionary mapping wildcards or filter functions
-            to its quantizer attributes. The wildcards or filter functions  are matched
-            against the quantizer module names. The quantizer modules have names ending with
-            ``weight_quantizer`` and ``input_quantizer`` and they perform weight quantization and
-            input quantization (or activation quantization) respectively. The quantizer modules
-            are instances of
-            :class:`TensorQuantizer <modelopt.torch.quantization.nn.modules.tensor_quantizer.TensorQuantizer>` and the
-            specified quantizer attributes describe its quantization behavior. See
-            :meth:`set_quantizer_by_cfg <modelopt.torch.quantization.conversion.set_quantizer_by_cfg>` for more details
-            on ``"quant_cfg"`` dictionary.
-
-            An example ``config`` dictionary is given below:
-
-            .. code-block::python
-
-                config = {
-
-                    "quant_cfg": {
-                        # "num_bits" specifies the number of bits for quantization
-                        # "axis" specifies the axis for quantization
-                        "*weight_quantizer": {"num_bits": 8, "axis": 0},
-                        "*input_quantizer": {"num_bits": 8, "axis": -1},
-
-                        # Default quantization settings
-                        "default": {"num_bits": 8, "axis": None},
-                    }
-                    "algorithm": "max"
-                }
-
-            Please see :mod:`config <modelopt.torch.quantization.config>` for more examples.
-
-        forward_loop: A callable that forwards all calibration data through the model. This is used
-            to gather statistics for calibration. It should take model as the argument. It does not need
-            to return anything. Here are a few examples for correct ``forward_loop`` definitions:
-            Example 1:
-
-            .. code-block::
-
-                    def forward_loop(model) -> None:
-                        # iterate over the data loader and forward data through the model
-                        for batch in data_loader:
-                            model(batch)
-
-            Example 2:
-
-            .. code-block::
-
-                    def forward_loop(model) -> float:
-                        # evaluate the model on the task
-                        return evaluate(model, task, ....)
-
-            Example 3:
-
-            .. code-block::
-
-                    def forward_loop(model) -> None:
-                        # run evaluation pipeline
-                        evaluator.model = model
-                        evaluator.evaluate()
-
-            .. note::
-
-                Calibration does not require forwarding the entire dataset through the model.
-                Please subsample the dataset or reduce the number of batches if needed.
-
-    Returns: A pytorch model which has been quantized and calibrated.
-    """
-    model = apply_mode(model, mode=[("quantize", config)], registry=QuantizeModeRegistry)
-    return calibrate(model, config["algorithm"], forward_loop=forward_loop)
-
-
-def disable_quantizer(model: nn.Module, wildcard_or_filter_func: Union[str, Callable]):
-    """Disable quantizer by wildcard or filter function."""
-    set_quantizer_attribute(model, wildcard_or_filter_func, {"enable": False})
-
-
-def enable_quantizer(model: nn.Module, wildcard_or_filter_func: Union[str, Callable]):
-    """Enable quantizer by wildcard or filter function."""
-    set_quantizer_attribute(model, wildcard_or_filter_func, {"enable": True})
-
-
-def print_quant_summary(model: nn.Module):
-    """Print summary of all quantizer modules in the model."""
-    count = 0
-    for name, mod in model.named_modules():
-        if isinstance(mod, TensorQuantizer):
-            print(f"{name:80} {mod}")
-            count += 1
-    print(f"{count} TensorQuantizers found in model")
-
-
-def fold_weight(model: nn.Module):
-    """Fold weight quantizer for fast evaluation."""
-    for name, module in model.named_modules():
-        if hasattr(module, "weight_quantizer") and hasattr(module, "weight"):
-            module.weight.data.copy_(
-                (module.weight_quantizer(module.weight.float())).to(module.weight.dtype)
-            )
-            module.weight_quantizer.disable()
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""User-facing quantization API."""
+from typing import Any, Callable, Dict, Optional, Union
+
+import torch.nn as nn
+
+from modelopt.torch.opt import apply_mode
+from modelopt.torch.opt.searcher import ForwardLoop
+
+from .conversion import set_quantizer_attribute
+from .mode import QuantizeModeRegistry
+from .model_calib import calibrate
+from .nn import TensorQuantizer
+
+__all__ = [
+    "quantize",
+    "disable_quantizer",
+    "enable_quantizer",
+    "print_quant_summary",
+    "fold_weight",
+]
+
+
+def quantize(
+    model: nn.Module,
+    config: Dict[str, Any],
+    forward_loop: Optional[ForwardLoop] = None,
+) -> nn.Module:
+    """Quantizes and calibrates the model.
+
+    This method performs replacement of modules with their quantized counterparts and
+    performs calibration as specified by ``quant_cfg``.
+    ``forward_loop`` is used to forward data through the model and gather statistics for calibration.
+
+    Args:
+        model: A pytorch model
+        config: A dictionary specifying the values for keys ``"quant_cfg"`` and ``"algorithm"``.
+            The ``"quant_cfg"`` key specifies the quantization configurations.
+            The ``"algorithm"`` key specifies the ``algorithm`` argument to
+            :meth:`calibrate <modelopt.torch.quantization.model_calib.calibrate>`.
+
+            Quantization configurations is a dictionary mapping wildcards or filter functions
+            to its quantizer attributes. The wildcards or filter functions  are matched
+            against the quantizer module names. The quantizer modules have names ending with
+            ``weight_quantizer`` and ``input_quantizer`` and they perform weight quantization and
+            input quantization (or activation quantization) respectively. The quantizer modules
+            are instances of
+            :class:`TensorQuantizer <modelopt.torch.quantization.nn.modules.tensor_quantizer.TensorQuantizer>` and the
+            specified quantizer attributes describe its quantization behavior. See
+            :meth:`set_quantizer_by_cfg <modelopt.torch.quantization.conversion.set_quantizer_by_cfg>` for more details
+            on ``"quant_cfg"`` dictionary.
+
+            An example ``config`` dictionary is given below:
+
+            .. code-block::python
+
+                config = {
+
+                    "quant_cfg": {
+                        # "num_bits" specifies the number of bits for quantization
+                        # "axis" specifies the axis for quantization
+                        "*weight_quantizer": {"num_bits": 8, "axis": 0},
+                        "*input_quantizer": {"num_bits": 8, "axis": -1},
+
+                        # Default quantization settings
+                        "default": {"num_bits": 8, "axis": None},
+                    }
+                    "algorithm": "max"
+                }
+
+            Please see :mod:`config <modelopt.torch.quantization.config>` for more examples.
+
+        forward_loop: A callable that forwards all calibration data through the model. This is used
+            to gather statistics for calibration. It should take model as the argument. It does not need
+            to return anything. Here are a few examples for correct ``forward_loop`` definitions:
+            Example 1:
+
+            .. code-block::
+
+                    def forward_loop(model) -> None:
+                        # iterate over the data loader and forward data through the model
+                        for batch in data_loader:
+                            model(batch)
+
+            Example 2:
+
+            .. code-block::
+
+                    def forward_loop(model) -> float:
+                        # evaluate the model on the task
+                        return evaluate(model, task, ....)
+
+            Example 3:
+
+            .. code-block::
+
+                    def forward_loop(model) -> None:
+                        # run evaluation pipeline
+                        evaluator.model = model
+                        evaluator.evaluate()
+
+            .. note::
+
+                Calibration does not require forwarding the entire dataset through the model.
+                Please subsample the dataset or reduce the number of batches if needed.
+
+    Returns: A pytorch model which has been quantized and calibrated.
+    """
+    model = apply_mode(model, mode=[("quantize", config)], registry=QuantizeModeRegistry)
+    return calibrate(model, config["algorithm"], forward_loop=forward_loop)
+
+
+def disable_quantizer(model: nn.Module, wildcard_or_filter_func: Union[str, Callable]):
+    """Disable quantizer by wildcard or filter function."""
+    set_quantizer_attribute(model, wildcard_or_filter_func, {"enable": False})
+
+
+def enable_quantizer(model: nn.Module, wildcard_or_filter_func: Union[str, Callable]):
+    """Enable quantizer by wildcard or filter function."""
+    set_quantizer_attribute(model, wildcard_or_filter_func, {"enable": True})
+
+
+def print_quant_summary(model: nn.Module):
+    """Print summary of all quantizer modules in the model."""
+    count = 0
+    for name, mod in model.named_modules():
+        if isinstance(mod, TensorQuantizer):
+            print(f"{name:80} {mod}")
+            count += 1
+    print(f"{count} TensorQuantizers found in model")
+
+
+def fold_weight(model: nn.Module):
+    """Fold weight quantizer for fast evaluation."""
+    for name, module in model.named_modules():
+        if hasattr(module, "weight_quantizer") and hasattr(module, "weight"):
+            module.weight.data.copy_(
+                (module.weight_quantizer(module.weight.float())).to(module.weight.dtype)
+            )
+            module.weight_quantizer.disable()
```

## modelopt/torch/quantization/optim.py

 * *Ordering differences only*

```diff
@@ -1,33 +1,33 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Deprecated. Placeholder module for throwing deprecated error."""
-
-from modelopt.torch.utils import DeprecatedError
-
-
-def match_parameters(*args, **kwargs):
-    """Deprecated. Placeholder function for throwing deprecated error."""
-    raise DeprecatedError("This method is deprecated. ")
-
-
-def group_parameters(*args, **kwargs):
-    """Deprecated. Placeholder function for throwing deprecated error."""
-    raise DeprecatedError("This method is deprecated. ")
-
-
-def freeze_parameters(*args, **kwargs):
-    """Deprecated. Placeholder function for throwing deprecated error."""
-    raise DeprecatedError("This module is deprecated. ")
-
-
-def quant_weight_inplace(*args, **kwargs):
-    """Deprecated. Placeholder function for throwing deprecated error."""
-    raise DeprecatedError("This method is deprecated. ")
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Deprecated. Placeholder module for throwing deprecated error."""
+
+from modelopt.torch.utils import DeprecatedError
+
+
+def match_parameters(*args, **kwargs):
+    """Deprecated. Placeholder function for throwing deprecated error."""
+    raise DeprecatedError("This method is deprecated. ")
+
+
+def group_parameters(*args, **kwargs):
+    """Deprecated. Placeholder function for throwing deprecated error."""
+    raise DeprecatedError("This method is deprecated. ")
+
+
+def freeze_parameters(*args, **kwargs):
+    """Deprecated. Placeholder function for throwing deprecated error."""
+    raise DeprecatedError("This module is deprecated. ")
+
+
+def quant_weight_inplace(*args, **kwargs):
+    """Deprecated. Placeholder function for throwing deprecated error."""
+    raise DeprecatedError("This method is deprecated. ")
```

## modelopt/torch/quantization/quant_modules.py

 * *Ordering differences only*

```diff
@@ -1,45 +1,45 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Deprecated. Placeholder module for throwing deprecated error."""
-
-from contextlib import contextmanager
-
-from modelopt.torch.utils import DeprecatedError
-
-
-def initialize(*args, **kwargs):
-    """Deprecated. This API is no longer supported.
-
-    Use :meth:`mtq.quantize() <modelopt.torch.quantization.model_quant.quantize>`
-    instead to quantize the model.
-    """
-    raise DeprecatedError(
-        "This API is no longer supported. Use `modelopt.torch.quantization.model_quant.quantize()`"
-        " API instead to quantize the model."
-    )
-
-
-def deactivate():
-    """Deprecated. This API is no longer supported."""
-    raise DeprecatedError(
-        "This API is no longer supported. Use "
-        "`modelopt.torch.quantization.model_quant.disable_quantizer()` API instead "
-        "to disable quantization."
-    )
-
-
-@contextmanager
-def enable_onnx_export():
-    """Deprecated. You no longer need to use this context manager while exporting to ONNX."""
-    raise DeprecatedError(
-        "You no longer need to use this context manager while exporting to ONNX! please call"
-        " `torch.onnx.export` directly."
-    )
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Deprecated. Placeholder module for throwing deprecated error."""
+
+from contextlib import contextmanager
+
+from modelopt.torch.utils import DeprecatedError
+
+
+def initialize(*args, **kwargs):
+    """Deprecated. This API is no longer supported.
+
+    Use :meth:`mtq.quantize() <modelopt.torch.quantization.model_quant.quantize>`
+    instead to quantize the model.
+    """
+    raise DeprecatedError(
+        "This API is no longer supported. Use `modelopt.torch.quantization.model_quant.quantize()`"
+        " API instead to quantize the model."
+    )
+
+
+def deactivate():
+    """Deprecated. This API is no longer supported."""
+    raise DeprecatedError(
+        "This API is no longer supported. Use "
+        "`modelopt.torch.quantization.model_quant.disable_quantizer()` API instead "
+        "to disable quantization."
+    )
+
+
+@contextmanager
+def enable_onnx_export():
+    """Deprecated. You no longer need to use this context manager while exporting to ONNX."""
+    raise DeprecatedError(
+        "You no longer need to use this context manager while exporting to ONNX! please call"
+        " `torch.onnx.export` directly."
+    )
```

## modelopt/torch/quantization/tensor_quant.py

 * *Ordering differences only*

```diff
@@ -1,731 +1,731 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Basic tensor quantization functions."""
-
-import numpy as np
-import torch
-import torch._C._onnx as _C_onnx
-from torch.autograd import Function
-from torch.onnx import symbolic_helper
-
-from modelopt.torch.quantization.utils import is_torch_library_supported
-
-
-def scaled_e4m3_impl(
-    inputs: torch.Tensor,  # TODO: check support for multiple inputs
-    amax: torch.Tensor,
-) -> torch.Tensor:
-    """Implementation of fake quantizing input to FP8.
-
-    Args:
-        inputs: Torch tensor.
-        amax: Absolute max range of the input tensor.
-
-    Returns:
-        Input tensors faked quantized to FP8.
-    """
-    zero_mask = inputs.abs() < 1.0 / (1 << 24)
-    from .extensions import cuda_ext_fp8
-
-    assert (
-        cuda_ext_fp8 is not None
-    ), "cuda_ext_fp8 could not be imported. E4M3 quantization requires CUDA and cuda_ext_fp8."
-
-    with torch.cuda.device(
-        None if inputs.device.index == torch.cuda.current_device() else inputs.device.index
-    ):
-        if amax is None:
-            outputs = cuda_ext_fp8.fake_e4m3fy(inputs)
-        else:
-            scale = 448.0 / amax
-            outputs = cuda_ext_fp8.fake_e4m3fy(inputs * scale) / scale
-
-    # Zero out values that are tiny.
-    # Tiny values could lead to tiny amax and then large scale which cause overflow/saturation
-    # and won't go back to normal value after dividing by scale. The right behavior is to mark them
-    # as zero which also get rid of inf/nan
-    outputs[zero_mask] = 0.0
-
-    return outputs
-
-
-def scaled_e4m3_abstract(
-    input: torch.Tensor,
-    amax: torch.Tensor,
-) -> torch.Tensor:
-    """Register an abstract implementation for scaled_e4m3.
-
-    This abstract function returns an empty tensor with the same shape and dtype.
-    """
-    output = torch.empty_like(input)
-
-    return output
-
-
-ScaledE4M3_op = scaled_e4m3_impl
-# Define torch.library custom op if supported
-if is_torch_library_supported():
-    try:
-        torch.library.define("trt::quantize_fp8", "(Tensor input, Tensor amax) -> Tensor")
-        scaled_e4m3_impl = torch.library.impl("trt::quantize_fp8", ["cpu", "cuda"])(
-            scaled_e4m3_impl
-        )
-        scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
-            scaled_e4m3_abstract
-        )
-        ScaledE4M3_op = torch.ops.trt.quantize_fp8
-    except (AttributeError, RuntimeError):
-        # torch.library is an experiemental feature, the function signatures may change overtime.
-        print(
-            "Unable to register operators with torch.library. Exporting quantized models with"
-            " torch.export will not be supported."
-        )
-
-
-class ScaledQuantDescriptor:
-    """Supportive descriptor of quantization.
-
-    Describe how a tensor should be quantized. A QuantDescriptor and a tensor defines a quantized
-    tensor.
-
-    Args:
-        num_bits: An integer or a tuple of two integers.
-            Specifically, `num_bits` can be:
-
-            #. A positive integer argument for integer quantization. `num_bits` specify
-                the number of bits used for integer quantization.
-
-            #. Constant integer tuple (E,M) for floating point quantization emulating
-                Nvidia's FPx quantization. E is the number of exponent bits and M is the number
-                of mantissa bits. Supported FPx quantizations: FP8 with (E=4, M=3).
-
-            Default: 8.
-        name: Seems a nice thing to have
-        fake_quant: A boolean. If True, use fake quantization mode. Default True.
-        axis: None, int or tuple of int. The specified axis/axes will have its own amax for
-            computing scaling factor. If None (the default), use per tensor scale. Must be in the
-            range [-rank(input_tensor), rank(input_tensor)). E.g. For a KCRS weight tensor,
-            ``quant_axis=(0)`` will yield per channel scaling.
-        block_sizes: None or a dictionary. The dictionary specifies
-            block quantization parameters. The keys are the axes for block quantization and the
-            values are block sizes for quantization along the respective axes. Keys must be in the
-            range ``[-rank(input_tensor), rank(input_tensor)]``. Values, which are the block sizes
-            for quantization must be positive integers.
-
-            In addition, there can be special string keys "type" and "scale_bits". Key "type"
-            should map to "dynamic" or "static" where "dynamic" indicates dynamic block quantization and "static"
-            indicates static calibrated block quantization. By default, the type is "static". Key "scale_bits"
-            specify the quantization bits for the per-block quantization scale factor
-            (i.e a double quantization scheme). By default per-block quantization scale is not quantized.
-
-            For example, ``block_sizes = {-1: 32}`` will quantize the last axis of the input tensor in
-            blocks of size 32 with static calibration and ``block_sizes = {-1: 32, "type": "dynamic"}``
-            will perform dynamic block quantization. If None, block
-            quantization is not performed. ``axis`` must be None when ``block_sizes`` is not None.
-
-        amax: A float or list/ndarray of floats of user specified absolute max range. If supplied,
-            ignore quant_axis and use this to quantize. If learn_amax is True, will be used to
-            initialize learnable amax.
-        learn_amax: A boolean. If True, learn amax.
-        scale_amax: A float. If supplied, multiply amax by scale_amax. Default None. It is useful
-            for some quick experiment.
-        calib_method: A string. One of ``["max", "histogram"]`` indicates which calibration to use.
-            Except the simple max calibration, other methods are all histogram based.
-        unsigned: A boolean. If True, use unsigned.
-        narrow_range: A boolean. if True, symmetric integer range for signed quantization is used.
-
-    Read-only properties:
-        - fake_quant:
-        - name:
-        - learn_amax:
-        - scale_amax:
-        - axis:
-        - calib_method:
-        - num_bits:
-        - amax:
-        - unsigned:
-    """
-
-    def __init__(
-        self,
-        num_bits=8,
-        name=None,
-        fake_quant=True,
-        axis=None,
-        block_sizes=None,
-        amax=None,
-        learn_amax=False,
-        scale_amax=None,
-        calib_method="max",
-        unsigned=False,
-        narrow_range=False,
-    ):
-        """Initialize QuantDescriptor."""
-        if isinstance(num_bits, int):
-            if num_bits < 0:
-                raise ValueError(f"num_bits must be > 0, not {num_bits}.")
-        elif num_bits not in [(4, 3), (2, 1)]:  # E4M3 and E2M1
-            raise TypeError(
-                "num_bits must be a postive integer or tuple such as (4, 3), (2, 1) etc., not"
-                f" {type(num_bits)}."
-            )
-
-        self._num_bits = num_bits
-        if not isinstance(name, str) and name is not None:
-            raise TypeError(f"name must be a string or None, not {type(name)}.")
-        self._name = name
-
-        self._fake_quant = fake_quant
-        self._axis = axis
-        self._block_sizes = None
-        if block_sizes is not None:
-            assert axis is None, "axis must be None when block_sizes is not None."
-            assert isinstance(block_sizes, dict), "block_sizes must be a dictionary."
-            assert all(
-                isinstance(key, int)
-                for key in self.get_block_quant_axes_and_sizes(block_sizes).keys()
-            ), "axes for block block quantization must be integers."
-            assert all(
-                isinstance(value, int) and value > 0
-                for value in self.get_block_quant_axes_and_sizes(block_sizes).values()
-            ), "block sizes for block quantization must be positive integers."
-            self._block_sizes = block_sizes
-
-        self._learn_amax = learn_amax
-        if self._learn_amax and self._axis is not None:
-            raise TypeError(
-                f"axis is ignored and must be None when learn_amax is true, got {type(self._axis)}."
-            )
-        if amax is not None:
-            if (
-                not isinstance(amax, float)
-                and not isinstance(amax, list)
-                and not isinstance(amax, np.ndarray)
-            ):
-                raise TypeError(f"amax must be float, list or ndarray, not {type(amax)}")
-            # Make it single precision array
-            self._amax = np.array(amax, dtype=np.float32)
-        else:
-            self._amax = amax
-
-        self._scale_amax = scale_amax
-        self._calib_method = calib_method
-        self._unsigned = unsigned
-        self._narrow_range = narrow_range
-
-    @property
-    def num_bits(self):
-        """Return num_bits."""
-        return self._num_bits
-
-    @property
-    def fake_quant(self):
-        """Return True if fake quantization is used."""
-        return self._fake_quant
-
-    @property
-    def axis(self):
-        """Return axis for quantization."""
-        return self._axis
-
-    @property
-    def block_sizes(self):
-        """Return block_sizes for quantization."""
-        return self._block_sizes
-
-    @staticmethod
-    def get_block_quant_axes_and_sizes(block_sizes: dict):
-        """Return axes and sizes for block quantization."""
-        if block_sizes is None:
-            return None
-        return {k: v for k, v in block_sizes.items() if k not in ["type", "scale_bits"]}
-
-    @property
-    def amax(self):
-        """Return amax."""
-        return self._amax
-
-    @property
-    def learn_amax(self):
-        """Return True if amax is learnable."""
-        return self._learn_amax
-
-    @property
-    def scale_amax(self):
-        """Return scale_amax."""
-        return self._scale_amax
-
-    @property
-    def name(self):
-        """Return name."""
-        return self._name
-
-    @property
-    def calib_method(self):
-        """Return calibration method."""
-        return self._calib_method
-
-    @property
-    def unsigned(self):
-        """Return True if unsigned integer range is used."""
-        return self._unsigned
-
-    @property
-    def narrow_range(self):
-        """Return True if symmetric integer range for signed quantization is used."""
-        return self._narrow_range
-
-    def __str__(self):
-        s = (self._name + ": ") if self._name is not None else "QuantDescriptor"
-        s += f"({'unsigned ' if self._unsigned else ''}{self._num_bits}bit"
-        s += " fake" if self._fake_quant else " real"
-        if self._block_sizes is not None:
-            s += f" block_sizes={self._block_sizes}"
-        else:
-            s += f" axis={self._axis if self._axis is not None else ' per-tensor'}"
-        if isinstance(self._amax, torch.Tensor):
-            amax_str = np.array2string(
-                self._amax.cpu().numpy().flatten(), edgeitems=1, formatter={"all": "{:.2e}".format}
-            )
-            s += f" amax={amax_str}"
-        elif self._amax is not None:
-            s += f" amax={self._amax}"
-            s += " full_range"
-        if self._learn_amax:
-            s += " learn_amax"
-        if self._scale_amax:
-            s += f" scale_amax={self._scale_amax}"
-        s += ")"
-        return s
-
-    def __eq__(self, rhs):
-        """Compare 2 descriptors."""
-        return self.__dict__ == rhs.__dict__
-
-    def dict(self):
-        """Serialize to dict.
-
-        The build-in __dict__ method returns all the attributes, which includes those have default value and have
-        protected prefix "_". This method only returns those have values other than the default one and don't have _ in
-        key. Construct a instance by dict returned by this method should get exactly the same instance.
-        """
-        obj_dict = {}
-        obj_dict["num_bits"] = self._num_bits
-        obj_dict["name"] = self._name
-
-        if not self._fake_quant:
-            obj_dict["fake_quant"] = self._fake_quant
-        if self._axis is not None:
-            obj_dict["axis"] = self._axis
-        if self._amax is not None:
-            obj_dict["amax"] = self._amax.tolist()
-        if self._scale_amax is not None:
-            obj_dict["scale_amax"] = self._scale_amax
-        if self._learn_amax:
-            obj_dict["learn_amax"] = self._learn_amax
-        if self._unsigned:
-            obj_dict["unsigned"] = self._unsigned
-
-        return obj_dict
-
-
-QuantDescriptor = ScaledQuantDescriptor
-
-# Predefined descriptors
-QUANT_DESC_8BIT_PER_TENSOR = QuantDescriptor(num_bits=8)
-QUANT_DESC_UNSIGNED_8BIT_PER_TENSOR = QuantDescriptor(num_bits=8, unsigned=True)
-QUANT_DESC_8BIT_CONV1D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
-QUANT_DESC_8BIT_CONV2D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
-QUANT_DESC_8BIT_CONV3D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
-QUANT_DESC_8BIT_LINEAR_WEIGHT_PER_ROW = QuantDescriptor(num_bits=8, axis=(0))
-QUANT_DESC_8BIT_CONVTRANSPOSE1D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
-QUANT_DESC_8BIT_CONVTRANSPOSE2D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
-QUANT_DESC_8BIT_CONVTRANSPOSE3D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
-
-
-@torch.jit.script
-def _fake_tensor_quant_backward(inputs, amax, grad_outputs):
-    zero = grad_outputs.new_zeros(1)
-    grad_inputs = torch.where(inputs.abs() <= amax, grad_outputs, zero)
-    return grad_inputs
-
-
-def _onnx_int8_helper(g, inputs, amax, num_bits, unsigned, narrow_range):
-    assert num_bits == 8, "Only INT8 ONNX export is supported for now."
-    maxbound = (1 << (num_bits - 1 + int(unsigned))) - 1
-
-    if amax.numel() == 1:
-        zero_point, axis = torch.tensor(0.0, device=amax.device), None
-    else:
-        amax_init_shape = amax.shape
-        amax = amax.squeeze().data
-        assert len(amax.shape) == 1, "ONNX does not support multi-axis quantization."
-        zero_point = torch.zeros_like(amax, dtype=torch.int32).data
-        axis = list(amax_init_shape).index(list(amax.shape)[0])
-
-    zero_point = g.op("Constant", value_t=zero_point)
-
-    if not unsigned:
-        assert not narrow_range, "ONNX does not support unsigned narrow range INT8."
-        zero_point = g.op("Cast", zero_point, to_i=_C_onnx.TensorProtoDataType.INT8)
-    else:
-        zero_point = g.op("Cast", zero_point, to_i=_C_onnx.TensorProtoDataType.UINT8)
-
-    amax = amax.to(torch.float32)
-    scale = amax / maxbound
-    scale.masked_fill_(scale == 0, 1.0)
-    scale = g.op("Constant", value_t=scale)
-
-    input_type = inputs.type().scalarType()
-
-    # Q inputs are currently constrained to FP32 due to a similar limitation in ORT
-    # custom ops, so cast the input if needed.
-    if input_type == "Half" or input_type == "BFloat16":
-        inputs = g.op("Cast", inputs, to_i=_C_onnx.TensorProtoDataType.FLOAT)
-
-    quantized = g.op("QuantizeLinear", inputs, scale, zero_point, axis_i=axis)
-    out = g.op("DequantizeLinear", quantized, scale, zero_point, axis_i=axis)
-
-    # DQ outputs are currently constrained to FP32 due to a similar limitation in ORT
-    # custom ops, so cast the output if needed.
-    if input_type == "Half":
-        out = g.op("Cast", out, to_i=_C_onnx.TensorProtoDataType.FLOAT16)
-    elif input_type == "BFloat16":
-        out = g.op("Cast", out, to_i=_C_onnx.TensorProtoDataType.BFLOAT16)
-
-    return out
-
-
-class FakeTensorQuantFunction(Function):
-    """Fake version of TensorQuantFunction use CUDA extension."""
-
-    @staticmethod
-    @symbolic_helper.parse_args("v", "t", "i", "b", "b")
-    def symbolic(g, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
-        """ONNX symbolic function."""
-        return _onnx_int8_helper(g, inputs, amax, num_bits, unsigned, narrow_range)
-
-    @staticmethod
-    def forward(ctx, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
-        """Forward method."""
-        ctx.save_for_backward(inputs, amax)
-
-        def legacy_quant_func():
-            # The LegacyFakeTensorQuantFunction support cpu and amax with any shape that can be broadcasted to inputs.
-            outputs, scale = _tensor_quant(inputs, amax, num_bits, unsigned, narrow_range)
-            return outputs / scale.to(inputs.dtype)
-
-        if not inputs.is_cuda:
-            outputs = legacy_quant_func()
-        else:
-            try:
-                from .extensions import cuda_ext
-
-                with torch.cuda.device(
-                    None
-                    if inputs.device.index == torch.cuda.current_device()
-                    else inputs.device.index
-                ):
-                    if amax.numel() == 1:
-                        outputs = cuda_ext.fake_tensor_quant(  # type: ignore[union-attr]
-                            inputs, amax, num_bits, unsigned, narrow_range
-                        )
-                    else:
-                        axis = amax.shape.index(amax.numel())
-                        outputs = cuda_ext.fake_tensor_quant_with_axis(  # type: ignore[union-attr]
-                            inputs, amax.squeeze(), axis, num_bits, unsigned, narrow_range
-                        )
-            except (AttributeError, ValueError):
-                # AttributeError: cuda_ext is not imported, possibly due to CPU only installation
-                # ValueError: cuda_ext is installed, but trying to perform multidimensional quantization (amax dim > 1)
-                outputs = legacy_quant_func()
-
-        return outputs
-
-    @staticmethod
-    def backward(ctx, grad_outputs):
-        """Implements straight through estimation with clipping."""
-        inputs, amax = ctx.saved_tensors
-        return _fake_tensor_quant_backward(inputs, amax, grad_outputs), None, None, None, None
-
-
-def _onnx_fp8_quantize(g, inputs, scale_inv):
-    """Helper Function for Quantization."""
-    output_shape = torch.onnx.symbolic_helper._get_tensor_sizes(inputs)
-
-    # Q inputs are currently constrained to FP32 due to a similar limitation in ORT
-    # custom ops, so cast the input if needed.
-    if inputs.type().scalarType() == "Half" or inputs.type().scalarType() == "BFloat16":
-        inputs = g.op("Cast", inputs, to_i=_C_onnx.TensorProtoDataType.FLOAT)
-
-    scale = g.op("Constant", value_t=torch.tensor(scale_inv))
-    q_op = g.op("trt::TRT_FP8QuantizeLinear", inputs, scale).setType(
-        inputs.type().with_dtype(torch.uint8).with_sizes(output_shape)
-    )
-    return q_op
-
-
-def _onnx_fp8_dequantize(g, inputs, scale_inv, otype=None):
-    """Helper Function for Dequantization."""
-    output_shape = torch.onnx.symbolic_helper._get_tensor_sizes(inputs)
-
-    scale = g.op("Constant", value_t=torch.tensor(scale_inv))
-    out = g.op("trt::TRT_FP8DequantizeLinear", inputs, scale).setType(
-        inputs.type().with_dtype(torch.float32).with_sizes(output_shape)
-    )
-
-    # DQ outputs are currently constrained to FP32 due to a similar limitation in ORT
-    # custom ops, so cast the output if needed.
-    if otype == "Half":
-        out = g.op("Cast", out, to_i=_C_onnx.TensorProtoDataType.FLOAT16)
-    elif otype == "BFloat16":
-        out = g.op("Cast", out, to_i=_C_onnx.TensorProtoDataType.BFLOAT16)
-    return out
-
-
-class ScaledE4M3Function(Function):
-    """E4M3fy input with scale."""
-
-    @staticmethod
-    @symbolic_helper.parse_args("v", "t", "i", "b", "b")
-    def symbolic(g, inputs, amax=None, E=4, M=3):  # noqa: N803
-        """ONNX symbolic function."""
-        if amax is None:
-            scale = 1.0
-        else:
-            scale = 448.0 / float(amax)
-        otype = inputs.type().scalarType()
-        q_tensor = _onnx_fp8_quantize(g, inputs, 1.0 / scale)
-        return _onnx_fp8_dequantize(g, q_tensor, 1.0 / scale, otype)
-
-    @staticmethod
-    # Default values could cause errors from TorchDynamo during torch.export
-    def forward(ctx, inputs, amax, E, M):  # noqa: N803
-        """Forward method."""
-        if E != 4 or M != 3:
-            raise NotImplementedError("Only support E=4 & M=3 for now.")
-
-        ctx.save_for_backward(inputs)
-        ctx.amax = amax
-        outputs = ScaledE4M3_op(inputs, amax)
-
-        return outputs
-
-    @staticmethod
-    def backward(ctx, grad_outputs):
-        """Implements straight through estimation with clipping."""
-        (inputs,) = ctx.saved_tensors
-        amax = torch.tensor(
-            ctx.amax if ctx.amax is not None else 448.0, dtype=torch.float32, device=inputs.device
-        )
-        grad_inputs = _fake_tensor_quant_backward(inputs, amax, grad_outputs)
-        return grad_inputs, None, None, None
-
-
-class TensorQuantFunction(Function):
-    """A universal tensor quantization function.
-
-    Take an input tensor, output an quantized tensor. The granularity of scale can be interpreted from the
-    shape of amax.
-    output_dtype indicates whether the quantized value will be stored in integer or float. The reason we want to store
-    it in float is the pytorch function takes the quantized value may not accept integer input, e.g. Conv2D.
-
-    It uses 2^num_bits -1 values instead of 2^num_bits. e.g., for num_bits=8, it uses [-127, 127] instead of [-128, 127]
-    """
-
-    @staticmethod
-    @symbolic_helper.parse_args("v", "t", "i", "b", "b")
-    def symbolic(g, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
-        """ONNX symbolic function."""
-        return _onnx_int8_helper(g, inputs, amax, num_bits, unsigned, narrow_range)
-
-    @staticmethod
-    def forward(ctx, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
-        """Forward method.
-
-        Follow tensorflow convention, max value is passed in and used to decide scale, instead of inputing scale
-        directly. Though inputing scale directly may be more natural to use.
-
-        Args:
-            ctx: A Context object to store tensors for backward.
-            inputs: A Tensor of type float32.
-            amax: A Tensor of type float32. Inputs will be quantized within range [-amax, amax]
-                amax will be broadcasted to inputs tensor.
-            num_bits: A integer used to calculate scaling factor, scale = (2^(num_bits-1) - 1) / max
-                Effectively, it indicates how many integer bits is used to represent the value. Default 8.
-            output_dtype: A type of Tensor. torch.int32 or torch.float32.
-            unsigned: A boolean. Use unsigned integer range. E.g. [0, 255] for num_bits=8. Default False.
-            narrow_range: A boolean. Use symmetric integer range for signed quantization
-                E.g. [-127,127] instead of [-128,127] for num_bits=8. Default True.
-
-        Returns:
-            outputs: A Tensor of type output_dtype.
-            scale: A Tensor of type float32. outputs / scale will dequantize outputs tensor.
-
-        Raises:
-            ValueError:
-        """
-        ctx.save_for_backward(inputs, amax)
-        outputs, scale = _tensor_quant(inputs, amax, num_bits, unsigned, narrow_range)
-        # Check if scale overflows FP16
-        if outputs.dtype == torch.half and scale.max() > 65504:
-            raise ValueError(f"scale is too large for FP16 with amax={amax}")
-        return outputs, scale.to(inputs.dtype)
-
-    @staticmethod
-    def backward(ctx, grad_outputs, grad_scale):
-        """Implements straight through estimation with clipping.
-
-        For -amax <= input <= amax the gradient passes straight through, otherwise the gradient is zero.
-
-        Args:
-            ctx: A Context object with saved tensors from forward.
-            grad_outputs: A tensor of gradient of outputs.
-            grad_scale: A tensor of gradient of scale.
-
-        Returns:
-            grad_inputs: A tensor of gradient.
-        """
-        inputs, amax = ctx.saved_tensors
-        zero = grad_outputs.new_zeros(1)  # create a zero tensor with the same type and device
-        grad_inputs = torch.where(inputs.abs() <= amax, grad_outputs, zero)
-        return grad_inputs, None, None, None, None
-
-
-class LegacyFakeTensorQuantFunction(Function):
-    """Fake version of TensorQuantFunction.
-
-    See comments of TensorQuantFunction, arguments are the same.
-    """
-
-    @staticmethod
-    def forward(ctx, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
-        """Forward method."""
-        ctx.save_for_backward(inputs, amax)
-        outputs, scale = _tensor_quant(inputs, amax, num_bits, unsigned, narrow_range)
-        return outputs / scale.to(inputs.dtype)
-
-    @staticmethod
-    def backward(ctx, grad_outputs):
-        """Implements straight through estimation."""
-        inputs, amax = ctx.saved_tensors
-        zero = grad_outputs.new_zeros(1)
-        grad_inputs = torch.where(inputs.abs() <= amax, grad_outputs, zero)
-        return grad_inputs, None, None, None, None
-
-
-def _tensor_quant(inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
-    """Shared function body between TensorQuantFunction and FakeTensorQuantFunction."""
-    # Fine scale, per channel scale will be handled by broadcasting, which could be tricky. Pop a warning.
-    if unsigned:
-        if inputs.min() < 0.0:
-            raise TypeError("Negative values encountered in unsigned quantization.")
-
-    # Computation must be in FP32 to prevent potential over flow.
-    input_dtype = inputs.dtype
-    if inputs.dtype == torch.half:
-        inputs = inputs.float()
-    if amax.dtype == torch.half:
-        amax = amax.float()
-
-    min_amax = amax.min()
-    if min_amax < 0:
-        raise ValueError("Negative values in amax")
-
-    max_bound = torch.tensor((2.0 ** (num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)
-    if unsigned:
-        min_bound = 0
-    elif narrow_range:
-        min_bound = -max_bound
-    else:
-        min_bound = -max_bound - 1
-    scale = max_bound / amax
-
-    epsilon = 1.0 / (1 << 24)
-    if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0
-        zero_amax_mask = amax <= epsilon
-        scale[zero_amax_mask] = 0  # Value quantized with amax=0 should all be 0
-
-    outputs = torch.clamp((inputs * scale).round_(), min_bound, max_bound)
-
-    if min_amax <= epsilon:
-        scale[zero_amax_mask] = (
-            1.0  # Return 1 makes more sense for values quantized to 0 with amax=0
-        )
-
-    if input_dtype == torch.half:
-        outputs = outputs.half()
-
-    return outputs, scale
-
-
-class FakeAffineTensorQuantFunction(Function):
-    """Fake version of affine quantization.
-
-    gemmlowp style scale+shift quantization. See more details in
-    https://github.com/google/gemmlowp/blob/master/doc/quantization.md.
-
-    We DO NOT recommend affine quantization on weights for performance reason. There might be value to affine quantize
-    activation as it can be cancelled by bias and comes with no performance penalty. This functionality is only added
-    for experimental purpose.
-    """
-
-    @staticmethod
-    def forward(ctx, inputs, min_range, max_range, num_bits=8):
-        """As it will be only applied on activation with per tensor granularity, broadcast is not needed.
-
-        Args:
-            ctx: Pytorch convention.
-            inputs: A Tensor of type float32.
-            min_range: A float.
-            max_range: A float.
-            num_bits: An integer
-
-        Returns:
-            outputs: A Tensor of type output_dtype
-        """
-        ctx.save_for_backward(inputs, min_range, max_range)
-
-        step_size = (max_range - min_range) / (2.0**num_bits - 1)
-
-        min_bound = -(2.0 ** (num_bits - 1))
-        max_bound = 2.0 ** (num_bits - 1) - 1
-
-        quant_zero = torch.round(min_range / step_size) - min_bound
-        quantized = torch.round(inputs / step_size) - quant_zero
-        quantized = torch.clamp(quantized, min_bound, max_bound)
-
-        outputs = (quantized + quant_zero) * step_size
-
-        return outputs
-
-    @staticmethod
-    def backward(ctx, grad_outputs):
-        """Implements straight through estimation with clipping.
-
-        Args:
-            ctx: Pytorch convention.
-            grad_output: A tensor of gradient of outputs.
-
-        Returns:
-            grad_inputs: A tensor of gradient
-        """
-        inputs, min_range, max_range = ctx.saved_tensors
-        zero = grad_outputs.new_zeros(1)
-        grad_inputs = torch.where((inputs <= max_range) * (inputs >= min_range), grad_outputs, zero)
-        return grad_inputs, None, None, None
-
-
-tensor_quant = TensorQuantFunction.apply
-legacy_fake_tensor_quant = LegacyFakeTensorQuantFunction.apply
-fake_tensor_quant = FakeTensorQuantFunction.apply
-fake_affine_tensor_quant = FakeAffineTensorQuantFunction.apply
-scaled_e4m3 = ScaledE4M3Function.apply
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Basic tensor quantization functions."""
+
+import numpy as np
+import torch
+import torch._C._onnx as _C_onnx
+from torch.autograd import Function
+from torch.onnx import symbolic_helper
+
+from modelopt.torch.quantization.utils import is_torch_library_supported
+
+
+def scaled_e4m3_impl(
+    inputs: torch.Tensor,  # TODO: check support for multiple inputs
+    amax: torch.Tensor,
+) -> torch.Tensor:
+    """Implementation of fake quantizing input to FP8.
+
+    Args:
+        inputs: Torch tensor.
+        amax: Absolute max range of the input tensor.
+
+    Returns:
+        Input tensors faked quantized to FP8.
+    """
+    zero_mask = inputs.abs() < 1.0 / (1 << 24)
+    from .extensions import cuda_ext_fp8
+
+    assert (
+        cuda_ext_fp8 is not None
+    ), "cuda_ext_fp8 could not be imported. E4M3 quantization requires CUDA and cuda_ext_fp8."
+
+    with torch.cuda.device(
+        None if inputs.device.index == torch.cuda.current_device() else inputs.device.index
+    ):
+        if amax is None:
+            outputs = cuda_ext_fp8.fake_e4m3fy(inputs)
+        else:
+            scale = 448.0 / amax
+            outputs = cuda_ext_fp8.fake_e4m3fy(inputs * scale) / scale
+
+    # Zero out values that are tiny.
+    # Tiny values could lead to tiny amax and then large scale which cause overflow/saturation
+    # and won't go back to normal value after dividing by scale. The right behavior is to mark them
+    # as zero which also get rid of inf/nan
+    outputs[zero_mask] = 0.0
+
+    return outputs
+
+
+def scaled_e4m3_abstract(
+    input: torch.Tensor,
+    amax: torch.Tensor,
+) -> torch.Tensor:
+    """Register an abstract implementation for scaled_e4m3.
+
+    This abstract function returns an empty tensor with the same shape and dtype.
+    """
+    output = torch.empty_like(input)
+
+    return output
+
+
+ScaledE4M3_op = scaled_e4m3_impl
+# Define torch.library custom op if supported
+if is_torch_library_supported():
+    try:
+        torch.library.define("trt::quantize_fp8", "(Tensor input, Tensor amax) -> Tensor")
+        scaled_e4m3_impl = torch.library.impl("trt::quantize_fp8", ["cpu", "cuda"])(
+            scaled_e4m3_impl
+        )
+        scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
+            scaled_e4m3_abstract
+        )
+        ScaledE4M3_op = torch.ops.trt.quantize_fp8
+    except (AttributeError, RuntimeError):
+        # torch.library is an experiemental feature, the function signatures may change overtime.
+        print(
+            "Unable to register operators with torch.library. Exporting quantized models with"
+            " torch.export will not be supported."
+        )
+
+
+class ScaledQuantDescriptor:
+    """Supportive descriptor of quantization.
+
+    Describe how a tensor should be quantized. A QuantDescriptor and a tensor defines a quantized
+    tensor.
+
+    Args:
+        num_bits: An integer or a tuple of two integers.
+            Specifically, `num_bits` can be:
+
+            #. A positive integer argument for integer quantization. `num_bits` specify
+                the number of bits used for integer quantization.
+
+            #. Constant integer tuple (E,M) for floating point quantization emulating
+                Nvidia's FPx quantization. E is the number of exponent bits and M is the number
+                of mantissa bits. Supported FPx quantizations: FP8 with (E=4, M=3).
+
+            Default: 8.
+        name: Seems a nice thing to have
+        fake_quant: A boolean. If True, use fake quantization mode. Default True.
+        axis: None, int or tuple of int. The specified axis/axes will have its own amax for
+            computing scaling factor. If None (the default), use per tensor scale. Must be in the
+            range [-rank(input_tensor), rank(input_tensor)). E.g. For a KCRS weight tensor,
+            ``quant_axis=(0)`` will yield per channel scaling.
+        block_sizes: None or a dictionary. The dictionary specifies
+            block quantization parameters. The keys are the axes for block quantization and the
+            values are block sizes for quantization along the respective axes. Keys must be in the
+            range ``[-rank(input_tensor), rank(input_tensor)]``. Values, which are the block sizes
+            for quantization must be positive integers.
+
+            In addition, there can be special string keys "type" and "scale_bits". Key "type"
+            should map to "dynamic" or "static" where "dynamic" indicates dynamic block quantization and "static"
+            indicates static calibrated block quantization. By default, the type is "static". Key "scale_bits"
+            specify the quantization bits for the per-block quantization scale factor
+            (i.e a double quantization scheme). By default per-block quantization scale is not quantized.
+
+            For example, ``block_sizes = {-1: 32}`` will quantize the last axis of the input tensor in
+            blocks of size 32 with static calibration and ``block_sizes = {-1: 32, "type": "dynamic"}``
+            will perform dynamic block quantization. If None, block
+            quantization is not performed. ``axis`` must be None when ``block_sizes`` is not None.
+
+        amax: A float or list/ndarray of floats of user specified absolute max range. If supplied,
+            ignore quant_axis and use this to quantize. If learn_amax is True, will be used to
+            initialize learnable amax.
+        learn_amax: A boolean. If True, learn amax.
+        scale_amax: A float. If supplied, multiply amax by scale_amax. Default None. It is useful
+            for some quick experiment.
+        calib_method: A string. One of ``["max", "histogram"]`` indicates which calibration to use.
+            Except the simple max calibration, other methods are all histogram based.
+        unsigned: A boolean. If True, use unsigned.
+        narrow_range: A boolean. if True, symmetric integer range for signed quantization is used.
+
+    Read-only properties:
+        - fake_quant:
+        - name:
+        - learn_amax:
+        - scale_amax:
+        - axis:
+        - calib_method:
+        - num_bits:
+        - amax:
+        - unsigned:
+    """
+
+    def __init__(
+        self,
+        num_bits=8,
+        name=None,
+        fake_quant=True,
+        axis=None,
+        block_sizes=None,
+        amax=None,
+        learn_amax=False,
+        scale_amax=None,
+        calib_method="max",
+        unsigned=False,
+        narrow_range=False,
+    ):
+        """Initialize QuantDescriptor."""
+        if isinstance(num_bits, int):
+            if num_bits < 0:
+                raise ValueError(f"num_bits must be > 0, not {num_bits}.")
+        elif num_bits not in [(4, 3), (2, 1)]:  # E4M3 and E2M1
+            raise TypeError(
+                "num_bits must be a postive integer or tuple such as (4, 3), (2, 1) etc., not"
+                f" {type(num_bits)}."
+            )
+
+        self._num_bits = num_bits
+        if not isinstance(name, str) and name is not None:
+            raise TypeError(f"name must be a string or None, not {type(name)}.")
+        self._name = name
+
+        self._fake_quant = fake_quant
+        self._axis = axis
+        self._block_sizes = None
+        if block_sizes is not None:
+            assert axis is None, "axis must be None when block_sizes is not None."
+            assert isinstance(block_sizes, dict), "block_sizes must be a dictionary."
+            assert all(
+                isinstance(key, int)
+                for key in self.get_block_quant_axes_and_sizes(block_sizes).keys()
+            ), "axes for block block quantization must be integers."
+            assert all(
+                isinstance(value, int) and value > 0
+                for value in self.get_block_quant_axes_and_sizes(block_sizes).values()
+            ), "block sizes for block quantization must be positive integers."
+            self._block_sizes = block_sizes
+
+        self._learn_amax = learn_amax
+        if self._learn_amax and self._axis is not None:
+            raise TypeError(
+                f"axis is ignored and must be None when learn_amax is true, got {type(self._axis)}."
+            )
+        if amax is not None:
+            if (
+                not isinstance(amax, float)
+                and not isinstance(amax, list)
+                and not isinstance(amax, np.ndarray)
+            ):
+                raise TypeError(f"amax must be float, list or ndarray, not {type(amax)}")
+            # Make it single precision array
+            self._amax = np.array(amax, dtype=np.float32)
+        else:
+            self._amax = amax
+
+        self._scale_amax = scale_amax
+        self._calib_method = calib_method
+        self._unsigned = unsigned
+        self._narrow_range = narrow_range
+
+    @property
+    def num_bits(self):
+        """Return num_bits."""
+        return self._num_bits
+
+    @property
+    def fake_quant(self):
+        """Return True if fake quantization is used."""
+        return self._fake_quant
+
+    @property
+    def axis(self):
+        """Return axis for quantization."""
+        return self._axis
+
+    @property
+    def block_sizes(self):
+        """Return block_sizes for quantization."""
+        return self._block_sizes
+
+    @staticmethod
+    def get_block_quant_axes_and_sizes(block_sizes: dict):
+        """Return axes and sizes for block quantization."""
+        if block_sizes is None:
+            return None
+        return {k: v for k, v in block_sizes.items() if k not in ["type", "scale_bits"]}
+
+    @property
+    def amax(self):
+        """Return amax."""
+        return self._amax
+
+    @property
+    def learn_amax(self):
+        """Return True if amax is learnable."""
+        return self._learn_amax
+
+    @property
+    def scale_amax(self):
+        """Return scale_amax."""
+        return self._scale_amax
+
+    @property
+    def name(self):
+        """Return name."""
+        return self._name
+
+    @property
+    def calib_method(self):
+        """Return calibration method."""
+        return self._calib_method
+
+    @property
+    def unsigned(self):
+        """Return True if unsigned integer range is used."""
+        return self._unsigned
+
+    @property
+    def narrow_range(self):
+        """Return True if symmetric integer range for signed quantization is used."""
+        return self._narrow_range
+
+    def __str__(self):
+        s = (self._name + ": ") if self._name is not None else "QuantDescriptor"
+        s += f"({'unsigned ' if self._unsigned else ''}{self._num_bits}bit"
+        s += " fake" if self._fake_quant else " real"
+        if self._block_sizes is not None:
+            s += f" block_sizes={self._block_sizes}"
+        else:
+            s += f" axis={self._axis if self._axis is not None else ' per-tensor'}"
+        if isinstance(self._amax, torch.Tensor):
+            amax_str = np.array2string(
+                self._amax.cpu().numpy().flatten(), edgeitems=1, formatter={"all": "{:.2e}".format}
+            )
+            s += f" amax={amax_str}"
+        elif self._amax is not None:
+            s += f" amax={self._amax}"
+            s += " full_range"
+        if self._learn_amax:
+            s += " learn_amax"
+        if self._scale_amax:
+            s += f" scale_amax={self._scale_amax}"
+        s += ")"
+        return s
+
+    def __eq__(self, rhs):
+        """Compare 2 descriptors."""
+        return self.__dict__ == rhs.__dict__
+
+    def dict(self):
+        """Serialize to dict.
+
+        The build-in __dict__ method returns all the attributes, which includes those have default value and have
+        protected prefix "_". This method only returns those have values other than the default one and don't have _ in
+        key. Construct a instance by dict returned by this method should get exactly the same instance.
+        """
+        obj_dict = {}
+        obj_dict["num_bits"] = self._num_bits
+        obj_dict["name"] = self._name
+
+        if not self._fake_quant:
+            obj_dict["fake_quant"] = self._fake_quant
+        if self._axis is not None:
+            obj_dict["axis"] = self._axis
+        if self._amax is not None:
+            obj_dict["amax"] = self._amax.tolist()
+        if self._scale_amax is not None:
+            obj_dict["scale_amax"] = self._scale_amax
+        if self._learn_amax:
+            obj_dict["learn_amax"] = self._learn_amax
+        if self._unsigned:
+            obj_dict["unsigned"] = self._unsigned
+
+        return obj_dict
+
+
+QuantDescriptor = ScaledQuantDescriptor
+
+# Predefined descriptors
+QUANT_DESC_8BIT_PER_TENSOR = QuantDescriptor(num_bits=8)
+QUANT_DESC_UNSIGNED_8BIT_PER_TENSOR = QuantDescriptor(num_bits=8, unsigned=True)
+QUANT_DESC_8BIT_CONV1D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
+QUANT_DESC_8BIT_CONV2D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
+QUANT_DESC_8BIT_CONV3D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
+QUANT_DESC_8BIT_LINEAR_WEIGHT_PER_ROW = QuantDescriptor(num_bits=8, axis=(0))
+QUANT_DESC_8BIT_CONVTRANSPOSE1D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
+QUANT_DESC_8BIT_CONVTRANSPOSE2D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
+QUANT_DESC_8BIT_CONVTRANSPOSE3D_WEIGHT_PER_CHANNEL = QuantDescriptor(num_bits=8, axis=(0))
+
+
+@torch.jit.script
+def _fake_tensor_quant_backward(inputs, amax, grad_outputs):
+    zero = grad_outputs.new_zeros(1)
+    grad_inputs = torch.where(inputs.abs() <= amax, grad_outputs, zero)
+    return grad_inputs
+
+
+def _onnx_int8_helper(g, inputs, amax, num_bits, unsigned, narrow_range):
+    assert num_bits == 8, "Only INT8 ONNX export is supported for now."
+    maxbound = (1 << (num_bits - 1 + int(unsigned))) - 1
+
+    if amax.numel() == 1:
+        zero_point, axis = torch.tensor(0.0, device=amax.device), None
+    else:
+        amax_init_shape = amax.shape
+        amax = amax.squeeze().data
+        assert len(amax.shape) == 1, "ONNX does not support multi-axis quantization."
+        zero_point = torch.zeros_like(amax, dtype=torch.int32).data
+        axis = list(amax_init_shape).index(list(amax.shape)[0])
+
+    zero_point = g.op("Constant", value_t=zero_point)
+
+    if not unsigned:
+        assert not narrow_range, "ONNX does not support unsigned narrow range INT8."
+        zero_point = g.op("Cast", zero_point, to_i=_C_onnx.TensorProtoDataType.INT8)
+    else:
+        zero_point = g.op("Cast", zero_point, to_i=_C_onnx.TensorProtoDataType.UINT8)
+
+    amax = amax.to(torch.float32)
+    scale = amax / maxbound
+    scale.masked_fill_(scale == 0, 1.0)
+    scale = g.op("Constant", value_t=scale)
+
+    input_type = inputs.type().scalarType()
+
+    # Q inputs are currently constrained to FP32 due to a similar limitation in ORT
+    # custom ops, so cast the input if needed.
+    if input_type == "Half" or input_type == "BFloat16":
+        inputs = g.op("Cast", inputs, to_i=_C_onnx.TensorProtoDataType.FLOAT)
+
+    quantized = g.op("QuantizeLinear", inputs, scale, zero_point, axis_i=axis)
+    out = g.op("DequantizeLinear", quantized, scale, zero_point, axis_i=axis)
+
+    # DQ outputs are currently constrained to FP32 due to a similar limitation in ORT
+    # custom ops, so cast the output if needed.
+    if input_type == "Half":
+        out = g.op("Cast", out, to_i=_C_onnx.TensorProtoDataType.FLOAT16)
+    elif input_type == "BFloat16":
+        out = g.op("Cast", out, to_i=_C_onnx.TensorProtoDataType.BFLOAT16)
+
+    return out
+
+
+class FakeTensorQuantFunction(Function):
+    """Fake version of TensorQuantFunction use CUDA extension."""
+
+    @staticmethod
+    @symbolic_helper.parse_args("v", "t", "i", "b", "b")
+    def symbolic(g, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
+        """ONNX symbolic function."""
+        return _onnx_int8_helper(g, inputs, amax, num_bits, unsigned, narrow_range)
+
+    @staticmethod
+    def forward(ctx, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
+        """Forward method."""
+        ctx.save_for_backward(inputs, amax)
+
+        def legacy_quant_func():
+            # The LegacyFakeTensorQuantFunction support cpu and amax with any shape that can be broadcasted to inputs.
+            outputs, scale = _tensor_quant(inputs, amax, num_bits, unsigned, narrow_range)
+            return outputs / scale.to(inputs.dtype)
+
+        if not inputs.is_cuda:
+            outputs = legacy_quant_func()
+        else:
+            try:
+                from .extensions import cuda_ext
+
+                with torch.cuda.device(
+                    None
+                    if inputs.device.index == torch.cuda.current_device()
+                    else inputs.device.index
+                ):
+                    if amax.numel() == 1:
+                        outputs = cuda_ext.fake_tensor_quant(  # type: ignore[union-attr]
+                            inputs, amax, num_bits, unsigned, narrow_range
+                        )
+                    else:
+                        axis = amax.shape.index(amax.numel())
+                        outputs = cuda_ext.fake_tensor_quant_with_axis(  # type: ignore[union-attr]
+                            inputs, amax.squeeze(), axis, num_bits, unsigned, narrow_range
+                        )
+            except (AttributeError, ValueError):
+                # AttributeError: cuda_ext is not imported, possibly due to CPU only installation
+                # ValueError: cuda_ext is installed, but trying to perform multidimensional quantization (amax dim > 1)
+                outputs = legacy_quant_func()
+
+        return outputs
+
+    @staticmethod
+    def backward(ctx, grad_outputs):
+        """Implements straight through estimation with clipping."""
+        inputs, amax = ctx.saved_tensors
+        return _fake_tensor_quant_backward(inputs, amax, grad_outputs), None, None, None, None
+
+
+def _onnx_fp8_quantize(g, inputs, scale_inv):
+    """Helper Function for Quantization."""
+    output_shape = torch.onnx.symbolic_helper._get_tensor_sizes(inputs)
+
+    # Q inputs are currently constrained to FP32 due to a similar limitation in ORT
+    # custom ops, so cast the input if needed.
+    if inputs.type().scalarType() == "Half" or inputs.type().scalarType() == "BFloat16":
+        inputs = g.op("Cast", inputs, to_i=_C_onnx.TensorProtoDataType.FLOAT)
+
+    scale = g.op("Constant", value_t=torch.tensor(scale_inv))
+    q_op = g.op("trt::TRT_FP8QuantizeLinear", inputs, scale).setType(
+        inputs.type().with_dtype(torch.uint8).with_sizes(output_shape)
+    )
+    return q_op
+
+
+def _onnx_fp8_dequantize(g, inputs, scale_inv, otype=None):
+    """Helper Function for Dequantization."""
+    output_shape = torch.onnx.symbolic_helper._get_tensor_sizes(inputs)
+
+    scale = g.op("Constant", value_t=torch.tensor(scale_inv))
+    out = g.op("trt::TRT_FP8DequantizeLinear", inputs, scale).setType(
+        inputs.type().with_dtype(torch.float32).with_sizes(output_shape)
+    )
+
+    # DQ outputs are currently constrained to FP32 due to a similar limitation in ORT
+    # custom ops, so cast the output if needed.
+    if otype == "Half":
+        out = g.op("Cast", out, to_i=_C_onnx.TensorProtoDataType.FLOAT16)
+    elif otype == "BFloat16":
+        out = g.op("Cast", out, to_i=_C_onnx.TensorProtoDataType.BFLOAT16)
+    return out
+
+
+class ScaledE4M3Function(Function):
+    """E4M3fy input with scale."""
+
+    @staticmethod
+    @symbolic_helper.parse_args("v", "t", "i", "b", "b")
+    def symbolic(g, inputs, amax=None, E=4, M=3):  # noqa: N803
+        """ONNX symbolic function."""
+        if amax is None:
+            scale = 1.0
+        else:
+            scale = 448.0 / float(amax)
+        otype = inputs.type().scalarType()
+        q_tensor = _onnx_fp8_quantize(g, inputs, 1.0 / scale)
+        return _onnx_fp8_dequantize(g, q_tensor, 1.0 / scale, otype)
+
+    @staticmethod
+    # Default values could cause errors from TorchDynamo during torch.export
+    def forward(ctx, inputs, amax, E, M):  # noqa: N803
+        """Forward method."""
+        if E != 4 or M != 3:
+            raise NotImplementedError("Only support E=4 & M=3 for now.")
+
+        ctx.save_for_backward(inputs)
+        ctx.amax = amax
+        outputs = ScaledE4M3_op(inputs, amax)
+
+        return outputs
+
+    @staticmethod
+    def backward(ctx, grad_outputs):
+        """Implements straight through estimation with clipping."""
+        (inputs,) = ctx.saved_tensors
+        amax = torch.tensor(
+            ctx.amax if ctx.amax is not None else 448.0, dtype=torch.float32, device=inputs.device
+        )
+        grad_inputs = _fake_tensor_quant_backward(inputs, amax, grad_outputs)
+        return grad_inputs, None, None, None
+
+
+class TensorQuantFunction(Function):
+    """A universal tensor quantization function.
+
+    Take an input tensor, output an quantized tensor. The granularity of scale can be interpreted from the
+    shape of amax.
+    output_dtype indicates whether the quantized value will be stored in integer or float. The reason we want to store
+    it in float is the pytorch function takes the quantized value may not accept integer input, e.g. Conv2D.
+
+    It uses 2^num_bits -1 values instead of 2^num_bits. e.g., for num_bits=8, it uses [-127, 127] instead of [-128, 127]
+    """
+
+    @staticmethod
+    @symbolic_helper.parse_args("v", "t", "i", "b", "b")
+    def symbolic(g, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
+        """ONNX symbolic function."""
+        return _onnx_int8_helper(g, inputs, amax, num_bits, unsigned, narrow_range)
+
+    @staticmethod
+    def forward(ctx, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
+        """Forward method.
+
+        Follow tensorflow convention, max value is passed in and used to decide scale, instead of inputing scale
+        directly. Though inputing scale directly may be more natural to use.
+
+        Args:
+            ctx: A Context object to store tensors for backward.
+            inputs: A Tensor of type float32.
+            amax: A Tensor of type float32. Inputs will be quantized within range [-amax, amax]
+                amax will be broadcasted to inputs tensor.
+            num_bits: A integer used to calculate scaling factor, scale = (2^(num_bits-1) - 1) / max
+                Effectively, it indicates how many integer bits is used to represent the value. Default 8.
+            output_dtype: A type of Tensor. torch.int32 or torch.float32.
+            unsigned: A boolean. Use unsigned integer range. E.g. [0, 255] for num_bits=8. Default False.
+            narrow_range: A boolean. Use symmetric integer range for signed quantization
+                E.g. [-127,127] instead of [-128,127] for num_bits=8. Default True.
+
+        Returns:
+            outputs: A Tensor of type output_dtype.
+            scale: A Tensor of type float32. outputs / scale will dequantize outputs tensor.
+
+        Raises:
+            ValueError:
+        """
+        ctx.save_for_backward(inputs, amax)
+        outputs, scale = _tensor_quant(inputs, amax, num_bits, unsigned, narrow_range)
+        # Check if scale overflows FP16
+        if outputs.dtype == torch.half and scale.max() > 65504:
+            raise ValueError(f"scale is too large for FP16 with amax={amax}")
+        return outputs, scale.to(inputs.dtype)
+
+    @staticmethod
+    def backward(ctx, grad_outputs, grad_scale):
+        """Implements straight through estimation with clipping.
+
+        For -amax <= input <= amax the gradient passes straight through, otherwise the gradient is zero.
+
+        Args:
+            ctx: A Context object with saved tensors from forward.
+            grad_outputs: A tensor of gradient of outputs.
+            grad_scale: A tensor of gradient of scale.
+
+        Returns:
+            grad_inputs: A tensor of gradient.
+        """
+        inputs, amax = ctx.saved_tensors
+        zero = grad_outputs.new_zeros(1)  # create a zero tensor with the same type and device
+        grad_inputs = torch.where(inputs.abs() <= amax, grad_outputs, zero)
+        return grad_inputs, None, None, None, None
+
+
+class LegacyFakeTensorQuantFunction(Function):
+    """Fake version of TensorQuantFunction.
+
+    See comments of TensorQuantFunction, arguments are the same.
+    """
+
+    @staticmethod
+    def forward(ctx, inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
+        """Forward method."""
+        ctx.save_for_backward(inputs, amax)
+        outputs, scale = _tensor_quant(inputs, amax, num_bits, unsigned, narrow_range)
+        return outputs / scale.to(inputs.dtype)
+
+    @staticmethod
+    def backward(ctx, grad_outputs):
+        """Implements straight through estimation."""
+        inputs, amax = ctx.saved_tensors
+        zero = grad_outputs.new_zeros(1)
+        grad_inputs = torch.where(inputs.abs() <= amax, grad_outputs, zero)
+        return grad_inputs, None, None, None, None
+
+
+def _tensor_quant(inputs, amax, num_bits=8, unsigned=False, narrow_range=True):
+    """Shared function body between TensorQuantFunction and FakeTensorQuantFunction."""
+    # Fine scale, per channel scale will be handled by broadcasting, which could be tricky. Pop a warning.
+    if unsigned:
+        if inputs.min() < 0.0:
+            raise TypeError("Negative values encountered in unsigned quantization.")
+
+    # Computation must be in FP32 to prevent potential over flow.
+    input_dtype = inputs.dtype
+    if inputs.dtype == torch.half:
+        inputs = inputs.float()
+    if amax.dtype == torch.half:
+        amax = amax.float()
+
+    min_amax = amax.min()
+    if min_amax < 0:
+        raise ValueError("Negative values in amax")
+
+    max_bound = torch.tensor((2.0 ** (num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)
+    if unsigned:
+        min_bound = 0
+    elif narrow_range:
+        min_bound = -max_bound
+    else:
+        min_bound = -max_bound - 1
+    scale = max_bound / amax
+
+    epsilon = 1.0 / (1 << 24)
+    if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0
+        zero_amax_mask = amax <= epsilon
+        scale[zero_amax_mask] = 0  # Value quantized with amax=0 should all be 0
+
+    outputs = torch.clamp((inputs * scale).round_(), min_bound, max_bound)
+
+    if min_amax <= epsilon:
+        scale[zero_amax_mask] = (
+            1.0  # Return 1 makes more sense for values quantized to 0 with amax=0
+        )
+
+    if input_dtype == torch.half:
+        outputs = outputs.half()
+
+    return outputs, scale
+
+
+class FakeAffineTensorQuantFunction(Function):
+    """Fake version of affine quantization.
+
+    gemmlowp style scale+shift quantization. See more details in
+    https://github.com/google/gemmlowp/blob/master/doc/quantization.md.
+
+    We DO NOT recommend affine quantization on weights for performance reason. There might be value to affine quantize
+    activation as it can be cancelled by bias and comes with no performance penalty. This functionality is only added
+    for experimental purpose.
+    """
+
+    @staticmethod
+    def forward(ctx, inputs, min_range, max_range, num_bits=8):
+        """As it will be only applied on activation with per tensor granularity, broadcast is not needed.
+
+        Args:
+            ctx: Pytorch convention.
+            inputs: A Tensor of type float32.
+            min_range: A float.
+            max_range: A float.
+            num_bits: An integer
+
+        Returns:
+            outputs: A Tensor of type output_dtype
+        """
+        ctx.save_for_backward(inputs, min_range, max_range)
+
+        step_size = (max_range - min_range) / (2.0**num_bits - 1)
+
+        min_bound = -(2.0 ** (num_bits - 1))
+        max_bound = 2.0 ** (num_bits - 1) - 1
+
+        quant_zero = torch.round(min_range / step_size) - min_bound
+        quantized = torch.round(inputs / step_size) - quant_zero
+        quantized = torch.clamp(quantized, min_bound, max_bound)
+
+        outputs = (quantized + quant_zero) * step_size
+
+        return outputs
+
+    @staticmethod
+    def backward(ctx, grad_outputs):
+        """Implements straight through estimation with clipping.
+
+        Args:
+            ctx: Pytorch convention.
+            grad_output: A tensor of gradient of outputs.
+
+        Returns:
+            grad_inputs: A tensor of gradient
+        """
+        inputs, min_range, max_range = ctx.saved_tensors
+        zero = grad_outputs.new_zeros(1)
+        grad_inputs = torch.where((inputs <= max_range) * (inputs >= min_range), grad_outputs, zero)
+        return grad_inputs, None, None, None
+
+
+tensor_quant = TensorQuantFunction.apply
+legacy_fake_tensor_quant = LegacyFakeTensorQuantFunction.apply
+fake_tensor_quant = FakeTensorQuantFunction.apply
+fake_affine_tensor_quant = FakeAffineTensorQuantFunction.apply
+scaled_e4m3 = ScaledE4M3Function.apply
```

## modelopt/torch/quantization/utils.py

 * *Ordering differences only*

```diff
@@ -1,167 +1,167 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Quantization utilities."""
-
-from contextlib import contextmanager
-
-import torch
-
-__all__ = [
-    "reduce_amax",
-    "is_quantized",
-    "is_quantized_layer_with_weight",
-    "is_quantized_column_parallel_linear",
-    "is_quantized_row_parallel_linear",
-    "replace_function",
-    "EXPORT_MODE",
-    "export_torch_mode",
-    "is_torch_library_supported",
-]
-
-
-def reduce_amax(input, axis=None, keepdims=True):
-    """Compute the absolute maximum value of a tensor.
-
-    Reduces input_tensor along the dimensions given in axis. Unless keepdims is true,
-    the rank of the tensor is reduced by 1 for each entry in axis. If keepdims is true,
-    the reduced dimensions are retained with length 1.
-
-    .. note::
-        Gradient computation is disabled as this function is never meant learning reduces amax
-
-    Args:
-        input: Input tensor
-        axis: The dimensions to reduce. None or int or tuple of ints. If None (the default),
-            reduces all dimensions. Must be in the range [-rank(input_tensor), rank(input_tensor)).
-        keepdims: A boolean. If true, retains reduced dimensions with length 1. Default True
-        granularity: DEPRECTED. specifies if the statistic has to be calculated at tensor or channel granularity
-
-    Returns:
-        The reduced tensor.
-
-    Raises:
-        ValueError: Any axis which doesn't make sense or is not supported
-        ValueError: If unknown granularity is passed in.
-    """
-    with torch.no_grad():
-        # A memory-efficient implementation that avoids copying input tensor
-        if axis is None:
-            max_val = torch.max(input)
-            min_val = torch.min(input)
-            output = torch.maximum(torch.abs(max_val), torch.abs(min_val))
-        else:
-            if isinstance(axis, int):
-                axis = (axis,)
-            max_val = torch.amax(input, dim=axis, keepdim=keepdims)
-            min_val = torch.amin(input, dim=axis, keepdim=keepdims)
-            output = torch.maximum(torch.abs(max_val), torch.abs(min_val))
-            if output.numel() == 1:
-                output.squeeze_()
-        return output
-
-
-def is_quantized(module):
-    """Check if a module is quantized."""
-    from modelopt.torch.quantization.nn import TensorQuantizer
-
-    for _module in module.modules():
-        if isinstance(_module, TensorQuantizer):
-            return True
-    return False
-
-
-def is_quantized_layer_with_weight(module):
-    """Check if a module is quantized with weights."""
-    return is_quantized(module) and getattr(module, "weight", None) is not None
-
-
-def is_quantized_linear(module):
-    """Check if a module is a quantized linear module."""
-    return (
-        hasattr(module, "input_quantizer")
-        and hasattr(module, "weight_quantizer")
-        and getattr(module, "weight", None) is not None
-        and module.weight.dim() == 2
-    )
-
-
-def _is_quantized_parallel_linear(module, mod_name):
-    parallel_layers = []
-    try:
-        import apex.transformer.tensor_parallel.layers as apex_parallel
-
-        parallel_layers.append(getattr(apex_parallel, mod_name))
-    except ImportError:
-        pass
-
-    try:
-        import megatron.core.tensor_parallel.layers as megatron_parallel
-
-        parallel_layers.append(getattr(megatron_parallel, mod_name))
-    except ImportError:
-        pass
-
-    return is_quantized_linear(module) and isinstance(module, tuple(parallel_layers))
-
-
-def is_quantized_column_parallel_linear(module):
-    """Check if a module is a quantized column parallel linear module."""
-    return _is_quantized_parallel_linear(module, "ColumnParallelLinear")
-
-
-def is_quantized_row_parallel_linear(module):
-    """Check if a module is a quantized row parallel linear module."""
-    return _is_quantized_parallel_linear(module, "RowParallelLinear")
-
-
-@contextmanager
-def replace_function(package, name, new_func):
-    """Replace a function with a new one within a context."""
-    old_func = getattr(package, name)
-    setattr(package, name, new_func)
-    setattr(package, "_" + name, old_func)
-    yield
-    setattr(package, name, old_func)
-    delattr(package, "_" + name)
-
-
-EXPORT_MODE: bool = False
-
-
-@contextmanager
-def export_torch_mode():
-    """Context manager enabling the export mode."""
-    global EXPORT_MODE
-    original_value = EXPORT_MODE
-    EXPORT_MODE = True
-    try:
-        yield
-    finally:
-        EXPORT_MODE = original_value
-
-
-def is_torch_export_mode():
-    """Check whether in the context of exporting model to torch."""
-    return EXPORT_MODE
-
-
-def is_torch_library_supported():
-    """Check if the installed PyTorch version meets or exceeds a specified version."""
-    ver_strs = torch.__version__.split(".")
-    major, minor = ver_strs[0], ver_strs[1]
-    # Require torch version >= 2.2.0
-    # Adding checks for `impl` and `impl_abstract` as they are experiemental features
-    return (
-        major >= "2"
-        and minor >= "2"
-        and hasattr(torch.library, "impl")
-        and hasattr(torch.library, "impl_abstract")
-    )
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Quantization utilities."""
+
+from contextlib import contextmanager
+
+import torch
+
+__all__ = [
+    "reduce_amax",
+    "is_quantized",
+    "is_quantized_layer_with_weight",
+    "is_quantized_column_parallel_linear",
+    "is_quantized_row_parallel_linear",
+    "replace_function",
+    "EXPORT_MODE",
+    "export_torch_mode",
+    "is_torch_library_supported",
+]
+
+
+def reduce_amax(input, axis=None, keepdims=True):
+    """Compute the absolute maximum value of a tensor.
+
+    Reduces input_tensor along the dimensions given in axis. Unless keepdims is true,
+    the rank of the tensor is reduced by 1 for each entry in axis. If keepdims is true,
+    the reduced dimensions are retained with length 1.
+
+    .. note::
+        Gradient computation is disabled as this function is never meant learning reduces amax
+
+    Args:
+        input: Input tensor
+        axis: The dimensions to reduce. None or int or tuple of ints. If None (the default),
+            reduces all dimensions. Must be in the range [-rank(input_tensor), rank(input_tensor)).
+        keepdims: A boolean. If true, retains reduced dimensions with length 1. Default True
+        granularity: DEPRECTED. specifies if the statistic has to be calculated at tensor or channel granularity
+
+    Returns:
+        The reduced tensor.
+
+    Raises:
+        ValueError: Any axis which doesn't make sense or is not supported
+        ValueError: If unknown granularity is passed in.
+    """
+    with torch.no_grad():
+        # A memory-efficient implementation that avoids copying input tensor
+        if axis is None:
+            max_val = torch.max(input)
+            min_val = torch.min(input)
+            output = torch.maximum(torch.abs(max_val), torch.abs(min_val))
+        else:
+            if isinstance(axis, int):
+                axis = (axis,)
+            max_val = torch.amax(input, dim=axis, keepdim=keepdims)
+            min_val = torch.amin(input, dim=axis, keepdim=keepdims)
+            output = torch.maximum(torch.abs(max_val), torch.abs(min_val))
+            if output.numel() == 1:
+                output.squeeze_()
+        return output
+
+
+def is_quantized(module):
+    """Check if a module is quantized."""
+    from modelopt.torch.quantization.nn import TensorQuantizer
+
+    for _module in module.modules():
+        if isinstance(_module, TensorQuantizer):
+            return True
+    return False
+
+
+def is_quantized_layer_with_weight(module):
+    """Check if a module is quantized with weights."""
+    return is_quantized(module) and getattr(module, "weight", None) is not None
+
+
+def is_quantized_linear(module):
+    """Check if a module is a quantized linear module."""
+    return (
+        hasattr(module, "input_quantizer")
+        and hasattr(module, "weight_quantizer")
+        and getattr(module, "weight", None) is not None
+        and module.weight.dim() == 2
+    )
+
+
+def _is_quantized_parallel_linear(module, mod_name):
+    parallel_layers = []
+    try:
+        import apex.transformer.tensor_parallel.layers as apex_parallel
+
+        parallel_layers.append(getattr(apex_parallel, mod_name))
+    except ImportError:
+        pass
+
+    try:
+        import megatron.core.tensor_parallel.layers as megatron_parallel
+
+        parallel_layers.append(getattr(megatron_parallel, mod_name))
+    except ImportError:
+        pass
+
+    return is_quantized_linear(module) and isinstance(module, tuple(parallel_layers))
+
+
+def is_quantized_column_parallel_linear(module):
+    """Check if a module is a quantized column parallel linear module."""
+    return _is_quantized_parallel_linear(module, "ColumnParallelLinear")
+
+
+def is_quantized_row_parallel_linear(module):
+    """Check if a module is a quantized row parallel linear module."""
+    return _is_quantized_parallel_linear(module, "RowParallelLinear")
+
+
+@contextmanager
+def replace_function(package, name, new_func):
+    """Replace a function with a new one within a context."""
+    old_func = getattr(package, name)
+    setattr(package, name, new_func)
+    setattr(package, "_" + name, old_func)
+    yield
+    setattr(package, name, old_func)
+    delattr(package, "_" + name)
+
+
+EXPORT_MODE: bool = False
+
+
+@contextmanager
+def export_torch_mode():
+    """Context manager enabling the export mode."""
+    global EXPORT_MODE
+    original_value = EXPORT_MODE
+    EXPORT_MODE = True
+    try:
+        yield
+    finally:
+        EXPORT_MODE = original_value
+
+
+def is_torch_export_mode():
+    """Check whether in the context of exporting model to torch."""
+    return EXPORT_MODE
+
+
+def is_torch_library_supported():
+    """Check if the installed PyTorch version meets or exceeds a specified version."""
+    ver_strs = torch.__version__.split(".")
+    major, minor = ver_strs[0], ver_strs[1]
+    # Require torch version >= 2.2.0
+    # Adding checks for `impl` and `impl_abstract` as they are experiemental features
+    return (
+        major >= "2"
+        and minor >= "2"
+        and hasattr(torch.library, "impl")
+        and hasattr(torch.library, "impl_abstract")
+    )
```

## modelopt/torch/quantization/calib/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Calibrator classes.
-
-``modelopt.torch.quantization.calib`` provides Calibrator classes that
-collect data statistics and determine modelopt.torch.quantization parameters.
-"""
-
-from .calibrator import *
-from .histogram import *
-from .max import *
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Calibrator classes.
+
+``modelopt.torch.quantization.calib`` provides Calibrator classes that
+collect data statistics and determine modelopt.torch.quantization parameters.
+"""
+
+from .calibrator import *
+from .histogram import *
+from .max import *
```

## modelopt/torch/quantization/calib/calibrator.py

 * *Ordering differences only*

```diff
@@ -1,58 +1,58 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Abstract base class for calibrators."""
-
-__all__ = ["_Calibrator"]
-
-
-class _Calibrator:
-    """Abstract base class of calibrators.
-
-    Args:
-        num_bits: An integer. Number of bits of quantization.
-        axis: A tuple. see QuantDescriptor.
-        unsigned: A boolean. using unsigned quantization.
-
-    Readonly Properties:
-        axis:
-    """
-
-    def __init__(self, num_bits=8, axis=None, unsigned=False):
-        """Initialize."""
-        self._num_bits = num_bits
-        self._axis = axis
-        self._unsigned = unsigned
-
-    def collect(self, x):
-        """Abstract method: collect tensor statistics used to compute amax.
-
-        Args:
-            x: A tensor
-        """
-        raise NotImplementedError
-
-    def reset(self):
-        """Abstract method: reset calibrator to initial state."""
-        raise NotImplementedError
-
-    def compute_amax(self, *args, **kwargs):
-        """Abstract method: compute the amax from the collected data.
-
-        Returns:
-            amax: a tensor
-        """
-        raise NotImplementedError
-
-    def __repr__(self):
-        s = "num_bits={_num_bits}"
-        s += " axis={_axis}"
-        s += " unsigned={_unsigned}"
-        return s.format(**self.__dict__)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Abstract base class for calibrators."""
+
+__all__ = ["_Calibrator"]
+
+
+class _Calibrator:
+    """Abstract base class of calibrators.
+
+    Args:
+        num_bits: An integer. Number of bits of quantization.
+        axis: A tuple. see QuantDescriptor.
+        unsigned: A boolean. using unsigned quantization.
+
+    Readonly Properties:
+        axis:
+    """
+
+    def __init__(self, num_bits=8, axis=None, unsigned=False):
+        """Initialize."""
+        self._num_bits = num_bits
+        self._axis = axis
+        self._unsigned = unsigned
+
+    def collect(self, x):
+        """Abstract method: collect tensor statistics used to compute amax.
+
+        Args:
+            x: A tensor
+        """
+        raise NotImplementedError
+
+    def reset(self):
+        """Abstract method: reset calibrator to initial state."""
+        raise NotImplementedError
+
+    def compute_amax(self, *args, **kwargs):
+        """Abstract method: compute the amax from the collected data.
+
+        Returns:
+            amax: a tensor
+        """
+        raise NotImplementedError
+
+    def __repr__(self):
+        s = "num_bits={_num_bits}"
+        s += " axis={_axis}"
+        s += " unsigned={_unsigned}"
+        return s.format(**self.__dict__)
```

## modelopt/torch/quantization/calib/histogram.py

 * *Ordering differences only*

```diff
@@ -1,427 +1,427 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Histogram based calibrators."""
-import warnings
-from collections import Counter
-
-import numpy as np
-import torch
-import torch.distributed as dist
-from scipy.stats import entropy
-
-from .. import nn as qnn
-from .. import utils as quant_utils
-from ..tensor_quant import fake_tensor_quant, scaled_e4m3
-from .calibrator import _Calibrator
-
-__all__ = ["HistogramCalibrator", "calibrate_weights"]
-
-
-class HistogramCalibrator(_Calibrator):
-    """Unified histogram calibrator.
-
-    Histogram will be only collected once. compute_amax() performs entropy, percentile, or mse
-        calibration based on arguments
-
-    Args:
-        num_bits: An integer. Number of bits of quantization.
-        axis: A tuple. see QuantDescriptor.
-        unsigned: A boolean. using unsigned quantization.
-        num_bins: An integer. Number of histograms bins. Default 2048.
-        grow_method: A string. DEPRECATED. default None.
-        skip_zeros: A boolean. If True, skips zeros when collecting data for histogram. Default False.
-        torch_hist: A boolean. If True, collect histogram by torch.histc instead of np.histogram. If input tensor
-            is on GPU, histc will also be running on GPU. Default True.
-    """
-
-    def __init__(
-        self,
-        num_bits=8,
-        axis=None,
-        unsigned=False,
-        num_bins=2048,
-        grow_method=None,
-        skip_zeros=False,
-        torch_hist=True,
-    ):
-        """Initialize."""
-        super(HistogramCalibrator, self).__init__(num_bits, axis, unsigned)
-        self._num_bins = num_bins
-        self._skip_zeros = skip_zeros
-
-        self._calib_bin_edges = None
-        self._calib_hist = None
-
-        self._torch_hist = torch_hist
-
-        if axis is not None:
-            raise NotImplementedError(
-                "Calibrator histogram collection only supports per tensor scaling"
-            )
-
-    def collect(self, x):
-        """Collect histogram."""
-        if torch.min(x) < 0.0:
-            x = x.abs()
-
-        x = x.float()
-
-        if not self._torch_hist:
-            x_np = x.cpu().detach().numpy()
-
-            if self._skip_zeros:
-                x_np = x_np[np.where(x_np != 0)]
-
-            if self._calib_bin_edges is None and self._calib_hist is None:
-                # first time it uses num_bins to compute histogram.
-                self._calib_hist, self._calib_bin_edges = np.histogram(x_np, bins=self._num_bins)
-            else:
-                temp_amax = np.max(x_np)
-                if temp_amax > self._calib_bin_edges[-1]:  # type: ignore
-                    # increase the number of bins
-                    width = self._calib_bin_edges[1] - self._calib_bin_edges[0]  # type: ignore
-                    # NOTE: np.arange may create an extra bin after the one containing temp_amax
-                    new_bin_edges = np.arange(
-                        self._calib_bin_edges[-1] + width, temp_amax + width, width  # type: ignore
-                    )
-                    self._calib_bin_edges = np.hstack((self._calib_bin_edges, new_bin_edges))
-                hist, self._calib_bin_edges = np.histogram(x_np, bins=self._calib_bin_edges)
-                hist[: len(self._calib_hist)] += self._calib_hist  # type: ignore
-                self._calib_hist = hist
-        else:
-            # This branch of code is designed to match numpy version as close as possible
-            with torch.no_grad():
-                if self._skip_zeros:
-                    x = x[torch.where(x != 0)]
-
-                # Because we collect histogram on absolute value, setting min=0 simplifying the rare case where
-                # minimum value is not exactly 0 and first batch collected has larger min value than later batches
-                x_max = x.max()
-                if self._calib_bin_edges is None and self._calib_hist is None:
-                    self._calib_hist = torch.histc(x, bins=self._num_bins, min=0, max=x_max)
-                    self._calib_bin_edges = torch.linspace(0, x_max, self._num_bins + 1)
-                else:
-                    if x_max > self._calib_bin_edges[-1]:  # type: ignore
-                        width = self._calib_bin_edges[1] - self._calib_bin_edges[0]  # type: ignore
-                        self._num_bins = int((x_max / width).ceil().item())
-                        self._calib_bin_edges = torch.arange(
-                            0, x_max + width, width, device=x.device
-                        )
-
-                    hist = torch.histc(x, bins=self._num_bins, min=0, max=self._calib_bin_edges[-1])  # type: ignore
-                    hist[: self._calib_hist.numel()] += self._calib_hist  # type: ignore
-                    self._calib_hist = hist
-
-    def reset(self):
-        """Reset the collected histogram."""
-        self._calib_bin_edges = None
-        self._calib_hist = None
-
-    def compute_amax(
-        self,
-        method: str,
-        *,
-        stride: int = 1,
-        start_bin: int = 128,
-        percentile: float = 99.99,
-    ):
-        """Compute the amax from the collected histogram.
-
-        Args:
-            method: A string. One of ['entropy', 'mse', 'percentile']
-
-        Keyword Arguments:
-            stride: An integer. Default 1
-            start_bin: An integer. Default 128
-            percentils: A float number between [0, 100]. Default 99.99.
-
-        Returns:
-            amax: a tensor
-        """
-        if dist.is_available() and dist.is_initialized():
-            warnings.warn(
-                "This method does not perform any synchronization across DistributedDataParallel"
-                " (DDP) https://pytorch.org/docs/stable/notes/ddp.html modules. The recommended"
-                " method is to use the same calibration dataset across all distributed data"
-                " parallel groups so that `amax` is the same for all DDP modules."
-            )
-
-        if isinstance(self._calib_hist, torch.Tensor):
-            calib_hist = self._calib_hist.int().cpu().numpy()
-            calib_bin_edges = self._calib_bin_edges.cpu().numpy()  # type: ignore
-        else:
-            calib_hist = self._calib_hist
-            calib_bin_edges = self._calib_bin_edges
-
-        if method == "entropy":
-            calib_amax = _compute_amax_entropy(
-                calib_hist, calib_bin_edges, self._num_bits, self._unsigned, stride, start_bin
-            )
-        elif method == "mse":
-            calib_amax = _compute_amax_mse(
-                calib_hist, calib_bin_edges, self._num_bits, self._unsigned, stride, start_bin
-            )
-        elif method == "percentile":
-            calib_amax = _compute_amax_percentile(calib_hist, calib_bin_edges, percentile)
-        else:
-            raise TypeError("Unknown calibration method {}".format(method))
-
-        return calib_amax
-
-    def __str__(self):
-        s = "HistogramCalibrator("
-        if self._calib_bin_edges is None:
-            bin_edge_str = "None"
-        else:
-            bin_edge_str = "[{:.3f}, ..., {:.3f}]({})".format(
-                self._calib_bin_edges[0], self._calib_bin_edges[-1], len(self._calib_bin_edges)
-            )
-        s += "calib_bin_edges={})".format(bin_edge_str)
-        return s
-
-    def __repr__(self):
-        s = "HistogramCalibrator("
-        s += super(HistogramCalibrator, self).__repr__()
-        s += " calib_bin_edges={_calib_bin_edges}"
-        s += " calib_hist={_calib_hist})"
-        return s.format(**self.__dict__)
-
-
-# Ideally, we want to decouple collector (collect histogram) and calibrator (compute amax) as opposed to
-# the current calibrator design. The following compute amax functions are broken out from the calibrator
-# as first step towards there.
-def _compute_amax_entropy(calib_hist, calib_bin_edges, num_bits, unsigned, stride=1, start_bin=128):
-    """Returns amax that minimizes KL-Divergence of the collected histogram."""
-    # If calibrator hasn't collected any data, return none
-    if calib_bin_edges is None and calib_hist is None:
-        return None
-
-    def _normalize_distr(distr):
-        summ = np.sum(distr)
-        if summ != 0:
-            distr = distr / summ
-
-    bins = calib_hist[:]
-    bins[0] = bins[1]
-
-    total_data = np.sum(bins)
-
-    divergences = []
-    arguments = []
-
-    # we are quantizing to 128 values + sign if num_bits=8
-    nbins = 1 << (num_bits - 1 + int(unsigned))
-
-    starting = start_bin
-    stop = len(bins)
-
-    new_density_counts = np.zeros(nbins, dtype=np.float64)
-
-    for i in range(starting, stop + 1, stride):
-        new_density_counts.fill(0)
-        space = np.linspace(0, i, num=nbins + 1)
-        digitized_space = np.digitize(range(i), space) - 1
-
-        digitized_space[bins[:i] == 0] = -1
-
-        for idx, digitized in enumerate(digitized_space):
-            if digitized != -1:
-                new_density_counts[digitized] += bins[idx]
-
-        counter = Counter(digitized_space)
-        for key, val in counter.items():
-            if key != -1:
-                new_density_counts[key] = new_density_counts[key] / val
-
-        new_density = np.zeros(i, dtype=np.float64)
-        for idx, digitized in enumerate(digitized_space):
-            if digitized != -1:
-                new_density[idx] = new_density_counts[digitized]
-
-        total_counts_new = np.sum(new_density) + np.sum(bins[i:])
-        _normalize_distr(new_density)
-
-        reference_density = np.array(bins[: len(digitized_space)])
-        reference_density[-1] += np.sum(bins[i:])
-
-        total_counts_old = np.sum(reference_density)
-        if round(total_counts_new) != total_data or round(total_counts_old) != total_data:
-            raise RuntimeError(
-                "Count mismatch! total_counts_new={}, total_counts_old={}, total_data={}".format(
-                    total_counts_new, total_counts_old, total_data
-                )
-            )
-
-        _normalize_distr(reference_density)
-
-        ent = entropy(reference_density, new_density)
-        divergences.append(ent)
-        arguments.append(i)
-
-    divergences = np.array(divergences)
-    last_argmin = len(divergences) - 1 - np.argmin(divergences[::-1])
-    calib_amax = calib_bin_edges[last_argmin * stride + starting]
-    calib_amax = torch.tensor(calib_amax.item())
-
-    return calib_amax
-
-
-def _compute_amax_mse(calib_hist, calib_bin_edges, num_bits, unsigned, stride=1, start_bin=128):
-    """Returns amax that minimizes MSE of the collected histogram."""
-    # If calibrator hasn't collected any data, return none
-    if calib_bin_edges is None and calib_hist is None:
-        return None
-
-    counts = torch.from_numpy(calib_hist[:]).float()
-    edges = torch.from_numpy(calib_bin_edges[:]).float()
-
-    device = None
-    if torch.cuda.is_available():
-        device = counts.device
-        counts = counts.cuda()
-        edges = edges.cuda()
-
-    centers = (edges[1:] + edges[:-1]) / 2
-
-    mses = []
-    arguments = []
-
-    for i in range(start_bin, len(centers), stride):
-        amax = centers[i]
-        if isinstance(num_bits, int) and num_bits >= 0:
-            quant_centers = fake_tensor_quant(centers, amax, num_bits, unsigned)
-        elif num_bits == (4, 3):
-            quant_centers = scaled_e4m3(centers, amax, num_bits[0], num_bits[1])
-        else:
-            raise TypeError("Invalid num_bits. num_bits must be a positive integer or tuple (4,3).")
-
-        mse = ((quant_centers - centers) ** 2 * counts).mean()
-
-        mses.append(mse.cpu())
-        arguments.append(i)
-
-    argmin = np.argmin(mses)
-    calib_amax = centers[arguments[argmin]]
-
-    if device is not None:
-        calib_amax = calib_amax.to(device)
-
-    return calib_amax
-
-
-def _compute_amax_percentile(calib_hist, calib_bin_edges, percentile):
-    """Returns amax that clips the percentile fraction of collected data."""
-    if percentile < 0 or percentile > 100:
-        raise ValueError("Invalid percentile. Must be in range 0 <= percentile <= 100.")
-
-    # If calibrator hasn't collected any data, return none
-    if calib_bin_edges is None and calib_hist is None:
-        return None
-
-    total = calib_hist.sum()
-    cdf = np.cumsum(calib_hist / total)
-    idx = np.searchsorted(cdf, percentile / 100)
-    calib_amax = calib_bin_edges[idx]
-    calib_amax = torch.tensor(calib_amax.item())
-
-    return calib_amax
-
-
-def calibrate_weights(model, method="percentile", perchannel=True, percentile=99.99, num_bins=2048):
-    """Calibrate weights of all child quantized modules.
-
-    Ideally, we would split calibration functionality to histogram collector and calibrator which
-    takes histogram and compute amax. But since we haven't decoupled collector and calibrator, it
-    is easier to create a separate function to calibrate weight.
-
-    .. note::
-        This function uses `method` specified by the argument to decide which method to use, NOT the one
-        specified by the calibrator embedded in weight_quantizer.
-        We haven't moved calibration to GPU, so everything is transfered to CPU
-
-    Args:
-        model: A torch.nn.Module.
-        method: A string of calibration method. Supports "mse" and "percentile". Default "percentile"
-        perchannel: A bool. Set channel/neuron axis if True. Default True.
-        percentile: A float. Default 99.99
-        num_bins: A integer. Number of bins of histogram. Default 2048.
-
-    """
-    for name, module in model.named_modules():
-        if hasattr(module, "weight") and hasattr(module, "weight_quantizer"):
-            num_bits = module.weight_quantizer.num_bits
-            unsigned = module.weight_quantizer.unsigned
-            channel_second_modules = (
-                qnn.QuantConvTranspose1d,
-                qnn.QuantConvTranspose2d,
-                qnn.QuantConvTranspose3d,
-            )
-            if perchannel:
-                axis = 1 if isinstance(module, channel_second_modules) else 0
-            else:
-                axis = None
-            axis_size = module.weight.shape[axis] if axis is not None else 1
-
-            # Histogram is always collected even if method is "max". Although "max" is supported here
-            # but it is not the primary usage of this function
-            if axis is None:
-                input_weights = module.weight.abs().cpu().detach().numpy()
-                calib_hist, calib_bin_edges = np.histogram(
-                    input_weights, bins=2048, range=(0, input_weights.max())
-                )
-                calib_hist = [calib_hist]
-                calib_bin_edges = [calib_bin_edges]
-            else:
-                calib_hist = []
-                calib_bin_edges = []
-                for i in range(axis_size):
-                    input_weights = (
-                        module.weight.index_select(
-                            axis, torch.tensor(i, device=module.weight.device)
-                        )
-                        .abs()
-                        .cpu()
-                        .detach()
-                        .numpy()
-                    )
-                    hist, bin_edges = np.histogram(
-                        input_weights, bins=num_bins, range=(0, input_weights.max())
-                    )
-                    calib_hist.append(hist)
-                    calib_bin_edges.append(bin_edges)
-
-            calib_amax = []
-            if method == "max":
-                reduce_axis = list(range(module.weight.dim()))
-                reduce_axis.remove(axis)  # type: ignore
-                calib_amax.append(quant_utils.reduce_amax(module.weight, axis=reduce_axis))
-            elif method == "mse":
-                for i in range(axis_size):
-                    calib_amax.append(
-                        _compute_amax_mse(calib_hist[i], calib_bin_edges[i], num_bits, unsigned)
-                    )
-            elif method == "percentile":
-                for i in range(axis_size):
-                    calib_amax.append(
-                        _compute_amax_percentile(calib_hist[i], calib_bin_edges[i], percentile)
-                    )
-            else:
-                raise TypeError("Unsupported calibration method {}".format(method))
-
-            if axis is None:
-                calib_amax_t = calib_amax[0]
-            else:
-                calib_amax_shape = [1] * module.weight.dim()
-                calib_amax_shape[axis] = module.weight.shape[axis]
-                calib_amax_t = torch.stack(calib_amax).reshape(calib_amax_shape)
-            if calib_amax_t.numel() == 1:
-                calib_amax_t.squeeze_()
-            module.weight_quantizer.amax = calib_amax_t.detach().cpu().numpy()
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Histogram based calibrators."""
+import warnings
+from collections import Counter
+
+import numpy as np
+import torch
+import torch.distributed as dist
+from scipy.stats import entropy
+
+from .. import nn as qnn
+from .. import utils as quant_utils
+from ..tensor_quant import fake_tensor_quant, scaled_e4m3
+from .calibrator import _Calibrator
+
+__all__ = ["HistogramCalibrator", "calibrate_weights"]
+
+
+class HistogramCalibrator(_Calibrator):
+    """Unified histogram calibrator.
+
+    Histogram will be only collected once. compute_amax() performs entropy, percentile, or mse
+        calibration based on arguments
+
+    Args:
+        num_bits: An integer. Number of bits of quantization.
+        axis: A tuple. see QuantDescriptor.
+        unsigned: A boolean. using unsigned quantization.
+        num_bins: An integer. Number of histograms bins. Default 2048.
+        grow_method: A string. DEPRECATED. default None.
+        skip_zeros: A boolean. If True, skips zeros when collecting data for histogram. Default False.
+        torch_hist: A boolean. If True, collect histogram by torch.histc instead of np.histogram. If input tensor
+            is on GPU, histc will also be running on GPU. Default True.
+    """
+
+    def __init__(
+        self,
+        num_bits=8,
+        axis=None,
+        unsigned=False,
+        num_bins=2048,
+        grow_method=None,
+        skip_zeros=False,
+        torch_hist=True,
+    ):
+        """Initialize."""
+        super(HistogramCalibrator, self).__init__(num_bits, axis, unsigned)
+        self._num_bins = num_bins
+        self._skip_zeros = skip_zeros
+
+        self._calib_bin_edges = None
+        self._calib_hist = None
+
+        self._torch_hist = torch_hist
+
+        if axis is not None:
+            raise NotImplementedError(
+                "Calibrator histogram collection only supports per tensor scaling"
+            )
+
+    def collect(self, x):
+        """Collect histogram."""
+        if torch.min(x) < 0.0:
+            x = x.abs()
+
+        x = x.float()
+
+        if not self._torch_hist:
+            x_np = x.cpu().detach().numpy()
+
+            if self._skip_zeros:
+                x_np = x_np[np.where(x_np != 0)]
+
+            if self._calib_bin_edges is None and self._calib_hist is None:
+                # first time it uses num_bins to compute histogram.
+                self._calib_hist, self._calib_bin_edges = np.histogram(x_np, bins=self._num_bins)
+            else:
+                temp_amax = np.max(x_np)
+                if temp_amax > self._calib_bin_edges[-1]:  # type: ignore
+                    # increase the number of bins
+                    width = self._calib_bin_edges[1] - self._calib_bin_edges[0]  # type: ignore
+                    # NOTE: np.arange may create an extra bin after the one containing temp_amax
+                    new_bin_edges = np.arange(
+                        self._calib_bin_edges[-1] + width, temp_amax + width, width  # type: ignore
+                    )
+                    self._calib_bin_edges = np.hstack((self._calib_bin_edges, new_bin_edges))
+                hist, self._calib_bin_edges = np.histogram(x_np, bins=self._calib_bin_edges)
+                hist[: len(self._calib_hist)] += self._calib_hist  # type: ignore
+                self._calib_hist = hist
+        else:
+            # This branch of code is designed to match numpy version as close as possible
+            with torch.no_grad():
+                if self._skip_zeros:
+                    x = x[torch.where(x != 0)]
+
+                # Because we collect histogram on absolute value, setting min=0 simplifying the rare case where
+                # minimum value is not exactly 0 and first batch collected has larger min value than later batches
+                x_max = x.max()
+                if self._calib_bin_edges is None and self._calib_hist is None:
+                    self._calib_hist = torch.histc(x, bins=self._num_bins, min=0, max=x_max)
+                    self._calib_bin_edges = torch.linspace(0, x_max, self._num_bins + 1)
+                else:
+                    if x_max > self._calib_bin_edges[-1]:  # type: ignore
+                        width = self._calib_bin_edges[1] - self._calib_bin_edges[0]  # type: ignore
+                        self._num_bins = int((x_max / width).ceil().item())
+                        self._calib_bin_edges = torch.arange(
+                            0, x_max + width, width, device=x.device
+                        )
+
+                    hist = torch.histc(x, bins=self._num_bins, min=0, max=self._calib_bin_edges[-1])  # type: ignore
+                    hist[: self._calib_hist.numel()] += self._calib_hist  # type: ignore
+                    self._calib_hist = hist
+
+    def reset(self):
+        """Reset the collected histogram."""
+        self._calib_bin_edges = None
+        self._calib_hist = None
+
+    def compute_amax(
+        self,
+        method: str,
+        *,
+        stride: int = 1,
+        start_bin: int = 128,
+        percentile: float = 99.99,
+    ):
+        """Compute the amax from the collected histogram.
+
+        Args:
+            method: A string. One of ['entropy', 'mse', 'percentile']
+
+        Keyword Arguments:
+            stride: An integer. Default 1
+            start_bin: An integer. Default 128
+            percentils: A float number between [0, 100]. Default 99.99.
+
+        Returns:
+            amax: a tensor
+        """
+        if dist.is_available() and dist.is_initialized():
+            warnings.warn(
+                "This method does not perform any synchronization across DistributedDataParallel"
+                " (DDP) https://pytorch.org/docs/stable/notes/ddp.html modules. The recommended"
+                " method is to use the same calibration dataset across all distributed data"
+                " parallel groups so that `amax` is the same for all DDP modules."
+            )
+
+        if isinstance(self._calib_hist, torch.Tensor):
+            calib_hist = self._calib_hist.int().cpu().numpy()
+            calib_bin_edges = self._calib_bin_edges.cpu().numpy()  # type: ignore
+        else:
+            calib_hist = self._calib_hist
+            calib_bin_edges = self._calib_bin_edges
+
+        if method == "entropy":
+            calib_amax = _compute_amax_entropy(
+                calib_hist, calib_bin_edges, self._num_bits, self._unsigned, stride, start_bin
+            )
+        elif method == "mse":
+            calib_amax = _compute_amax_mse(
+                calib_hist, calib_bin_edges, self._num_bits, self._unsigned, stride, start_bin
+            )
+        elif method == "percentile":
+            calib_amax = _compute_amax_percentile(calib_hist, calib_bin_edges, percentile)
+        else:
+            raise TypeError("Unknown calibration method {}".format(method))
+
+        return calib_amax
+
+    def __str__(self):
+        s = "HistogramCalibrator("
+        if self._calib_bin_edges is None:
+            bin_edge_str = "None"
+        else:
+            bin_edge_str = "[{:.3f}, ..., {:.3f}]({})".format(
+                self._calib_bin_edges[0], self._calib_bin_edges[-1], len(self._calib_bin_edges)
+            )
+        s += "calib_bin_edges={})".format(bin_edge_str)
+        return s
+
+    def __repr__(self):
+        s = "HistogramCalibrator("
+        s += super(HistogramCalibrator, self).__repr__()
+        s += " calib_bin_edges={_calib_bin_edges}"
+        s += " calib_hist={_calib_hist})"
+        return s.format(**self.__dict__)
+
+
+# Ideally, we want to decouple collector (collect histogram) and calibrator (compute amax) as opposed to
+# the current calibrator design. The following compute amax functions are broken out from the calibrator
+# as first step towards there.
+def _compute_amax_entropy(calib_hist, calib_bin_edges, num_bits, unsigned, stride=1, start_bin=128):
+    """Returns amax that minimizes KL-Divergence of the collected histogram."""
+    # If calibrator hasn't collected any data, return none
+    if calib_bin_edges is None and calib_hist is None:
+        return None
+
+    def _normalize_distr(distr):
+        summ = np.sum(distr)
+        if summ != 0:
+            distr = distr / summ
+
+    bins = calib_hist[:]
+    bins[0] = bins[1]
+
+    total_data = np.sum(bins)
+
+    divergences = []
+    arguments = []
+
+    # we are quantizing to 128 values + sign if num_bits=8
+    nbins = 1 << (num_bits - 1 + int(unsigned))
+
+    starting = start_bin
+    stop = len(bins)
+
+    new_density_counts = np.zeros(nbins, dtype=np.float64)
+
+    for i in range(starting, stop + 1, stride):
+        new_density_counts.fill(0)
+        space = np.linspace(0, i, num=nbins + 1)
+        digitized_space = np.digitize(range(i), space) - 1
+
+        digitized_space[bins[:i] == 0] = -1
+
+        for idx, digitized in enumerate(digitized_space):
+            if digitized != -1:
+                new_density_counts[digitized] += bins[idx]
+
+        counter = Counter(digitized_space)
+        for key, val in counter.items():
+            if key != -1:
+                new_density_counts[key] = new_density_counts[key] / val
+
+        new_density = np.zeros(i, dtype=np.float64)
+        for idx, digitized in enumerate(digitized_space):
+            if digitized != -1:
+                new_density[idx] = new_density_counts[digitized]
+
+        total_counts_new = np.sum(new_density) + np.sum(bins[i:])
+        _normalize_distr(new_density)
+
+        reference_density = np.array(bins[: len(digitized_space)])
+        reference_density[-1] += np.sum(bins[i:])
+
+        total_counts_old = np.sum(reference_density)
+        if round(total_counts_new) != total_data or round(total_counts_old) != total_data:
+            raise RuntimeError(
+                "Count mismatch! total_counts_new={}, total_counts_old={}, total_data={}".format(
+                    total_counts_new, total_counts_old, total_data
+                )
+            )
+
+        _normalize_distr(reference_density)
+
+        ent = entropy(reference_density, new_density)
+        divergences.append(ent)
+        arguments.append(i)
+
+    divergences = np.array(divergences)
+    last_argmin = len(divergences) - 1 - np.argmin(divergences[::-1])
+    calib_amax = calib_bin_edges[last_argmin * stride + starting]
+    calib_amax = torch.tensor(calib_amax.item())
+
+    return calib_amax
+
+
+def _compute_amax_mse(calib_hist, calib_bin_edges, num_bits, unsigned, stride=1, start_bin=128):
+    """Returns amax that minimizes MSE of the collected histogram."""
+    # If calibrator hasn't collected any data, return none
+    if calib_bin_edges is None and calib_hist is None:
+        return None
+
+    counts = torch.from_numpy(calib_hist[:]).float()
+    edges = torch.from_numpy(calib_bin_edges[:]).float()
+
+    device = None
+    if torch.cuda.is_available():
+        device = counts.device
+        counts = counts.cuda()
+        edges = edges.cuda()
+
+    centers = (edges[1:] + edges[:-1]) / 2
+
+    mses = []
+    arguments = []
+
+    for i in range(start_bin, len(centers), stride):
+        amax = centers[i]
+        if isinstance(num_bits, int) and num_bits >= 0:
+            quant_centers = fake_tensor_quant(centers, amax, num_bits, unsigned)
+        elif num_bits == (4, 3):
+            quant_centers = scaled_e4m3(centers, amax, num_bits[0], num_bits[1])
+        else:
+            raise TypeError("Invalid num_bits. num_bits must be a positive integer or tuple (4,3).")
+
+        mse = ((quant_centers - centers) ** 2 * counts).mean()
+
+        mses.append(mse.cpu())
+        arguments.append(i)
+
+    argmin = np.argmin(mses)
+    calib_amax = centers[arguments[argmin]]
+
+    if device is not None:
+        calib_amax = calib_amax.to(device)
+
+    return calib_amax
+
+
+def _compute_amax_percentile(calib_hist, calib_bin_edges, percentile):
+    """Returns amax that clips the percentile fraction of collected data."""
+    if percentile < 0 or percentile > 100:
+        raise ValueError("Invalid percentile. Must be in range 0 <= percentile <= 100.")
+
+    # If calibrator hasn't collected any data, return none
+    if calib_bin_edges is None and calib_hist is None:
+        return None
+
+    total = calib_hist.sum()
+    cdf = np.cumsum(calib_hist / total)
+    idx = np.searchsorted(cdf, percentile / 100)
+    calib_amax = calib_bin_edges[idx]
+    calib_amax = torch.tensor(calib_amax.item())
+
+    return calib_amax
+
+
+def calibrate_weights(model, method="percentile", perchannel=True, percentile=99.99, num_bins=2048):
+    """Calibrate weights of all child quantized modules.
+
+    Ideally, we would split calibration functionality to histogram collector and calibrator which
+    takes histogram and compute amax. But since we haven't decoupled collector and calibrator, it
+    is easier to create a separate function to calibrate weight.
+
+    .. note::
+        This function uses `method` specified by the argument to decide which method to use, NOT the one
+        specified by the calibrator embedded in weight_quantizer.
+        We haven't moved calibration to GPU, so everything is transfered to CPU
+
+    Args:
+        model: A torch.nn.Module.
+        method: A string of calibration method. Supports "mse" and "percentile". Default "percentile"
+        perchannel: A bool. Set channel/neuron axis if True. Default True.
+        percentile: A float. Default 99.99
+        num_bins: A integer. Number of bins of histogram. Default 2048.
+
+    """
+    for name, module in model.named_modules():
+        if hasattr(module, "weight") and hasattr(module, "weight_quantizer"):
+            num_bits = module.weight_quantizer.num_bits
+            unsigned = module.weight_quantizer.unsigned
+            channel_second_modules = (
+                qnn.QuantConvTranspose1d,
+                qnn.QuantConvTranspose2d,
+                qnn.QuantConvTranspose3d,
+            )
+            if perchannel:
+                axis = 1 if isinstance(module, channel_second_modules) else 0
+            else:
+                axis = None
+            axis_size = module.weight.shape[axis] if axis is not None else 1
+
+            # Histogram is always collected even if method is "max". Although "max" is supported here
+            # but it is not the primary usage of this function
+            if axis is None:
+                input_weights = module.weight.abs().cpu().detach().numpy()
+                calib_hist, calib_bin_edges = np.histogram(
+                    input_weights, bins=2048, range=(0, input_weights.max())
+                )
+                calib_hist = [calib_hist]
+                calib_bin_edges = [calib_bin_edges]
+            else:
+                calib_hist = []
+                calib_bin_edges = []
+                for i in range(axis_size):
+                    input_weights = (
+                        module.weight.index_select(
+                            axis, torch.tensor(i, device=module.weight.device)
+                        )
+                        .abs()
+                        .cpu()
+                        .detach()
+                        .numpy()
+                    )
+                    hist, bin_edges = np.histogram(
+                        input_weights, bins=num_bins, range=(0, input_weights.max())
+                    )
+                    calib_hist.append(hist)
+                    calib_bin_edges.append(bin_edges)
+
+            calib_amax = []
+            if method == "max":
+                reduce_axis = list(range(module.weight.dim()))
+                reduce_axis.remove(axis)  # type: ignore
+                calib_amax.append(quant_utils.reduce_amax(module.weight, axis=reduce_axis))
+            elif method == "mse":
+                for i in range(axis_size):
+                    calib_amax.append(
+                        _compute_amax_mse(calib_hist[i], calib_bin_edges[i], num_bits, unsigned)
+                    )
+            elif method == "percentile":
+                for i in range(axis_size):
+                    calib_amax.append(
+                        _compute_amax_percentile(calib_hist[i], calib_bin_edges[i], percentile)
+                    )
+            else:
+                raise TypeError("Unsupported calibration method {}".format(method))
+
+            if axis is None:
+                calib_amax_t = calib_amax[0]
+            else:
+                calib_amax_shape = [1] * module.weight.dim()
+                calib_amax_shape[axis] = module.weight.shape[axis]
+                calib_amax_t = torch.stack(calib_amax).reshape(calib_amax_shape)
+            if calib_amax_t.numel() == 1:
+                calib_amax_t.squeeze_()
+            module.weight_quantizer.amax = calib_amax_t.detach().cpu().numpy()
```

## modelopt/torch/quantization/calib/max.py

 * *Ordering differences only*

```diff
@@ -1,98 +1,98 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Calibrator that returns the absolute max of all collected tensors."""
-
-import torch
-
-from .. import utils as quant_utils
-from .calibrator import _Calibrator
-
-__all__ = ["MaxCalibrator"]
-
-
-class MaxCalibrator(_Calibrator):
-    """Max calibrator, tracks the maximum value globally.
-
-    Args:
-        calib_desc: A MaxCalibDescriptor.
-        num_bits: An integer. Number of bits of quantization.
-        axis: A tuple. see QuantDescriptor.
-        unsigned: A boolean. using unsigned quantization.
-
-    Readonly Properties:
-        amaxs: A list of amax. Numpy array is saved as it is likely to be used for some plot.
-    """
-
-    def __init__(self, num_bits=8, axis=None, unsigned=False, track_amax=False):
-        """Initialize."""
-        super(MaxCalibrator, self).__init__(num_bits, axis, unsigned)
-        self._track_amax = track_amax
-        if self._track_amax:
-            self._amaxs = []  # shall we have a better name?
-        self._calib_amax = None
-
-    @property
-    def amaxs(self):
-        """Returns the list of amax`s collected so far."""
-        return self._amaxs
-
-    @torch.no_grad()
-    def collect(self, x):
-        """Tracks the absolute max of all tensors.
-
-        Args:
-            x: A tensor
-
-        Raises:
-            RuntimeError: If amax shape changes
-        """
-        # Swap axis to reduce.
-        axis = self._axis if isinstance(self._axis, (list, tuple)) else [self._axis]
-        # Handle negative axis.
-        axis = [x.dim() + i if isinstance(i, int) and i < 0 else i for i in axis]
-        reduce_axis = []
-        for i in range(x.dim()):
-            if i not in axis:
-                reduce_axis.append(i)
-        local_amax = quant_utils.reduce_amax(x, axis=reduce_axis).detach()
-        if self._calib_amax is None:
-            self._calib_amax = local_amax
-        else:
-            if local_amax.shape != self._calib_amax.shape:
-                raise RuntimeError("amax shape changed!")
-            self._calib_amax = torch.max(self._calib_amax, local_amax)
-
-        if self._track_amax:
-            self._amaxs.append(local_amax.cpu().numpy())
-
-    def reset(self):
-        """Reset the collected absolute max."""
-        self._calib_amax = None
-
-    def compute_amax(self):
-        """Return the absolute max of all tensors collected."""
-        return self._calib_amax
-
-    def __str__(self):
-        s = "MaxCalibrator("
-        s += "track_amax={_track_amax}"
-        s += ")"
-        return s.format(**self.__dict__)
-
-    def __repr__(self):
-        s = "MaxCalibrator("
-        s += super(MaxCalibrator, self).__repr__()
-        s += " calib_amax={_calib_amax}"
-        s += " track_amax={_track_amax}"
-        if self._track_amax:
-            s += " amaxs={_amaxs}"
-        s += ")"
-        return s.format(**self.__dict__)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Calibrator that returns the absolute max of all collected tensors."""
+
+import torch
+
+from .. import utils as quant_utils
+from .calibrator import _Calibrator
+
+__all__ = ["MaxCalibrator"]
+
+
+class MaxCalibrator(_Calibrator):
+    """Max calibrator, tracks the maximum value globally.
+
+    Args:
+        calib_desc: A MaxCalibDescriptor.
+        num_bits: An integer. Number of bits of quantization.
+        axis: A tuple. see QuantDescriptor.
+        unsigned: A boolean. using unsigned quantization.
+
+    Readonly Properties:
+        amaxs: A list of amax. Numpy array is saved as it is likely to be used for some plot.
+    """
+
+    def __init__(self, num_bits=8, axis=None, unsigned=False, track_amax=False):
+        """Initialize."""
+        super(MaxCalibrator, self).__init__(num_bits, axis, unsigned)
+        self._track_amax = track_amax
+        if self._track_amax:
+            self._amaxs = []  # shall we have a better name?
+        self._calib_amax = None
+
+    @property
+    def amaxs(self):
+        """Returns the list of amax`s collected so far."""
+        return self._amaxs
+
+    @torch.no_grad()
+    def collect(self, x):
+        """Tracks the absolute max of all tensors.
+
+        Args:
+            x: A tensor
+
+        Raises:
+            RuntimeError: If amax shape changes
+        """
+        # Swap axis to reduce.
+        axis = self._axis if isinstance(self._axis, (list, tuple)) else [self._axis]
+        # Handle negative axis.
+        axis = [x.dim() + i if isinstance(i, int) and i < 0 else i for i in axis]
+        reduce_axis = []
+        for i in range(x.dim()):
+            if i not in axis:
+                reduce_axis.append(i)
+        local_amax = quant_utils.reduce_amax(x, axis=reduce_axis).detach()
+        if self._calib_amax is None:
+            self._calib_amax = local_amax
+        else:
+            if local_amax.shape != self._calib_amax.shape:
+                raise RuntimeError("amax shape changed!")
+            self._calib_amax = torch.max(self._calib_amax, local_amax)
+
+        if self._track_amax:
+            self._amaxs.append(local_amax.cpu().numpy())
+
+    def reset(self):
+        """Reset the collected absolute max."""
+        self._calib_amax = None
+
+    def compute_amax(self):
+        """Return the absolute max of all tensors collected."""
+        return self._calib_amax
+
+    def __str__(self):
+        s = "MaxCalibrator("
+        s += "track_amax={_track_amax}"
+        s += ")"
+        return s.format(**self.__dict__)
+
+    def __repr__(self):
+        s = "MaxCalibrator("
+        s += super(MaxCalibrator, self).__repr__()
+        s += " calib_amax={_calib_amax}"
+        s += " track_amax={_track_amax}"
+        if self._track_amax:
+            s += " amaxs={_amaxs}"
+        s += ")"
+        return s.format(**self.__dict__)
```

## modelopt/torch/quantization/nn/__init__.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Modules with quantization support."""
-
-from .modules.clip import *
-from .modules.quant_activations import *
-from .modules.quant_batchnorm import *
-from .modules.quant_conv import *
-from .modules.quant_instancenorm import *
-from .modules.quant_linear import *
-from .modules.quant_module import *
-from .modules.quant_pooling import *
-from .modules.tensor_quantizer import *
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Modules with quantization support."""
+
+from .modules.clip import *
+from .modules.quant_activations import *
+from .modules.quant_batchnorm import *
+from .modules.quant_conv import *
+from .modules.quant_instancenorm import *
+from .modules.quant_linear import *
+from .modules.quant_module import *
+from .modules.quant_pooling import *
+from .modules.tensor_quantizer import *
```

## modelopt/torch/quantization/nn/functional.py

 * *Ordering differences only*

```diff
@@ -1,66 +1,66 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Some supportive functions."""
-import warnings
-
-import torch
-from torch.autograd import Function
-
-
-class ClipFunction(Function):
-    """An universal tensor clip function.
-
-    Pytorch's clamp() only supports scalar range and doesn't support broadcast. This implementation uses min/max which
-    is more genaral. The gradient is defined according to IBM's PACT paper https://arxiv.org/abs/1805.06085, which is
-    also the behavior of Tensorflow's clip_by_value()
-    """
-
-    @staticmethod
-    def forward(ctx, input, clip_value_min, clip_value_max):
-        """Forward pass for the clip function."""
-        output = torch.min(input, clip_value_max)
-        output = torch.max(output, clip_value_min)
-        ctx.save_for_backward(input, clip_value_min, clip_value_max)
-        return output
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        """Backward pass for the clip function."""
-        input, clip_value_min, clip_value_max = ctx.saved_tensors
-        min_mask = (input > clip_value_min).to(grad_output.dtype)
-        max_mask = (input < clip_value_max).to(grad_output.dtype)
-        grad_input = grad_output * min_mask * max_mask
-
-        if clip_value_min.requires_grad or clip_value_max.requires_grad:
-            warnings.warn("Learning enabled for clip min/max. This is an experimental feature.")
-        if clip_value_min.numel() != 1 or clip_value_max.numel() != 1:
-            raise ValueError(
-                "Learnable min/max can only be scalar, got size %s and %s."
-                % (clip_value_min.size(), clip_value_max.size())
-            )
-
-        # Ensure the dtypes of min/max grads matches the input dtype
-        # This might be necessary if running w/ AMP which will cast to fp32 before `sum()`
-        grad_clip_value_min = (
-            (grad_output * (1.0 - min_mask)).sum().to(clip_value_min.dtype)
-            if clip_value_min.requires_grad
-            else None
-        )
-        grad_clip_value_max = (
-            (grad_output * (1.0 - max_mask)).sum().to(clip_value_min.dtype)
-            if clip_value_max.requires_grad
-            else None
-        )
-
-        return grad_input, grad_clip_value_min, grad_clip_value_max
-
-
-clip = ClipFunction.apply
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Some supportive functions."""
+import warnings
+
+import torch
+from torch.autograd import Function
+
+
+class ClipFunction(Function):
+    """An universal tensor clip function.
+
+    Pytorch's clamp() only supports scalar range and doesn't support broadcast. This implementation uses min/max which
+    is more genaral. The gradient is defined according to IBM's PACT paper https://arxiv.org/abs/1805.06085, which is
+    also the behavior of Tensorflow's clip_by_value()
+    """
+
+    @staticmethod
+    def forward(ctx, input, clip_value_min, clip_value_max):
+        """Forward pass for the clip function."""
+        output = torch.min(input, clip_value_max)
+        output = torch.max(output, clip_value_min)
+        ctx.save_for_backward(input, clip_value_min, clip_value_max)
+        return output
+
+    @staticmethod
+    def backward(ctx, grad_output):
+        """Backward pass for the clip function."""
+        input, clip_value_min, clip_value_max = ctx.saved_tensors
+        min_mask = (input > clip_value_min).to(grad_output.dtype)
+        max_mask = (input < clip_value_max).to(grad_output.dtype)
+        grad_input = grad_output * min_mask * max_mask
+
+        if clip_value_min.requires_grad or clip_value_max.requires_grad:
+            warnings.warn("Learning enabled for clip min/max. This is an experimental feature.")
+        if clip_value_min.numel() != 1 or clip_value_max.numel() != 1:
+            raise ValueError(
+                "Learnable min/max can only be scalar, got size %s and %s."
+                % (clip_value_min.size(), clip_value_max.size())
+            )
+
+        # Ensure the dtypes of min/max grads matches the input dtype
+        # This might be necessary if running w/ AMP which will cast to fp32 before `sum()`
+        grad_clip_value_min = (
+            (grad_output * (1.0 - min_mask)).sum().to(clip_value_min.dtype)
+            if clip_value_min.requires_grad
+            else None
+        )
+        grad_clip_value_max = (
+            (grad_output * (1.0 - max_mask)).sum().to(clip_value_min.dtype)
+            if clip_value_max.requires_grad
+            else None
+        )
+
+        return grad_input, grad_clip_value_min, grad_clip_value_max
+
+
+clip = ClipFunction.apply
```

## modelopt/torch/quantization/nn/modules/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Modules with quantization support."""
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Modules with quantization support."""
```

## modelopt/torch/quantization/nn/modules/_utils.py

 * *Ordering differences only*

```diff
@@ -1,207 +1,207 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Some helper functions for implementing quantized modules"""
-import copy
-import inspect
-
-from torch import nn
-
-from ...tensor_quant import QUANT_DESC_8BIT_PER_TENSOR, QuantDescriptor
-from .tensor_quantizer import TensorQuantizer
-
-
-class QuantMixin:
-    """Mixin class for adding basic quantization logic to quantized modules"""
-
-    default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
-    default_quant_desc_weight = QUANT_DESC_8BIT_PER_TENSOR
-    default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
-
-    @classmethod
-    def set_default_quant_desc_input(cls, value):
-        """
-        Args:
-            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
-        """
-        if not isinstance(value, QuantDescriptor):
-            raise ValueError("{} is not an instance of QuantDescriptor!")
-        cls.default_quant_desc_input = copy.deepcopy(value)
-
-    @classmethod
-    def set_default_quant_desc_weight(cls, value):
-        """
-        Args:
-            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
-        """
-        if not isinstance(value, QuantDescriptor):
-            raise ValueError("{} is not an instance of QuantDescriptor!")
-        cls.default_quant_desc_weight = copy.deepcopy(value)
-
-    @classmethod
-    def set_default_quant_desc_output(cls, value):
-        """
-        Args:
-            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
-        """
-        if not isinstance(value, QuantDescriptor):
-            raise ValueError("{} is not an instance of QuantDescriptor!")
-        cls.default_quant_desc_output = copy.deepcopy(value)
-
-    def init_quantizer(
-        self, quant_desc_input, quant_desc_weight, num_layers=None, quant_desc_output=None
-    ):
-        """Helper function for __init__ of quantized module
-
-        Create input and weight quantizer based on quant_desc passed by kwargs, or default of the class.
-
-        Args:
-            quant_desc_input: An instance of
-                :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
-            quant_desc_weight: An instance of
-                :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
-            num_layers: An integer. Default None. If not None, create a list of quantizers.
-        """
-        if not inspect.stack()[1].function == "__init__":
-            raise TypeError(
-                "{} should be only called by __init__ of quantized module.".format(__name__)
-            )
-
-        if quant_desc_output is None:
-            quant_desc_output = QuantMixin.default_quant_desc_output
-
-        self._fake_quant = True
-        if (
-            (not quant_desc_input.fake_quant)
-            or (not quant_desc_weight.fake_quant)
-            or (not quant_desc_output.fake_quant)
-        ):
-            raise ValueError("Only fake quantization is supported!")
-
-        if num_layers is None:
-            self._input_quantizer = TensorQuantizer(quant_desc_input)
-            self._weight_quantizer = TensorQuantizer(quant_desc_weight)
-            self._output_quantizer = TensorQuantizer(quant_desc_output)
-            self._output_quantizer.disable()
-        else:
-            self._input_quantizers = nn.ModuleList(
-                [TensorQuantizer(quant_desc_input) for _ in range(num_layers)]
-            )
-            self._weight_quantizers = nn.ModuleList(
-                [TensorQuantizer(quant_desc_weight) for _ in range(num_layers)]
-            )
-            self._output_quantizers = nn.ModuleList(
-                [TensorQuantizer(quant_desc_output) for _ in range(num_layers)]
-            )
-            for quantizer in self._output_quantizers:
-                quantizer.disable()
-
-    @property
-    def input_quantizer(self):
-        return self._input_quantizer
-
-    @property
-    def weight_quantizer(self):
-        return self._weight_quantizer
-
-    @property
-    def output_quantizer(self):
-        return self._output_quantizer
-
-
-class QuantInputMixin:
-    """Mixin class for adding basic quantization logic to quantized modules"""
-
-    default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
-    default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
-
-    @classmethod
-    def set_default_quant_desc_input(cls, value):
-        """
-        Args:
-            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
-        """
-        if not isinstance(value, QuantDescriptor):
-            raise ValueError("{} is not an instance of QuantDescriptor!")
-        cls.default_quant_desc_input = copy.deepcopy(value)
-
-    @classmethod
-    def set_default_quant_desc_output(cls, value):
-        """
-        Args:
-            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
-        """
-        if not isinstance(value, QuantDescriptor):
-            raise ValueError("{} is not an instance of QuantDescriptor!")
-        cls.default_quant_desc_output = copy.deepcopy(value)
-
-    def init_quantizer(self, quant_desc_input, quant_desc_output=None):
-        """Helper function for __init__ of simple quantized module
-
-        Create input quantizer based on quant_desc passed by kwargs, or default of the class.
-
-        Args:
-            quant_desc_input: An instance of
-                :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
-        """
-        if not inspect.stack()[1].function == "__init__":
-            raise TypeError(
-                "{} should be only called by __init__ of quantized module.".format(__name__)
-            )
-
-        if quant_desc_output is None:
-            quant_desc_output = QuantMixin.default_quant_desc_output
-
-        self._fake_quant = True
-        if not quant_desc_input.fake_quant:
-            raise ValueError("Only fake quantization is supported!")
-
-        self._input_quantizer = TensorQuantizer(quant_desc_input)
-        self._output_quantizer = TensorQuantizer(quant_desc_output)
-        self._output_quantizer.disable()
-
-    @property
-    def input_quantizer(self):
-        return self._input_quantizer
-
-    @property
-    def output_quantizer(self):
-        return self._output_quantizer
-
-
-# TODO: Remove this function
-def pop_quant_desc_in_kwargs(quant_cls, input_only=False, **kwargs):
-    """Pop quant descriptors in kwargs
-
-    If there is no descriptor in kwargs, the default one in quant_cls will be used
-
-    Arguments:
-       quant_cls: A class that has default quantization descriptors
-       input_only: A boolean. If True, pop quant_desc_input only, not quant_desc_weight. Default false.
-
-    Keyword Arguments:
-       quant_desc_input: An instance of
-            :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`.
-           Quantization descriptor of input.
-       quant_desc_weight: An instance of
-            :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`.
-           Quantization descriptor of weight.
-    """
-    quant_desc_input = kwargs.pop("quant_desc_input", quant_cls.default_quant_desc_input)
-    if not input_only:
-        quant_desc_weight = kwargs.pop("quant_desc_weight", quant_cls.default_quant_desc_weight)
-
-    # Check if anything is left in **kwargs
-    if kwargs:
-        raise TypeError("Unused keys: {}".format(kwargs.keys()))
-
-    if input_only:
-        return quant_desc_input
-    return quant_desc_input, quant_desc_weight
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Some helper functions for implementing quantized modules"""
+import copy
+import inspect
+
+from torch import nn
+
+from ...tensor_quant import QUANT_DESC_8BIT_PER_TENSOR, QuantDescriptor
+from .tensor_quantizer import TensorQuantizer
+
+
+class QuantMixin:
+    """Mixin class for adding basic quantization logic to quantized modules"""
+
+    default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
+    default_quant_desc_weight = QUANT_DESC_8BIT_PER_TENSOR
+    default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
+
+    @classmethod
+    def set_default_quant_desc_input(cls, value):
+        """
+        Args:
+            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
+        """
+        if not isinstance(value, QuantDescriptor):
+            raise ValueError("{} is not an instance of QuantDescriptor!")
+        cls.default_quant_desc_input = copy.deepcopy(value)
+
+    @classmethod
+    def set_default_quant_desc_weight(cls, value):
+        """
+        Args:
+            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
+        """
+        if not isinstance(value, QuantDescriptor):
+            raise ValueError("{} is not an instance of QuantDescriptor!")
+        cls.default_quant_desc_weight = copy.deepcopy(value)
+
+    @classmethod
+    def set_default_quant_desc_output(cls, value):
+        """
+        Args:
+            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
+        """
+        if not isinstance(value, QuantDescriptor):
+            raise ValueError("{} is not an instance of QuantDescriptor!")
+        cls.default_quant_desc_output = copy.deepcopy(value)
+
+    def init_quantizer(
+        self, quant_desc_input, quant_desc_weight, num_layers=None, quant_desc_output=None
+    ):
+        """Helper function for __init__ of quantized module
+
+        Create input and weight quantizer based on quant_desc passed by kwargs, or default of the class.
+
+        Args:
+            quant_desc_input: An instance of
+                :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
+            quant_desc_weight: An instance of
+                :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
+            num_layers: An integer. Default None. If not None, create a list of quantizers.
+        """
+        if not inspect.stack()[1].function == "__init__":
+            raise TypeError(
+                "{} should be only called by __init__ of quantized module.".format(__name__)
+            )
+
+        if quant_desc_output is None:
+            quant_desc_output = QuantMixin.default_quant_desc_output
+
+        self._fake_quant = True
+        if (
+            (not quant_desc_input.fake_quant)
+            or (not quant_desc_weight.fake_quant)
+            or (not quant_desc_output.fake_quant)
+        ):
+            raise ValueError("Only fake quantization is supported!")
+
+        if num_layers is None:
+            self._input_quantizer = TensorQuantizer(quant_desc_input)
+            self._weight_quantizer = TensorQuantizer(quant_desc_weight)
+            self._output_quantizer = TensorQuantizer(quant_desc_output)
+            self._output_quantizer.disable()
+        else:
+            self._input_quantizers = nn.ModuleList(
+                [TensorQuantizer(quant_desc_input) for _ in range(num_layers)]
+            )
+            self._weight_quantizers = nn.ModuleList(
+                [TensorQuantizer(quant_desc_weight) for _ in range(num_layers)]
+            )
+            self._output_quantizers = nn.ModuleList(
+                [TensorQuantizer(quant_desc_output) for _ in range(num_layers)]
+            )
+            for quantizer in self._output_quantizers:
+                quantizer.disable()
+
+    @property
+    def input_quantizer(self):
+        return self._input_quantizer
+
+    @property
+    def weight_quantizer(self):
+        return self._weight_quantizer
+
+    @property
+    def output_quantizer(self):
+        return self._output_quantizer
+
+
+class QuantInputMixin:
+    """Mixin class for adding basic quantization logic to quantized modules"""
+
+    default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
+    default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
+
+    @classmethod
+    def set_default_quant_desc_input(cls, value):
+        """
+        Args:
+            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
+        """
+        if not isinstance(value, QuantDescriptor):
+            raise ValueError("{} is not an instance of QuantDescriptor!")
+        cls.default_quant_desc_input = copy.deepcopy(value)
+
+    @classmethod
+    def set_default_quant_desc_output(cls, value):
+        """
+        Args:
+            value: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
+        """
+        if not isinstance(value, QuantDescriptor):
+            raise ValueError("{} is not an instance of QuantDescriptor!")
+        cls.default_quant_desc_output = copy.deepcopy(value)
+
+    def init_quantizer(self, quant_desc_input, quant_desc_output=None):
+        """Helper function for __init__ of simple quantized module
+
+        Create input quantizer based on quant_desc passed by kwargs, or default of the class.
+
+        Args:
+            quant_desc_input: An instance of
+                :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`
+        """
+        if not inspect.stack()[1].function == "__init__":
+            raise TypeError(
+                "{} should be only called by __init__ of quantized module.".format(__name__)
+            )
+
+        if quant_desc_output is None:
+            quant_desc_output = QuantMixin.default_quant_desc_output
+
+        self._fake_quant = True
+        if not quant_desc_input.fake_quant:
+            raise ValueError("Only fake quantization is supported!")
+
+        self._input_quantizer = TensorQuantizer(quant_desc_input)
+        self._output_quantizer = TensorQuantizer(quant_desc_output)
+        self._output_quantizer.disable()
+
+    @property
+    def input_quantizer(self):
+        return self._input_quantizer
+
+    @property
+    def output_quantizer(self):
+        return self._output_quantizer
+
+
+# TODO: Remove this function
+def pop_quant_desc_in_kwargs(quant_cls, input_only=False, **kwargs):
+    """Pop quant descriptors in kwargs
+
+    If there is no descriptor in kwargs, the default one in quant_cls will be used
+
+    Arguments:
+       quant_cls: A class that has default quantization descriptors
+       input_only: A boolean. If True, pop quant_desc_input only, not quant_desc_weight. Default false.
+
+    Keyword Arguments:
+       quant_desc_input: An instance of
+            :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`.
+           Quantization descriptor of input.
+       quant_desc_weight: An instance of
+            :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`.
+           Quantization descriptor of weight.
+    """
+    quant_desc_input = kwargs.pop("quant_desc_input", quant_cls.default_quant_desc_input)
+    if not input_only:
+        quant_desc_weight = kwargs.pop("quant_desc_weight", quant_cls.default_quant_desc_weight)
+
+    # Check if anything is left in **kwargs
+    if kwargs:
+        raise TypeError("Unused keys: {}".format(kwargs.keys()))
+
+    if input_only:
+        return quant_desc_input
+    return quant_desc_input, quant_desc_weight
```

## modelopt/torch/quantization/nn/modules/clip.py

 * *Ordering differences only*

```diff
@@ -1,58 +1,58 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Implement a clip module as pytorch only has a simple clamp function."""
-import torch
-from torch import nn
-from torch.nn.parameter import Parameter
-
-from .. import functional as QF
-
-__all__ = ["Clip"]
-
-
-class Clip(nn.Module):
-    """Clip tensor.
-
-    Args:
-        clip_value_min: A number or tensor of lower bound to clip
-        clip_value_max: A number of tensor of upper bound to clip
-        learn_min: A boolean. If True, learn min. clip_value_min will be used to initialize. Default False
-        learn_max: A boolean. Similar as learn_min but for max.
-
-    Raises:
-        ValueError:
-    """
-
-    def __init__(self, clip_value_min, clip_value_max, learn_min=False, learn_max=False):
-        """Initialize."""
-        super(Clip, self).__init__()
-        if learn_min:
-            if not isinstance(clip_value_min, float) and clip_value_min.size != 1:
-                raise ValueError(
-                    "clip_value_min/clip_value_max must be scalar for initilizing learnable range."
-                )
-            self.clip_value_min = Parameter(torch.tensor(clip_value_min))
-        else:
-            self.clip_value_min = clip_value_min
-
-        if learn_max:
-            if not isinstance(clip_value_max, float) and clip_value_max.size != 1:
-                raise ValueError(
-                    "clip_value_min/clip_value_max must be scalar for initilizing learnable range."
-                )
-            self.clip_value_max = Parameter(torch.tensor(clip_value_max))
-        else:
-            self.clip_value_max = clip_value_max
-
-    def forward(self, inputs):
-        """Clip input tensor."""
-        outputs = QF.clip(inputs, self.clip_value_min, self.clip_value_max)
-        return outputs
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Implement a clip module as pytorch only has a simple clamp function."""
+import torch
+from torch import nn
+from torch.nn.parameter import Parameter
+
+from .. import functional as QF
+
+__all__ = ["Clip"]
+
+
+class Clip(nn.Module):
+    """Clip tensor.
+
+    Args:
+        clip_value_min: A number or tensor of lower bound to clip
+        clip_value_max: A number of tensor of upper bound to clip
+        learn_min: A boolean. If True, learn min. clip_value_min will be used to initialize. Default False
+        learn_max: A boolean. Similar as learn_min but for max.
+
+    Raises:
+        ValueError:
+    """
+
+    def __init__(self, clip_value_min, clip_value_max, learn_min=False, learn_max=False):
+        """Initialize."""
+        super(Clip, self).__init__()
+        if learn_min:
+            if not isinstance(clip_value_min, float) and clip_value_min.size != 1:
+                raise ValueError(
+                    "clip_value_min/clip_value_max must be scalar for initilizing learnable range."
+                )
+            self.clip_value_min = Parameter(torch.tensor(clip_value_min))
+        else:
+            self.clip_value_min = clip_value_min
+
+        if learn_max:
+            if not isinstance(clip_value_max, float) and clip_value_max.size != 1:
+                raise ValueError(
+                    "clip_value_min/clip_value_max must be scalar for initilizing learnable range."
+                )
+            self.clip_value_max = Parameter(torch.tensor(clip_value_max))
+        else:
+            self.clip_value_max = clip_value_max
+
+    def forward(self, inputs):
+        """Clip input tensor."""
+        outputs = QF.clip(inputs, self.clip_value_min, self.clip_value_max)
+        return outputs
```

## modelopt/torch/quantization/nn/modules/quant_activations.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Quantized activations module."""
-
-import torch.nn as nn
-
-from .quant_module import QuantInputBase, QuantModuleRegistry
-from .tensor_quantizer import TensorQuantizer
-
-
-class _QuantActivation(QuantInputBase):
-
-    @property
-    def input_quantizer(self):
-        return self._input_act_quantizer
-
-    @property
-    def output_quantizer(self):
-        return self._output_act_quantizer
-
-    def _setup(self):
-        self._register_temp_attribute(
-            "_input_act_quantizer", TensorQuantizer(self.default_quant_desc_input)
-        )
-        self._register_temp_attribute(
-            "_output_act_quantizer", TensorQuantizer(self.default_quant_desc_output)
-        )
-        self._output_act_quantizer.disable()
-
-
-QuantModuleRegistry.register({nn.LeakyReLU: "nn.LeakyReLU"})(_QuantActivation)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Quantized activations module."""
+
+import torch.nn as nn
+
+from .quant_module import QuantInputBase, QuantModuleRegistry
+from .tensor_quantizer import TensorQuantizer
+
+
+class _QuantActivation(QuantInputBase):
+
+    @property
+    def input_quantizer(self):
+        return self._input_act_quantizer
+
+    @property
+    def output_quantizer(self):
+        return self._output_act_quantizer
+
+    def _setup(self):
+        self._register_temp_attribute(
+            "_input_act_quantizer", TensorQuantizer(self.default_quant_desc_input)
+        )
+        self._register_temp_attribute(
+            "_output_act_quantizer", TensorQuantizer(self.default_quant_desc_output)
+        )
+        self._output_act_quantizer.disable()
+
+
+QuantModuleRegistry.register({nn.LeakyReLU: "nn.LeakyReLU"})(_QuantActivation)
```

## modelopt/torch/quantization/nn/modules/quant_batchnorm.py

 * *Ordering differences only*

```diff
@@ -1,41 +1,41 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Quantized batch normalization module."""
-
-import torch.nn as nn
-
-from .quant_module import QuantInputBase, QuantModuleRegistry
-from .tensor_quantizer import TensorQuantizer
-
-
-class _QuantBatchNormNd(QuantInputBase):
-
-    @property
-    def input_quantizer(self):
-        return self._input_bn_quantizer
-
-    @property
-    def output_quantizer(self):
-        return self._output_bn_quantizer
-
-    def _setup(self):
-        self._register_temp_attribute(
-            "_input_bn_quantizer", TensorQuantizer(self.default_quant_desc_input)
-        )
-        self._register_temp_attribute(
-            "_output_bn_quantizer", TensorQuantizer(self.default_quant_desc_output)
-        )
-        self._output_bn_quantizer.disable()
-
-
-QuantModuleRegistry.register({nn.BatchNorm1d: "nn.BatchNorm1d"})(_QuantBatchNormNd)
-QuantModuleRegistry.register({nn.BatchNorm2d: "nn.BatchNorm2d"})(_QuantBatchNormNd)
-QuantModuleRegistry.register({nn.BatchNorm3d: "nn.BatchNorm3d"})(_QuantBatchNormNd)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Quantized batch normalization module."""
+
+import torch.nn as nn
+
+from .quant_module import QuantInputBase, QuantModuleRegistry
+from .tensor_quantizer import TensorQuantizer
+
+
+class _QuantBatchNormNd(QuantInputBase):
+
+    @property
+    def input_quantizer(self):
+        return self._input_bn_quantizer
+
+    @property
+    def output_quantizer(self):
+        return self._output_bn_quantizer
+
+    def _setup(self):
+        self._register_temp_attribute(
+            "_input_bn_quantizer", TensorQuantizer(self.default_quant_desc_input)
+        )
+        self._register_temp_attribute(
+            "_output_bn_quantizer", TensorQuantizer(self.default_quant_desc_output)
+        )
+        self._output_bn_quantizer.disable()
+
+
+QuantModuleRegistry.register({nn.BatchNorm1d: "nn.BatchNorm1d"})(_QuantBatchNormNd)
+QuantModuleRegistry.register({nn.BatchNorm2d: "nn.BatchNorm2d"})(_QuantBatchNormNd)
+QuantModuleRegistry.register({nn.BatchNorm3d: "nn.BatchNorm3d"})(_QuantBatchNormNd)
```

## modelopt/torch/quantization/nn/modules/quant_conv.py

 * *Ordering differences only*

```diff
@@ -1,118 +1,118 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Quantized convolution."""
-import torch.nn as nn
-
-from modelopt.torch.quantization import tensor_quant
-
-from .quant_module import QuantLinearConvBase, QuantModuleRegistry, _LegacyQuantLinearConvBaseMixin
-
-__all__ = [
-    "Conv2d",
-    "QuantConv2d",
-    "Conv3d",
-    "QuantConv3d",
-    "Conv1d",
-    "QuantConv1d",
-    "ConvTranspose1d",
-    "ConvTranspose2d",
-    "ConvTranspose3d",
-    "QuantConvTranspose1d",
-    "QuantConvTranspose2d",
-    "QuantConvTranspose3d",
-]
-
-
-@QuantModuleRegistry.register({nn.Conv1d: "nn.Conv1d"})
-class _QuantConv1d(QuantLinearConvBase):
-    """Quantized 1D convolution."""
-
-    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONV1D_WEIGHT_PER_CHANNEL
-
-
-class QuantConv1d(_LegacyQuantLinearConvBaseMixin, nn.Conv1d):
-    """Quantized 1D convolution."""
-
-    default_quant_desc_weight = _QuantConv1d.default_quant_desc_weight
-
-
-@QuantModuleRegistry.register({nn.Conv2d: "nn.Conv2d"})
-class _QuantConv2d(QuantLinearConvBase):
-    """Quantized 2D convolution."""
-
-    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONV2D_WEIGHT_PER_CHANNEL
-
-
-class QuantConv2d(_LegacyQuantLinearConvBaseMixin, nn.Conv2d):
-    """Quantized 2D convolution."""
-
-    default_quant_desc_weight = _QuantConv2d.default_quant_desc_weight
-
-
-@QuantModuleRegistry.register({nn.Conv3d: "nn.Conv3d"})
-class _QuantConv3d(QuantLinearConvBase):
-    """Quantized 3D convolution."""
-
-    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONV3D_WEIGHT_PER_CHANNEL
-
-
-class QuantConv3d(_LegacyQuantLinearConvBaseMixin, nn.Conv3d):
-    """Quantized 3D convolution."""
-
-    default_quant_desc_weight = _QuantConv3d.default_quant_desc_weight
-
-
-@QuantModuleRegistry.register({nn.ConvTranspose1d: "nn.ConvTranspose1d"})
-class _QuantConvTranspose1d(QuantLinearConvBase):
-    """Quantized 1D transposed convolution."""
-
-    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONVTRANSPOSE1D_WEIGHT_PER_CHANNEL
-
-
-class QuantConvTranspose1d(_LegacyQuantLinearConvBaseMixin, nn.ConvTranspose1d):
-    """Quantized 1D transposed convolution."""
-
-    default_quant_desc_weight = _QuantConvTranspose1d.default_quant_desc_weight
-
-
-@QuantModuleRegistry.register({nn.ConvTranspose2d: "nn.ConvTranspose2d"})
-class _QuantConvTranspose2d(QuantLinearConvBase):
-    """Quantized 2D transposed convolution."""
-
-    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONVTRANSPOSE2D_WEIGHT_PER_CHANNEL
-
-
-class QuantConvTranspose2d(_LegacyQuantLinearConvBaseMixin, nn.ConvTranspose2d):
-    """Quantized 2D transposed convolution."""
-
-    default_quant_desc_weight = _QuantConvTranspose2d.default_quant_desc_weight
-
-
-@QuantModuleRegistry.register({nn.ConvTranspose3d: "nn.ConvTranspose3d"})
-class _QuantConvTranspose3d(QuantLinearConvBase):
-    """Quantized 3D transposed convolution."""
-
-    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONVTRANSPOSE3D_WEIGHT_PER_CHANNEL
-
-
-class QuantConvTranspose3d(_LegacyQuantLinearConvBaseMixin, nn.ConvTranspose3d):
-    """Quantized 3D transposed convolution."""
-
-    default_quant_desc_weight = _QuantConvTranspose3d.default_quant_desc_weight
-
-
-# Define alias with Quant prefix
-Conv1d = QuantConv1d
-Conv2d = QuantConv2d
-Conv3d = QuantConv3d
-ConvTranspose1d = QuantConvTranspose1d
-ConvTranspose2d = QuantConvTranspose2d
-ConvTranspose3d = QuantConvTranspose3d
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Quantized convolution."""
+import torch.nn as nn
+
+from modelopt.torch.quantization import tensor_quant
+
+from .quant_module import QuantLinearConvBase, QuantModuleRegistry, _LegacyQuantLinearConvBaseMixin
+
+__all__ = [
+    "Conv2d",
+    "QuantConv2d",
+    "Conv3d",
+    "QuantConv3d",
+    "Conv1d",
+    "QuantConv1d",
+    "ConvTranspose1d",
+    "ConvTranspose2d",
+    "ConvTranspose3d",
+    "QuantConvTranspose1d",
+    "QuantConvTranspose2d",
+    "QuantConvTranspose3d",
+]
+
+
+@QuantModuleRegistry.register({nn.Conv1d: "nn.Conv1d"})
+class _QuantConv1d(QuantLinearConvBase):
+    """Quantized 1D convolution."""
+
+    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONV1D_WEIGHT_PER_CHANNEL
+
+
+class QuantConv1d(_LegacyQuantLinearConvBaseMixin, nn.Conv1d):
+    """Quantized 1D convolution."""
+
+    default_quant_desc_weight = _QuantConv1d.default_quant_desc_weight
+
+
+@QuantModuleRegistry.register({nn.Conv2d: "nn.Conv2d"})
+class _QuantConv2d(QuantLinearConvBase):
+    """Quantized 2D convolution."""
+
+    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONV2D_WEIGHT_PER_CHANNEL
+
+
+class QuantConv2d(_LegacyQuantLinearConvBaseMixin, nn.Conv2d):
+    """Quantized 2D convolution."""
+
+    default_quant_desc_weight = _QuantConv2d.default_quant_desc_weight
+
+
+@QuantModuleRegistry.register({nn.Conv3d: "nn.Conv3d"})
+class _QuantConv3d(QuantLinearConvBase):
+    """Quantized 3D convolution."""
+
+    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONV3D_WEIGHT_PER_CHANNEL
+
+
+class QuantConv3d(_LegacyQuantLinearConvBaseMixin, nn.Conv3d):
+    """Quantized 3D convolution."""
+
+    default_quant_desc_weight = _QuantConv3d.default_quant_desc_weight
+
+
+@QuantModuleRegistry.register({nn.ConvTranspose1d: "nn.ConvTranspose1d"})
+class _QuantConvTranspose1d(QuantLinearConvBase):
+    """Quantized 1D transposed convolution."""
+
+    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONVTRANSPOSE1D_WEIGHT_PER_CHANNEL
+
+
+class QuantConvTranspose1d(_LegacyQuantLinearConvBaseMixin, nn.ConvTranspose1d):
+    """Quantized 1D transposed convolution."""
+
+    default_quant_desc_weight = _QuantConvTranspose1d.default_quant_desc_weight
+
+
+@QuantModuleRegistry.register({nn.ConvTranspose2d: "nn.ConvTranspose2d"})
+class _QuantConvTranspose2d(QuantLinearConvBase):
+    """Quantized 2D transposed convolution."""
+
+    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONVTRANSPOSE2D_WEIGHT_PER_CHANNEL
+
+
+class QuantConvTranspose2d(_LegacyQuantLinearConvBaseMixin, nn.ConvTranspose2d):
+    """Quantized 2D transposed convolution."""
+
+    default_quant_desc_weight = _QuantConvTranspose2d.default_quant_desc_weight
+
+
+@QuantModuleRegistry.register({nn.ConvTranspose3d: "nn.ConvTranspose3d"})
+class _QuantConvTranspose3d(QuantLinearConvBase):
+    """Quantized 3D transposed convolution."""
+
+    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_CONVTRANSPOSE3D_WEIGHT_PER_CHANNEL
+
+
+class QuantConvTranspose3d(_LegacyQuantLinearConvBaseMixin, nn.ConvTranspose3d):
+    """Quantized 3D transposed convolution."""
+
+    default_quant_desc_weight = _QuantConvTranspose3d.default_quant_desc_weight
+
+
+# Define alias with Quant prefix
+Conv1d = QuantConv1d
+Conv2d = QuantConv2d
+Conv3d = QuantConv3d
+ConvTranspose1d = QuantConvTranspose1d
+ConvTranspose2d = QuantConvTranspose2d
+ConvTranspose3d = QuantConvTranspose3d
```

## modelopt/torch/quantization/nn/modules/quant_instancenorm.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Quantized instance normalization module."""
-
-import torch.nn as nn
-
-from .quant_module import QuantInputBase, QuantModuleRegistry, _LegacyQuantInputBaseMixin
-
-__all__ = ["QuantInstanceNorm1d", "QuantInstanceNorm2d", "QuantInstanceNorm3d"]
-
-
-class QuantInstanceNorm1d(_LegacyQuantInputBaseMixin, nn.InstanceNorm1d):
-    """Applies Quantized Instance Normalization over a 3D input."""
-
-    pass
-
-
-class QuantInstanceNorm2d(_LegacyQuantInputBaseMixin, nn.InstanceNorm2d):
-    """Applies Quantized Instance Normalization over a 4D input."""
-
-    pass
-
-
-class QuantInstanceNorm3d(_LegacyQuantInputBaseMixin, nn.InstanceNorm3d):
-    """Applies Quantized Instance Normalization over a 5D input."""
-
-    pass
-
-
-QuantModuleRegistry.register({nn.InstanceNorm1d: "nn.InstanceNorm1d"})(QuantInputBase)
-QuantModuleRegistry.register({nn.InstanceNorm2d: "nn.InstanceNorm2d"})(QuantInputBase)
-QuantModuleRegistry.register({nn.InstanceNorm3d: "nn.InstanceNorm3d"})(QuantInputBase)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Quantized instance normalization module."""
+
+import torch.nn as nn
+
+from .quant_module import QuantInputBase, QuantModuleRegistry, _LegacyQuantInputBaseMixin
+
+__all__ = ["QuantInstanceNorm1d", "QuantInstanceNorm2d", "QuantInstanceNorm3d"]
+
+
+class QuantInstanceNorm1d(_LegacyQuantInputBaseMixin, nn.InstanceNorm1d):
+    """Applies Quantized Instance Normalization over a 3D input."""
+
+    pass
+
+
+class QuantInstanceNorm2d(_LegacyQuantInputBaseMixin, nn.InstanceNorm2d):
+    """Applies Quantized Instance Normalization over a 4D input."""
+
+    pass
+
+
+class QuantInstanceNorm3d(_LegacyQuantInputBaseMixin, nn.InstanceNorm3d):
+    """Applies Quantized Instance Normalization over a 5D input."""
+
+    pass
+
+
+QuantModuleRegistry.register({nn.InstanceNorm1d: "nn.InstanceNorm1d"})(QuantInputBase)
+QuantModuleRegistry.register({nn.InstanceNorm2d: "nn.InstanceNorm2d"})(QuantInputBase)
+QuantModuleRegistry.register({nn.InstanceNorm3d: "nn.InstanceNorm3d"})(QuantInputBase)
```

## modelopt/torch/quantization/nn/modules/quant_linear.py

 * *Ordering differences only*

```diff
@@ -1,73 +1,73 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Quantized Linear."""
-
-import types
-
-import torch.nn as nn
-
-from modelopt.torch.quantization import tensor_quant
-
-from .quant_module import QuantLinearConvBase, QuantModuleRegistry, _LegacyQuantLinearConvBaseMixin
-
-__all__ = ["Linear", "QuantLinear"]
-
-
-@QuantModuleRegistry.register({nn.Linear: "nn.Linear"})
-class _QuantLinear(QuantLinearConvBase):
-    """Quantized base class for nn.Linear type classes."""
-
-    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_LINEAR_WEIGHT_PER_ROW
-
-    @staticmethod
-    def quantized_linear_fn(package, func_name, self, input, weight, *args, **kwargs):
-        """Quantized version of a generic linear functional."""
-        output = getattr(package, func_name)(
-            self.input_quantizer(input),
-            self.weight_quantizer(weight),
-            *args,
-            **kwargs,
-        )
-        return self.output_quantizer(output)
-
-    def _setup(self):
-        super()._setup()
-        if not hasattr(self.forward, "__func__") or (
-            self.forward.__func__ is not self.__class__.forward
-        ):
-            if hasattr(self, "_hf_hook"):
-                # The forward of this module has been monkey patched by HF accelerate
-                # So it will not call _QuantLinear forward function
-                # We need to remove the monkey patching, update the forward method and add the hook back
-
-                from accelerate.hooks import add_hook_to_module
-
-                assert hasattr(self, "_old_forward")
-
-                hook = self._hf_hook
-                delattr(self, "_hf_hook")
-                delattr(self, "_old_forward")
-                self.forward = types.MethodType(self.__class__.forward, self)
-                add_hook_to_module(self, hook)
-            else:
-                raise RuntimeError(
-                    "Received a module with monkey patched forward method. Quantization will not"
-                    " work."
-                )
-
-
-class QuantLinear(_LegacyQuantLinearConvBaseMixin, nn.Linear):
-    """Quantized version of nn.Linear."""
-
-    default_quant_desc_weight = _QuantLinear.default_quant_desc_weight
-
-
-Linear = QuantLinear
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Quantized Linear."""
+
+import types
+
+import torch.nn as nn
+
+from modelopt.torch.quantization import tensor_quant
+
+from .quant_module import QuantLinearConvBase, QuantModuleRegistry, _LegacyQuantLinearConvBaseMixin
+
+__all__ = ["Linear", "QuantLinear"]
+
+
+@QuantModuleRegistry.register({nn.Linear: "nn.Linear"})
+class _QuantLinear(QuantLinearConvBase):
+    """Quantized base class for nn.Linear type classes."""
+
+    default_quant_desc_weight = tensor_quant.QUANT_DESC_8BIT_LINEAR_WEIGHT_PER_ROW
+
+    @staticmethod
+    def quantized_linear_fn(package, func_name, self, input, weight, *args, **kwargs):
+        """Quantized version of a generic linear functional."""
+        output = getattr(package, func_name)(
+            self.input_quantizer(input),
+            self.weight_quantizer(weight),
+            *args,
+            **kwargs,
+        )
+        return self.output_quantizer(output)
+
+    def _setup(self):
+        super()._setup()
+        if not hasattr(self.forward, "__func__") or (
+            self.forward.__func__ is not self.__class__.forward
+        ):
+            if hasattr(self, "_hf_hook"):
+                # The forward of this module has been monkey patched by HF accelerate
+                # So it will not call _QuantLinear forward function
+                # We need to remove the monkey patching, update the forward method and add the hook back
+
+                from accelerate.hooks import add_hook_to_module
+
+                assert hasattr(self, "_old_forward")
+
+                hook = self._hf_hook
+                delattr(self, "_hf_hook")
+                delattr(self, "_old_forward")
+                self.forward = types.MethodType(self.__class__.forward, self)
+                add_hook_to_module(self, hook)
+            else:
+                raise RuntimeError(
+                    "Received a module with monkey patched forward method. Quantization will not"
+                    " work."
+                )
+
+
+class QuantLinear(_LegacyQuantLinearConvBaseMixin, nn.Linear):
+    """Quantized version of nn.Linear."""
+
+    default_quant_desc_weight = _QuantLinear.default_quant_desc_weight
+
+
+Linear = QuantLinear
```

## modelopt/torch/quantization/nn/modules/quant_module.py

 * *Ordering differences only*

```diff
@@ -1,177 +1,177 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Base class for quantization modules."""
-import contextlib
-import copy
-from typing import Union
-
-import torch
-
-from modelopt.torch.opt.dynamic import DynamicModule, _DMRegistryCls
-from modelopt.torch.quantization.utils import is_torch_export_mode
-
-from ...tensor_quant import QUANT_DESC_8BIT_PER_TENSOR, QuantDescriptor
-from .tensor_quantizer import SequentialQuantizer, TensorQuantizer
-
-__all__ = ["QuantInputBase", "QuantLinearConvBase", "QuantModuleRegistry"]
-
-QuantModuleRegistry = _DMRegistryCls("Quant")
-
-
-class QuantInputBase(DynamicModule):
-    """Base class for modules where the input is quantized."""
-
-    input_quantizer: Union[TensorQuantizer, SequentialQuantizer]
-    output_quantizer: Union[TensorQuantizer, SequentialQuantizer]
-    default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
-    default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
-
-    def forward(self, input, *args, **kwargs):
-        """Quantize the input before calling the original forward method."""
-        input = self.input_quantizer(input)
-        output = super().forward(input, *args, **kwargs)
-        if isinstance(output, tuple):
-            return (self.output_quantizer(output[0]), *output[1:])
-        return self.output_quantizer(output)
-
-    def _setup(self):
-        """Patch the module's forward method to quantize the input."""
-        self._register_temp_attribute(
-            "input_quantizer", TensorQuantizer(self.default_quant_desc_input)
-        )
-        self._register_temp_attribute(
-            "output_quantizer", TensorQuantizer(self.default_quant_desc_output)
-        )
-        self.output_quantizer.disable()
-
-
-class QuantLinearConvBase(QuantInputBase):
-    """Base class for quantized linear modules.
-
-    Quantized linear modules are modules where both the input and the weight are quantized.
-    """
-
-    weight_quantizer: Union[TensorQuantizer, SequentialQuantizer]
-    _enable_weight_quantization: bool
-    default_quant_desc_weight = QUANT_DESC_8BIT_PER_TENSOR
-
-    @contextlib.contextmanager
-    def quantize_weight(self):
-        """Context in which `self.weight` is quantized."""
-        self._enable_weight_quantization = True
-        yield
-        self._enable_weight_quantization = False
-
-    @staticmethod
-    def _get_quantized_weight(module: "QuantLinearConvBase", weight: torch.Tensor) -> torch.Tensor:
-        if module._enable_weight_quantization or is_torch_export_mode():
-            return module.weight_quantizer(weight)
-        return weight
-
-    def forward(self, input, *args, **kwargs):
-        """Quantize the input and the weight before calling the original forward method."""
-        # self.quntize_weight() setting attributes is not allowed for torch.export.
-        if is_torch_export_mode():
-            return super().forward(input, *args, **kwargs)
-        with self.quantize_weight():
-            return super().forward(input, *args, **kwargs)
-
-    def _setup(self):
-        super()._setup()
-        self._register_temp_attribute(
-            "weight_quantizer", TensorQuantizer(self.default_quant_desc_weight)
-        )
-        self._register_temp_attribute("_enable_weight_quantization", False)
-        self._register_dynamic_attribute("weight", self._get_quantized_weight)
-
-    @staticmethod
-    def initialize_quantizer_with_dummy_states(module):
-        """Initialize the quantizer states with dummy values with the correct type and device."""
-        # Lets do a local import; nn modules should not import from model_calib
-        from modelopt.torch.quantization.model_calib import max_calibrate
-
-        def _initialize_activation_quantizer_amax(quantizer, device, dtype):
-            if not getattr(quantizer, "_has_amax", False):
-                return
-            # We need the outputs to initialize the amax in this case; Weights alone are not enough
-            if (
-                quantizer.block_sizes is not None
-                and quantizer.block_sizes.get("type", None) != "dynamic"
-            ):
-                return
-            if quantizer.axis is not None:
-                return
-            quantizer.amax = torch.tensor(1, device=device, dtype=dtype)
-
-        device, dtype = module.weight.device, module.weight.dtype
-
-        for input_quantizer in SequentialQuantizer.tensor_quantizer_iterator(
-            getattr(module, "input_quantizer", None)
-        ):
-            if getattr(input_quantizer, "_has_pre_quant_scale", False):
-                input_quantizer.pre_quant_scale = torch.ones(
-                    module.weight.shape[1], device=device, dtype=dtype
-                )
-
-            _initialize_activation_quantizer_amax(input_quantizer, device, dtype)
-
-        for output_quantizer in SequentialQuantizer.tensor_quantizer_iterator(
-            getattr(module, "output_quantizer", None)
-        ):
-            _initialize_activation_quantizer_amax(output_quantizer, device, dtype)
-
-        for weight_quantizer in SequentialQuantizer.tensor_quantizer_iterator(
-            getattr(module, "weight_quantizer", None)
-        ):
-            if getattr(weight_quantizer, "_has_amax", False):
-                max_calibrate(
-                    weight_quantizer, lambda weight_quantizer: weight_quantizer(module.weight)
-                )
-
-
-class _LegacyQuantInputBaseMixin:
-    """A mixin to support legacy quantized modules which needs to have an __init__ method."""
-
-    _quantized_cls = QuantInputBase
-    default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
-    default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
-
-    def __init__(self, *args, quant_desc_input=None, **kwargs):
-        """Initialize the module with its original __init__ and patch its forward."""
-        self.default_quant_desc_input = quant_desc_input or self.default_quant_desc_input
-        super().__init__(*args, **kwargs)
-        QuantModuleRegistry.convert(self)
-
-    @classmethod
-    def set_default_quant_desc_input(cls, value):
-        """Set the class default input quantization descriptor (legacy method)."""
-        if not isinstance(value, QuantDescriptor):
-            raise ValueError("{} is not an instance of QuantDescriptor!")
-        cls.default_quant_desc_input = copy.deepcopy(value)
-
-
-class _LegacyQuantLinearConvBaseMixin(_LegacyQuantInputBaseMixin):
-    """A mixin to support legacy quantized modules which needs to have an __init__ method."""
-
-    _quantized_cls = QuantLinearConvBase
-    default_quant_desc_weight = QUANT_DESC_8BIT_PER_TENSOR
-
-    def __init__(self, *args, quant_desc_input=None, quant_desc_weight=None, **kwargs):
-        """Initialize the module with its original __init__ and patch its forward."""
-        self.default_quant_desc_weight = quant_desc_weight or self.default_quant_desc_weight
-        super().__init__(*args, quant_desc_input=quant_desc_input, **kwargs)
-
-    @classmethod
-    def set_default_quant_desc_weight(cls, value):
-        """Set the class default weight quantization descriptor (legacy method)."""
-        if not isinstance(value, QuantDescriptor):
-            raise ValueError("{} is not an instance of QuantDescriptor!")
-        cls.default_quant_desc_weight = copy.deepcopy(value)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Base class for quantization modules."""
+import contextlib
+import copy
+from typing import Union
+
+import torch
+
+from modelopt.torch.opt.dynamic import DynamicModule, _DMRegistryCls
+from modelopt.torch.quantization.utils import is_torch_export_mode
+
+from ...tensor_quant import QUANT_DESC_8BIT_PER_TENSOR, QuantDescriptor
+from .tensor_quantizer import SequentialQuantizer, TensorQuantizer
+
+__all__ = ["QuantInputBase", "QuantLinearConvBase", "QuantModuleRegistry"]
+
+QuantModuleRegistry = _DMRegistryCls("Quant")
+
+
+class QuantInputBase(DynamicModule):
+    """Base class for modules where the input is quantized."""
+
+    input_quantizer: Union[TensorQuantizer, SequentialQuantizer]
+    output_quantizer: Union[TensorQuantizer, SequentialQuantizer]
+    default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
+    default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
+
+    def forward(self, input, *args, **kwargs):
+        """Quantize the input before calling the original forward method."""
+        input = self.input_quantizer(input)
+        output = super().forward(input, *args, **kwargs)
+        if isinstance(output, tuple):
+            return (self.output_quantizer(output[0]), *output[1:])
+        return self.output_quantizer(output)
+
+    def _setup(self):
+        """Patch the module's forward method to quantize the input."""
+        self._register_temp_attribute(
+            "input_quantizer", TensorQuantizer(self.default_quant_desc_input)
+        )
+        self._register_temp_attribute(
+            "output_quantizer", TensorQuantizer(self.default_quant_desc_output)
+        )
+        self.output_quantizer.disable()
+
+
+class QuantLinearConvBase(QuantInputBase):
+    """Base class for quantized linear modules.
+
+    Quantized linear modules are modules where both the input and the weight are quantized.
+    """
+
+    weight_quantizer: Union[TensorQuantizer, SequentialQuantizer]
+    _enable_weight_quantization: bool
+    default_quant_desc_weight = QUANT_DESC_8BIT_PER_TENSOR
+
+    @contextlib.contextmanager
+    def quantize_weight(self):
+        """Context in which `self.weight` is quantized."""
+        self._enable_weight_quantization = True
+        yield
+        self._enable_weight_quantization = False
+
+    @staticmethod
+    def _get_quantized_weight(module: "QuantLinearConvBase", weight: torch.Tensor) -> torch.Tensor:
+        if module._enable_weight_quantization or is_torch_export_mode():
+            return module.weight_quantizer(weight)
+        return weight
+
+    def forward(self, input, *args, **kwargs):
+        """Quantize the input and the weight before calling the original forward method."""
+        # self.quntize_weight() setting attributes is not allowed for torch.export.
+        if is_torch_export_mode():
+            return super().forward(input, *args, **kwargs)
+        with self.quantize_weight():
+            return super().forward(input, *args, **kwargs)
+
+    def _setup(self):
+        super()._setup()
+        self._register_temp_attribute(
+            "weight_quantizer", TensorQuantizer(self.default_quant_desc_weight)
+        )
+        self._register_temp_attribute("_enable_weight_quantization", False)
+        self._register_dynamic_attribute("weight", self._get_quantized_weight)
+
+    @staticmethod
+    def initialize_quantizer_with_dummy_states(module):
+        """Initialize the quantizer states with dummy values with the correct type and device."""
+        # Lets do a local import; nn modules should not import from model_calib
+        from modelopt.torch.quantization.model_calib import max_calibrate
+
+        def _initialize_activation_quantizer_amax(quantizer, device, dtype):
+            if not getattr(quantizer, "_has_amax", False):
+                return
+            # We need the outputs to initialize the amax in this case; Weights alone are not enough
+            if (
+                quantizer.block_sizes is not None
+                and quantizer.block_sizes.get("type", None) != "dynamic"
+            ):
+                return
+            if quantizer.axis is not None:
+                return
+            quantizer.amax = torch.tensor(1, device=device, dtype=dtype)
+
+        device, dtype = module.weight.device, module.weight.dtype
+
+        for input_quantizer in SequentialQuantizer.tensor_quantizer_iterator(
+            getattr(module, "input_quantizer", None)
+        ):
+            if getattr(input_quantizer, "_has_pre_quant_scale", False):
+                input_quantizer.pre_quant_scale = torch.ones(
+                    module.weight.shape[1], device=device, dtype=dtype
+                )
+
+            _initialize_activation_quantizer_amax(input_quantizer, device, dtype)
+
+        for output_quantizer in SequentialQuantizer.tensor_quantizer_iterator(
+            getattr(module, "output_quantizer", None)
+        ):
+            _initialize_activation_quantizer_amax(output_quantizer, device, dtype)
+
+        for weight_quantizer in SequentialQuantizer.tensor_quantizer_iterator(
+            getattr(module, "weight_quantizer", None)
+        ):
+            if getattr(weight_quantizer, "_has_amax", False):
+                max_calibrate(
+                    weight_quantizer, lambda weight_quantizer: weight_quantizer(module.weight)
+                )
+
+
+class _LegacyQuantInputBaseMixin:
+    """A mixin to support legacy quantized modules which needs to have an __init__ method."""
+
+    _quantized_cls = QuantInputBase
+    default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
+    default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
+
+    def __init__(self, *args, quant_desc_input=None, **kwargs):
+        """Initialize the module with its original __init__ and patch its forward."""
+        self.default_quant_desc_input = quant_desc_input or self.default_quant_desc_input
+        super().__init__(*args, **kwargs)
+        QuantModuleRegistry.convert(self)
+
+    @classmethod
+    def set_default_quant_desc_input(cls, value):
+        """Set the class default input quantization descriptor (legacy method)."""
+        if not isinstance(value, QuantDescriptor):
+            raise ValueError("{} is not an instance of QuantDescriptor!")
+        cls.default_quant_desc_input = copy.deepcopy(value)
+
+
+class _LegacyQuantLinearConvBaseMixin(_LegacyQuantInputBaseMixin):
+    """A mixin to support legacy quantized modules which needs to have an __init__ method."""
+
+    _quantized_cls = QuantLinearConvBase
+    default_quant_desc_weight = QUANT_DESC_8BIT_PER_TENSOR
+
+    def __init__(self, *args, quant_desc_input=None, quant_desc_weight=None, **kwargs):
+        """Initialize the module with its original __init__ and patch its forward."""
+        self.default_quant_desc_weight = quant_desc_weight or self.default_quant_desc_weight
+        super().__init__(*args, quant_desc_input=quant_desc_input, **kwargs)
+
+    @classmethod
+    def set_default_quant_desc_weight(cls, value):
+        """Set the class default weight quantization descriptor (legacy method)."""
+        if not isinstance(value, QuantDescriptor):
+            raise ValueError("{} is not an instance of QuantDescriptor!")
+        cls.default_quant_desc_weight = copy.deepcopy(value)
```

## modelopt/torch/quantization/nn/modules/quant_pooling.py

 * *Ordering differences only*

```diff
@@ -1,112 +1,112 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Quantized Pooling modules."""
-
-from torch.nn.modules import pooling
-
-from .quant_module import QuantInputBase, QuantModuleRegistry, _LegacyQuantInputBaseMixin
-
-__all__ = [
-    "MaxPool1d",
-    "QuantMaxPool1d",
-    "MaxPool2d",
-    "QuantMaxPool2d",
-    "MaxPool3d",
-    "QuantMaxPool3d",
-    "AvgPool1d",
-    "QuantAvgPool1d",
-    "AvgPool2d",
-    "QuantAvgPool2d",
-    "AvgPool3d",
-    "QuantAvgPool3d",
-    "AdaptiveAvgPool1d",
-    "QuantAdaptiveAvgPool1d",
-    "AdaptiveAvgPool2d",
-    "QuantAdaptiveAvgPool2d",
-    "AdaptiveAvgPool3d",
-    "QuantAdaptiveAvgPool3d",
-]
-
-
-class QuantMaxPool1d(_LegacyQuantInputBaseMixin, pooling.MaxPool1d):
-    """Quantized version of nn.MaxPool1d."""
-
-    pass
-
-
-class QuantMaxPool2d(_LegacyQuantInputBaseMixin, pooling.MaxPool2d):
-    """Quantized version of nn.MaxPool2d."""
-
-    pass
-
-
-class QuantMaxPool3d(_LegacyQuantInputBaseMixin, pooling.MaxPool3d):
-    """Quantized version of nn.MaxPool3d."""
-
-    pass
-
-
-class QuantAvgPool1d(_LegacyQuantInputBaseMixin, pooling.AvgPool1d):
-    """Quantized version of nn.AvgPool1d."""
-
-    pass
-
-
-class QuantAvgPool2d(_LegacyQuantInputBaseMixin, pooling.AvgPool2d):
-    """Quantized version of nn.AvgPool2d."""
-
-    pass
-
-
-class QuantAvgPool3d(_LegacyQuantInputBaseMixin, pooling.AvgPool3d):
-    """Quantized version of nn.AvgPool3d."""
-
-    pass
-
-
-class QuantAdaptiveAvgPool1d(_LegacyQuantInputBaseMixin, pooling.AdaptiveAvgPool1d):
-    """Quantized version of nn.AdaptiveAvgPool1d."""
-
-    pass
-
-
-class QuantAdaptiveAvgPool2d(_LegacyQuantInputBaseMixin, pooling.AdaptiveAvgPool2d):
-    """Quantized version of nn.AdaptiveAvgPool2d."""
-
-    pass
-
-
-class QuantAdaptiveAvgPool3d(_LegacyQuantInputBaseMixin, pooling.AdaptiveAvgPool3d):
-    """Quantized version of nn.AdaptiveAvgPool3d."""
-
-    pass
-
-
-# Define alias with Quant prefix
-MaxPool1d = QuantMaxPool1d
-MaxPool2d = QuantMaxPool2d
-MaxPool3d = QuantMaxPool3d
-AvgPool1d = QuantAvgPool1d
-AvgPool2d = QuantAvgPool2d
-AvgPool3d = QuantAvgPool3d
-AdaptiveAvgPool1d = QuantAdaptiveAvgPool1d
-AdaptiveAvgPool2d = QuantAdaptiveAvgPool2d
-AdaptiveAvgPool3d = QuantAdaptiveAvgPool3d
-
-QuantModuleRegistry.register({pooling.MaxPool1d: "nn.MaxPool1d"})(QuantInputBase)
-QuantModuleRegistry.register({pooling.MaxPool2d: "nn.MaxPool2d"})(QuantInputBase)
-QuantModuleRegistry.register({pooling.MaxPool3d: "nn.MaxPool3d"})(QuantInputBase)
-QuantModuleRegistry.register({pooling.AvgPool1d: "nn.AvgPool1d"})(QuantInputBase)
-QuantModuleRegistry.register({pooling.AvgPool2d: "nn.AvgPool2d"})(QuantInputBase)
-QuantModuleRegistry.register({pooling.AvgPool3d: "nn.AvgPool3d"})(QuantInputBase)
-QuantModuleRegistry.register({pooling.AdaptiveAvgPool1d: "nn.AdaptiveAvgPool1d"})(QuantInputBase)
-QuantModuleRegistry.register({pooling.AdaptiveAvgPool2d: "nn.AdaptiveAvgPool2d"})(QuantInputBase)
-QuantModuleRegistry.register({pooling.AdaptiveAvgPool3d: "nn.AdaptiveAvgPool3d"})(QuantInputBase)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Quantized Pooling modules."""
+
+from torch.nn.modules import pooling
+
+from .quant_module import QuantInputBase, QuantModuleRegistry, _LegacyQuantInputBaseMixin
+
+__all__ = [
+    "MaxPool1d",
+    "QuantMaxPool1d",
+    "MaxPool2d",
+    "QuantMaxPool2d",
+    "MaxPool3d",
+    "QuantMaxPool3d",
+    "AvgPool1d",
+    "QuantAvgPool1d",
+    "AvgPool2d",
+    "QuantAvgPool2d",
+    "AvgPool3d",
+    "QuantAvgPool3d",
+    "AdaptiveAvgPool1d",
+    "QuantAdaptiveAvgPool1d",
+    "AdaptiveAvgPool2d",
+    "QuantAdaptiveAvgPool2d",
+    "AdaptiveAvgPool3d",
+    "QuantAdaptiveAvgPool3d",
+]
+
+
+class QuantMaxPool1d(_LegacyQuantInputBaseMixin, pooling.MaxPool1d):
+    """Quantized version of nn.MaxPool1d."""
+
+    pass
+
+
+class QuantMaxPool2d(_LegacyQuantInputBaseMixin, pooling.MaxPool2d):
+    """Quantized version of nn.MaxPool2d."""
+
+    pass
+
+
+class QuantMaxPool3d(_LegacyQuantInputBaseMixin, pooling.MaxPool3d):
+    """Quantized version of nn.MaxPool3d."""
+
+    pass
+
+
+class QuantAvgPool1d(_LegacyQuantInputBaseMixin, pooling.AvgPool1d):
+    """Quantized version of nn.AvgPool1d."""
+
+    pass
+
+
+class QuantAvgPool2d(_LegacyQuantInputBaseMixin, pooling.AvgPool2d):
+    """Quantized version of nn.AvgPool2d."""
+
+    pass
+
+
+class QuantAvgPool3d(_LegacyQuantInputBaseMixin, pooling.AvgPool3d):
+    """Quantized version of nn.AvgPool3d."""
+
+    pass
+
+
+class QuantAdaptiveAvgPool1d(_LegacyQuantInputBaseMixin, pooling.AdaptiveAvgPool1d):
+    """Quantized version of nn.AdaptiveAvgPool1d."""
+
+    pass
+
+
+class QuantAdaptiveAvgPool2d(_LegacyQuantInputBaseMixin, pooling.AdaptiveAvgPool2d):
+    """Quantized version of nn.AdaptiveAvgPool2d."""
+
+    pass
+
+
+class QuantAdaptiveAvgPool3d(_LegacyQuantInputBaseMixin, pooling.AdaptiveAvgPool3d):
+    """Quantized version of nn.AdaptiveAvgPool3d."""
+
+    pass
+
+
+# Define alias with Quant prefix
+MaxPool1d = QuantMaxPool1d
+MaxPool2d = QuantMaxPool2d
+MaxPool3d = QuantMaxPool3d
+AvgPool1d = QuantAvgPool1d
+AvgPool2d = QuantAvgPool2d
+AvgPool3d = QuantAvgPool3d
+AdaptiveAvgPool1d = QuantAdaptiveAvgPool1d
+AdaptiveAvgPool2d = QuantAdaptiveAvgPool2d
+AdaptiveAvgPool3d = QuantAdaptiveAvgPool3d
+
+QuantModuleRegistry.register({pooling.MaxPool1d: "nn.MaxPool1d"})(QuantInputBase)
+QuantModuleRegistry.register({pooling.MaxPool2d: "nn.MaxPool2d"})(QuantInputBase)
+QuantModuleRegistry.register({pooling.MaxPool3d: "nn.MaxPool3d"})(QuantInputBase)
+QuantModuleRegistry.register({pooling.AvgPool1d: "nn.AvgPool1d"})(QuantInputBase)
+QuantModuleRegistry.register({pooling.AvgPool2d: "nn.AvgPool2d"})(QuantInputBase)
+QuantModuleRegistry.register({pooling.AvgPool3d: "nn.AvgPool3d"})(QuantInputBase)
+QuantModuleRegistry.register({pooling.AdaptiveAvgPool1d: "nn.AdaptiveAvgPool1d"})(QuantInputBase)
+QuantModuleRegistry.register({pooling.AdaptiveAvgPool2d: "nn.AdaptiveAvgPool2d"})(QuantInputBase)
+QuantModuleRegistry.register({pooling.AdaptiveAvgPool3d: "nn.AdaptiveAvgPool3d"})(QuantInputBase)
```

## modelopt/torch/quantization/nn/modules/tensor_quantizer.py

 * *Ordering differences only*

```diff
@@ -1,842 +1,842 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""TensorQuantizer Module."""
-import contextlib
-import math
-import warnings
-from typing import Any, Dict, List, Optional
-
-import torch
-import torch.distributed as dist
-import torch.nn.functional as F
-from packaging.version import Version
-from torch import nn
-
-from modelopt.torch.quantization.utils import is_torch_export_mode
-from modelopt.torch.utils import standardize_constructor_args
-from modelopt.torch.utils.distributed import DistributedProcessGroup
-
-from ... import calib
-from ... import utils as quant_utils
-from ...tensor_quant import (
-    QuantDescriptor,
-    fake_tensor_quant,
-    scaled_e4m3,
-    tensor_quant,
-)
-from .clip import Clip
-
-__all__ = ["TensorQuantizer", "SequentialQuantizer"]
-
-
-class TensorQuantizer(nn.Module):
-    """Tensor quantizer module.
-
-    This module uses tensor_quant or fake_tensor_quant function to quantize a tensor. And wrappers
-    variable, moving statistics we'd want when training a quantized network.
-
-    Experimental features:
-        * ``clip`` stage learns range before enabling quantization.
-        * ``calib`` stage runs calibration
-
-    Args:
-        quant_desc: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`.
-        disabled: A boolean. If True, by pass the whole module returns input. Default False.
-        if_quant: A boolean. If True, run main quantization body. Default True.
-        if_clip: A boolean. If True, clip before quantization and learn amax. Default False.
-        if_calib: A boolean. If True, run calibration. Not implemented yet. Settings of calibration
-            will probably go to :class:`QuantDescriptor <modelopt.torch.quantization.QuantDescriptor>`.
-
-    Readonly Properties:
-        - axis:
-        - fake_quant:
-        - scale:
-        - step_size:
-
-    Mutable Properties:
-        - num_bits:
-        - unsigned:
-        - amax:
-    """
-
-    def __init__(
-        self,
-        quant_desc=QuantDescriptor(),
-        disabled=False,
-        if_quant=True,
-        if_clip=False,
-        if_calib=False,
-    ):
-        """Initialize quantizer and set up required variables."""
-        super(TensorQuantizer, self).__init__()
-        # Expand quant_desc. Use quant_desc.dict would be easier, but adding one-by-one explicitly gives more control
-        self._num_bits = quant_desc.num_bits
-        self._fake_quant = quant_desc.fake_quant
-        self._axis = quant_desc.axis
-        self._block_sizes = quant_desc.block_sizes
-        self._scale_amax = quant_desc.scale_amax
-        self._learn_amax = quant_desc.learn_amax
-        self._unsigned = quant_desc.unsigned
-        self._narrow_range = quant_desc.narrow_range
-
-        self._scale = None if not quant_desc.fake_quant else 1.0
-        self._disabled = disabled
-        self._if_quant = if_quant
-        self._if_clip = False
-        self._if_calib = if_calib
-        self._enable_pre_quant_scale = True
-
-        if quant_desc.amax is not None:
-            self.register_buffer("_amax", torch.tensor(quant_desc.amax))
-
-        # Clip module consumes a lot of memory, so only create it if learn_amax is True
-        if self._learn_amax:
-            init_amax = quant_desc.amax if quant_desc.amax is not None else 1.0
-            self.clip = Clip(-init_amax, init_amax, learn_min=True, learn_max=True)
-            # It makes more sense to enable clip stage (which learns amax) if learn_amax is true
-            self.enable_clip()
-        if if_clip:
-            self.enable_clip()
-
-        if quant_desc.calib_method == "histogram":
-            calib_cls = calib.HistogramCalibrator
-        elif quant_desc.calib_method == "max":
-            calib_cls = calib.MaxCalibrator
-        else:
-            raise ValueError(f"Unknown calib_method: {quant_desc.calib_method}")
-        self._calibrator = calib_cls(
-            num_bits=self._num_bits, axis=self._axis, unsigned=self._unsigned
-        )
-
-    @property
-    def num_bits(self):
-        """Return num_bits for quantization."""
-        return self._num_bits
-
-    @num_bits.setter
-    def num_bits(self, value):
-        self._num_bits = value
-
-    @property
-    def maxbound(self):
-        """Return maxbound for quantization."""
-        if self._num_bits == (4, 3):
-            return 448.0
-        return (1 << (self._num_bits - 1 + int(self._unsigned))) - 1
-
-    @property
-    def unsigned(self):
-        """Return True if unsigned quantization is used."""
-        return self._unsigned
-
-    @unsigned.setter
-    def unsigned(self, value):
-        self._unsigned = value
-
-    @property
-    def scale(self):
-        """Return scale used for quantization."""
-        return self._scale
-
-    @property
-    def pre_quant_scale(self):
-        """Return pre_quant_scale used for smoothquant."""
-        if not hasattr(self, "_pre_quant_scale") or not self._enable_pre_quant_scale:
-            return None
-        return self._pre_quant_scale
-
-    @pre_quant_scale.setter
-    def pre_quant_scale(self, value):
-        assert value is not None, "pre_quant_scale cannot be set to None."
-        assert (
-            self._enable_pre_quant_scale
-        ), "pre_quant_scale cannot be set when forward_with_pre_quant_scale is False."
-        if not hasattr(self, "_pre_quant_scale"):
-            self.register_buffer("_pre_quant_scale", torch.tensor(value))
-        else:
-            value = torch.tensor(value, device=self._pre_quant_scale.device)
-            if self._pre_quant_scale.shape != value.shape:
-                raise RuntimeError("Changing shape when setting pre_quant_scale is not allowed.")
-            self._pre_quant_scale.data.copy_(value.data)
-
-    @property
-    def amax(self):
-        """Return amax for quantization."""
-        if not hasattr(self, "_amax"):
-            return None
-        return self._amax
-
-    @amax.setter
-    def amax(self, value):
-        assert value is not None, "amax cannot be set to None."
-
-        if not hasattr(self, "_amax"):
-            self.register_buffer("_amax", torch.tensor(value))
-        else:
-            value = torch.tensor(value, device=self._amax.device)
-            if self._amax.shape != value.shape:
-                raise RuntimeError("Changing shape when setting amax is not allowed.")
-            self._amax.data.copy_(value.data)
-
-    def reset_amax(self):
-        """Reset amax to None."""
-        if hasattr(self, "_amax"):
-            delattr(self, "_amax")
-        self._calibrator.reset()
-
-    @property
-    def step_size(self):
-        """Return step size for integer quantization."""
-        if not hasattr(self, "_amax"):
-            warnings.warn("step_size is undefined under dynamic amax mode!")
-            return None
-        assert isinstance(
-            self._num_bits, int
-        ), "Step size is not defined for non-integer quantization."
-        return self._amax / (2.0 ** (self._num_bits - 1 + int(self._unsigned)) - 1.0)
-
-    @property
-    def axis(self):
-        """Return axis for quantization."""
-        return self._axis
-
-    @axis.setter
-    def axis(self, value):
-        self._axis = value
-
-    @property
-    def block_sizes(self):
-        """Return block_sizes for quantization."""
-        return self._block_sizes
-
-    @block_sizes.setter
-    def block_sizes(self, value):
-        self._axis = None
-        self._block_sizes = value
-
-    @property
-    def fake_quant(self):
-        """Return True if fake quantization is used."""
-        return self._fake_quant
-
-    @property
-    def narrow_range(self):
-        """Return True if symmetric integer range for signed quantization is used."""
-        return self._narrow_range
-
-    @narrow_range.setter
-    def narrow_range(self, value):
-        self._narrow_range = value
-
-    @property
-    def is_enabled(self):
-        """Return true if the modules is not disabled."""
-        return not self._disabled
-
-    def disable(self):
-        """Bypass the module.
-
-        Neither of calibration, clipping and quantization will be performed if the module is disabled.
-        """
-        self._disabled = True
-
-    def enable(self):
-        """Enable the module."""
-        self._disabled = False
-
-    def disable_clip(self):
-        """Disable clip stage."""
-        self._if_clip = False
-        self.clip.clip_value_min.requires_grad = False
-        self.clip.clip_value_max.requires_grad = False
-
-    def enable_clip(self):
-        """Enable clip stage."""
-        if not self._learn_amax:
-            raise ValueError("learn_amax is False. Cannot enable clip.")
-        self.clip.clip_value_min.requires_grad = True
-        self.clip.clip_value_max.requires_grad = True
-        self._if_clip = True
-
-    def disable_calib(self):
-        """Disable calibration."""
-        self._if_calib = False
-
-    def enable_calib(self):
-        """Enable calibration."""
-        if self._calibrator is None:
-            raise ValueError("Calibrator was not created, cannot enable calibration.")
-        self._if_calib = True
-
-    def disable_quant(self):
-        """Disable quantization."""
-        self._if_quant = False
-
-    def enable_quant(self):
-        """Enable quantization."""
-        self._if_quant = True
-
-    def load_calib_amax(self, *args, **kwargs):
-        """Load amax from calibrator.
-
-        Updates the amax buffer with value computed by the calibrator, creating it if necessary.
-        ``*args`` and ``**kwargs`` are directly passed to ``compute_amax``, except ``"strict"`` in
-        ``kwargs``. Refer to ``compute_amax`` for more details.
-        """
-        strict = kwargs.pop("strict", True)
-        if getattr(self, "_calibrator", None) is None:
-            raise RuntimeError("Calibrator not created.")
-        calib_amax = self._calibrator.compute_amax(*args, **kwargs)
-        if calib_amax is None:
-            err_msg = (
-                "Calibrator returned None. This usually happens when calibrator hasn't seen any"
-                " tensor."
-            )
-            if not strict:
-                warnings.warn(err_msg)
-                warnings.warn("Set amax to NaN!")
-                calib_amax = torch.tensor(math.nan)
-            else:
-                raise RuntimeError(
-                    err_msg
-                    + " Passing 'strict=False' to `load_calib_amax()` will ignore the error."
-                )
-        if not hasattr(self, "_amax"):
-            self.register_buffer("_amax", calib_amax.data)
-        else:
-            self._amax.copy_(calib_amax)
-
-    def init_learn_amax(self):
-        """Initialize learned amax from fixed amax."""
-        if self._learn_amax is False:
-            raise RuntimeError("Called init_learn_amax with learn_amax=False.")
-
-        if self._amax.numel() != 1:
-            warnings.warn("Per channel learned amax not supported. Initializing with max(amax).")
-            init_amax = torch.max(self._amax)
-        else:
-            init_amax = self._amax
-        self.clip.clip_value_min.data.copy_(-init_amax.data)
-        self.clip.clip_value_max.data.copy_(init_amax.data)
-
-    def _get_amax(self, inputs):
-        """Get amax from buffer or compute it dynamically."""
-        if hasattr(self, "_amax"):
-            amax = self._amax
-        else:
-            if self._axis is None:
-                reduce_axis = None
-            else:
-                reduce_axis = []
-                # Swap axis to reduce
-                axis = self._axis if isinstance(self._axis, (list, tuple)) else [self._axis]
-                for i in range(inputs.dim()):
-                    if not (i in axis or (i - inputs.dim()) in axis):
-                        reduce_axis.append(i)
-            amax = quant_utils.reduce_amax(inputs, axis=reduce_axis, keepdims=True).detach()
-        if self._scale_amax is not None:
-            amax = amax.detach() * self._scale_amax
-
-        amax = amax.data
-        return amax
-
-    def _quant_forward(self, inputs):
-        """Quantized forward pass."""
-        if self._learn_amax:
-            inputs = self.clip(inputs)
-            amax = torch.max(-self.clip.clip_value_min, self.clip.clip_value_max).detach()
-        else:
-            amax = self._get_amax(inputs)
-
-        if self._fake_quant:
-            if isinstance(self._num_bits, tuple):
-                E, M = self._num_bits  # noqa: N806
-                outputs = scaled_e4m3(inputs, self._get_amax(inputs), E, M)
-            else:
-                outputs = fake_tensor_quant(
-                    inputs, amax, self._num_bits, self._unsigned, self._narrow_range
-                )
-        else:
-            assert not isinstance(
-                self._num_bits, tuple
-            ), "only fake quantization supports ExMy type quantization."
-            outputs, self._scale = tensor_quant(inputs, amax, self._num_bits, self._unsigned)
-
-        return outputs
-
-    def _check_onnx_readiness(self, inputs):
-        """Check if quantizer is ready for ONNX export."""
-        assert hasattr(self, "_amax"), (
-            "Quantizer has not been calibrated. ONNX export requires the quantizer to be"
-            " calibrated.Calibrate and load amax before exporting to ONNX."
-        )
-
-        if self._if_calib:
-            warnings.warn(
-                "Quantizer is in calibration mode. "
-                "Please complete calibration before exporting to ONNX for correct results."
-            )
-
-        amax = self._get_amax(inputs)
-
-        # We only support scalar amax for E4M3 ONNX export
-        if isinstance(self.num_bits, tuple):
-            assert amax.numel() == 1, (
-                "E4M3 supports ONNX export only for per-tensor quantization."
-                " Per-tensor quantization requires scalar amax. "
-                f"Received non-scalar amax of shape: {amax.shape}"
-            )
-
-        if self.block_sizes is not None:
-            raise Exception("Blockquant does not support ONNX export.")
-
-    def _setup_for_blockquant(self, inputs: torch.Tensor):
-        # Get reshape sizes and paddings for block-quantization
-        def get_axis_quant_params(ax):
-            ax = ax if ax in self.block_sizes else ax - inputs.dim()
-            bsize = self.block_sizes.get(ax, None)
-            padding, ax_slice = None, None
-            if bsize is not None and inputs.shape[ax] % bsize != 0:
-                padding = (bsize - (inputs.shape[ax] % bsize), 0)
-                ax_slice = slice(inputs.shape[ax])
-            return bsize, padding, ax_slice
-
-        def set_quant_params(axis, block_reshape_size, padding, slices, amax_shape=None):
-            self._axis = tuple(axis)
-            if hasattr(self, "_calibrator"):
-                self._calibrator._axis = self._axis
-            self._original_shape = inputs.shape
-            self._block_reshape_size = torch.Size(block_reshape_size)
-            if padding is not None:
-                self._padding = tuple(padding)
-                self._original_shape = F.pad(inputs, self._padding, "constant", 0).shape
-            if slices is not None:
-                self._slices = slices
-            if amax_shape:
-                self._amax_shape_for_export = amax_shape
-
-        # Reshape size have already been set
-        if hasattr(self, "_block_reshape_size"):
-            return
-
-        reshape_size, quantize_axis, paddings, slices = [], [], [], []
-
-        # special handling for block-quantization along the last axis:
-        # flatten the input for faster execution
-        if (self.block_sizes.get(inputs.dim() - 1, None) or self.block_sizes.get(-1, None)) and len(
-            QuantDescriptor.get_block_quant_axes_and_sizes(self.block_sizes)
-        ) == 1:
-            bsize, padding, ax_slice = get_axis_quant_params(inputs.dim() - 1)
-            slices = None if ax_slice is None else (*(slice(None),) * (inputs.dim() - 1), ax_slice)
-            padding = padding if not padding else tuple(reversed(padding))
-            amax_shape_for_export = (*(inputs.shape[:-1]), -1)
-            set_quant_params((0,), (-1, bsize), padding, slices, amax_shape_for_export)
-            return
-
-        for ax in range(inputs.dim()):
-            bsize, padding, ax_slice = get_axis_quant_params(ax)
-            paddings.append(padding)
-            slices.append(ax_slice)
-            if bsize is not None:
-                reshape_size.extend([math.ceil(inputs.shape[ax] / bsize), bsize])
-                quantize_axis.extend([True, False])
-            else:
-                reshape_size.append(inputs.shape[ax])
-                quantize_axis.append(True)
-
-        quant_axis = [i for i in range(len(quantize_axis)) if quantize_axis[i]]
-
-        if all(s is None for s in slices):
-            slices = None
-        else:
-            slices = [s if s else slice(None) for s in slices]
-
-        if all(p is None for p in paddings):
-            paddings = None
-        else:
-            new_paddings = []
-            for padding in paddings:
-                if not (new_paddings or padding):
-                    continue
-                new_paddings.extend(padding if padding else (0, 0))
-            paddings = tuple(reversed(new_paddings))
-
-        set_quant_params(quant_axis, reshape_size, paddings, slices)
-
-    def _process_for_blockquant(self, inputs: torch.Tensor):
-        if hasattr(self, "_padding"):
-            inputs = F.pad(inputs, self._padding, "constant", 0)
-        assert inputs.shape == self._original_shape, (
-            f"Input shape has changed from {self._original_shape} to {inputs.shape}."
-            " Block-quantization requires a fixed input shape."
-        )
-        inputs = inputs.reshape(self._block_reshape_size)
-        return inputs
-
-    def _reset_to_original_shape(self, outputs: torch.Tensor):
-        outputs = outputs.reshape(self._original_shape)
-        if hasattr(self, "_slices"):
-            outputs = outputs[self._slices]
-        return outputs
-
-    def export_amax(self) -> Optional[torch.Tensor]:
-        """Export correctly formatted/shaped amax."""
-        if self.block_sizes is not None and self.block_sizes.get("type", None) == "dynamic":
-            raise NotImplementedError(
-                "Exporting amax for dynamic block quantization is not supported."
-            )
-
-        if self.amax is None:
-            return None
-
-        if not hasattr(self, "_amax_shape_for_export"):
-            amax = self.amax
-        else:
-            amax = self.amax.reshape(self._amax_shape_for_export)
-        amax[amax == 0] = self.maxbound
-        clamp_min, clamp_max = torch.finfo(amax.dtype).tiny, torch.finfo(amax.dtype).max
-        amax = amax.clamp(min=clamp_min, max=clamp_max)
-
-        if self.block_sizes is None:
-            # tensorrt_llm assumes the scaling_factor dim >= 1 for per-tensor.
-            if self.axis is None:
-                amax = amax.unsqueeze(0)
-
-            # If single-axis quantization, squeeze amax
-            elif isinstance(self.axis, int) or (
-                isinstance(self.axis, (list, tuple)) and len(self.axis) == 1
-            ):
-                amax = amax.squeeze()
-
-        return amax
-
-    def forward(self, inputs):
-        """Apply tensor_quant function to inputs.
-
-        Args:
-            inputs: A Tensor of type float32.
-
-        Returns:
-            outputs: A Tensor of type output_dtype
-        """
-        # Early return if nothing is collected during the forward (e.g. MoE)
-        if len(inputs) == 0:
-            return inputs
-
-        # Activation scaling for smoothquant
-        if self.pre_quant_scale is not None:
-            inputs = inputs * self.pre_quant_scale
-
-        if self._disabled:
-            return inputs
-
-        # GLOBALS could break TorchDynamo for some Pytorch versions (i.e., 2.3.0)
-        if not is_torch_export_mode():
-            # GLOBALS.in_onnx_export is only available in torch >= 1.13
-            if Version(torch.__version__) >= Version("1.13"):
-                from torch.onnx._globals import GLOBALS
-
-                if GLOBALS.in_onnx_export:
-                    self._check_onnx_readiness(inputs)
-
-        if self.block_sizes is not None and not self.block_sizes.get("type", None) == "dynamic":
-            # Dynamic block quantization is handled seperately by the quantization kernels
-            self._setup_for_blockquant(inputs)
-            inputs = self._process_for_blockquant(inputs)
-
-        outputs = inputs
-
-        if self._if_calib:
-            if self._calibrator is None:
-                raise RuntimeError("Calibrator was not created.")
-            # Shape is only known when it sees the first tensor
-            self._calibrator.collect(inputs)
-
-        if self._if_clip:
-            if not self._learn_amax:
-                raise RuntimeError("Clip without learning amax is not implemented.")
-            outputs = self.clip(inputs)
-
-        if self._if_quant:
-            outputs = self._quant_forward(inputs)
-
-        if self.block_sizes is not None and not self.block_sizes.get("type", None) == "dynamic":
-            outputs = self._reset_to_original_shape(outputs)
-
-        return outputs
-
-    def _short_amax(self, fmt=".4f"):
-        """Short description of amax.
-
-        Returns:
-            'dynamic': if _amax is not registered
-            'amax': if _amax is per-tensor
-            '[min, max](size)': if _amax is per-channel
-        """
-        if not hasattr(self, "_amax"):
-            return "dynamic"
-        if self._amax is None:
-            return "None"
-        if self._amax.numel() == 1:
-            return f"{self._amax.item():{fmt}}"
-        return (
-            f"[{self._amax.min().item():{fmt}},"
-            f" {self._amax.max().item():{fmt}}]({self._amax.numel()})"
-        )
-
-    def extra_repr(self):
-        """Set the extra information about this module."""
-        if self._disabled:
-            return "disabled"
-        s = f"{'unsigned ' if self._unsigned else ''}{self._num_bits} bit"
-        s += " narrow" if (self._narrow_range) else ""
-        s += " fake" if (self._fake_quant) else ""
-        if self.block_sizes is not None:
-            s += f" block_sizes={self._block_sizes},"
-        else:
-            s += f" axis={self._axis}" if self._axis is not None else " per-tensor"
-        s += f" amax={self._short_amax()}"
-        s += f" *{self._scale_amax}" if self._scale_amax else ""
-        s += " pre_quant_scale" if self.pre_quant_scale is not None else ""
-        s += " learned" if (self._learn_amax) else ""
-        s += (
-            f" calibrator={self._calibrator.__class__.__name__}"
-            if (self._calibrator is not None)
-            else ""
-        )
-        s += " quant" if (self._if_quant) else ""
-        s += " clip" if (self._if_clip) else ""
-        s += " calib" if (self._if_calib) else ""
-        return s
-
-    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):
-        """Overloaded module function.
-
-        Adds warnings during state_dict loading.
-        A workaround is implemented for loading amax from checkpoint and only supports CUDA.
-
-        Args:
-            state_dict: A dict containing the state of the top level module
-            prefix: A string that prefixes all of this modules state in state_dict, e.g. 'model.conv1.'
-        """
-        dst_has_amax = "_amax" in self._buffers
-        src_has_amax = prefix + "_amax" in state_dict
-
-        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-
-        if not src_has_amax and dst_has_amax:
-            warnings.warn(f"{prefix[:-1]}: No amax in state_dict.")
-        elif src_has_amax and not dst_has_amax:
-            warnings.warn(
-                f"{prefix[:-1]}: No '_amax' buffer to load amax into."
-                " '_amax` will be created as WAR for now. "
-                "This behavior will change in future."
-            )
-            self.register_buffer("_amax", state_dict[prefix + "_amax"].data.to(device))
-        elif (
-            src_has_amax
-            and dst_has_amax
-            and (self._amax.shape != state_dict[prefix + "_amax"].shape)
-        ):
-            # This is a workaround to support the sharded checkpoint loading in Megatron.
-            # The sharded checkpoint has the amax in a different shape.
-            assert self._amax.numel() == state_dict[prefix + "_amax"].numel(), (
-                f" {prefix[:-1]}: amax state cannot be loaded. Expected {self._amax.shape}, got"
-                f" {state_dict[prefix + '_amax'].shape}"
-            )
-            state_dict[prefix + "_amax"] = state_dict[prefix + "_amax"].view(self._amax.shape)
-
-        dst_has_pre_quant_scale = "_pre_quant_scale" in self._buffers
-        src_has_pre_quant_scale = prefix + "_pre_quant_scale" in state_dict
-
-        if not src_has_pre_quant_scale and dst_has_pre_quant_scale:
-            warnings.warn(f"{prefix[:-1]}: No pre_quant_scale in state_dict.")
-        elif src_has_pre_quant_scale and not dst_has_pre_quant_scale:
-            warnings.warn(
-                f"{prefix[:-1]}: No '_pre_quant_scale' buffer to load pre_quant_scale into."
-                " '_pre_quant_scale` will be created as WAR for now. "
-                "This behavior will change in future."
-            )
-            self.register_buffer(
-                "_pre_quant_scale", state_dict[prefix + "_pre_quant_scale"].data.to(device)
-            )
-
-        super(TensorQuantizer, self)._load_from_state_dict(state_dict, prefix, *args, **kwargs)
-
-    def _get_skip_properties_for_modelopt_state(self):
-        return {"clip", "_calibrator", "_original_shape", "_block_reshape_size", "_padding"}
-
-    def _get_properties_for_modelopt_state(self):
-        return (
-            self.__dict__.keys()
-            - nn.Module().__dict__.keys()
-            - self._get_skip_properties_for_modelopt_state()
-        )
-
-    def get_modelopt_state(self) -> Dict[str, Any]:
-        """Get meta state to be saved in checkpoint."""
-        modelopt_state = {}
-        for k in self._get_properties_for_modelopt_state():
-            modelopt_state[k] = getattr(self, k)
-
-        if hasattr(self, "_amax"):
-            modelopt_state["_has_amax"] = True
-
-        if hasattr(self, "_pre_quant_scale"):
-            modelopt_state["_has_pre_quant_scale"] = True
-
-        if hasattr(self, "clip"):
-            modelopt_state["_init_clip"] = True
-
-        return modelopt_state
-
-    def set_from_modelopt_state(self, modelopt_state, prefix=""):
-        """Set meta state from checkpoint."""
-        # Set all properties except the skip properties; this is done for backward compatibility
-        for key in modelopt_state.keys() - self._get_skip_properties_for_modelopt_state():
-            setattr(self, key, modelopt_state[key])
-
-        if "_init_clip" in modelopt_state:
-            # clip min and max parameters will be loaded from checkpoint
-            self.clip = Clip(-1.0, 1.0, learn_min=True, learn_max=True)
-
-        # Create a temporary variable to indicate if the quantizer had amax in the checkpoint
-        self._has_amax = modelopt_state.get("_has_amax", "_amax" in modelopt_state)
-
-        # Create a temporary variable to indicate if the quantizer had pre_quant_scale in the checkpoint
-        self._has_pre_quant_scale = modelopt_state.get(
-            "_has_pre_quant_scale", "_pre_quant_scale" in modelopt_state
-        )
-
-    def clean_up_after_set_from_modelopt_state(self, prefix=""):
-        """Clean up temporary variables created during set_from_modelopt_state."""
-        warning_msg = (
-            f"Could not initialize the quantizer states for {prefix}. The quantizer"
-            " states after `load_state_dict` could be in the wrong device. Please move"
-            " the modules to the correct device after loading the state dict."
-        )
-
-        if hasattr(self, "_has_amax"):
-            if self._has_amax and self.amax is None:
-                warnings.warn(warning_msg, UserWarning)
-            delattr(self, "_has_amax")
-
-        if hasattr(self, "_has_pre_quant_scale"):
-            if self._has_pre_quant_scale and self.pre_quant_scale is None:
-                warnings.warn(warning_msg, UserWarning)
-            delattr(self, "_has_pre_quant_scale")
-
-    # TODO: [OMNIML-823] type specification & validation for attributes
-    def set_from_attribute_dict(self, attribute_dict: Dict[str, Any]):
-        """Set quantizer attributes from attribute_dict."""
-        if "num_bits" in attribute_dict:
-            self.num_bits = attribute_dict["num_bits"]
-        if "axis" in attribute_dict:
-            self.axis = attribute_dict["axis"]
-            if hasattr(self, "_calibrator"):
-                self._calibrator._axis = attribute_dict["axis"]
-        if "block_sizes" in attribute_dict:
-            assert (
-                not attribute_dict["block_sizes"] or attribute_dict.get("axis", None) is None
-            ), "axis must be None when block_sizes is not None."
-            if attribute_dict["block_sizes"].get("type", None) == "dynamic":
-                assert (
-                    len(
-                        QuantDescriptor.get_block_quant_axes_and_sizes(
-                            attribute_dict["block_sizes"]
-                        )
-                    )
-                    == 1
-                ), "Dynamic block quantization only supports quantization last axis."
-            self.block_sizes = attribute_dict["block_sizes"]
-        if "calibrator" in attribute_dict:
-            calib_cls, args, kwargs = standardize_constructor_args(attribute_dict["calibrator"])
-            self._calibrator = calib_cls(*args, **kwargs)
-        if "enable" in attribute_dict:
-            if attribute_dict["enable"]:
-                self.enable()
-            else:
-                self.disable()
-
-    def sync_amax_across_distributed_group(self, parallel_group: DistributedProcessGroup):
-        """Synchronize the amax across all ranks in the given group."""
-        if parallel_group.is_initialized() and self.amax is not None:
-            dist.all_reduce(self.amax, op=dist.ReduceOp.MAX, group=parallel_group.group)
-
-
-class SequentialQuantizer(nn.Sequential):
-    """A sequential container for  :class:`TensorQuantizer` modules.
-
-    This modules is used to quantize a tensor in multiple formats sequentially. It takes as input
-    :class:`TensorQuantizer` modules and containerize them similar to :class:`torch.nn.Sequential`.
-
-    Args:
-        quantizers (TensorQuantizer): :class:`TensorQuantizer` modules to be added to the container.
-
-    """
-
-    def __init__(self, *quantizers: TensorQuantizer):  # noqa: N803
-        """Initialize SequentialQuantizer module."""
-        assert not any(
-            not isinstance(q, TensorQuantizer) for q in quantizers
-        ), "All quantizers must be a TensorQuantizer."
-        super().__init__(*quantizers)
-
-    def get_modelopt_state(self) -> Dict[str, Any]:
-        """Get meta state to be saved in checkpoint."""
-        return {"num_quantizers": len(self), "is_sequential_quantizer": True}
-
-    def disable(self):
-        """Disable the quantizer modules."""
-        for quantizer in self:
-            quantizer.disable()
-
-    def set_from_attribute_dict(self, attributes: List[Dict[str, Any]]):
-        """Set the attributes of contained quantizers from a list of attribute_dicts."""
-        for attribute, quantizer in zip(attributes, self):
-            quantizer.set_from_attribute_dict(attribute)
-
-    @staticmethod
-    @contextlib.contextmanager
-    def replace_sequential_quantizer_with_single_quantizer(model, indx: int = 0):
-        """Replace instances of :class:`SequentialQuantizer` in the model with single quantizers.
-
-        The quantizer indexed by ``indx`` from the sequential quantizer is used to replace it.
-        This method is useful for individually calibrating the quantizers in a sequential quantizer.
-        """
-        for name, module in list(model.named_modules()):
-            if isinstance(module, SequentialQuantizer):
-                assert len(module) > indx
-                parent_module = model.get_submodule(name.rpartition(".")[0])
-                setattr(parent_module, "_original_" + name.rpartition(".")[-1], module)
-                setattr(parent_module, name.rpartition(".")[-1], module[indx])
-
-        yield
-
-        for name, module in list(model.named_modules()):
-            if isinstance(module, SequentialQuantizer) and "_original_" in name.rpartition(".")[-1]:
-                parent_module = model.get_submodule(name.rpartition(".")[0])
-                original_name = name.rpartition(".")[-1].replace("_original_", "")
-                setattr(parent_module, original_name, module)
-                delattr(parent_module, name.rpartition(".")[-1])
-
-    @staticmethod
-    def tensor_quantizer_iterator(quantizers):
-        """Iterator for the quantizers in the container (but yield itself if its a TensorQuantizer)."""
-        if quantizers is None:
-            return
-        if isinstance(quantizers, TensorQuantizer):
-            yield quantizers
-        elif isinstance(quantizers, SequentialQuantizer):
-            for quantizer in quantizers:
-                yield quantizer
-        else:
-            raise ValueError("Invalid quantizer type.")
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""TensorQuantizer Module."""
+import contextlib
+import math
+import warnings
+from typing import Any, Dict, List, Optional
+
+import torch
+import torch.distributed as dist
+import torch.nn.functional as F
+from packaging.version import Version
+from torch import nn
+
+from modelopt.torch.quantization.utils import is_torch_export_mode
+from modelopt.torch.utils import standardize_constructor_args
+from modelopt.torch.utils.distributed import DistributedProcessGroup
+
+from ... import calib
+from ... import utils as quant_utils
+from ...tensor_quant import (
+    QuantDescriptor,
+    fake_tensor_quant,
+    scaled_e4m3,
+    tensor_quant,
+)
+from .clip import Clip
+
+__all__ = ["TensorQuantizer", "SequentialQuantizer"]
+
+
+class TensorQuantizer(nn.Module):
+    """Tensor quantizer module.
+
+    This module uses tensor_quant or fake_tensor_quant function to quantize a tensor. And wrappers
+    variable, moving statistics we'd want when training a quantized network.
+
+    Experimental features:
+        * ``clip`` stage learns range before enabling quantization.
+        * ``calib`` stage runs calibration
+
+    Args:
+        quant_desc: An instance of :class:`QuantDescriptor <modelopt.torch.quantization.tensor_quant.QuantDescriptor>`.
+        disabled: A boolean. If True, by pass the whole module returns input. Default False.
+        if_quant: A boolean. If True, run main quantization body. Default True.
+        if_clip: A boolean. If True, clip before quantization and learn amax. Default False.
+        if_calib: A boolean. If True, run calibration. Not implemented yet. Settings of calibration
+            will probably go to :class:`QuantDescriptor <modelopt.torch.quantization.QuantDescriptor>`.
+
+    Readonly Properties:
+        - axis:
+        - fake_quant:
+        - scale:
+        - step_size:
+
+    Mutable Properties:
+        - num_bits:
+        - unsigned:
+        - amax:
+    """
+
+    def __init__(
+        self,
+        quant_desc=QuantDescriptor(),
+        disabled=False,
+        if_quant=True,
+        if_clip=False,
+        if_calib=False,
+    ):
+        """Initialize quantizer and set up required variables."""
+        super(TensorQuantizer, self).__init__()
+        # Expand quant_desc. Use quant_desc.dict would be easier, but adding one-by-one explicitly gives more control
+        self._num_bits = quant_desc.num_bits
+        self._fake_quant = quant_desc.fake_quant
+        self._axis = quant_desc.axis
+        self._block_sizes = quant_desc.block_sizes
+        self._scale_amax = quant_desc.scale_amax
+        self._learn_amax = quant_desc.learn_amax
+        self._unsigned = quant_desc.unsigned
+        self._narrow_range = quant_desc.narrow_range
+
+        self._scale = None if not quant_desc.fake_quant else 1.0
+        self._disabled = disabled
+        self._if_quant = if_quant
+        self._if_clip = False
+        self._if_calib = if_calib
+        self._enable_pre_quant_scale = True
+
+        if quant_desc.amax is not None:
+            self.register_buffer("_amax", torch.tensor(quant_desc.amax))
+
+        # Clip module consumes a lot of memory, so only create it if learn_amax is True
+        if self._learn_amax:
+            init_amax = quant_desc.amax if quant_desc.amax is not None else 1.0
+            self.clip = Clip(-init_amax, init_amax, learn_min=True, learn_max=True)
+            # It makes more sense to enable clip stage (which learns amax) if learn_amax is true
+            self.enable_clip()
+        if if_clip:
+            self.enable_clip()
+
+        if quant_desc.calib_method == "histogram":
+            calib_cls = calib.HistogramCalibrator
+        elif quant_desc.calib_method == "max":
+            calib_cls = calib.MaxCalibrator
+        else:
+            raise ValueError(f"Unknown calib_method: {quant_desc.calib_method}")
+        self._calibrator = calib_cls(
+            num_bits=self._num_bits, axis=self._axis, unsigned=self._unsigned
+        )
+
+    @property
+    def num_bits(self):
+        """Return num_bits for quantization."""
+        return self._num_bits
+
+    @num_bits.setter
+    def num_bits(self, value):
+        self._num_bits = value
+
+    @property
+    def maxbound(self):
+        """Return maxbound for quantization."""
+        if self._num_bits == (4, 3):
+            return 448.0
+        return (1 << (self._num_bits - 1 + int(self._unsigned))) - 1
+
+    @property
+    def unsigned(self):
+        """Return True if unsigned quantization is used."""
+        return self._unsigned
+
+    @unsigned.setter
+    def unsigned(self, value):
+        self._unsigned = value
+
+    @property
+    def scale(self):
+        """Return scale used for quantization."""
+        return self._scale
+
+    @property
+    def pre_quant_scale(self):
+        """Return pre_quant_scale used for smoothquant."""
+        if not hasattr(self, "_pre_quant_scale") or not self._enable_pre_quant_scale:
+            return None
+        return self._pre_quant_scale
+
+    @pre_quant_scale.setter
+    def pre_quant_scale(self, value):
+        assert value is not None, "pre_quant_scale cannot be set to None."
+        assert (
+            self._enable_pre_quant_scale
+        ), "pre_quant_scale cannot be set when forward_with_pre_quant_scale is False."
+        if not hasattr(self, "_pre_quant_scale"):
+            self.register_buffer("_pre_quant_scale", torch.tensor(value))
+        else:
+            value = torch.tensor(value, device=self._pre_quant_scale.device)
+            if self._pre_quant_scale.shape != value.shape:
+                raise RuntimeError("Changing shape when setting pre_quant_scale is not allowed.")
+            self._pre_quant_scale.data.copy_(value.data)
+
+    @property
+    def amax(self):
+        """Return amax for quantization."""
+        if not hasattr(self, "_amax"):
+            return None
+        return self._amax
+
+    @amax.setter
+    def amax(self, value):
+        assert value is not None, "amax cannot be set to None."
+
+        if not hasattr(self, "_amax"):
+            self.register_buffer("_amax", torch.tensor(value))
+        else:
+            value = torch.tensor(value, device=self._amax.device)
+            if self._amax.shape != value.shape:
+                raise RuntimeError("Changing shape when setting amax is not allowed.")
+            self._amax.data.copy_(value.data)
+
+    def reset_amax(self):
+        """Reset amax to None."""
+        if hasattr(self, "_amax"):
+            delattr(self, "_amax")
+        self._calibrator.reset()
+
+    @property
+    def step_size(self):
+        """Return step size for integer quantization."""
+        if not hasattr(self, "_amax"):
+            warnings.warn("step_size is undefined under dynamic amax mode!")
+            return None
+        assert isinstance(
+            self._num_bits, int
+        ), "Step size is not defined for non-integer quantization."
+        return self._amax / (2.0 ** (self._num_bits - 1 + int(self._unsigned)) - 1.0)
+
+    @property
+    def axis(self):
+        """Return axis for quantization."""
+        return self._axis
+
+    @axis.setter
+    def axis(self, value):
+        self._axis = value
+
+    @property
+    def block_sizes(self):
+        """Return block_sizes for quantization."""
+        return self._block_sizes
+
+    @block_sizes.setter
+    def block_sizes(self, value):
+        self._axis = None
+        self._block_sizes = value
+
+    @property
+    def fake_quant(self):
+        """Return True if fake quantization is used."""
+        return self._fake_quant
+
+    @property
+    def narrow_range(self):
+        """Return True if symmetric integer range for signed quantization is used."""
+        return self._narrow_range
+
+    @narrow_range.setter
+    def narrow_range(self, value):
+        self._narrow_range = value
+
+    @property
+    def is_enabled(self):
+        """Return true if the modules is not disabled."""
+        return not self._disabled
+
+    def disable(self):
+        """Bypass the module.
+
+        Neither of calibration, clipping and quantization will be performed if the module is disabled.
+        """
+        self._disabled = True
+
+    def enable(self):
+        """Enable the module."""
+        self._disabled = False
+
+    def disable_clip(self):
+        """Disable clip stage."""
+        self._if_clip = False
+        self.clip.clip_value_min.requires_grad = False
+        self.clip.clip_value_max.requires_grad = False
+
+    def enable_clip(self):
+        """Enable clip stage."""
+        if not self._learn_amax:
+            raise ValueError("learn_amax is False. Cannot enable clip.")
+        self.clip.clip_value_min.requires_grad = True
+        self.clip.clip_value_max.requires_grad = True
+        self._if_clip = True
+
+    def disable_calib(self):
+        """Disable calibration."""
+        self._if_calib = False
+
+    def enable_calib(self):
+        """Enable calibration."""
+        if self._calibrator is None:
+            raise ValueError("Calibrator was not created, cannot enable calibration.")
+        self._if_calib = True
+
+    def disable_quant(self):
+        """Disable quantization."""
+        self._if_quant = False
+
+    def enable_quant(self):
+        """Enable quantization."""
+        self._if_quant = True
+
+    def load_calib_amax(self, *args, **kwargs):
+        """Load amax from calibrator.
+
+        Updates the amax buffer with value computed by the calibrator, creating it if necessary.
+        ``*args`` and ``**kwargs`` are directly passed to ``compute_amax``, except ``"strict"`` in
+        ``kwargs``. Refer to ``compute_amax`` for more details.
+        """
+        strict = kwargs.pop("strict", True)
+        if getattr(self, "_calibrator", None) is None:
+            raise RuntimeError("Calibrator not created.")
+        calib_amax = self._calibrator.compute_amax(*args, **kwargs)
+        if calib_amax is None:
+            err_msg = (
+                "Calibrator returned None. This usually happens when calibrator hasn't seen any"
+                " tensor."
+            )
+            if not strict:
+                warnings.warn(err_msg)
+                warnings.warn("Set amax to NaN!")
+                calib_amax = torch.tensor(math.nan)
+            else:
+                raise RuntimeError(
+                    err_msg
+                    + " Passing 'strict=False' to `load_calib_amax()` will ignore the error."
+                )
+        if not hasattr(self, "_amax"):
+            self.register_buffer("_amax", calib_amax.data)
+        else:
+            self._amax.copy_(calib_amax)
+
+    def init_learn_amax(self):
+        """Initialize learned amax from fixed amax."""
+        if self._learn_amax is False:
+            raise RuntimeError("Called init_learn_amax with learn_amax=False.")
+
+        if self._amax.numel() != 1:
+            warnings.warn("Per channel learned amax not supported. Initializing with max(amax).")
+            init_amax = torch.max(self._amax)
+        else:
+            init_amax = self._amax
+        self.clip.clip_value_min.data.copy_(-init_amax.data)
+        self.clip.clip_value_max.data.copy_(init_amax.data)
+
+    def _get_amax(self, inputs):
+        """Get amax from buffer or compute it dynamically."""
+        if hasattr(self, "_amax"):
+            amax = self._amax
+        else:
+            if self._axis is None:
+                reduce_axis = None
+            else:
+                reduce_axis = []
+                # Swap axis to reduce
+                axis = self._axis if isinstance(self._axis, (list, tuple)) else [self._axis]
+                for i in range(inputs.dim()):
+                    if not (i in axis or (i - inputs.dim()) in axis):
+                        reduce_axis.append(i)
+            amax = quant_utils.reduce_amax(inputs, axis=reduce_axis, keepdims=True).detach()
+        if self._scale_amax is not None:
+            amax = amax.detach() * self._scale_amax
+
+        amax = amax.data
+        return amax
+
+    def _quant_forward(self, inputs):
+        """Quantized forward pass."""
+        if self._learn_amax:
+            inputs = self.clip(inputs)
+            amax = torch.max(-self.clip.clip_value_min, self.clip.clip_value_max).detach()
+        else:
+            amax = self._get_amax(inputs)
+
+        if self._fake_quant:
+            if isinstance(self._num_bits, tuple):
+                E, M = self._num_bits  # noqa: N806
+                outputs = scaled_e4m3(inputs, self._get_amax(inputs), E, M)
+            else:
+                outputs = fake_tensor_quant(
+                    inputs, amax, self._num_bits, self._unsigned, self._narrow_range
+                )
+        else:
+            assert not isinstance(
+                self._num_bits, tuple
+            ), "only fake quantization supports ExMy type quantization."
+            outputs, self._scale = tensor_quant(inputs, amax, self._num_bits, self._unsigned)
+
+        return outputs
+
+    def _check_onnx_readiness(self, inputs):
+        """Check if quantizer is ready for ONNX export."""
+        assert hasattr(self, "_amax"), (
+            "Quantizer has not been calibrated. ONNX export requires the quantizer to be"
+            " calibrated.Calibrate and load amax before exporting to ONNX."
+        )
+
+        if self._if_calib:
+            warnings.warn(
+                "Quantizer is in calibration mode. "
+                "Please complete calibration before exporting to ONNX for correct results."
+            )
+
+        amax = self._get_amax(inputs)
+
+        # We only support scalar amax for E4M3 ONNX export
+        if isinstance(self.num_bits, tuple):
+            assert amax.numel() == 1, (
+                "E4M3 supports ONNX export only for per-tensor quantization."
+                " Per-tensor quantization requires scalar amax. "
+                f"Received non-scalar amax of shape: {amax.shape}"
+            )
+
+        if self.block_sizes is not None:
+            raise Exception("Blockquant does not support ONNX export.")
+
+    def _setup_for_blockquant(self, inputs: torch.Tensor):
+        # Get reshape sizes and paddings for block-quantization
+        def get_axis_quant_params(ax):
+            ax = ax if ax in self.block_sizes else ax - inputs.dim()
+            bsize = self.block_sizes.get(ax, None)
+            padding, ax_slice = None, None
+            if bsize is not None and inputs.shape[ax] % bsize != 0:
+                padding = (bsize - (inputs.shape[ax] % bsize), 0)
+                ax_slice = slice(inputs.shape[ax])
+            return bsize, padding, ax_slice
+
+        def set_quant_params(axis, block_reshape_size, padding, slices, amax_shape=None):
+            self._axis = tuple(axis)
+            if hasattr(self, "_calibrator"):
+                self._calibrator._axis = self._axis
+            self._original_shape = inputs.shape
+            self._block_reshape_size = torch.Size(block_reshape_size)
+            if padding is not None:
+                self._padding = tuple(padding)
+                self._original_shape = F.pad(inputs, self._padding, "constant", 0).shape
+            if slices is not None:
+                self._slices = slices
+            if amax_shape:
+                self._amax_shape_for_export = amax_shape
+
+        # Reshape size have already been set
+        if hasattr(self, "_block_reshape_size"):
+            return
+
+        reshape_size, quantize_axis, paddings, slices = [], [], [], []
+
+        # special handling for block-quantization along the last axis:
+        # flatten the input for faster execution
+        if (self.block_sizes.get(inputs.dim() - 1, None) or self.block_sizes.get(-1, None)) and len(
+            QuantDescriptor.get_block_quant_axes_and_sizes(self.block_sizes)
+        ) == 1:
+            bsize, padding, ax_slice = get_axis_quant_params(inputs.dim() - 1)
+            slices = None if ax_slice is None else (*(slice(None),) * (inputs.dim() - 1), ax_slice)
+            padding = padding if not padding else tuple(reversed(padding))
+            amax_shape_for_export = (*(inputs.shape[:-1]), -1)
+            set_quant_params((0,), (-1, bsize), padding, slices, amax_shape_for_export)
+            return
+
+        for ax in range(inputs.dim()):
+            bsize, padding, ax_slice = get_axis_quant_params(ax)
+            paddings.append(padding)
+            slices.append(ax_slice)
+            if bsize is not None:
+                reshape_size.extend([math.ceil(inputs.shape[ax] / bsize), bsize])
+                quantize_axis.extend([True, False])
+            else:
+                reshape_size.append(inputs.shape[ax])
+                quantize_axis.append(True)
+
+        quant_axis = [i for i in range(len(quantize_axis)) if quantize_axis[i]]
+
+        if all(s is None for s in slices):
+            slices = None
+        else:
+            slices = [s if s else slice(None) for s in slices]
+
+        if all(p is None for p in paddings):
+            paddings = None
+        else:
+            new_paddings = []
+            for padding in paddings:
+                if not (new_paddings or padding):
+                    continue
+                new_paddings.extend(padding if padding else (0, 0))
+            paddings = tuple(reversed(new_paddings))
+
+        set_quant_params(quant_axis, reshape_size, paddings, slices)
+
+    def _process_for_blockquant(self, inputs: torch.Tensor):
+        if hasattr(self, "_padding"):
+            inputs = F.pad(inputs, self._padding, "constant", 0)
+        assert inputs.shape == self._original_shape, (
+            f"Input shape has changed from {self._original_shape} to {inputs.shape}."
+            " Block-quantization requires a fixed input shape."
+        )
+        inputs = inputs.reshape(self._block_reshape_size)
+        return inputs
+
+    def _reset_to_original_shape(self, outputs: torch.Tensor):
+        outputs = outputs.reshape(self._original_shape)
+        if hasattr(self, "_slices"):
+            outputs = outputs[self._slices]
+        return outputs
+
+    def export_amax(self) -> Optional[torch.Tensor]:
+        """Export correctly formatted/shaped amax."""
+        if self.block_sizes is not None and self.block_sizes.get("type", None) == "dynamic":
+            raise NotImplementedError(
+                "Exporting amax for dynamic block quantization is not supported."
+            )
+
+        if self.amax is None:
+            return None
+
+        if not hasattr(self, "_amax_shape_for_export"):
+            amax = self.amax
+        else:
+            amax = self.amax.reshape(self._amax_shape_for_export)
+        amax[amax == 0] = self.maxbound
+        clamp_min, clamp_max = torch.finfo(amax.dtype).tiny, torch.finfo(amax.dtype).max
+        amax = amax.clamp(min=clamp_min, max=clamp_max)
+
+        if self.block_sizes is None:
+            # tensorrt_llm assumes the scaling_factor dim >= 1 for per-tensor.
+            if self.axis is None:
+                amax = amax.unsqueeze(0)
+
+            # If single-axis quantization, squeeze amax
+            elif isinstance(self.axis, int) or (
+                isinstance(self.axis, (list, tuple)) and len(self.axis) == 1
+            ):
+                amax = amax.squeeze()
+
+        return amax
+
+    def forward(self, inputs):
+        """Apply tensor_quant function to inputs.
+
+        Args:
+            inputs: A Tensor of type float32.
+
+        Returns:
+            outputs: A Tensor of type output_dtype
+        """
+        # Early return if nothing is collected during the forward (e.g. MoE)
+        if len(inputs) == 0:
+            return inputs
+
+        # Activation scaling for smoothquant
+        if self.pre_quant_scale is not None:
+            inputs = inputs * self.pre_quant_scale
+
+        if self._disabled:
+            return inputs
+
+        # GLOBALS could break TorchDynamo for some Pytorch versions (i.e., 2.3.0)
+        if not is_torch_export_mode():
+            # GLOBALS.in_onnx_export is only available in torch >= 1.13
+            if Version(torch.__version__) >= Version("1.13"):
+                from torch.onnx._globals import GLOBALS
+
+                if GLOBALS.in_onnx_export:
+                    self._check_onnx_readiness(inputs)
+
+        if self.block_sizes is not None and not self.block_sizes.get("type", None) == "dynamic":
+            # Dynamic block quantization is handled seperately by the quantization kernels
+            self._setup_for_blockquant(inputs)
+            inputs = self._process_for_blockquant(inputs)
+
+        outputs = inputs
+
+        if self._if_calib:
+            if self._calibrator is None:
+                raise RuntimeError("Calibrator was not created.")
+            # Shape is only known when it sees the first tensor
+            self._calibrator.collect(inputs)
+
+        if self._if_clip:
+            if not self._learn_amax:
+                raise RuntimeError("Clip without learning amax is not implemented.")
+            outputs = self.clip(inputs)
+
+        if self._if_quant:
+            outputs = self._quant_forward(inputs)
+
+        if self.block_sizes is not None and not self.block_sizes.get("type", None) == "dynamic":
+            outputs = self._reset_to_original_shape(outputs)
+
+        return outputs
+
+    def _short_amax(self, fmt=".4f"):
+        """Short description of amax.
+
+        Returns:
+            'dynamic': if _amax is not registered
+            'amax': if _amax is per-tensor
+            '[min, max](size)': if _amax is per-channel
+        """
+        if not hasattr(self, "_amax"):
+            return "dynamic"
+        if self._amax is None:
+            return "None"
+        if self._amax.numel() == 1:
+            return f"{self._amax.item():{fmt}}"
+        return (
+            f"[{self._amax.min().item():{fmt}},"
+            f" {self._amax.max().item():{fmt}}]({self._amax.numel()})"
+        )
+
+    def extra_repr(self):
+        """Set the extra information about this module."""
+        if self._disabled:
+            return "disabled"
+        s = f"{'unsigned ' if self._unsigned else ''}{self._num_bits} bit"
+        s += " narrow" if (self._narrow_range) else ""
+        s += " fake" if (self._fake_quant) else ""
+        if self.block_sizes is not None:
+            s += f" block_sizes={self._block_sizes},"
+        else:
+            s += f" axis={self._axis}" if self._axis is not None else " per-tensor"
+        s += f" amax={self._short_amax()}"
+        s += f" *{self._scale_amax}" if self._scale_amax else ""
+        s += " pre_quant_scale" if self.pre_quant_scale is not None else ""
+        s += " learned" if (self._learn_amax) else ""
+        s += (
+            f" calibrator={self._calibrator.__class__.__name__}"
+            if (self._calibrator is not None)
+            else ""
+        )
+        s += " quant" if (self._if_quant) else ""
+        s += " clip" if (self._if_clip) else ""
+        s += " calib" if (self._if_calib) else ""
+        return s
+
+    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):
+        """Overloaded module function.
+
+        Adds warnings during state_dict loading.
+        A workaround is implemented for loading amax from checkpoint and only supports CUDA.
+
+        Args:
+            state_dict: A dict containing the state of the top level module
+            prefix: A string that prefixes all of this modules state in state_dict, e.g. 'model.conv1.'
+        """
+        dst_has_amax = "_amax" in self._buffers
+        src_has_amax = prefix + "_amax" in state_dict
+
+        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+
+        if not src_has_amax and dst_has_amax:
+            warnings.warn(f"{prefix[:-1]}: No amax in state_dict.")
+        elif src_has_amax and not dst_has_amax:
+            warnings.warn(
+                f"{prefix[:-1]}: No '_amax' buffer to load amax into."
+                " '_amax` will be created as WAR for now. "
+                "This behavior will change in future."
+            )
+            self.register_buffer("_amax", state_dict[prefix + "_amax"].data.to(device))
+        elif (
+            src_has_amax
+            and dst_has_amax
+            and (self._amax.shape != state_dict[prefix + "_amax"].shape)
+        ):
+            # This is a workaround to support the sharded checkpoint loading in Megatron.
+            # The sharded checkpoint has the amax in a different shape.
+            assert self._amax.numel() == state_dict[prefix + "_amax"].numel(), (
+                f" {prefix[:-1]}: amax state cannot be loaded. Expected {self._amax.shape}, got"
+                f" {state_dict[prefix + '_amax'].shape}"
+            )
+            state_dict[prefix + "_amax"] = state_dict[prefix + "_amax"].view(self._amax.shape)
+
+        dst_has_pre_quant_scale = "_pre_quant_scale" in self._buffers
+        src_has_pre_quant_scale = prefix + "_pre_quant_scale" in state_dict
+
+        if not src_has_pre_quant_scale and dst_has_pre_quant_scale:
+            warnings.warn(f"{prefix[:-1]}: No pre_quant_scale in state_dict.")
+        elif src_has_pre_quant_scale and not dst_has_pre_quant_scale:
+            warnings.warn(
+                f"{prefix[:-1]}: No '_pre_quant_scale' buffer to load pre_quant_scale into."
+                " '_pre_quant_scale` will be created as WAR for now. "
+                "This behavior will change in future."
+            )
+            self.register_buffer(
+                "_pre_quant_scale", state_dict[prefix + "_pre_quant_scale"].data.to(device)
+            )
+
+        super(TensorQuantizer, self)._load_from_state_dict(state_dict, prefix, *args, **kwargs)
+
+    def _get_skip_properties_for_modelopt_state(self):
+        return {"clip", "_calibrator", "_original_shape", "_block_reshape_size", "_padding"}
+
+    def _get_properties_for_modelopt_state(self):
+        return (
+            self.__dict__.keys()
+            - nn.Module().__dict__.keys()
+            - self._get_skip_properties_for_modelopt_state()
+        )
+
+    def get_modelopt_state(self) -> Dict[str, Any]:
+        """Get meta state to be saved in checkpoint."""
+        modelopt_state = {}
+        for k in self._get_properties_for_modelopt_state():
+            modelopt_state[k] = getattr(self, k)
+
+        if hasattr(self, "_amax"):
+            modelopt_state["_has_amax"] = True
+
+        if hasattr(self, "_pre_quant_scale"):
+            modelopt_state["_has_pre_quant_scale"] = True
+
+        if hasattr(self, "clip"):
+            modelopt_state["_init_clip"] = True
+
+        return modelopt_state
+
+    def set_from_modelopt_state(self, modelopt_state, prefix=""):
+        """Set meta state from checkpoint."""
+        # Set all properties except the skip properties; this is done for backward compatibility
+        for key in modelopt_state.keys() - self._get_skip_properties_for_modelopt_state():
+            setattr(self, key, modelopt_state[key])
+
+        if "_init_clip" in modelopt_state:
+            # clip min and max parameters will be loaded from checkpoint
+            self.clip = Clip(-1.0, 1.0, learn_min=True, learn_max=True)
+
+        # Create a temporary variable to indicate if the quantizer had amax in the checkpoint
+        self._has_amax = modelopt_state.get("_has_amax", "_amax" in modelopt_state)
+
+        # Create a temporary variable to indicate if the quantizer had pre_quant_scale in the checkpoint
+        self._has_pre_quant_scale = modelopt_state.get(
+            "_has_pre_quant_scale", "_pre_quant_scale" in modelopt_state
+        )
+
+    def clean_up_after_set_from_modelopt_state(self, prefix=""):
+        """Clean up temporary variables created during set_from_modelopt_state."""
+        warning_msg = (
+            f"Could not initialize the quantizer states for {prefix}. The quantizer"
+            " states after `load_state_dict` could be in the wrong device. Please move"
+            " the modules to the correct device after loading the state dict."
+        )
+
+        if hasattr(self, "_has_amax"):
+            if self._has_amax and self.amax is None:
+                warnings.warn(warning_msg, UserWarning)
+            delattr(self, "_has_amax")
+
+        if hasattr(self, "_has_pre_quant_scale"):
+            if self._has_pre_quant_scale and self.pre_quant_scale is None:
+                warnings.warn(warning_msg, UserWarning)
+            delattr(self, "_has_pre_quant_scale")
+
+    # TODO: [OMNIML-823] type specification & validation for attributes
+    def set_from_attribute_dict(self, attribute_dict: Dict[str, Any]):
+        """Set quantizer attributes from attribute_dict."""
+        if "num_bits" in attribute_dict:
+            self.num_bits = attribute_dict["num_bits"]
+        if "axis" in attribute_dict:
+            self.axis = attribute_dict["axis"]
+            if hasattr(self, "_calibrator"):
+                self._calibrator._axis = attribute_dict["axis"]
+        if "block_sizes" in attribute_dict:
+            assert (
+                not attribute_dict["block_sizes"] or attribute_dict.get("axis", None) is None
+            ), "axis must be None when block_sizes is not None."
+            if attribute_dict["block_sizes"].get("type", None) == "dynamic":
+                assert (
+                    len(
+                        QuantDescriptor.get_block_quant_axes_and_sizes(
+                            attribute_dict["block_sizes"]
+                        )
+                    )
+                    == 1
+                ), "Dynamic block quantization only supports quantization last axis."
+            self.block_sizes = attribute_dict["block_sizes"]
+        if "calibrator" in attribute_dict:
+            calib_cls, args, kwargs = standardize_constructor_args(attribute_dict["calibrator"])
+            self._calibrator = calib_cls(*args, **kwargs)
+        if "enable" in attribute_dict:
+            if attribute_dict["enable"]:
+                self.enable()
+            else:
+                self.disable()
+
+    def sync_amax_across_distributed_group(self, parallel_group: DistributedProcessGroup):
+        """Synchronize the amax across all ranks in the given group."""
+        if parallel_group.is_initialized() and self.amax is not None:
+            dist.all_reduce(self.amax, op=dist.ReduceOp.MAX, group=parallel_group.group)
+
+
+class SequentialQuantizer(nn.Sequential):
+    """A sequential container for  :class:`TensorQuantizer` modules.
+
+    This modules is used to quantize a tensor in multiple formats sequentially. It takes as input
+    :class:`TensorQuantizer` modules and containerize them similar to :class:`torch.nn.Sequential`.
+
+    Args:
+        quantizers (TensorQuantizer): :class:`TensorQuantizer` modules to be added to the container.
+
+    """
+
+    def __init__(self, *quantizers: TensorQuantizer):  # noqa: N803
+        """Initialize SequentialQuantizer module."""
+        assert not any(
+            not isinstance(q, TensorQuantizer) for q in quantizers
+        ), "All quantizers must be a TensorQuantizer."
+        super().__init__(*quantizers)
+
+    def get_modelopt_state(self) -> Dict[str, Any]:
+        """Get meta state to be saved in checkpoint."""
+        return {"num_quantizers": len(self), "is_sequential_quantizer": True}
+
+    def disable(self):
+        """Disable the quantizer modules."""
+        for quantizer in self:
+            quantizer.disable()
+
+    def set_from_attribute_dict(self, attributes: List[Dict[str, Any]]):
+        """Set the attributes of contained quantizers from a list of attribute_dicts."""
+        for attribute, quantizer in zip(attributes, self):
+            quantizer.set_from_attribute_dict(attribute)
+
+    @staticmethod
+    @contextlib.contextmanager
+    def replace_sequential_quantizer_with_single_quantizer(model, indx: int = 0):
+        """Replace instances of :class:`SequentialQuantizer` in the model with single quantizers.
+
+        The quantizer indexed by ``indx`` from the sequential quantizer is used to replace it.
+        This method is useful for individually calibrating the quantizers in a sequential quantizer.
+        """
+        for name, module in list(model.named_modules()):
+            if isinstance(module, SequentialQuantizer):
+                assert len(module) > indx
+                parent_module = model.get_submodule(name.rpartition(".")[0])
+                setattr(parent_module, "_original_" + name.rpartition(".")[-1], module)
+                setattr(parent_module, name.rpartition(".")[-1], module[indx])
+
+        yield
+
+        for name, module in list(model.named_modules()):
+            if isinstance(module, SequentialQuantizer) and "_original_" in name.rpartition(".")[-1]:
+                parent_module = model.get_submodule(name.rpartition(".")[0])
+                original_name = name.rpartition(".")[-1].replace("_original_", "")
+                setattr(parent_module, original_name, module)
+                delattr(parent_module, name.rpartition(".")[-1])
+
+    @staticmethod
+    def tensor_quantizer_iterator(quantizers):
+        """Iterator for the quantizers in the container (but yield itself if its a TensorQuantizer)."""
+        if quantizers is None:
+            return
+        if isinstance(quantizers, TensorQuantizer):
+            yield quantizers
+        elif isinstance(quantizers, SequentialQuantizer):
+            for quantizer in quantizers:
+                yield quantizer
+        else:
+            raise ValueError("Invalid quantizer type.")
```

## modelopt/torch/quantization/plugins/__init__.py

```diff
@@ -1,57 +1,59 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Handles quantization plugins to correctly quantize third-party modules.
-
-Please check out the source code of this module for examples of how plugins work and how you can
-write your own one. Currently, we support plugins for
-
-- :meth:`apex<modelopt.torch.quantization.plugins.apex>`
-- :meth:`huggingface<modelopt.torch.quantization.plugins.huggingface>`
-- :meth:`megatron<modelopt.torch.quantization.plugins.megatron>`
-- :meth:`nemo<modelopt.torch.quantization.plugins.nemo>`
-"""
-import warnings
-
-try:
-    from .huggingface import *
-except ImportError:
-    pass
-except Exception as e:
-    warnings.warn(f"Failed to import huggingface plugin due to: {repr(e)}")
-
-
-try:
-    from .diffusers import *
-except ImportError:
-    pass
-except Exception as e:
-    warnings.warn(f"Failed to import diffusers plugin due to: {repr(e)}")
-
-try:
-    from .apex import *
-except ImportError:
-    pass
-except Exception as e:
-    warnings.warn(f"Failed to import apex plugin due to: {repr(e)}")
-
-try:
-    from .nemo import *
-except ImportError:
-    pass
-except Exception as e:
-    warnings.warn(f"Failed to import nemo plugin due to: {repr(e)}")
-
-try:
-    from .megatron import *
-except ImportError:
-    pass
-except Exception as e:
-    warnings.warn(f"Failed to import megatron plugin due to: {repr(e)}")
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Handles quantization plugins to correctly quantize third-party modules.
+
+Please check out the source code of this module for examples of how plugins work and how you can
+write your own one. Currently, we support plugins for
+
+- :meth:`apex<modelopt.torch.quantization.plugins.apex>`
+- :meth:`diffusers<modelopt.torch.quantization.plugins.diffusers>`
+- :meth:`huggingface<modelopt.torch.quantization.plugins.huggingface>`
+- :meth:`megatron<modelopt.torch.quantization.plugins.megatron>`
+- :meth:`nemo<modelopt.torch.quantization.plugins.nemo>`
+"""
+import warnings
+
+try:
+    from .apex import *
+except ImportError:
+    pass
+except Exception as e:
+    warnings.warn(f"Failed to import apex plugin due to: {repr(e)}")
+
+
+try:
+    from .diffusers import *
+except ImportError:
+    pass
+except Exception as e:
+    warnings.warn(f"Failed to import diffusers plugin due to: {repr(e)}")
+
+try:
+    from .huggingface import *
+except ImportError:
+    pass
+except Exception as e:
+    warnings.warn(f"Failed to import huggingface plugin due to: {repr(e)}")
+
+
+try:
+    from .megatron import *
+except ImportError:
+    pass
+except Exception as e:
+    warnings.warn(f"Failed to import megatron plugin due to: {repr(e)}")
+
+try:
+    from .nemo import *
+except ImportError:
+    pass
+except Exception as e:
+    warnings.warn(f"Failed to import nemo plugin due to: {repr(e)}")
```

## modelopt/torch/quantization/plugins/apex.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Support quantization for apex linear layers."""
-
-
-from functools import partial
-
-import apex.transformer.tensor_parallel.layers as apex_parallel
-
-from modelopt.torch.opt.dynamic import DynamicModule
-from modelopt.torch.quantization.nn.modules.quant_linear import _QuantLinear
-
-from ..nn import QuantModuleRegistry
-from .custom import _ParallelLinear
-
-
-@QuantModuleRegistry.register(
-    {
-        apex_parallel.ColumnParallelLinear: "apex_ColumnParallelLinear",
-        apex_parallel.RowParallelLinear: "apex_RowParallelLinear",
-    }
-)
-class _ApexParallelLinear(DynamicModule):
-    def _setup(self):
-        quantized_linear_fn = partial(
-            _QuantLinear.quantized_linear_fn,
-            apex_parallel,
-            "linear_with_grad_accumulation_and_async_allreduce",
-            self,
-        )
-        self._forward_impl = quantized_linear_fn
-        _ParallelLinear._setup(self)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Support quantization for apex linear layers."""
+
+
+from functools import partial
+
+import apex.transformer.tensor_parallel.layers as apex_parallel
+
+from modelopt.torch.opt.dynamic import DynamicModule
+from modelopt.torch.quantization.nn.modules.quant_linear import _QuantLinear
+
+from ..nn import QuantModuleRegistry
+from .custom import _ParallelLinear
+
+
+@QuantModuleRegistry.register(
+    {
+        apex_parallel.ColumnParallelLinear: "apex_ColumnParallelLinear",
+        apex_parallel.RowParallelLinear: "apex_RowParallelLinear",
+    }
+)
+class _ApexParallelLinear(DynamicModule):
+    def _setup(self):
+        quantized_linear_fn = partial(
+            _QuantLinear.quantized_linear_fn,
+            apex_parallel,
+            "linear_with_grad_accumulation_and_async_allreduce",
+            self,
+        )
+        self._forward_impl = quantized_linear_fn
+        _ParallelLinear._setup(self)
```

## modelopt/torch/quantization/plugins/custom.py

 * *Ordering differences only*

```diff
@@ -1,102 +1,102 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Custom plugin base modules and utilities for quantization."""
-
-from contextlib import ExitStack, contextmanager
-from functools import partial
-from types import ModuleType
-from typing import Callable, Iterator, List, Tuple
-
-from modelopt.torch.opt.dynamic import DynamicModule
-from modelopt.torch.quantization.nn import TensorQuantizer
-from modelopt.torch.quantization.nn.modules.quant_linear import _QuantLinear
-
-from ..utils import replace_function
-
-try:
-    from .huggingface import register_falcon_linears_on_the_fly
-except ImportError:
-
-    def _dummy_register(model):
-        pass
-
-    register_falcon_linears_on_the_fly = _dummy_register
-
-
-# TODO: This is a temporary solution
-# In future implement a decorator to register methods updating QUANT_MODULE on the fly
-def register_custom_model_plugins_on_the_fly(model):
-    """Registers custom modules as QUANT_MODULE on the fly."""
-    register_falcon_linears_on_the_fly(model)
-
-
-@contextmanager
-def _multi_context(*cms):
-    """Context manager enabling variable number of context managers."""
-    with ExitStack() as stack:
-        yield [stack.enter_context(cls) for cls in cms]
-
-
-class _QuantFunctionalMixin(DynamicModule):
-    """Mixin class for quantized functionals.
-
-    Often we need to replace a functional with a quantized version. This class provides a way to do that.
-    """
-
-    # List of functionals to replace with quantized versions, e.g. [(package, func_name, quantized_func), ...]
-    _functionals_to_replace: List[Tuple[ModuleType, str, Callable]] = []
-
-    @property
-    def functionals_to_replace(self) -> Iterator[Tuple[ModuleType, str, Callable]]:
-        return (
-            (package, func_name, quantized_func)
-            for package, func_name, quantized_func in self._functionals_to_replace
-            if hasattr(package, func_name)
-        )
-
-    def forward(self, *args, **kwargs):
-        with _multi_context(
-            *(
-                replace_function(package, func_name, quantized_func)
-                for package, func_name, quantized_func in self.functionals_to_replace
-            )
-        ):
-            return super().forward(*args, **kwargs)
-
-
-class _ParallelLinear(_QuantFunctionalMixin):
-    """Quantized base class for ParallelLinear type classes.
-
-    For this type of modules, we need to quantize the inputs and weights just before calling the actual linear
-    functional. This is accomplished by replacing the linear functional with a custom one that quantizes the inputs
-    and weights before calling the original functional.
-    """
-
-    # List of functionals to replace [(package, func_name), ...]
-    _functionals_to_replace: List[Tuple[ModuleType, str]] = []
-
-    @property
-    def functionals_to_replace(self) -> Iterator[Tuple[ModuleType, str, Callable]]:
-        for package, func_name in self._functionals_to_replace:
-            if not hasattr(package, func_name):
-                continue
-            quantized_func = partial(
-                _QuantLinear.quantized_linear_fn, package, "_" + func_name, self
-            )
-            if hasattr(getattr(package, func_name), "__dict__"):
-                quantized_func.__dict__.update(getattr(package, func_name).__dict__)
-            yield package, func_name, quantized_func
-
-    def _setup(self):
-        self.input_quantizer = TensorQuantizer(_QuantLinear.default_quant_desc_input)
-        self.weight_quantizer = TensorQuantizer(_QuantLinear.default_quant_desc_weight)
-        self.output_quantizer = TensorQuantizer(_QuantLinear.default_quant_desc_output)
-        self.output_quantizer.disable()
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Custom plugin base modules and utilities for quantization."""
+
+from contextlib import ExitStack, contextmanager
+from functools import partial
+from types import ModuleType
+from typing import Callable, Iterator, List, Tuple
+
+from modelopt.torch.opt.dynamic import DynamicModule
+from modelopt.torch.quantization.nn import TensorQuantizer
+from modelopt.torch.quantization.nn.modules.quant_linear import _QuantLinear
+
+from ..utils import replace_function
+
+try:
+    from .huggingface import register_falcon_linears_on_the_fly
+except ImportError:
+
+    def _dummy_register(model):
+        pass
+
+    register_falcon_linears_on_the_fly = _dummy_register
+
+
+# TODO: This is a temporary solution
+# In future implement a decorator to register methods updating QUANT_MODULE on the fly
+def register_custom_model_plugins_on_the_fly(model):
+    """Registers custom modules as QUANT_MODULE on the fly."""
+    register_falcon_linears_on_the_fly(model)
+
+
+@contextmanager
+def _multi_context(*cms):
+    """Context manager enabling variable number of context managers."""
+    with ExitStack() as stack:
+        yield [stack.enter_context(cls) for cls in cms]
+
+
+class _QuantFunctionalMixin(DynamicModule):
+    """Mixin class for quantized functionals.
+
+    Often we need to replace a functional with a quantized version. This class provides a way to do that.
+    """
+
+    # List of functionals to replace with quantized versions, e.g. [(package, func_name, quantized_func), ...]
+    _functionals_to_replace: List[Tuple[ModuleType, str, Callable]] = []
+
+    @property
+    def functionals_to_replace(self) -> Iterator[Tuple[ModuleType, str, Callable]]:
+        return (
+            (package, func_name, quantized_func)
+            for package, func_name, quantized_func in self._functionals_to_replace
+            if hasattr(package, func_name)
+        )
+
+    def forward(self, *args, **kwargs):
+        with _multi_context(
+            *(
+                replace_function(package, func_name, quantized_func)
+                for package, func_name, quantized_func in self.functionals_to_replace
+            )
+        ):
+            return super().forward(*args, **kwargs)
+
+
+class _ParallelLinear(_QuantFunctionalMixin):
+    """Quantized base class for ParallelLinear type classes.
+
+    For this type of modules, we need to quantize the inputs and weights just before calling the actual linear
+    functional. This is accomplished by replacing the linear functional with a custom one that quantizes the inputs
+    and weights before calling the original functional.
+    """
+
+    # List of functionals to replace [(package, func_name), ...]
+    _functionals_to_replace: List[Tuple[ModuleType, str]] = []
+
+    @property
+    def functionals_to_replace(self) -> Iterator[Tuple[ModuleType, str, Callable]]:
+        for package, func_name in self._functionals_to_replace:
+            if not hasattr(package, func_name):
+                continue
+            quantized_func = partial(
+                _QuantLinear.quantized_linear_fn, package, "_" + func_name, self
+            )
+            if hasattr(getattr(package, func_name), "__dict__"):
+                quantized_func.__dict__.update(getattr(package, func_name).__dict__)
+            yield package, func_name, quantized_func
+
+    def _setup(self):
+        self.input_quantizer = TensorQuantizer(_QuantLinear.default_quant_desc_input)
+        self.weight_quantizer = TensorQuantizer(_QuantLinear.default_quant_desc_weight)
+        self.output_quantizer = TensorQuantizer(_QuantLinear.default_quant_desc_output)
+        self.output_quantizer.disable()
```

## modelopt/torch/quantization/plugins/diffusers.py

 * *Ordering differences only*

```diff
@@ -1,86 +1,86 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-"""Support quantization of diffusers layers."""
-from functools import partial
-from types import ModuleType
-from typing import Callable, Iterator, Tuple
-
-import torch
-from diffusers.models.attention_processor import Attention
-from diffusers.models.lora import LoRACompatibleConv, LoRACompatibleLinear
-
-from modelopt.torch.quantization.nn import (
-    QuantConv2d,
-    QuantInputBase,
-    QuantLinear,
-    QuantLinearConvBase,
-    QuantModuleRegistry,
-    TensorQuantizer,
-)
-
-from .custom import _QuantFunctionalMixin
-
-
-class _QuantLoRACompatibleLinearConvBase(QuantLinearConvBase):
-    def _setup(self):
-        assert self.lora_layer is None, (
-            f"To quantize {self}, lora_layer should be None. Please fuse the LoRA layer before"
-            " quantization."
-        )
-        return super()._setup()
-
-
-@QuantModuleRegistry.register({LoRACompatibleConv: "LoRACompatibleConv"})
-class _QuantLoRACompatibleConv(_QuantLoRACompatibleLinearConvBase):
-    default_quant_desc_weight = QuantConv2d.default_quant_desc_weight
-
-
-@QuantModuleRegistry.register({LoRACompatibleLinear: "LoRACompatibleLinear"})
-class _QuantLoRACompatibleLinear(_QuantLoRACompatibleLinearConvBase):
-    default_quant_desc_weight = QuantLinear.default_quant_desc_weight
-
-
-def _quantized_bmm(self, input, mat2, *args, **kwargs):
-    attn, v = input, mat2
-    return self.bmm2_output_quantizer(
-        torch._bmm(self.softmax_quantizer(attn), self.v_bmm_quantizer(v), *args, **kwargs)
-    )
-
-
-def _quantized_baddbmm(self, input, batch1, batch2, *args, **kwargs):
-    q, k = batch1, batch2
-    return torch._baddbmm(input, self.q_bmm_quantizer(q), self.k_bmm_quantizer(k), *args, **kwargs)
-
-
-class _QuantAttention(_QuantFunctionalMixin):
-    """FP8 processor for performing attention-related computations."""
-
-    _functionals_to_replace = [
-        (torch, "bmm", _quantized_bmm),
-        (torch, "baddbmm", _quantized_baddbmm),
-    ]
-
-    @property
-    def functionals_to_replace(self) -> Iterator[Tuple[ModuleType, str, Callable]]:
-        for package, func_name, quantized_func in self._functionals_to_replace:
-            if not hasattr(package, func_name):
-                continue
-            quantized_func = partial(quantized_func, self)
-            yield package, func_name, quantized_func
-
-    def _setup(self):
-        self.q_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
-        self.k_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
-        self.v_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
-        self.softmax_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
-        self.bmm2_output_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
-
-
-QuantModuleRegistry.register({Attention: "Attention"})(_QuantAttention)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+"""Support quantization of diffusers layers."""
+from functools import partial
+from types import ModuleType
+from typing import Callable, Iterator, Tuple
+
+import torch
+from diffusers.models.attention_processor import Attention
+from diffusers.models.lora import LoRACompatibleConv, LoRACompatibleLinear
+
+from modelopt.torch.quantization.nn import (
+    QuantConv2d,
+    QuantInputBase,
+    QuantLinear,
+    QuantLinearConvBase,
+    QuantModuleRegistry,
+    TensorQuantizer,
+)
+
+from .custom import _QuantFunctionalMixin
+
+
+class _QuantLoRACompatibleLinearConvBase(QuantLinearConvBase):
+    def _setup(self):
+        assert self.lora_layer is None, (
+            f"To quantize {self}, lora_layer should be None. Please fuse the LoRA layer before"
+            " quantization."
+        )
+        return super()._setup()
+
+
+@QuantModuleRegistry.register({LoRACompatibleConv: "LoRACompatibleConv"})
+class _QuantLoRACompatibleConv(_QuantLoRACompatibleLinearConvBase):
+    default_quant_desc_weight = QuantConv2d.default_quant_desc_weight
+
+
+@QuantModuleRegistry.register({LoRACompatibleLinear: "LoRACompatibleLinear"})
+class _QuantLoRACompatibleLinear(_QuantLoRACompatibleLinearConvBase):
+    default_quant_desc_weight = QuantLinear.default_quant_desc_weight
+
+
+def _quantized_bmm(self, input, mat2, *args, **kwargs):
+    attn, v = input, mat2
+    return self.bmm2_output_quantizer(
+        torch._bmm(self.softmax_quantizer(attn), self.v_bmm_quantizer(v), *args, **kwargs)
+    )
+
+
+def _quantized_baddbmm(self, input, batch1, batch2, *args, **kwargs):
+    q, k = batch1, batch2
+    return torch._baddbmm(input, self.q_bmm_quantizer(q), self.k_bmm_quantizer(k), *args, **kwargs)
+
+
+class _QuantAttention(_QuantFunctionalMixin):
+    """FP8 processor for performing attention-related computations."""
+
+    _functionals_to_replace = [
+        (torch, "bmm", _quantized_bmm),
+        (torch, "baddbmm", _quantized_baddbmm),
+    ]
+
+    @property
+    def functionals_to_replace(self) -> Iterator[Tuple[ModuleType, str, Callable]]:
+        for package, func_name, quantized_func in self._functionals_to_replace:
+            if not hasattr(package, func_name):
+                continue
+            quantized_func = partial(quantized_func, self)
+            yield package, func_name, quantized_func
+
+    def _setup(self):
+        self.q_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
+        self.k_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
+        self.v_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
+        self.softmax_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
+        self.bmm2_output_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
+
+
+QuantModuleRegistry.register({Attention: "Attention"})(_QuantAttention)
```

## modelopt/torch/quantization/plugins/huggingface.py

 * *Ordering differences only*

```diff
@@ -1,56 +1,56 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Support quantization for huggingface layers."""
-import torch
-import torch.nn as nn
-import transformers
-
-from modelopt.torch.opt.dynamic import DynamicModule
-
-from ..nn import QuantModuleRegistry
-from ..nn.modules.quant_linear import _QuantLinear
-
-__all__ = []
-
-
-# transformers.modeling_utils.Conv1D used in HF-GPT2 is not a real Conv1D
-# It is actually a Linear layer where weight is transposed and torch.addmm is used
-@QuantModuleRegistry.register({transformers.modeling_utils.Conv1D: "Conv1D"})
-class _QuantConv1D(_QuantLinear):
-    @classmethod
-    @torch.no_grad()
-    def convert(cls, module: nn.Module) -> "_QuantConv1D":
-        module.weight = nn.Parameter(module.weight.T)
-        module.out_features, module.in_features = module.weight.shape
-        # We want the forward method of nn.Linear to be called instead of the forward method of Conv1D
-        dyn_cls: DynamicModule = QuantModuleRegistry.get(nn.Linear)
-        return dyn_cls.convert(module)
-
-
-if hasattr(transformers.models, "falcon") and hasattr(
-    transformers.models.falcon.modeling_falcon, "FalconLinear"
-):
-    QuantModuleRegistry.register(
-        {transformers.models.falcon.modeling_falcon.FalconLinear: "FalconLinear"}
-    )(_QuantLinear)
-
-
-def register_falcon_linears_on_the_fly(model):
-    """Register Falcon linear modules as a QUANT_MODULE.
-
-    Certain falcon models (for example, falcon 40b) use remote code, which are loaded dynamically, to build their model.
-    Therefore, we need to register the linear on the fly before quantization.
-    """
-    if type(model).__name__ in ["RWForCausalLM", "FalconForCausalLM"]:
-        linear_type = type(model.transformer.h[0].self_attention.dense)
-        # Create a QuantFalconLinear class on the fly
-        if QuantModuleRegistry.get(linear_type) is None:
-            QuantModuleRegistry.register({linear_type: linear_type.__name__})(_QuantLinear)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Support quantization for huggingface layers."""
+import torch
+import torch.nn as nn
+import transformers
+
+from modelopt.torch.opt.dynamic import DynamicModule
+
+from ..nn import QuantModuleRegistry
+from ..nn.modules.quant_linear import _QuantLinear
+
+__all__ = []
+
+
+# transformers.modeling_utils.Conv1D used in HF-GPT2 is not a real Conv1D
+# It is actually a Linear layer where weight is transposed and torch.addmm is used
+@QuantModuleRegistry.register({transformers.modeling_utils.Conv1D: "Conv1D"})
+class _QuantConv1D(_QuantLinear):
+    @classmethod
+    @torch.no_grad()
+    def convert(cls, module: nn.Module) -> "_QuantConv1D":
+        module.weight = nn.Parameter(module.weight.T)
+        module.out_features, module.in_features = module.weight.shape
+        # We want the forward method of nn.Linear to be called instead of the forward method of Conv1D
+        dyn_cls: DynamicModule = QuantModuleRegistry.get(nn.Linear)
+        return dyn_cls.convert(module)
+
+
+if hasattr(transformers.models, "falcon") and hasattr(
+    transformers.models.falcon.modeling_falcon, "FalconLinear"
+):
+    QuantModuleRegistry.register(
+        {transformers.models.falcon.modeling_falcon.FalconLinear: "FalconLinear"}
+    )(_QuantLinear)
+
+
+def register_falcon_linears_on_the_fly(model):
+    """Register Falcon linear modules as a QUANT_MODULE.
+
+    Certain falcon models (for example, falcon 40b) use remote code, which are loaded dynamically, to build their model.
+    Therefore, we need to register the linear on the fly before quantization.
+    """
+    if type(model).__name__ in ["RWForCausalLM", "FalconForCausalLM"]:
+        linear_type = type(model.transformer.h[0].self_attention.dense)
+        # Create a QuantFalconLinear class on the fly
+        if QuantModuleRegistry.get(linear_type) is None:
+            QuantModuleRegistry.register({linear_type: linear_type.__name__})(_QuantLinear)
```

## modelopt/torch/quantization/plugins/megatron.py

 * *Ordering differences only*

```diff
@@ -1,99 +1,99 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Support quantization for megatron linear layers."""
-
-
-import megatron.core.tensor_parallel.layers as megatron_parallel
-from megatron.core.transformer.utils import make_sharded_tensors_for_checkpoint
-
-from ..nn import QuantModuleRegistry
-from ..tensor_quant import QuantDescriptor
-from .custom import _ParallelLinear
-
-__all__ = []
-
-
-class _MegatronParallelLinear(_ParallelLinear):
-    _functionals_to_replace = [
-        (megatron_parallel, "linear_with_grad_accumulation_and_async_allreduce"),
-        (megatron_parallel, "linear_with_frozen_weight"),
-    ]
-
-    def _process_weight_quantizer_amax(self, k, v, quantizer_state_dict):
-        if v.ndim == 0:
-            quantizer_state_dict[k] = v.view(-1)
-        elif v.ndim == 2:
-            quantizer_state_dict[k] = v.view(self.weight.shape[0], -1)
-        else:
-            raise ValueError(f"Invalid weight quantizer {k} amax: {v}, {v.shape}")
-
-    def _process_activation_quantizer_amax(self, k, v, quantizer_state_dict):
-        assert v.ndim == 0, f"Invalid activation quantizer amax: {v}, {v.shape}"
-        quantizer_state_dict[k] = v.view(-1)
-
-    def _process_activation_quantizer_pre_quant_scale(self, k, v, quantizer_state_dict):
-        assert v.ndim == 1, f"Invalid activation quantizer pre_quant_scale: {v}, {v.shape}"
-        quantizer_state_dict[k] = v
-
-    def _get_shard_axis_dict(self):
-        raise NotImplementedError
-
-    def sharded_state_dict(self, prefix="", sharded_offsets=(), metadata=None):
-        sharded_state_dict = super().sharded_state_dict(prefix, sharded_offsets)
-
-        quantizer_state_dict, sharded_axis_dict = {}, self._get_shard_axis_dict()
-        for k, v in self.state_dict(prefix="", keep_vars=True).items():
-            if "weight_quantizer." in k:
-                assert k.endswith("._amax"), f"Invalid weight quantizer state: {k}"
-                self._process_weight_quantizer_amax(k, v, quantizer_state_dict)
-            elif ("input_quantizer" in k or "output_quantizer" in k) and k.endswith("._amax"):
-                self._process_activation_quantizer_amax(k, v, quantizer_state_dict)
-            elif k.endswith("input_quantizer._pre_quant_scale"):
-                self._process_activation_quantizer_pre_quant_scale(k, v, quantizer_state_dict)
-
-        sharded_state_dict.update(
-            **make_sharded_tensors_for_checkpoint(
-                quantizer_state_dict, prefix, sharded_axis_dict, sharded_offsets
-            )
-        )
-        return sharded_state_dict
-
-
-@QuantModuleRegistry.register(
-    {megatron_parallel.ColumnParallelLinear: "megatron_ColumnParallelLinear"}
-)
-class _MegatronColumnParallelLinear(_MegatronParallelLinear):
-    def _get_shard_axis_dict(self):
-        shard_axis_dict = {}
-        for k, v in self.state_dict(prefix="", keep_vars=True).items():
-            if "weight_quantizer." in k and v.ndim != 0:
-                shard_axis_dict[k] = 0
-        return shard_axis_dict
-
-
-@QuantModuleRegistry.register({megatron_parallel.RowParallelLinear: "megatron_RowParallelLinear"})
-class _MegatronRowParallelLinear(_MegatronParallelLinear):
-    def _get_shard_axis_dict(self):
-        shard_axis_dict = {}
-        for k, v in self.state_dict(prefix="", keep_vars=True).items():
-            if "weight_quantizer" in k:
-                assert "._amax" in k, f"Invalid weight quantizer state: {k}"
-                submodule_name = k.split("weight_quantizer")[-1].split("._amax")[0].split(".")[-1]
-                quantizer = self.weight_quantizer.get_submodule(submodule_name)
-                # The weights are split across dim -1; Only static block quantization requires sharding
-                if quantizer.block_sizes and quantizer.block_sizes.get("type", None) != "dynamic":
-                    assert (-1 in quantizer.block_sizes or 1 in quantizer.block_sizes) and len(
-                        QuantDescriptor.get_block_quant_axes_and_sizes(quantizer.block_sizes)
-                    ) == 1, f"Invalid block sizes: {quantizer.block_sizes}"
-                    shard_axis_dict[k] = 1
-            elif "input_quantizer._pre_quant_scale" in k:
-                shard_axis_dict[k] = 0
-        return shard_axis_dict
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Support quantization for megatron linear layers."""
+
+
+import megatron.core.tensor_parallel.layers as megatron_parallel
+from megatron.core.transformer.utils import make_sharded_tensors_for_checkpoint
+
+from ..nn import QuantModuleRegistry
+from ..tensor_quant import QuantDescriptor
+from .custom import _ParallelLinear
+
+__all__ = []
+
+
+class _MegatronParallelLinear(_ParallelLinear):
+    _functionals_to_replace = [
+        (megatron_parallel, "linear_with_grad_accumulation_and_async_allreduce"),
+        (megatron_parallel, "linear_with_frozen_weight"),
+    ]
+
+    def _process_weight_quantizer_amax(self, k, v, quantizer_state_dict):
+        if v.ndim == 0:
+            quantizer_state_dict[k] = v.view(-1)
+        elif v.ndim == 2:
+            quantizer_state_dict[k] = v.view(self.weight.shape[0], -1)
+        else:
+            raise ValueError(f"Invalid weight quantizer {k} amax: {v}, {v.shape}")
+
+    def _process_activation_quantizer_amax(self, k, v, quantizer_state_dict):
+        assert v.ndim == 0, f"Invalid activation quantizer amax: {v}, {v.shape}"
+        quantizer_state_dict[k] = v.view(-1)
+
+    def _process_activation_quantizer_pre_quant_scale(self, k, v, quantizer_state_dict):
+        assert v.ndim == 1, f"Invalid activation quantizer pre_quant_scale: {v}, {v.shape}"
+        quantizer_state_dict[k] = v
+
+    def _get_shard_axis_dict(self):
+        raise NotImplementedError
+
+    def sharded_state_dict(self, prefix="", sharded_offsets=(), metadata=None):
+        sharded_state_dict = super().sharded_state_dict(prefix, sharded_offsets)
+
+        quantizer_state_dict, sharded_axis_dict = {}, self._get_shard_axis_dict()
+        for k, v in self.state_dict(prefix="", keep_vars=True).items():
+            if "weight_quantizer." in k:
+                assert k.endswith("._amax"), f"Invalid weight quantizer state: {k}"
+                self._process_weight_quantizer_amax(k, v, quantizer_state_dict)
+            elif ("input_quantizer" in k or "output_quantizer" in k) and k.endswith("._amax"):
+                self._process_activation_quantizer_amax(k, v, quantizer_state_dict)
+            elif k.endswith("input_quantizer._pre_quant_scale"):
+                self._process_activation_quantizer_pre_quant_scale(k, v, quantizer_state_dict)
+
+        sharded_state_dict.update(
+            **make_sharded_tensors_for_checkpoint(
+                quantizer_state_dict, prefix, sharded_axis_dict, sharded_offsets
+            )
+        )
+        return sharded_state_dict
+
+
+@QuantModuleRegistry.register(
+    {megatron_parallel.ColumnParallelLinear: "megatron_ColumnParallelLinear"}
+)
+class _MegatronColumnParallelLinear(_MegatronParallelLinear):
+    def _get_shard_axis_dict(self):
+        shard_axis_dict = {}
+        for k, v in self.state_dict(prefix="", keep_vars=True).items():
+            if "weight_quantizer." in k and v.ndim != 0:
+                shard_axis_dict[k] = 0
+        return shard_axis_dict
+
+
+@QuantModuleRegistry.register({megatron_parallel.RowParallelLinear: "megatron_RowParallelLinear"})
+class _MegatronRowParallelLinear(_MegatronParallelLinear):
+    def _get_shard_axis_dict(self):
+        shard_axis_dict = {}
+        for k, v in self.state_dict(prefix="", keep_vars=True).items():
+            if "weight_quantizer" in k:
+                assert "._amax" in k, f"Invalid weight quantizer state: {k}"
+                submodule_name = k.split("weight_quantizer")[-1].split("._amax")[0].split(".")[-1]
+                quantizer = self.weight_quantizer.get_submodule(submodule_name)
+                # The weights are split across dim -1; Only static block quantization requires sharding
+                if quantizer.block_sizes and quantizer.block_sizes.get("type", None) != "dynamic":
+                    assert (-1 in quantizer.block_sizes or 1 in quantizer.block_sizes) and len(
+                        QuantDescriptor.get_block_quant_axes_and_sizes(quantizer.block_sizes)
+                    ) == 1, f"Invalid block sizes: {quantizer.block_sizes}"
+                    shard_axis_dict[k] = 1
+            elif "input_quantizer._pre_quant_scale" in k:
+                shard_axis_dict[k] = 0
+        return shard_axis_dict
```

## modelopt/torch/quantization/plugins/nemo.py

 * *Ordering differences only*

```diff
@@ -1,61 +1,61 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Customization for Nemo Megatron GPT."""
-from functools import partial
-from types import ModuleType
-from typing import Callable, Iterator, Tuple
-
-import torch
-
-# New nemo version should depend on megatron.core
-from nemo.collections.nlp.modules.common.megatron.attention import CoreAttention
-
-from ..nn import QuantInputBase, QuantModuleRegistry, TensorQuantizer
-from .custom import _QuantFunctionalMixin
-
-__all__ = []
-
-
-def _quantized_bmm(self, input, mat2, *args, **kwargs):
-    """Quantized version of BMM2 in nemo CoreAttention."""
-    attn, v = input, mat2
-    return torch._bmm(attn, self.v_bmm_quantizer(v), *args, **kwargs)
-
-
-def _quantized_baddbmm(self, input, batch1, batch2, *args, **kwargs):
-    """Quantized version of BMM1 in nemo CoreAttention."""
-    q, k = batch1, batch2
-    return torch._baddbmm(input, self.q_bmm_quantizer(q), self.k_bmm_quantizer(k), *args, **kwargs)
-
-
-class _QuantCoreAttention(_QuantFunctionalMixin):
-    """Quantized base class for CoreAttention."""
-
-    _functionals_to_replace = [
-        (torch, "bmm", _quantized_bmm),
-        (torch, "baddbmm", _quantized_baddbmm),
-    ]
-
-    @property
-    def functionals_to_replace(self) -> Iterator[Tuple[ModuleType, str, Callable]]:
-        for package, func_name, quantized_func in self._functionals_to_replace:
-            if not hasattr(package, func_name):
-                continue
-            quantized_func = partial(quantized_func, self)
-            yield package, func_name, quantized_func
-
-    def _setup(self):
-        self.q_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
-        self.k_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
-        self.v_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
-
-
-QuantModuleRegistry.register({CoreAttention: "nemo_core_attention"})(_QuantCoreAttention)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Customization for Nemo Megatron GPT."""
+from functools import partial
+from types import ModuleType
+from typing import Callable, Iterator, Tuple
+
+import torch
+
+# New nemo version should depend on megatron.core
+from nemo.collections.nlp.modules.common.megatron.attention import CoreAttention
+
+from ..nn import QuantInputBase, QuantModuleRegistry, TensorQuantizer
+from .custom import _QuantFunctionalMixin
+
+__all__ = []
+
+
+def _quantized_bmm(self, input, mat2, *args, **kwargs):
+    """Quantized version of BMM2 in nemo CoreAttention."""
+    attn, v = input, mat2
+    return torch._bmm(attn, self.v_bmm_quantizer(v), *args, **kwargs)
+
+
+def _quantized_baddbmm(self, input, batch1, batch2, *args, **kwargs):
+    """Quantized version of BMM1 in nemo CoreAttention."""
+    q, k = batch1, batch2
+    return torch._baddbmm(input, self.q_bmm_quantizer(q), self.k_bmm_quantizer(k), *args, **kwargs)
+
+
+class _QuantCoreAttention(_QuantFunctionalMixin):
+    """Quantized base class for CoreAttention."""
+
+    _functionals_to_replace = [
+        (torch, "bmm", _quantized_bmm),
+        (torch, "baddbmm", _quantized_baddbmm),
+    ]
+
+    @property
+    def functionals_to_replace(self) -> Iterator[Tuple[ModuleType, str, Callable]]:
+        for package, func_name, quantized_func in self._functionals_to_replace:
+            if not hasattr(package, func_name):
+                continue
+            quantized_func = partial(quantized_func, self)
+            yield package, func_name, quantized_func
+
+    def _setup(self):
+        self.q_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
+        self.k_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
+        self.v_bmm_quantizer = TensorQuantizer(QuantInputBase.default_quant_desc_input)
+
+
+QuantModuleRegistry.register({CoreAttention: "nemo_core_attention"})(_QuantCoreAttention)
```

## modelopt/torch/quantization/src/tensor_quant.cpp

 * *Ordering differences only*

```diff
@@ -1,57 +1,57 @@
-/*
- * SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.
- * All rights reserved. SPDX-License-Identifier: LicenseRef-NvidiaProprietary
- *
- * NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
- * property and proprietary rights in and to this material, related
- * documentation and any modifications thereto. Any use, reproduction,
- * disclosure or distribution of this material and related documentation
- * without an express license agreement from NVIDIA CORPORATION or
- * its affiliates is strictly prohibited.
- */
-
-#include <torch/extension.h>
-
-void fake_tensor_quant_cuda_inplace(at::Tensor, at::Tensor, int, bool, bool);
-at::Tensor fake_tensor_quant_cuda(at::Tensor, at::Tensor, int, bool, bool);
-at::Tensor fake_tensor_quant_with_axis_cuda(at::Tensor, at::Tensor, int, int, bool, bool);
-float bits_to_bound(int, int);
-at::Tensor fake_e4m3fy_cuda(at::Tensor inputs);
-
-void fake_tensor_quant_(at::Tensor inputs, at::Tensor amax, int num_bits = 8,
-                        bool is_unsigned = false, bool narrow_range = true) {
-  TORCH_CHECK(inputs.is_cuda());
-  TORCH_CHECK(inputs.is_contiguous()) // in-place on non-contiguous tensor is
-                                      // more difficult
-  TORCH_CHECK(amax.numel(), 1);
-  fake_tensor_quant_cuda_inplace(inputs, amax, num_bits, is_unsigned, narrow_range);
-}
-// TODO: Can we add support for CPU tensors here?
-at::Tensor fake_tensor_quant(at::Tensor inputs, at::Tensor amax, int num_bits = 8,
-                             bool is_unsigned = false, bool narrow_range = true) {
-  TORCH_CHECK(inputs.is_cuda());
-  TORCH_CHECK(amax.numel(), 1);
-  return fake_tensor_quant_cuda(inputs.contiguous(), amax.contiguous(), num_bits, is_unsigned,
-                                narrow_range);
-}
-
-at::Tensor fake_tensor_quant_with_axis(at::Tensor inputs, at::Tensor amax, int axis,
-                                       int num_bits = 8, bool is_unsigned = false,
-                                       bool narrow_range = true) {
-  TORCH_CHECK(inputs.is_cuda());
-  TORCH_CHECK(amax.numel(), inputs.size(axis));
-  return fake_tensor_quant_with_axis_cuda(inputs.contiguous(), amax.contiguous(), axis, num_bits,
-                                          is_unsigned, narrow_range);
-}
-
-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("fake_tensor_quant_", &fake_tensor_quant_, "Fake Tensor Quant Inplace", py::arg("inputs"),
-        py::arg("amax"), py::arg("num_bits") = 8, py::arg("unsigned") = false,
-        py::arg("narrow_range") = true);
-  m.def("fake_tensor_quant", &fake_tensor_quant, "Fake Tensor Quant", py::arg("inputs"),
-        py::arg("amax"), py::arg("num_bits") = 8, py::arg("unsigned") = false,
-        py::arg("narrow_range") = true);
-  m.def("fake_tensor_quant_with_axis", &fake_tensor_quant_with_axis, "Fake Tensor Quant with axis",
-        py::arg("inputs"), py::arg("amax"), py::arg("axis"), py::arg("num_bits") = 8,
-        py::arg("unsigned") = false, py::arg("narrow_range") = true);
-}
+/*
+ * SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.
+ * All rights reserved. SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+ *
+ * NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+ * property and proprietary rights in and to this material, related
+ * documentation and any modifications thereto. Any use, reproduction,
+ * disclosure or distribution of this material and related documentation
+ * without an express license agreement from NVIDIA CORPORATION or
+ * its affiliates is strictly prohibited.
+ */
+
+#include <torch/extension.h>
+
+void fake_tensor_quant_cuda_inplace(at::Tensor, at::Tensor, int, bool, bool);
+at::Tensor fake_tensor_quant_cuda(at::Tensor, at::Tensor, int, bool, bool);
+at::Tensor fake_tensor_quant_with_axis_cuda(at::Tensor, at::Tensor, int, int, bool, bool);
+float bits_to_bound(int, int);
+at::Tensor fake_e4m3fy_cuda(at::Tensor inputs);
+
+void fake_tensor_quant_(at::Tensor inputs, at::Tensor amax, int num_bits = 8,
+                        bool is_unsigned = false, bool narrow_range = true) {
+  TORCH_CHECK(inputs.is_cuda());
+  TORCH_CHECK(inputs.is_contiguous()) // in-place on non-contiguous tensor is
+                                      // more difficult
+  TORCH_CHECK(amax.numel(), 1);
+  fake_tensor_quant_cuda_inplace(inputs, amax, num_bits, is_unsigned, narrow_range);
+}
+// TODO: Can we add support for CPU tensors here?
+at::Tensor fake_tensor_quant(at::Tensor inputs, at::Tensor amax, int num_bits = 8,
+                             bool is_unsigned = false, bool narrow_range = true) {
+  TORCH_CHECK(inputs.is_cuda());
+  TORCH_CHECK(amax.numel(), 1);
+  return fake_tensor_quant_cuda(inputs.contiguous(), amax.contiguous(), num_bits, is_unsigned,
+                                narrow_range);
+}
+
+at::Tensor fake_tensor_quant_with_axis(at::Tensor inputs, at::Tensor amax, int axis,
+                                       int num_bits = 8, bool is_unsigned = false,
+                                       bool narrow_range = true) {
+  TORCH_CHECK(inputs.is_cuda());
+  TORCH_CHECK(amax.numel(), inputs.size(axis));
+  return fake_tensor_quant_with_axis_cuda(inputs.contiguous(), amax.contiguous(), axis, num_bits,
+                                          is_unsigned, narrow_range);
+}
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
+  m.def("fake_tensor_quant_", &fake_tensor_quant_, "Fake Tensor Quant Inplace", py::arg("inputs"),
+        py::arg("amax"), py::arg("num_bits") = 8, py::arg("unsigned") = false,
+        py::arg("narrow_range") = true);
+  m.def("fake_tensor_quant", &fake_tensor_quant, "Fake Tensor Quant", py::arg("inputs"),
+        py::arg("amax"), py::arg("num_bits") = 8, py::arg("unsigned") = false,
+        py::arg("narrow_range") = true);
+  m.def("fake_tensor_quant_with_axis", &fake_tensor_quant_with_axis, "Fake Tensor Quant with axis",
+        py::arg("inputs"), py::arg("amax"), py::arg("axis"), py::arg("num_bits") = 8,
+        py::arg("unsigned") = false, py::arg("narrow_range") = true);
+}
```

## modelopt/torch/quantization/src/tensor_quant_fp8.cpp

 * *Ordering differences only*

```diff
@@ -1,36 +1,36 @@
-/*
- * SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.
- * All rights reserved. SPDX-License-Identifier: LicenseRef-NvidiaProprietary
- *
- * NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
- * property and proprietary rights in and to this material, related
- * documentation and any modifications thereto. Any use, reproduction,
- * disclosure or distribution of this material and related documentation
- * without an express license agreement from NVIDIA CORPORATION or
- * its affiliates is strictly prohibited.
- */
-
-#include <ATen/ATen.h>
-#include <cuda_fp8.h>
-#include <torch/extension.h>
-
-at::Tensor fake_e4m3fy_cuda(at::Tensor inputs);
-
-at::Tensor fake_e4m3fy(at::Tensor inputs) {
-  if (inputs.is_cuda()) {
-    return fake_e4m3fy_cuda(inputs.contiguous());
-  } else {
-    TORCH_CHECK(inputs.dtype() == at::ScalarType::Float);
-    TORCH_CHECK(inputs.is_contiguous());
-    auto out = at::zeros_like(inputs);
-    for (int i = 0; i < inputs.numel(); ++i) {
-      out.data_ptr<float>()[i] =
-          static_cast<float>(static_cast<__nv_fp8_e4m3>(inputs.data_ptr<float>()[i]));
-    }
-    return out;
-  }
-}
-
-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("fake_e4m3fy", &fake_e4m3fy, "Reduce precision to E4M3", py::arg("inputs"));
-}
+/*
+ * SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.
+ * All rights reserved. SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+ *
+ * NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+ * property and proprietary rights in and to this material, related
+ * documentation and any modifications thereto. Any use, reproduction,
+ * disclosure or distribution of this material and related documentation
+ * without an express license agreement from NVIDIA CORPORATION or
+ * its affiliates is strictly prohibited.
+ */
+
+#include <ATen/ATen.h>
+#include <cuda_fp8.h>
+#include <torch/extension.h>
+
+at::Tensor fake_e4m3fy_cuda(at::Tensor inputs);
+
+at::Tensor fake_e4m3fy(at::Tensor inputs) {
+  if (inputs.is_cuda()) {
+    return fake_e4m3fy_cuda(inputs.contiguous());
+  } else {
+    TORCH_CHECK(inputs.dtype() == at::ScalarType::Float);
+    TORCH_CHECK(inputs.is_contiguous());
+    auto out = at::zeros_like(inputs);
+    for (int i = 0; i < inputs.numel(); ++i) {
+      out.data_ptr<float>()[i] =
+          static_cast<float>(static_cast<__nv_fp8_e4m3>(inputs.data_ptr<float>()[i]));
+    }
+    return out;
+  }
+}
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
+  m.def("fake_e4m3fy", &fake_e4m3fy, "Reduce precision to E4M3", py::arg("inputs"));
+}
```

## modelopt/torch/quantization/src/tensor_quant_gpu.cu

 * *Ordering differences only*

```diff
@@ -1,130 +1,130 @@
-/*
- * SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.
- * All rights reserved. SPDX-License-Identifier: LicenseRef-NvidiaProprietary
- *
- * NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
- * property and proprietary rights in and to this material, related
- * documentation and any modifications thereto. Any use, reproduction,
- * disclosure or distribution of this material and related documentation
- * without an express license agreement from NVIDIA CORPORATION or
- * its affiliates is strictly prohibited.
- */
-
-#include <ATen/ATen.h>
-#include <cuda.h>
-#include <cuda_fp16.h>
-#include <cuda_runtime.h>
-#include <math.h>
-#include <torch/extension.h>
-
-#define BLOCK_SIZE 128
-#define EPSILON (1. / (1 << 24)) // Minimum representable of fp16
-
-#define AT_DISPATCH_CASE_FLOATING_TYPES(...)                                                       \
-  AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__)                                            \
-  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)                                             \
-  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)                                              \
-  AT_DISPATCH_CASE(at::ScalarType::BFloat16, __VA_ARGS__)
-
-#define AT_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...)                                                \
-  AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))
-
-__host__ __device__ float bits_to_bound(int num_bits, int is_unsigned) {
-  float bound = (1 << (num_bits - 1 + int(is_unsigned))) - 1;
-  return bound;
-}
-
-__device__ float fake_tensor_quant_device(float input, float amax, int min_bound, int max_bound) {
-  CUDA_KERNEL_ASSERT(amax >= 0);
-
-  if (amax < EPSILON) {
-    return 0.f;
-  }
-
-  float scale = max_bound / amax;
-  float output = rint(input * scale);
-  output = output > max_bound ? max_bound : output;
-  output = output < min_bound ? min_bound : output;
-
-  return output / scale;
-}
-
-template <typename T>
-__global__ void fake_tensor_quant_kernel(const T *inputs, size_t n, T *outputs, const float *amax,
-                                         int num_bits = 8, bool is_unsigned = false,
-                                         bool narrow_range = true) {
-  int tid = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (tid < n) {
-    if (is_unsigned) {
-      CUDA_KERNEL_ASSERT(inputs[tid] >= 0);
-    }
-    float bound = bits_to_bound(num_bits, is_unsigned);
-    float max_bound = bound;
-    float min_bound = -(bound + !narrow_range);
-    outputs[tid] = fake_tensor_quant_device((float)inputs[tid], amax[0], min_bound, max_bound);
-  }
-}
-
-void fake_tensor_quant_cuda_inplace(at::Tensor inputs, at::Tensor amax, int num_bits = 8,
-                                    bool is_unsigned = false, bool narrow_range = true) {
-  size_t numel = inputs.numel();
-  AT_DISPATCH_FLOATING_TYPES(inputs.type().scalarType(), "fake_tensor_quant_cuda_inplace", [&] {
-    fake_tensor_quant_kernel<<<numel / BLOCK_SIZE + 1, BLOCK_SIZE>>>(
-        inputs.data_ptr<scalar_t>(), numel, inputs.data_ptr<scalar_t>(),
-        amax.to(at::ScalarType::Float).data_ptr<float>(), num_bits, is_unsigned, narrow_range);
-  });
-}
-
-at::Tensor fake_tensor_quant_cuda(at::Tensor inputs, at::Tensor amax, int num_bits = 8,
-                                  bool is_unsigned = false, bool narrow_range = true) {
-  size_t numel = inputs.numel();
-  auto outputs = torch::empty_like(inputs);
-  AT_DISPATCH_FLOATING_TYPES(inputs.type().scalarType(), "fake_tensor_quant_cuda", [&] {
-    fake_tensor_quant_kernel<<<numel / BLOCK_SIZE + 1, BLOCK_SIZE>>>(
-        inputs.data_ptr<scalar_t>(), numel, outputs.data_ptr<scalar_t>(),
-        amax.to(at::ScalarType::Float).data_ptr<float>(), num_bits, is_unsigned, narrow_range);
-  });
-
-  return outputs;
-}
-
-template <typename T>
-__global__ void
-fake_tensor_quant_with_axis_cuda_kernel(const T *inputs, size_t n, T *outputs, const float *amax,
-                                        int axis_size, int outer_size, int num_bits = 8,
-                                        bool is_unsigned = false, bool narrow_range = true) {
-  int tid = blockIdx.x * blockDim.x + threadIdx.x;
-
-  float bound = bits_to_bound(num_bits, is_unsigned);
-  float max_bound = bound;
-  float min_bound = -(bound + !narrow_range);
-
-  for (int idx = 4 * tid; idx < 4 * (tid + 1) && idx < n; ++idx) {
-    if (is_unsigned) {
-      CUDA_KERNEL_ASSERT(inputs[idx] >= 0);
-    }
-    int axis_idx = (idx / outer_size) % axis_size;
-
-    outputs[idx] =
-        fake_tensor_quant_device((float)inputs[idx], amax[axis_idx], min_bound, max_bound);
-  }
-}
-
-at::Tensor fake_tensor_quant_with_axis_cuda(at::Tensor inputs, at::Tensor amax, int axis,
-                                            int num_bits = 8, bool is_unsigned = false,
-                                            bool narrow_range = true) {
-  auto outputs = torch::empty_like(inputs);
-  size_t numel = inputs.numel();
-  int axis_size = inputs.size(axis);
-
-  int outer_size = inputs.stride(axis);
-
-  AT_DISPATCH_FLOATING_TYPES(inputs.type().scalarType(), "fake_tensor_quant_cuda_with_axis", [&] {
-    fake_tensor_quant_with_axis_cuda_kernel<<<numel / (BLOCK_SIZE * 4) + 1, BLOCK_SIZE>>>(
-        inputs.data_ptr<scalar_t>(), numel, outputs.data_ptr<scalar_t>(),
-        amax.to(at::ScalarType::Float).data_ptr<float>(), axis_size, outer_size, num_bits,
-        is_unsigned, narrow_range);
-  });
-  return outputs;
-}
+/*
+ * SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.
+ * All rights reserved. SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+ *
+ * NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+ * property and proprietary rights in and to this material, related
+ * documentation and any modifications thereto. Any use, reproduction,
+ * disclosure or distribution of this material and related documentation
+ * without an express license agreement from NVIDIA CORPORATION or
+ * its affiliates is strictly prohibited.
+ */
+
+#include <ATen/ATen.h>
+#include <cuda.h>
+#include <cuda_fp16.h>
+#include <cuda_runtime.h>
+#include <math.h>
+#include <torch/extension.h>
+
+#define BLOCK_SIZE 128
+#define EPSILON (1. / (1 << 24)) // Minimum representable of fp16
+
+#define AT_DISPATCH_CASE_FLOATING_TYPES(...)                                                       \
+  AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__)                                            \
+  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)                                             \
+  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)                                              \
+  AT_DISPATCH_CASE(at::ScalarType::BFloat16, __VA_ARGS__)
+
+#define AT_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...)                                                \
+  AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))
+
+__host__ __device__ float bits_to_bound(int num_bits, int is_unsigned) {
+  float bound = (1 << (num_bits - 1 + int(is_unsigned))) - 1;
+  return bound;
+}
+
+__device__ float fake_tensor_quant_device(float input, float amax, int min_bound, int max_bound) {
+  CUDA_KERNEL_ASSERT(amax >= 0);
+
+  if (amax < EPSILON) {
+    return 0.f;
+  }
+
+  float scale = max_bound / amax;
+  float output = rint(input * scale);
+  output = output > max_bound ? max_bound : output;
+  output = output < min_bound ? min_bound : output;
+
+  return output / scale;
+}
+
+template <typename T>
+__global__ void fake_tensor_quant_kernel(const T *inputs, size_t n, T *outputs, const float *amax,
+                                         int num_bits = 8, bool is_unsigned = false,
+                                         bool narrow_range = true) {
+  int tid = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (tid < n) {
+    if (is_unsigned) {
+      CUDA_KERNEL_ASSERT(inputs[tid] >= 0);
+    }
+    float bound = bits_to_bound(num_bits, is_unsigned);
+    float max_bound = bound;
+    float min_bound = -(bound + !narrow_range);
+    outputs[tid] = fake_tensor_quant_device((float)inputs[tid], amax[0], min_bound, max_bound);
+  }
+}
+
+void fake_tensor_quant_cuda_inplace(at::Tensor inputs, at::Tensor amax, int num_bits = 8,
+                                    bool is_unsigned = false, bool narrow_range = true) {
+  size_t numel = inputs.numel();
+  AT_DISPATCH_FLOATING_TYPES(inputs.type().scalarType(), "fake_tensor_quant_cuda_inplace", [&] {
+    fake_tensor_quant_kernel<<<numel / BLOCK_SIZE + 1, BLOCK_SIZE>>>(
+        inputs.data_ptr<scalar_t>(), numel, inputs.data_ptr<scalar_t>(),
+        amax.to(at::ScalarType::Float).data_ptr<float>(), num_bits, is_unsigned, narrow_range);
+  });
+}
+
+at::Tensor fake_tensor_quant_cuda(at::Tensor inputs, at::Tensor amax, int num_bits = 8,
+                                  bool is_unsigned = false, bool narrow_range = true) {
+  size_t numel = inputs.numel();
+  auto outputs = torch::empty_like(inputs);
+  AT_DISPATCH_FLOATING_TYPES(inputs.type().scalarType(), "fake_tensor_quant_cuda", [&] {
+    fake_tensor_quant_kernel<<<numel / BLOCK_SIZE + 1, BLOCK_SIZE>>>(
+        inputs.data_ptr<scalar_t>(), numel, outputs.data_ptr<scalar_t>(),
+        amax.to(at::ScalarType::Float).data_ptr<float>(), num_bits, is_unsigned, narrow_range);
+  });
+
+  return outputs;
+}
+
+template <typename T>
+__global__ void
+fake_tensor_quant_with_axis_cuda_kernel(const T *inputs, size_t n, T *outputs, const float *amax,
+                                        int axis_size, int outer_size, int num_bits = 8,
+                                        bool is_unsigned = false, bool narrow_range = true) {
+  int tid = blockIdx.x * blockDim.x + threadIdx.x;
+
+  float bound = bits_to_bound(num_bits, is_unsigned);
+  float max_bound = bound;
+  float min_bound = -(bound + !narrow_range);
+
+  for (int idx = 4 * tid; idx < 4 * (tid + 1) && idx < n; ++idx) {
+    if (is_unsigned) {
+      CUDA_KERNEL_ASSERT(inputs[idx] >= 0);
+    }
+    int axis_idx = (idx / outer_size) % axis_size;
+
+    outputs[idx] =
+        fake_tensor_quant_device((float)inputs[idx], amax[axis_idx], min_bound, max_bound);
+  }
+}
+
+at::Tensor fake_tensor_quant_with_axis_cuda(at::Tensor inputs, at::Tensor amax, int axis,
+                                            int num_bits = 8, bool is_unsigned = false,
+                                            bool narrow_range = true) {
+  auto outputs = torch::empty_like(inputs);
+  size_t numel = inputs.numel();
+  int axis_size = inputs.size(axis);
+
+  int outer_size = inputs.stride(axis);
+
+  AT_DISPATCH_FLOATING_TYPES(inputs.type().scalarType(), "fake_tensor_quant_cuda_with_axis", [&] {
+    fake_tensor_quant_with_axis_cuda_kernel<<<numel / (BLOCK_SIZE * 4) + 1, BLOCK_SIZE>>>(
+        inputs.data_ptr<scalar_t>(), numel, outputs.data_ptr<scalar_t>(),
+        amax.to(at::ScalarType::Float).data_ptr<float>(), axis_size, outer_size, num_bits,
+        is_unsigned, narrow_range);
+  });
+  return outputs;
+}
```

## modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu

 * *Ordering differences only*

```diff
@@ -1,45 +1,45 @@
-/*
- * SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.
- * All rights reserved. SPDX-License-Identifier: LicenseRef-NvidiaProprietary
- *
- * NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
- * property and proprietary rights in and to this material, related
- * documentation and any modifications thereto. Any use, reproduction,
- * disclosure or distribution of this material and related documentation
- * without an express license agreement from NVIDIA CORPORATION or
- * its affiliates is strictly prohibited.
- */
-
-#include <ATen/ATen.h>
-#include <cuda_fp8.h>
-#include <torch/extension.h>
-
-#define BLOCK_SIZE 128
-
-#define AT_DISPATCH_CASE_FLOATING_TYPES(...)                                                       \
-  AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__)                                            \
-  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)                                             \
-  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)                                              \
-  AT_DISPATCH_CASE(at::ScalarType::BFloat16, __VA_ARGS__)
-
-#define AT_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...)                                                \
-  AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))
-
-template <typename T> __global__ void fake_e4m3fy_kernel(const T *inputs, size_t n, T *outputs) {
-  int tid = blockIdx.x * blockDim.x + threadIdx.x;
-
-  for (int idx = 4 * tid; idx < 4 * (tid + 1) && idx < n; ++idx) {
-    outputs[idx] = static_cast<T>(
-        static_cast<float>(static_cast<__nv_fp8_e4m3>(static_cast<float>(inputs[idx]))));
-  }
-}
-
-at::Tensor fake_e4m3fy_cuda(at::Tensor inputs) {
-  size_t numel = inputs.numel();
-  auto outputs = torch::empty_like(inputs);
-  AT_DISPATCH_FLOATING_TYPES(inputs.type().scalarType(), "fake_e4m3fy_cuda", [&] {
-    fake_e4m3fy_kernel<<<numel / (BLOCK_SIZE * 4) + 1, BLOCK_SIZE>>>(
-        inputs.data_ptr<scalar_t>(), numel, outputs.data_ptr<scalar_t>());
-  });
-  return outputs;
-}
+/*
+ * SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES.
+ * All rights reserved. SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+ *
+ * NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+ * property and proprietary rights in and to this material, related
+ * documentation and any modifications thereto. Any use, reproduction,
+ * disclosure or distribution of this material and related documentation
+ * without an express license agreement from NVIDIA CORPORATION or
+ * its affiliates is strictly prohibited.
+ */
+
+#include <ATen/ATen.h>
+#include <cuda_fp8.h>
+#include <torch/extension.h>
+
+#define BLOCK_SIZE 128
+
+#define AT_DISPATCH_CASE_FLOATING_TYPES(...)                                                       \
+  AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__)                                            \
+  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)                                             \
+  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)                                              \
+  AT_DISPATCH_CASE(at::ScalarType::BFloat16, __VA_ARGS__)
+
+#define AT_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...)                                                \
+  AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))
+
+template <typename T> __global__ void fake_e4m3fy_kernel(const T *inputs, size_t n, T *outputs) {
+  int tid = blockIdx.x * blockDim.x + threadIdx.x;
+
+  for (int idx = 4 * tid; idx < 4 * (tid + 1) && idx < n; ++idx) {
+    outputs[idx] = static_cast<T>(
+        static_cast<float>(static_cast<__nv_fp8_e4m3>(static_cast<float>(inputs[idx]))));
+  }
+}
+
+at::Tensor fake_e4m3fy_cuda(at::Tensor inputs) {
+  size_t numel = inputs.numel();
+  auto outputs = torch::empty_like(inputs);
+  AT_DISPATCH_FLOATING_TYPES(inputs.type().scalarType(), "fake_e4m3fy_cuda", [&] {
+    fake_e4m3fy_kernel<<<numel / (BLOCK_SIZE * 4) + 1, BLOCK_SIZE>>>(
+        inputs.data_ptr<scalar_t>(), numel, outputs.data_ptr<scalar_t>());
+  });
+  return outputs;
+}
```

## modelopt/torch/sparsity/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""API for sparsification algorithms."""
-
-from . import mode, module, plugins
-from .sparsification import *
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""API for sparsification algorithms."""
+
+from . import mode, module, plugins
+from .sparsification import *
```

## modelopt/torch/sparsity/config.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Default configurations for sparsity modes."""
-
-
-from pydantic import create_model
-
-from modelopt.torch.opt.config import ModeloptBaseConfig, get_kwargs_for_create_model_with_rules
-
-from .module import SpDMRegistry
-
-SparseMagnitudeConfig = create_model(
-    "SparseMagnitudeConfig",
-    **get_kwargs_for_create_model_with_rules(
-        registry=SpDMRegistry,
-        default_rules={
-            "nn.Linear": {"*": {}, "*lm_head*": None},
-            "nn.Conv2d": {"*": {}, "*lm_head*": None},
-        },
-        doc='Configuration for the ``"sparse_magnitude"`` mode.',
-    ),
-)
-
-
-SparseGPTConfig = create_model(
-    "SparseGPTConfig",
-    **get_kwargs_for_create_model_with_rules(
-        registry=SpDMRegistry,
-        default_rules={
-            "nn.Linear": {"*": {}, "*lm_head*": None},
-            "nn.Conv2d": {"*": {}, "*lm_head*": None},
-        },
-        doc='Configuration for the ``"sparse_gpt"`` mode.',
-    ),
-)
-
-
-class ExportSparseConfig(ModeloptBaseConfig):
-    """Configuration (empty!) for the ``"export_sparse"`` mode."""
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Default configurations for sparsity modes."""
+
+
+from pydantic import create_model
+
+from modelopt.torch.opt.config import ModeloptBaseConfig, get_kwargs_for_create_model_with_rules
+
+from .module import SpDMRegistry
+
+SparseMagnitudeConfig = create_model(
+    "SparseMagnitudeConfig",
+    **get_kwargs_for_create_model_with_rules(
+        registry=SpDMRegistry,
+        default_rules={
+            "nn.Linear": {"*": {}, "*lm_head*": None},
+            "nn.Conv2d": {"*": {}, "*lm_head*": None},
+        },
+        doc='Configuration for the ``"sparse_magnitude"`` mode.',
+    ),
+)
+
+
+SparseGPTConfig = create_model(
+    "SparseGPTConfig",
+    **get_kwargs_for_create_model_with_rules(
+        registry=SpDMRegistry,
+        default_rules={
+            "nn.Linear": {"*": {}, "*lm_head*": None},
+            "nn.Conv2d": {"*": {}, "*lm_head*": None},
+        },
+        doc='Configuration for the ``"sparse_gpt"`` mode.',
+    ),
+)
+
+
+class ExportSparseConfig(ModeloptBaseConfig):
+    """Configuration (empty!) for the ``"export_sparse"`` mode."""
```

## modelopt/torch/sparsity/magnitude.py

 * *Ordering differences only*

```diff
@@ -1,144 +1,144 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Magnitude-base sparsity inspired by NVIDIA ASP (Automatic SParsity)."""
-
-import re
-import warnings
-from itertools import permutations
-from typing import Tuple
-
-import torch
-import torch.nn as nn
-
-from .module import SparseModule
-from .searcher import BaseSparseSearcher
-
-
-def get_nmprune_info(pattern: str) -> Tuple[bool, int, int]:
-    """Gets the n:m sparsity pattern information from a given string."""
-    nm_prune = re.search(r"(\d+):(\d+) sparsity", pattern)
-    if nm_prune is not None:
-        n, m = map(int, nm_prune.groups())
-        return nm_prune is not None, n, m
-    return False, 0, 0
-
-
-def fill(x):
-    """Calculates the ratio of non-zero elements in a tensor."""
-    return float(x.nonzero().size(0)) / torch.numel(x)
-
-
-def reshape_1d(matrix, m):
-    """Reshapes a given matrix into m-dimensional vectors: (h,w) -> (hw/m, m)."""
-    if matrix.shape[1] % m > 0:
-        new_cols = matrix.shape[1] + (m - matrix.shape[1] % m)
-        mat = matrix.new_empty(matrix.shape[0], new_cols).fill_(0)
-        mat[:, : matrix.shape[1]] = matrix
-
-        return mat.view(-1, m), mat.shape
-    else:
-        return matrix.view(-1, m), matrix.shape
-
-
-def compute_valid_1d_patterns(m, n):
-    """Computes all possible m:n patterns in a 1D vector.
-
-    The function generates a tensor of size m with n ones and (m-n) zeros.
-    It then generates all permutations of this tensor, removes duplicates,
-    and returns the unique patterns as a tensor.
-    """
-    patterns = torch.zeros(m)
-    patterns[:n] = 1
-    valid_patterns = torch.tensor(list(set(permutations(patterns.tolist()))))
-    return valid_patterns
-
-
-def mn_1d_best(matrix, m, n):
-    """Finds the best m:n pattern in a given matrix.
-
-    The function computes all possible m:n patterns and selects the one
-    that maximizes the sum of non-masked weights in the matrix. The selected
-    pattern is then used to create a mask for the matrix.
-    """
-    patterns = compute_valid_1d_patterns(m, n).to(matrix.device)
-
-    # Find the best m:n pattern (sum of non-masked weights).
-    mask = torch.IntTensor(matrix.shape).fill_(1).view(-1, m)
-    mat, _ = reshape_1d(matrix, m)
-    pmax = torch.argmax(torch.matmul(mat.abs(), patterns.t()), dim=1)
-    mask[:] = patterns[pmax[:]]
-    mask = mask.view(matrix.shape)
-    return mask
-
-
-def m4n2_1d(mat):
-    """Finds the best 2:4 pattern in a given matrix."""
-    return mn_1d_best(mat, 4, 2)
-
-
-def create_asp_mask(tensor: nn.Parameter, pattern: str) -> torch.BoolTensor:
-    """Creates a mask for a given tensor based on a specified sparse pattern.
-
-    The function reshapes the tensor and applies the specified pattern to create a sparse mask.
-    The default pattern is m4n2_1d, which finds the best 2:4 sparsity pattern in the tensor.
-    """
-    pattern_method_lut = {BaseSparseSearcher._pattern_2_4: m4n2_1d}
-    if pattern not in pattern_method_lut:
-        raise NotImplementedError(f"Unsupported pattern {pattern} for ASP sparsity")
-    func = pattern_method_lut[pattern]
-
-    shape = tensor.shape
-    tensor.type()
-    t = tensor.float().contiguous()
-
-    # 1d-tensor
-    if len(shape) == 1:
-        t = t.view(1, shape[0])
-        mask = func(t)
-    # 2d-tensor (K, C)
-    elif len(shape) == 2:
-        # linear
-        t = t.view(shape[0], shape[1])
-        mask = func(t)
-    # 3d-tensor (K, C, R)
-    elif len(shape) == 3:
-        # 1d convs
-        t = t.permute(0, 2, 1).contiguous().view(shape[0] * shape[2], shape[1])
-        mask = func(t)
-        mask = mask.view(shape[0], shape[2], shape[1]).permute(0, 2, 1).contiguous()
-    # 4d-tensor (K, C, R, S)
-    elif len(shape) == 4:
-        # 2d convs
-        t = t.permute(2, 3, 0, 1).contiguous().view(shape[2] * shape[3] * shape[0], shape[1])
-        mask = func(t)
-        mask = mask.view(shape[2], shape[3], shape[0], shape[1]).permute(2, 3, 0, 1).contiguous()
-
-    return mask.view(shape).to(dtype=torch.bool)
-
-
-class MagnitudeSearcher(BaseSparseSearcher):
-    """Searcher for magnitude-based sparsity."""
-
-    def _check_weight_size(self, weight: torch.nn.Parameter, mod_name: str) -> bool:
-        """Check if the weight size is supported."""
-        # rules from ASP
-        if weight.size(0) % 8 != 0 or weight.size(1) % 16 != 0:
-            warnings.warn(
-                f"Skipping pruning {mod_name} of size={str(weight.size())} and"
-                f" type={str(weight.dtype)} for sparsity"
-            )
-            return False
-
-        return True
-
-    def _compute_mask(self, module: SparseModule) -> torch.BoolTensor:
-        """Compute the mask (and weight update) for the given module."""
-        return create_asp_mask(module.weight, self.config["pattern"])
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Magnitude-base sparsity inspired by NVIDIA ASP (Automatic SParsity)."""
+
+import re
+import warnings
+from itertools import permutations
+from typing import Tuple
+
+import torch
+import torch.nn as nn
+
+from .module import SparseModule
+from .searcher import BaseSparseSearcher
+
+
+def get_nmprune_info(pattern: str) -> Tuple[bool, int, int]:
+    """Gets the n:m sparsity pattern information from a given string."""
+    nm_prune = re.search(r"(\d+):(\d+) sparsity", pattern)
+    if nm_prune is not None:
+        n, m = map(int, nm_prune.groups())
+        return nm_prune is not None, n, m
+    return False, 0, 0
+
+
+def fill(x):
+    """Calculates the ratio of non-zero elements in a tensor."""
+    return float(x.nonzero().size(0)) / torch.numel(x)
+
+
+def reshape_1d(matrix, m):
+    """Reshapes a given matrix into m-dimensional vectors: (h,w) -> (hw/m, m)."""
+    if matrix.shape[1] % m > 0:
+        new_cols = matrix.shape[1] + (m - matrix.shape[1] % m)
+        mat = matrix.new_empty(matrix.shape[0], new_cols).fill_(0)
+        mat[:, : matrix.shape[1]] = matrix
+
+        return mat.view(-1, m), mat.shape
+    else:
+        return matrix.view(-1, m), matrix.shape
+
+
+def compute_valid_1d_patterns(m, n):
+    """Computes all possible m:n patterns in a 1D vector.
+
+    The function generates a tensor of size m with n ones and (m-n) zeros.
+    It then generates all permutations of this tensor, removes duplicates,
+    and returns the unique patterns as a tensor.
+    """
+    patterns = torch.zeros(m)
+    patterns[:n] = 1
+    valid_patterns = torch.tensor(list(set(permutations(patterns.tolist()))))
+    return valid_patterns
+
+
+def mn_1d_best(matrix, m, n):
+    """Finds the best m:n pattern in a given matrix.
+
+    The function computes all possible m:n patterns and selects the one
+    that maximizes the sum of non-masked weights in the matrix. The selected
+    pattern is then used to create a mask for the matrix.
+    """
+    patterns = compute_valid_1d_patterns(m, n).to(matrix.device)
+
+    # Find the best m:n pattern (sum of non-masked weights).
+    mask = torch.IntTensor(matrix.shape).fill_(1).view(-1, m)
+    mat, _ = reshape_1d(matrix, m)
+    pmax = torch.argmax(torch.matmul(mat.abs(), patterns.t()), dim=1)
+    mask[:] = patterns[pmax[:]]
+    mask = mask.view(matrix.shape)
+    return mask
+
+
+def m4n2_1d(mat):
+    """Finds the best 2:4 pattern in a given matrix."""
+    return mn_1d_best(mat, 4, 2)
+
+
+def create_asp_mask(tensor: nn.Parameter, pattern: str) -> torch.BoolTensor:
+    """Creates a mask for a given tensor based on a specified sparse pattern.
+
+    The function reshapes the tensor and applies the specified pattern to create a sparse mask.
+    The default pattern is m4n2_1d, which finds the best 2:4 sparsity pattern in the tensor.
+    """
+    pattern_method_lut = {BaseSparseSearcher._pattern_2_4: m4n2_1d}
+    if pattern not in pattern_method_lut:
+        raise NotImplementedError(f"Unsupported pattern {pattern} for ASP sparsity")
+    func = pattern_method_lut[pattern]
+
+    shape = tensor.shape
+    tensor.type()
+    t = tensor.float().contiguous()
+
+    # 1d-tensor
+    if len(shape) == 1:
+        t = t.view(1, shape[0])
+        mask = func(t)
+    # 2d-tensor (K, C)
+    elif len(shape) == 2:
+        # linear
+        t = t.view(shape[0], shape[1])
+        mask = func(t)
+    # 3d-tensor (K, C, R)
+    elif len(shape) == 3:
+        # 1d convs
+        t = t.permute(0, 2, 1).contiguous().view(shape[0] * shape[2], shape[1])
+        mask = func(t)
+        mask = mask.view(shape[0], shape[2], shape[1]).permute(0, 2, 1).contiguous()
+    # 4d-tensor (K, C, R, S)
+    elif len(shape) == 4:
+        # 2d convs
+        t = t.permute(2, 3, 0, 1).contiguous().view(shape[2] * shape[3] * shape[0], shape[1])
+        mask = func(t)
+        mask = mask.view(shape[2], shape[3], shape[0], shape[1]).permute(2, 3, 0, 1).contiguous()
+
+    return mask.view(shape).to(dtype=torch.bool)
+
+
+class MagnitudeSearcher(BaseSparseSearcher):
+    """Searcher for magnitude-based sparsity."""
+
+    def _check_weight_size(self, weight: torch.nn.Parameter, mod_name: str) -> bool:
+        """Check if the weight size is supported."""
+        # rules from ASP
+        if weight.size(0) % 8 != 0 or weight.size(1) % 16 != 0:
+            warnings.warn(
+                f"Skipping pruning {mod_name} of size={str(weight.size())} and"
+                f" type={str(weight.dtype)} for sparsity"
+            )
+            return False
+
+        return True
+
+    def _compute_mask(self, module: SparseModule) -> torch.BoolTensor:
+        """Compute the mask (and weight update) for the given module."""
+        return create_asp_mask(module.weight, self.config["pattern"])
```

## modelopt/torch/sparsity/mode.py

 * *Ordering differences only*

```diff
@@ -1,201 +1,201 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Sparsity mode descriptor."""
-from typing import Optional, Set, Type
-
-from torch import nn
-
-from modelopt.torch.opt.config import ModeloptBaseConfig
-from modelopt.torch.opt.conversion import ApplyModeError
-from modelopt.torch.opt.dynamic import DynamicSpace
-from modelopt.torch.opt.mode import (
-    ConvertEntrypoint,
-    ConvertReturnType,
-    MetadataDict,
-    RestoreEntrypoint,
-    UpdateEntrypoint,
-    _ModeDescriptor,
-    _ModeRegistryCls,
-)
-from modelopt.torch.opt.searcher import BaseSearcher
-from modelopt.torch.utils import (
-    compare_dict,
-    unwrap_model,
-)
-
-from .config import ExportSparseConfig, SparseGPTConfig, SparseMagnitudeConfig
-from .magnitude import MagnitudeSearcher
-from .module import SpDMRegistry
-from .sparsegpt import SparseGPTSearcher
-
-SparsityModeRegistry = _ModeRegistryCls()
-
-
-def convert_sparse_model(model: nn.Module, config: ModeloptBaseConfig) -> ConvertReturnType:
-    """Function for converting a model to a sparsity meta-model."""
-    # we use the search space utility here with a custom registry to convert the model
-    dynamic_space = DynamicSpace(model)
-    dynamic_space.convert_to_dynamic(config.model_dump(), SpDMRegistry)
-
-    return dynamic_space.model, {"subnet_config": DynamicSpace(model).config()}
-
-
-def restore_sparse_model(
-    model: nn.Module, config: ModeloptBaseConfig, metadata: MetadataDict
-) -> nn.Module:
-    """Function for restoring a previously convert model to a sparsity meta-model."""
-    assert "subnet_config" in metadata
-    model, _ = convert_sparse_model(model, config)
-    DynamicSpace(model).select(metadata["subnet_config"])
-
-    return model
-
-
-def update_sparse_metadata(
-    model: nn.Module, config: ModeloptBaseConfig, metadata: MetadataDict
-) -> None:
-    """Update subnet config to current subnet config of model."""
-    metadata["subnet_config"] = DynamicSpace(model).config()
-
-
-def export_sparse(model: nn.Module, config: ExportSparseConfig) -> ConvertReturnType:
-    """Export a sparse model to a regular model."""
-    # sanity check to avoid DP/DDP here in the entrypoint
-    model = unwrap_model(model, raise_error=True)
-
-    # store config from model if we can find it for a future convert/restore process
-    metadata = {"subnet_config": DynamicSpace(model).config()}
-
-    # export model in-place
-    model = DynamicSpace(model).export(SpDMRegistry)
-
-    return model, metadata
-
-
-def restore_export_sparse(
-    model: nn.Module, config: ExportSparseConfig, metadata: MetadataDict
-) -> nn.Module:
-    """Restore & export a sparse model to a regular model."""
-    # select activated/deactivated sparse modules
-    DynamicSpace(model).select(metadata["subnet_config"])
-
-    # run export
-    model, metadata_new = export_sparse(model, config)
-
-    # double check metadata
-    unmatched_keys = compare_dict(metadata, metadata_new)
-    if unmatched_keys:
-        raise ApplyModeError(f"Unmatched metadata={unmatched_keys}!")
-
-    return model
-
-
-@SparsityModeRegistry.register_mode
-class SparseMagnitudeModeDescriptor(_ModeDescriptor):
-    """Class to define and describe magnitude-based sparsification."""
-
-    @property
-    def name(self) -> str:
-        """Returns the name of the mode."""
-        return "sparse_magnitude"
-
-    @property
-    def config_class(self) -> Type[ModeloptBaseConfig]:
-        """Specifies the config class for the mode."""
-        return SparseMagnitudeConfig
-
-    @property
-    def next_modes(self) -> Optional[Set[str]]:
-        """Specifies the next modes for the mode."""
-        return {"export_sparse", "kd_loss", "quantize"}
-
-    @property
-    def export_mode(self) -> Optional[str]:
-        """The mode that corresponds to the export mode of this mode."""
-        return "export_sparse"
-
-    @property
-    def search_algorithm(self) -> Type[BaseSearcher]:
-        """Specifies the search algorithm for the mode."""
-        return MagnitudeSearcher
-
-    @property
-    def convert(self) -> ConvertEntrypoint:
-        """The mode's entrypoint for converting a model."""
-        return convert_sparse_model
-
-    @property
-    def restore(self) -> RestoreEntrypoint:
-        """The mode's entrypoint for restoring a model."""
-        return restore_sparse_model
-
-    @property
-    def update_for_save(self) -> UpdateEntrypoint:
-        """The mode's entrypoint for updating the models metadata."""
-        return update_sparse_metadata
-
-    @property
-    def update_for_new_mode(self) -> UpdateEntrypoint:
-        """The mode's entrypoint for updating the models metadata."""
-        return update_sparse_metadata
-
-
-@SparsityModeRegistry.register_mode
-class SparseGPTModeDescriptor(SparseMagnitudeModeDescriptor):
-    """Class to define and describe sparsification based on SparseGPT."""
-
-    @property
-    def name(self) -> str:
-        """Returns the name of the mode."""
-        return "sparsegpt"
-
-    @property
-    def config_class(self) -> Type[ModeloptBaseConfig]:
-        """Specifies the config class for the mode."""
-        return SparseGPTConfig
-
-    @property
-    def search_algorithm(self) -> Type[BaseSearcher]:
-        """Specifies the search algorithm for the mode."""
-        return SparseGPTSearcher
-
-
-@SparsityModeRegistry.register_mode
-class ExportSparseModeDescriptor(_ModeDescriptor):
-    """Class to describe the ``"export_sparse"`` mode.
-
-    The properties of this mode can be inspected via the source code.
-    """
-
-    @property
-    def name(self) -> str:
-        """Returns the value (str representation) of the mode."""
-        return "export_sparse"
-
-    @property
-    def config_class(self) -> Type[ModeloptBaseConfig]:
-        """Specifies the config class for the mode."""
-        return ExportSparseConfig
-
-    @property
-    def is_export_mode(self) -> bool:
-        """Specifies if this mode is an export mode."""
-        return True
-
-    @property
-    def convert(self) -> ConvertEntrypoint:
-        """The mode's entrypoint for converting a model."""
-        return export_sparse
-
-    @property
-    def restore(self) -> RestoreEntrypoint:
-        """The mode's entrypoint for restoring a model."""
-        return restore_export_sparse
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Sparsity mode descriptor."""
+from typing import Optional, Set, Type
+
+from torch import nn
+
+from modelopt.torch.opt.config import ModeloptBaseConfig
+from modelopt.torch.opt.conversion import ApplyModeError
+from modelopt.torch.opt.dynamic import DynamicSpace
+from modelopt.torch.opt.mode import (
+    ConvertEntrypoint,
+    ConvertReturnType,
+    MetadataDict,
+    RestoreEntrypoint,
+    UpdateEntrypoint,
+    _ModeDescriptor,
+    _ModeRegistryCls,
+)
+from modelopt.torch.opt.searcher import BaseSearcher
+from modelopt.torch.utils import (
+    compare_dict,
+    unwrap_model,
+)
+
+from .config import ExportSparseConfig, SparseGPTConfig, SparseMagnitudeConfig
+from .magnitude import MagnitudeSearcher
+from .module import SpDMRegistry
+from .sparsegpt import SparseGPTSearcher
+
+SparsityModeRegistry = _ModeRegistryCls()
+
+
+def convert_sparse_model(model: nn.Module, config: ModeloptBaseConfig) -> ConvertReturnType:
+    """Function for converting a model to a sparsity meta-model."""
+    # we use the search space utility here with a custom registry to convert the model
+    dynamic_space = DynamicSpace(model)
+    dynamic_space.convert_to_dynamic(config.model_dump(), SpDMRegistry)
+
+    return dynamic_space.model, {"subnet_config": DynamicSpace(model).config()}
+
+
+def restore_sparse_model(
+    model: nn.Module, config: ModeloptBaseConfig, metadata: MetadataDict
+) -> nn.Module:
+    """Function for restoring a previously convert model to a sparsity meta-model."""
+    assert "subnet_config" in metadata
+    model, _ = convert_sparse_model(model, config)
+    DynamicSpace(model).select(metadata["subnet_config"])
+
+    return model
+
+
+def update_sparse_metadata(
+    model: nn.Module, config: ModeloptBaseConfig, metadata: MetadataDict
+) -> None:
+    """Update subnet config to current subnet config of model."""
+    metadata["subnet_config"] = DynamicSpace(model).config()
+
+
+def export_sparse(model: nn.Module, config: ExportSparseConfig) -> ConvertReturnType:
+    """Export a sparse model to a regular model."""
+    # sanity check to avoid DP/DDP here in the entrypoint
+    model = unwrap_model(model, raise_error=True)
+
+    # store config from model if we can find it for a future convert/restore process
+    metadata = {"subnet_config": DynamicSpace(model).config()}
+
+    # export model in-place
+    model = DynamicSpace(model).export(SpDMRegistry)
+
+    return model, metadata
+
+
+def restore_export_sparse(
+    model: nn.Module, config: ExportSparseConfig, metadata: MetadataDict
+) -> nn.Module:
+    """Restore & export a sparse model to a regular model."""
+    # select activated/deactivated sparse modules
+    DynamicSpace(model).select(metadata["subnet_config"])
+
+    # run export
+    model, metadata_new = export_sparse(model, config)
+
+    # double check metadata
+    unmatched_keys = compare_dict(metadata, metadata_new)
+    if unmatched_keys:
+        raise ApplyModeError(f"Unmatched metadata={unmatched_keys}!")
+
+    return model
+
+
+@SparsityModeRegistry.register_mode
+class SparseMagnitudeModeDescriptor(_ModeDescriptor):
+    """Class to define and describe magnitude-based sparsification."""
+
+    @property
+    def name(self) -> str:
+        """Returns the name of the mode."""
+        return "sparse_magnitude"
+
+    @property
+    def config_class(self) -> Type[ModeloptBaseConfig]:
+        """Specifies the config class for the mode."""
+        return SparseMagnitudeConfig
+
+    @property
+    def next_modes(self) -> Optional[Set[str]]:
+        """Specifies the next modes for the mode."""
+        return {"export_sparse", "kd_loss", "quantize"}
+
+    @property
+    def export_mode(self) -> Optional[str]:
+        """The mode that corresponds to the export mode of this mode."""
+        return "export_sparse"
+
+    @property
+    def search_algorithm(self) -> Type[BaseSearcher]:
+        """Specifies the search algorithm for the mode."""
+        return MagnitudeSearcher
+
+    @property
+    def convert(self) -> ConvertEntrypoint:
+        """The mode's entrypoint for converting a model."""
+        return convert_sparse_model
+
+    @property
+    def restore(self) -> RestoreEntrypoint:
+        """The mode's entrypoint for restoring a model."""
+        return restore_sparse_model
+
+    @property
+    def update_for_save(self) -> UpdateEntrypoint:
+        """The mode's entrypoint for updating the models metadata."""
+        return update_sparse_metadata
+
+    @property
+    def update_for_new_mode(self) -> UpdateEntrypoint:
+        """The mode's entrypoint for updating the models metadata."""
+        return update_sparse_metadata
+
+
+@SparsityModeRegistry.register_mode
+class SparseGPTModeDescriptor(SparseMagnitudeModeDescriptor):
+    """Class to define and describe sparsification based on SparseGPT."""
+
+    @property
+    def name(self) -> str:
+        """Returns the name of the mode."""
+        return "sparsegpt"
+
+    @property
+    def config_class(self) -> Type[ModeloptBaseConfig]:
+        """Specifies the config class for the mode."""
+        return SparseGPTConfig
+
+    @property
+    def search_algorithm(self) -> Type[BaseSearcher]:
+        """Specifies the search algorithm for the mode."""
+        return SparseGPTSearcher
+
+
+@SparsityModeRegistry.register_mode
+class ExportSparseModeDescriptor(_ModeDescriptor):
+    """Class to describe the ``"export_sparse"`` mode.
+
+    The properties of this mode can be inspected via the source code.
+    """
+
+    @property
+    def name(self) -> str:
+        """Returns the value (str representation) of the mode."""
+        return "export_sparse"
+
+    @property
+    def config_class(self) -> Type[ModeloptBaseConfig]:
+        """Specifies the config class for the mode."""
+        return ExportSparseConfig
+
+    @property
+    def is_export_mode(self) -> bool:
+        """Specifies if this mode is an export mode."""
+        return True
+
+    @property
+    def convert(self) -> ConvertEntrypoint:
+        """The mode's entrypoint for converting a model."""
+        return export_sparse
+
+    @property
+    def restore(self) -> RestoreEntrypoint:
+        """The mode's entrypoint for restoring a model."""
+        return restore_export_sparse
```

## modelopt/torch/sparsity/module.py

 * *Ordering differences only*

```diff
@@ -1,84 +1,84 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Dynamic class for all sparse modules."""
-
-from typing import Optional
-
-import torch
-from torch import nn
-
-from modelopt.torch.opt.dynamic import DynamicModule, _DMRegistryCls
-from modelopt.torch.opt.hparam import Hparam
-
-__all__ = ["SparseModule", "SpDMRegistry"]
-
-
-SpDMRegistry = _DMRegistryCls(prefix="Sparse")  # global instance for the sparsity registry
-
-
-@SpDMRegistry.register({nn.Linear: "nn.Linear", nn.Conv2d: "nn.Conv2d"})
-class SparseModule(DynamicModule):
-    """Base dynamic class for all sparse modules."""
-
-    @staticmethod
-    def _get_weight(mod: "SparseModule", weight: torch.Tensor) -> torch.Tensor:
-        if mod.is_sparse and mod._weight_mask is not None:
-            masked_weight = weight * mod._weight_mask
-            # Quick workaround for the custom attribute for Megatron.
-            # TODO: maybe we need a more general way for customized attributes
-            if hasattr(weight, "main_grad"):
-                masked_weight.main_grad = weight.main_grad
-            return masked_weight
-        return weight
-
-    def _setup(self):
-        # define hparam to check if sparsity is activated
-        hp = Hparam([0, -1], original=0)
-        hp.active = 0
-        self._register_hparam("is_sparse", hp)
-
-        # define the sparse mask here (don't pre-allocate memory to maximize memory savings)
-        self._register_temp_attribute("_weight_mask", None, lambda m, n, v: m.register_buffer(n, v))
-
-        # register dynamic attributes of the class
-        self._register_dynamic_attribute("weight", self._get_weight)
-
-    def modify(self, *args, **kwargs):
-        """Initialize the sparsity mask when this is called.
-
-        Note that for any module that is not frozen via ``None`` in the rules, this function will be
-        called. Hence, we use this function to initialize the sparsity mask only when necessary.
-        """
-        hp = self.get_hparam("is_sparse")
-        if -1 in hp.choices and self._weight_mask is None:
-            hp.active = -1
-            self._weight_mask = torch.ones_like(self.weight, dtype=torch.bool)
-
-    def set_mask(self, value: Optional[torch.BoolTensor]):
-        """Set the active sparse mask of the module weights."""
-        if value is None:
-            self._weight_mask = None
-            return
-
-        # sanity checks on the mask
-        w_shape = self.weight.shape
-        assert value is not None, "Mask cannot be None."
-        assert value.shape == w_shape, f"Mask must have shape {w_shape}, got {value.shape} instead."
-        assert value.dtype == torch.bool, f"Mask must be of type torch.bool, but got {value.dtype}."
-
-        # assign mask
-        with torch.no_grad():
-            if torch.all(value):
-                self._weight_mask = None
-            elif self._weight_mask is None:
-                self._weight_mask = value.detach().clone().to(self.weight.device)
-            else:
-                self._weight_mask.copy_(value.to(self._weight_mask.device))
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Dynamic class for all sparse modules."""
+
+from typing import Optional
+
+import torch
+from torch import nn
+
+from modelopt.torch.opt.dynamic import DynamicModule, _DMRegistryCls
+from modelopt.torch.opt.hparam import Hparam
+
+__all__ = ["SparseModule", "SpDMRegistry"]
+
+
+SpDMRegistry = _DMRegistryCls(prefix="Sparse")  # global instance for the sparsity registry
+
+
+@SpDMRegistry.register({nn.Linear: "nn.Linear", nn.Conv2d: "nn.Conv2d"})
+class SparseModule(DynamicModule):
+    """Base dynamic class for all sparse modules."""
+
+    @staticmethod
+    def _get_weight(mod: "SparseModule", weight: torch.Tensor) -> torch.Tensor:
+        if mod.is_sparse and mod._weight_mask is not None:
+            masked_weight = weight * mod._weight_mask
+            # Quick workaround for the custom attribute for Megatron.
+            # TODO: maybe we need a more general way for customized attributes
+            if hasattr(weight, "main_grad"):
+                masked_weight.main_grad = weight.main_grad
+            return masked_weight
+        return weight
+
+    def _setup(self):
+        # define hparam to check if sparsity is activated
+        hp = Hparam([0, -1], original=0)
+        hp.active = 0
+        self._register_hparam("is_sparse", hp)
+
+        # define the sparse mask here (don't pre-allocate memory to maximize memory savings)
+        self._register_temp_attribute("_weight_mask", None, lambda m, n, v: m.register_buffer(n, v))
+
+        # register dynamic attributes of the class
+        self._register_dynamic_attribute("weight", self._get_weight)
+
+    def modify(self, *args, **kwargs):
+        """Initialize the sparsity mask when this is called.
+
+        Note that for any module that is not frozen via ``None`` in the rules, this function will be
+        called. Hence, we use this function to initialize the sparsity mask only when necessary.
+        """
+        hp = self.get_hparam("is_sparse")
+        if -1 in hp.choices and self._weight_mask is None:
+            hp.active = -1
+            self._weight_mask = torch.ones_like(self.weight, dtype=torch.bool)
+
+    def set_mask(self, value: Optional[torch.BoolTensor]):
+        """Set the active sparse mask of the module weights."""
+        if value is None:
+            self._weight_mask = None
+            return
+
+        # sanity checks on the mask
+        w_shape = self.weight.shape
+        assert value is not None, "Mask cannot be None."
+        assert value.shape == w_shape, f"Mask must have shape {w_shape}, got {value.shape} instead."
+        assert value.dtype == torch.bool, f"Mask must be of type torch.bool, but got {value.dtype}."
+
+        # assign mask
+        with torch.no_grad():
+            if torch.all(value):
+                self._weight_mask = None
+            elif self._weight_mask is None:
+                self._weight_mask = value.detach().clone().to(self.weight.device)
+            else:
+                self._weight_mask.copy_(value.to(self._weight_mask.device))
```

## modelopt/torch/sparsity/searcher.py

 * *Ordering differences only*

```diff
@@ -1,83 +1,83 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Searcher interface for sparsity algorithms."""
-
-from abc import abstractmethod
-from typing import Iterator, Optional, Tuple
-
-import torch
-import torch.nn as nn
-
-from modelopt.torch.opt.searcher import BaseSearcher, SearchConfig, SearchStateDict
-from modelopt.torch.utils import print_rank_0
-
-from . import magnitude as asp
-from .module import SparseModule
-
-
-class BaseSparseSearcher(BaseSearcher):
-    """A generic sparse mask searching algorithm."""
-
-    _pattern_2_4 = "2:4 sparsity"
-
-    @property
-    def default_search_config(self) -> SearchConfig:
-        """Get the default config for the searcher."""
-        return {
-            **super().default_search_config,
-            "score_func": lambda _: 0.0,
-            "pattern": self._pattern_2_4,
-        }
-
-    @property
-    def default_state_dict(self) -> SearchStateDict:
-        """Return default state dict."""
-        return {}
-
-    def sanitize_search_config(self, config: Optional[SearchConfig]) -> SearchStateDict:
-        """Sanitize the search config dict."""
-        config_sanitized = super().sanitize_search_config(config)
-
-        # sanity check of sparsity format
-        is_nm_prune, n, m = asp.get_nmprune_info(config_sanitized["pattern"])
-        assert (
-            is_nm_prune and n == 2 and m == 4
-        ), f"Unsupported pattern {self.config['pattern']} for sparsity"
-
-        return config_sanitized
-
-    @abstractmethod
-    def _check_weight_size(self, weight, mod_name) -> bool:
-        """Check if the weight size is supported by the algorithm."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def _compute_mask(self, module: SparseModule) -> torch.BoolTensor:
-        """Compute the mask and update weight for a given module."""
-        raise NotImplementedError
-
-    def _named_sparsifiable_modules(self) -> Iterator[Tuple[str, nn.Module]]:
-        """Get the named sparsifiable modules."""
-        for name, module in self.model.named_modules():
-            if (
-                isinstance(module, SparseModule)
-                and module.is_sparse
-                and self._check_weight_size(module.weight, name)
-            ):
-                yield name, module
-
-    def run_search(self):
-        """Search for sparse mask."""
-        for name, module in self._named_sparsifiable_modules():
-            # compute the mask (and potentially weight update inside compute_mask)
-            print_rank_0(f"Searching for sparse mask and weight update for module {name}.")
-            with torch.no_grad():
-                module.set_mask(self._compute_mask(module))
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Searcher interface for sparsity algorithms."""
+
+from abc import abstractmethod
+from typing import Iterator, Optional, Tuple
+
+import torch
+import torch.nn as nn
+
+from modelopt.torch.opt.searcher import BaseSearcher, SearchConfig, SearchStateDict
+from modelopt.torch.utils import print_rank_0
+
+from . import magnitude as asp
+from .module import SparseModule
+
+
+class BaseSparseSearcher(BaseSearcher):
+    """A generic sparse mask searching algorithm."""
+
+    _pattern_2_4 = "2:4 sparsity"
+
+    @property
+    def default_search_config(self) -> SearchConfig:
+        """Get the default config for the searcher."""
+        return {
+            **super().default_search_config,
+            "score_func": lambda _: 0.0,
+            "pattern": self._pattern_2_4,
+        }
+
+    @property
+    def default_state_dict(self) -> SearchStateDict:
+        """Return default state dict."""
+        return {}
+
+    def sanitize_search_config(self, config: Optional[SearchConfig]) -> SearchStateDict:
+        """Sanitize the search config dict."""
+        config_sanitized = super().sanitize_search_config(config)
+
+        # sanity check of sparsity format
+        is_nm_prune, n, m = asp.get_nmprune_info(config_sanitized["pattern"])
+        assert (
+            is_nm_prune and n == 2 and m == 4
+        ), f"Unsupported pattern {self.config['pattern']} for sparsity"
+
+        return config_sanitized
+
+    @abstractmethod
+    def _check_weight_size(self, weight, mod_name) -> bool:
+        """Check if the weight size is supported by the algorithm."""
+        raise NotImplementedError
+
+    @abstractmethod
+    def _compute_mask(self, module: SparseModule) -> torch.BoolTensor:
+        """Compute the mask and update weight for a given module."""
+        raise NotImplementedError
+
+    def _named_sparsifiable_modules(self) -> Iterator[Tuple[str, nn.Module]]:
+        """Get the named sparsifiable modules."""
+        for name, module in self.model.named_modules():
+            if (
+                isinstance(module, SparseModule)
+                and module.is_sparse
+                and self._check_weight_size(module.weight, name)
+            ):
+                yield name, module
+
+    def run_search(self):
+        """Search for sparse mask."""
+        for name, module in self._named_sparsifiable_modules():
+            # compute the mask (and potentially weight update inside compute_mask)
+            print_rank_0(f"Searching for sparse mask and weight update for module {name}.")
+            with torch.no_grad():
+                module.set_mask(self._compute_mask(module))
```

## modelopt/torch/sparsity/sparsegpt.py

 * *Ordering differences only*

```diff
@@ -1,268 +1,268 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions of SparseGPT."""
-
-import math
-import warnings
-from typing import Tuple
-
-import torch
-import torch.nn as nn
-
-from modelopt.torch.opt.searcher import SearchConfig
-from modelopt.torch.utils import print_rank_0
-
-from .magnitude import get_nmprune_info
-from .module import SparseModule
-from .searcher import BaseSparseSearcher
-
-
-def invert(hessian: torch.Tensor) -> torch.Tensor:
-    """Invert a Hessian matrix."""
-    try:
-        hessian_inv = torch.linalg.cholesky(hessian)
-        hessian_inv = torch.cholesky_inverse(hessian_inv)
-        hessian_inv = torch.linalg.cholesky(hessian_inv, upper=True)
-    except RuntimeError:
-        cols = hessian.size(0)
-        eps = 1e-6 * torch.eye(cols).to(hessian.device)
-        hessian_inv = torch.cholesky_inverse(torch.linalg.cholesky(hessian + eps))
-
-    return hessian_inv
-
-
-def prepare(
-    tensor: torch.Tensor, hessian: torch.Tensor, hessian_damp: float
-) -> Tuple[torch.Tensor, torch.Tensor]:
-    """Prepare the inverse Hessian matrix."""
-    weight = tensor.detach().clone()
-    # move the hessian matrix from CPU to GPU for acceleration
-    hessian = hessian.to(weight.device)
-    if len(weight.size()) == 4:
-        weight = weight.flatten(1)
-
-    zero = torch.diag(hessian) == 0
-    hessian[zero, zero] = 1
-    weight[:, zero] = 0
-
-    damp = hessian_damp * torch.mean(torch.diag(hessian))
-    cols = weight.size(1)
-    diag = torch.arange(cols)
-    hessian[diag, diag] += damp
-
-    hessian_inv = invert(hessian)
-
-    # remove the Hessian matrix to save GPU memory
-    del hessian
-    torch.cuda.empty_cache()
-
-    return weight, hessian_inv
-
-
-@torch.no_grad()
-def create_sgpt_mask(
-    tensor: torch.Tensor, hessian: torch.Tensor, config: SearchConfig
-) -> torch.Tensor:
-    """Create a sparse mask for the given tensor."""
-    shape = tensor.size()
-    weight, hessian_inv = prepare(tensor, hessian, config["hessian_damp"])
-    rows, cols = weight.size()
-    hessian_inv_diag = torch.diagonal(hessian_inv, dim1=0, dim2=1)
-
-    is_nm_prune, n, m = get_nmprune_info(config["pattern"])
-    col_bs = config["col_block_size"]
-    row_bs = config["row_block_size"]
-    # if row_bs is not specified, prune the whole weight block
-    if row_bs == -1:
-        row_bs = rows
-
-    for r1 in range(0, rows, row_bs):
-        r2 = min(r1 + row_bs, rows)
-        # the mask of the weights not to be pruned
-        w_rows = weight[r1:r2].float()
-
-        # pruning the weight block W[row:row+row_bs, i1:i1+col_bs]
-        for i1 in range(0, cols, col_bs):
-            i2 = min(i1 + col_bs, cols)
-            w_blk = w_rows[:, i1:i2].clone()
-            q_blk = torch.zeros_like(w_blk)
-            # the error of the weights to be pruned
-            delta_blk = torch.zeros_like(w_blk)
-            hinv_blk = hessian_inv[i1:i2, i1:i2]
-            hinv_diag_blk = hessian_inv_diag[i1:i2]
-
-            errors_blk = (w_blk**2) / (hinv_diag_blk**2 + 1e-9)
-            if torch.isnan(errors_blk).any():
-                print("nan in errors_blk.")
-
-            mask_blk = torch.zeros_like(w_blk, dtype=torch.bool)
-
-            for j in range(i2 - i1):
-                # compute the error of the weights to be pruned
-                w = w_blk[:, j]
-                d = hinv_diag_blk[j]
-                if is_nm_prune and j % m == 0:
-                    errors_blk = (w_blk[:, j : j + m] ** 2) / (hinv_diag_blk[j : j + m] ** 2 + 1e-9)
-                    mask_blk.scatter_(
-                        1, j + torch.topk(errors_blk, n, dim=1, largest=False)[1], True
-                    )
-
-                q = w.clone()
-                q[mask_blk[:, j]] = 0
-                q_blk[:, j] = q
-
-                # update the remaining weights in the col_bs block to compensate the error caused by pruning W[:, j]
-                err = (w - q) / d
-                w_blk[:, j:] -= err.unsqueeze(1).matmul(hinv_blk[j, j:].unsqueeze(0))
-                delta_blk[:, j] = err
-
-            # compensate the error caused by pruning W[:, i: i + col_bs] with the weights update in W[:, i + col_bs:]
-            w_rows[:, i1:i2] = q_blk
-            w_rows[:, i2:] -= delta_blk.matmul(hessian_inv[i1:i2, i2:])
-            if torch.isnan(w_rows[:, i2:]).any():
-                print("nan")
-
-        weight[r1:r2] = w_rows
-
-    mask = weight != 0
-
-    return mask.view(shape)
-
-
-class SparseGPTSearcher(BaseSparseSearcher):
-    """SparseGPT-based sparse mask searching algorithm."""
-
-    @property
-    def default_search_config(self) -> SearchConfig:
-        """Get the default config for the searcher."""
-        return {
-            **super().default_search_config,
-            "col_block_size": 128,  # column block size in sparsegpt
-            "row_block_size": -1,  # row block size in sparsegpt
-            "hessian_damp": 0.1,  # hessian damp in sparsegpt
-            "calib_size": 256,  # calibration size for hessian matrix calculation
-            "device": "cuda",  # device of hessian matrix
-        }
-
-    def _check_weight_size(self, weight, mod_name) -> bool:
-        """Check if the weight size is supported by SparseGPT."""
-        _, _, m = get_nmprune_info(self.config["pattern"])
-
-        # the column size must be divisible by m
-        if weight.size(0) % m != 0 or weight.size(1) % m != 0:
-            warnings.warn(
-                f"Skipping pruning {mod_name} of size={str(weight.size())} and"
-                f" type={str(weight.dtype)} for SparseGPT"
-            )
-            return False
-
-        return True
-
-    def _compute_mask(self, module: SparseModule) -> torch.BoolTensor:
-        """Compute the mask (and weight update) for the given module."""
-        return create_sgpt_mask(module.weight, module.hessian, self.config)
-
-    def before_search(self):
-        """Register the forward hook to collect the hessian matrix."""
-        handles = []
-        for _, module in self._named_sparsifiable_modules():
-            # setup and register the forward hook
-            self._setup_forward_hook(module)
-            handles.append(module.register_forward_hook(self._hook_compute_hessian))
-
-        print_rank_0(f"Collecting Hessian statistics for {len(handles)} modules.")
-
-        # run a forward loop to collect the hessian matrix
-        assert self.forward_loop is not None, "Please provide `data_loader` or `forward_loop`!"
-        self.forward_loop(self.model)
-
-        # remove the forward hooks
-        for handle in handles:
-            handle.remove()
-
-    def after_search(self):
-        """Remove Hessian artifcats from network."""
-        for _, module in self._named_sparsifiable_modules():
-            del module.hessian
-            del module.samples
-
-    @staticmethod
-    def _is_memory_sufficient(device_id, threshold):
-        """Check if the memory usage on the CUDA device is below the threshold."""
-        total_memory = torch.cuda.get_device_properties(device_id).total_memory
-        allocated_memory = torch.cuda.memory_allocated(device_id)
-        free_memory = total_memory - allocated_memory
-        return free_memory / total_memory > (1 - threshold)
-
-    @classmethod
-    def _setup_forward_hook(cls, mod: SparseModule) -> None:
-        """Setup the attributes we need for our forward hook during the SparseGPT search."""
-        # initialize the hessian matrix
-        if isinstance(mod, nn.Conv2d):
-            # For conv2d layers, the hessian matrix is calculated as X * X^T, where X is the
-            # flattened weight matrix.
-            cols = mod.weight.size(1) * mod.weight.size(2) * mod.weight.size(3)
-        else:
-            # For linear layers, the hessian matrix is calculated as X * X^T, where X is the
-            # weight matrix.
-            cols = mod.weight.size(1)
-
-        target_device = mod.weight.device
-        # Hessian matrix is stored in the GPU memory by default
-        if target_device.type == "cuda" and cls._is_memory_sufficient(target_device.index, 0.8):
-            hessian = torch.zeros((cols, cols), dtype=torch.float32).to(target_device)
-        else:
-            hessian = torch.zeros((cols, cols), dtype=torch.float32).to("cpu")
-
-        # store the hessian matrix and the number of samples
-        # TODO: this should probably be improved eventually!!
-        mod.hessian = hessian
-        mod.samples = 0
-
-    @classmethod
-    def _hook_compute_hessian(cls, mod: nn.Module, inp: torch.Tensor, out: torch.Tensor):
-        with torch.inference_mode():
-            # TODO: move the hessian matrix to GPU memory if possible
-            if isinstance(inp, tuple):
-                inp = inp[0]
-            # use torch.float32 to avoid overflow in Hessian
-            if len(inp.shape) == 2:
-                inp = inp.unsqueeze(0)
-            tmp = inp.shape[0]
-            # nn.Linear and *ParallelLinear in mcore
-            if "Linear" in type(mod).__name__:
-                if len(inp.shape) == 3:
-                    inp = inp.reshape((-1, inp.shape[-1]))
-                inp.t_()
-            if isinstance(mod, nn.Conv2d):
-                unfold = nn.Unfold(
-                    mod.kernel_size,
-                    dilation=mod.dilation,
-                    stride=mod.stride,
-                )
-                inp = unfold(inp)
-                inp = inp.permute([1, 0, 2])
-                inp = inp.flatten(1)
-            mod.hessian *= mod.samples / (mod.samples + tmp)
-            mod.samples += tmp
-            inp = math.sqrt(2 / mod.samples) * inp.float()
-
-            # the hessian matrix is calculated as X * X^T
-            target_device = mod.hessian.device
-            if mod.hessian.device.type == "cuda":
-                if cls._is_memory_sufficient(mod.hessian.device.index, 0.8):
-                    mod.hessian += inp.matmul(inp.t()).to(mod.hessian.device)
-                else:
-                    target_device = "cpu"
-                    mod.hessian = mod.hessian.to("cpu")
-            mod.hessian += inp.matmul(inp.t()).to(target_device)
-
-            assert not torch.isinf(mod.hessian).any(), "Hessian contains inf"
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions of SparseGPT."""
+
+import math
+import warnings
+from typing import Tuple
+
+import torch
+import torch.nn as nn
+
+from modelopt.torch.opt.searcher import SearchConfig
+from modelopt.torch.utils import print_rank_0
+
+from .magnitude import get_nmprune_info
+from .module import SparseModule
+from .searcher import BaseSparseSearcher
+
+
+def invert(hessian: torch.Tensor) -> torch.Tensor:
+    """Invert a Hessian matrix."""
+    try:
+        hessian_inv = torch.linalg.cholesky(hessian)
+        hessian_inv = torch.cholesky_inverse(hessian_inv)
+        hessian_inv = torch.linalg.cholesky(hessian_inv, upper=True)
+    except RuntimeError:
+        cols = hessian.size(0)
+        eps = 1e-6 * torch.eye(cols).to(hessian.device)
+        hessian_inv = torch.cholesky_inverse(torch.linalg.cholesky(hessian + eps))
+
+    return hessian_inv
+
+
+def prepare(
+    tensor: torch.Tensor, hessian: torch.Tensor, hessian_damp: float
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """Prepare the inverse Hessian matrix."""
+    weight = tensor.detach().clone()
+    # move the hessian matrix from CPU to GPU for acceleration
+    hessian = hessian.to(weight.device)
+    if len(weight.size()) == 4:
+        weight = weight.flatten(1)
+
+    zero = torch.diag(hessian) == 0
+    hessian[zero, zero] = 1
+    weight[:, zero] = 0
+
+    damp = hessian_damp * torch.mean(torch.diag(hessian))
+    cols = weight.size(1)
+    diag = torch.arange(cols)
+    hessian[diag, diag] += damp
+
+    hessian_inv = invert(hessian)
+
+    # remove the Hessian matrix to save GPU memory
+    del hessian
+    torch.cuda.empty_cache()
+
+    return weight, hessian_inv
+
+
+@torch.no_grad()
+def create_sgpt_mask(
+    tensor: torch.Tensor, hessian: torch.Tensor, config: SearchConfig
+) -> torch.Tensor:
+    """Create a sparse mask for the given tensor."""
+    shape = tensor.size()
+    weight, hessian_inv = prepare(tensor, hessian, config["hessian_damp"])
+    rows, cols = weight.size()
+    hessian_inv_diag = torch.diagonal(hessian_inv, dim1=0, dim2=1)
+
+    is_nm_prune, n, m = get_nmprune_info(config["pattern"])
+    col_bs = config["col_block_size"]
+    row_bs = config["row_block_size"]
+    # if row_bs is not specified, prune the whole weight block
+    if row_bs == -1:
+        row_bs = rows
+
+    for r1 in range(0, rows, row_bs):
+        r2 = min(r1 + row_bs, rows)
+        # the mask of the weights not to be pruned
+        w_rows = weight[r1:r2].float()
+
+        # pruning the weight block W[row:row+row_bs, i1:i1+col_bs]
+        for i1 in range(0, cols, col_bs):
+            i2 = min(i1 + col_bs, cols)
+            w_blk = w_rows[:, i1:i2].clone()
+            q_blk = torch.zeros_like(w_blk)
+            # the error of the weights to be pruned
+            delta_blk = torch.zeros_like(w_blk)
+            hinv_blk = hessian_inv[i1:i2, i1:i2]
+            hinv_diag_blk = hessian_inv_diag[i1:i2]
+
+            errors_blk = (w_blk**2) / (hinv_diag_blk**2 + 1e-9)
+            if torch.isnan(errors_blk).any():
+                print("nan in errors_blk.")
+
+            mask_blk = torch.zeros_like(w_blk, dtype=torch.bool)
+
+            for j in range(i2 - i1):
+                # compute the error of the weights to be pruned
+                w = w_blk[:, j]
+                d = hinv_diag_blk[j]
+                if is_nm_prune and j % m == 0:
+                    errors_blk = (w_blk[:, j : j + m] ** 2) / (hinv_diag_blk[j : j + m] ** 2 + 1e-9)
+                    mask_blk.scatter_(
+                        1, j + torch.topk(errors_blk, n, dim=1, largest=False)[1], True
+                    )
+
+                q = w.clone()
+                q[mask_blk[:, j]] = 0
+                q_blk[:, j] = q
+
+                # update the remaining weights in the col_bs block to compensate the error caused by pruning W[:, j]
+                err = (w - q) / d
+                w_blk[:, j:] -= err.unsqueeze(1).matmul(hinv_blk[j, j:].unsqueeze(0))
+                delta_blk[:, j] = err
+
+            # compensate the error caused by pruning W[:, i: i + col_bs] with the weights update in W[:, i + col_bs:]
+            w_rows[:, i1:i2] = q_blk
+            w_rows[:, i2:] -= delta_blk.matmul(hessian_inv[i1:i2, i2:])
+            if torch.isnan(w_rows[:, i2:]).any():
+                print("nan")
+
+        weight[r1:r2] = w_rows
+
+    mask = weight != 0
+
+    return mask.view(shape)
+
+
+class SparseGPTSearcher(BaseSparseSearcher):
+    """SparseGPT-based sparse mask searching algorithm."""
+
+    @property
+    def default_search_config(self) -> SearchConfig:
+        """Get the default config for the searcher."""
+        return {
+            **super().default_search_config,
+            "col_block_size": 128,  # column block size in sparsegpt
+            "row_block_size": -1,  # row block size in sparsegpt
+            "hessian_damp": 0.1,  # hessian damp in sparsegpt
+            "calib_size": 256,  # calibration size for hessian matrix calculation
+            "device": "cuda",  # device of hessian matrix
+        }
+
+    def _check_weight_size(self, weight, mod_name) -> bool:
+        """Check if the weight size is supported by SparseGPT."""
+        _, _, m = get_nmprune_info(self.config["pattern"])
+
+        # the column size must be divisible by m
+        if weight.size(0) % m != 0 or weight.size(1) % m != 0:
+            warnings.warn(
+                f"Skipping pruning {mod_name} of size={str(weight.size())} and"
+                f" type={str(weight.dtype)} for SparseGPT"
+            )
+            return False
+
+        return True
+
+    def _compute_mask(self, module: SparseModule) -> torch.BoolTensor:
+        """Compute the mask (and weight update) for the given module."""
+        return create_sgpt_mask(module.weight, module.hessian, self.config)
+
+    def before_search(self):
+        """Register the forward hook to collect the hessian matrix."""
+        handles = []
+        for _, module in self._named_sparsifiable_modules():
+            # setup and register the forward hook
+            self._setup_forward_hook(module)
+            handles.append(module.register_forward_hook(self._hook_compute_hessian))
+
+        print_rank_0(f"Collecting Hessian statistics for {len(handles)} modules.")
+
+        # run a forward loop to collect the hessian matrix
+        assert self.forward_loop is not None, "Please provide `data_loader` or `forward_loop`!"
+        self.forward_loop(self.model)
+
+        # remove the forward hooks
+        for handle in handles:
+            handle.remove()
+
+    def after_search(self):
+        """Remove Hessian artifcats from network."""
+        for _, module in self._named_sparsifiable_modules():
+            del module.hessian
+            del module.samples
+
+    @staticmethod
+    def _is_memory_sufficient(device_id, threshold):
+        """Check if the memory usage on the CUDA device is below the threshold."""
+        total_memory = torch.cuda.get_device_properties(device_id).total_memory
+        allocated_memory = torch.cuda.memory_allocated(device_id)
+        free_memory = total_memory - allocated_memory
+        return free_memory / total_memory > (1 - threshold)
+
+    @classmethod
+    def _setup_forward_hook(cls, mod: SparseModule) -> None:
+        """Setup the attributes we need for our forward hook during the SparseGPT search."""
+        # initialize the hessian matrix
+        if isinstance(mod, nn.Conv2d):
+            # For conv2d layers, the hessian matrix is calculated as X * X^T, where X is the
+            # flattened weight matrix.
+            cols = mod.weight.size(1) * mod.weight.size(2) * mod.weight.size(3)
+        else:
+            # For linear layers, the hessian matrix is calculated as X * X^T, where X is the
+            # weight matrix.
+            cols = mod.weight.size(1)
+
+        target_device = mod.weight.device
+        # Hessian matrix is stored in the GPU memory by default
+        if target_device.type == "cuda" and cls._is_memory_sufficient(target_device.index, 0.8):
+            hessian = torch.zeros((cols, cols), dtype=torch.float32).to(target_device)
+        else:
+            hessian = torch.zeros((cols, cols), dtype=torch.float32).to("cpu")
+
+        # store the hessian matrix and the number of samples
+        # TODO: this should probably be improved eventually!!
+        mod.hessian = hessian
+        mod.samples = 0
+
+    @classmethod
+    def _hook_compute_hessian(cls, mod: nn.Module, inp: torch.Tensor, out: torch.Tensor):
+        with torch.inference_mode():
+            # TODO: move the hessian matrix to GPU memory if possible
+            if isinstance(inp, tuple):
+                inp = inp[0]
+            # use torch.float32 to avoid overflow in Hessian
+            if len(inp.shape) == 2:
+                inp = inp.unsqueeze(0)
+            tmp = inp.shape[0]
+            # nn.Linear and *ParallelLinear in mcore
+            if "Linear" in type(mod).__name__:
+                if len(inp.shape) == 3:
+                    inp = inp.reshape((-1, inp.shape[-1]))
+                inp.t_()
+            if isinstance(mod, nn.Conv2d):
+                unfold = nn.Unfold(
+                    mod.kernel_size,
+                    dilation=mod.dilation,
+                    stride=mod.stride,
+                )
+                inp = unfold(inp)
+                inp = inp.permute([1, 0, 2])
+                inp = inp.flatten(1)
+            mod.hessian *= mod.samples / (mod.samples + tmp)
+            mod.samples += tmp
+            inp = math.sqrt(2 / mod.samples) * inp.float()
+
+            # the hessian matrix is calculated as X * X^T
+            target_device = mod.hessian.device
+            if mod.hessian.device.type == "cuda":
+                if cls._is_memory_sufficient(mod.hessian.device.index, 0.8):
+                    mod.hessian += inp.matmul(inp.t()).to(mod.hessian.device)
+                else:
+                    target_device = "cpu"
+                    mod.hessian = mod.hessian.to("cpu")
+            mod.hessian += inp.matmul(inp.t()).to(target_device)
+
+            assert not torch.isinf(mod.hessian).any(), "Hessian contains inf"
```

## modelopt/torch/sparsity/sparsification.py

 * *Ordering differences only*

```diff
@@ -1,117 +1,117 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""High-level API to automatically sparsify your model with various algorithms."""
-from typing import Any, Dict, Optional, Tuple, Type
-
-from torch import nn
-
-from modelopt.torch.opt.conversion import apply_mode, get_mode
-from modelopt.torch.opt.mode import ModeLike
-from modelopt.torch.opt.searcher import BaseSearcher, SearchConfig
-from modelopt.torch.utils import unwrap_model
-
-from .mode import SparsityModeRegistry
-
-__all__ = ["sparsify", "export"]
-
-
-def sparsify(
-    model: nn.Module, mode: ModeLike, config: Optional[SearchConfig] = None
-) -> Tuple[nn.Module, Dict[str, Any]]:
-    """Sparsify a given model and search for they optimal sparsified weights.
-
-    Args:
-        model: A standard model that contains standard building blocks to be sparsified in-place.
-        mode: A (list of) string(s) or Mode(s) or a list of tuples containing the mode and its
-            config indicating the desired mode(s) (and configurations) for the convert
-            process. Modes set up the model for different algorithms for model optimization. The
-            following modes are available:
-
-            *   :meth:`"sparse_magnitude"<modelopt.torch.sparsity.mode.SparseMagnitudeModeDescriptor>`:
-                The ``model`` will be sparsified according to the magnitude of weights in each
-                layer. The mode's config is described in
-                :meth:`SparseMagnitudeConfig<modelopt.torch.sparsity.config.SparseMagnitudeConfig>`.
-            *   :meth:`"sparsegpt"<modelopt.torch.sparsity.mode.SparseGPTModeDescriptor>`:
-                The ``model`` will be sparsified and weights are updated optimally using an Hessian
-                approximation of the loss function (see SparseGPT paper for details). The mode's
-                config is described in
-                :meth:`SparseGPTConfig<modelopt.torch.sparsity.config.SparseGPTConfig>`.
-
-            If the mode argument is specified as a dictionary, the keys should indicate the mode and
-            the values specify the per-mode configuration. If not provided, then default
-            configuration would be used.
-
-        config: Additional optional arguments to configure the search. Currently, we support:
-
-            * ``verbose``: Whether to print detailed search stats during search.
-            * ``forward_loop``: A ``Callable`` that takes a model as input and runs a forward loop
-                on it. It is recommended to choose the data loader used inside the forward loop
-                carefully to reduce the runtime. Cannot be provided at the same time as
-                ``data_loader`` and ``collect_func``.
-            * ``data_loader``: An iterator yielding batches of data for calibrating the
-              normalization layers in the model or compute gradient scores. It is recommended to use
-              the same data loader as for training but with significantly fewer iterations. Cannot
-              be provided at the same time as ``forward_loop``.
-            * ``collect_func``: A ``Callable`` that takes a batch of data from the data loader as
-              input and returns the input to ``model.forward()`` as described in
-              :meth:`run_forward_loop <modelopt.torch.utils.network.run_forward_loop>`. Cannot
-              be provided at the same time as ``forward_loop``.
-
-            .. note::
-
-                Additional configuration options may be added by individual algorithms. Please
-                refer to the documentation of the individual algorithms for more information.
-
-    Returns: A sparsified model
-
-    .. note::
-
-        The given model is sparsified in-place. The returned model is thus a reference to the same
-        model instance as the input model.
-    """
-    # apply sparsity to the model
-    model = apply_mode(model, mode, registry=SparsityModeRegistry)
-
-    # retrieve searcher class
-    searcher_cls: Type[BaseSearcher] = getattr(get_mode(model), "search_algorithm")
-
-    # run search+sparsification algorithm
-    searcher = searcher_cls()
-    searcher.search(model, {}, tuple(), config)
-
-    # return the sparsified model
-    return model
-
-
-def export(model: nn.Module) -> nn.Module:
-    """Export a sparse dynamic model to a regular model.
-
-    This should be done after the model is fine-tuned and the weights are fixed.
-
-    .. warning::
-
-        After the call to ``export()``, the sparsity mask will no longer be enforced. This means any
-        future weight updates would destroy the sparsity pattern. If you want to continue training,
-        call ``export()`` after training is finished.
-    """
-    # unwrap a DP/DDP model
-    model = unwrap_model(
-        model,
-        warn=True,
-        msg=(
-            f"Unwrapping a {type(model).__name__} model for export! Note that the export is"
-            " in-place and the model wrapper should be re-created after export since the wrapper"
-            " might not support changing parameters after initialization."
-        ),
-    )
-
-    # apply export mode and return model
-    return apply_mode(model, "export_sparse", registry=SparsityModeRegistry)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""High-level API to automatically sparsify your model with various algorithms."""
+from typing import Any, Dict, Optional, Tuple, Type
+
+from torch import nn
+
+from modelopt.torch.opt.conversion import apply_mode, get_mode
+from modelopt.torch.opt.mode import ModeLike
+from modelopt.torch.opt.searcher import BaseSearcher, SearchConfig
+from modelopt.torch.utils import unwrap_model
+
+from .mode import SparsityModeRegistry
+
+__all__ = ["sparsify", "export"]
+
+
+def sparsify(
+    model: nn.Module, mode: ModeLike, config: Optional[SearchConfig] = None
+) -> Tuple[nn.Module, Dict[str, Any]]:
+    """Sparsify a given model and search for they optimal sparsified weights.
+
+    Args:
+        model: A standard model that contains standard building blocks to be sparsified in-place.
+        mode: A (list of) string(s) or Mode(s) or a list of tuples containing the mode and its
+            config indicating the desired mode(s) (and configurations) for the convert
+            process. Modes set up the model for different algorithms for model optimization. The
+            following modes are available:
+
+            *   :meth:`"sparse_magnitude"<modelopt.torch.sparsity.mode.SparseMagnitudeModeDescriptor>`:
+                The ``model`` will be sparsified according to the magnitude of weights in each
+                layer. The mode's config is described in
+                :meth:`SparseMagnitudeConfig<modelopt.torch.sparsity.config.SparseMagnitudeConfig>`.
+            *   :meth:`"sparsegpt"<modelopt.torch.sparsity.mode.SparseGPTModeDescriptor>`:
+                The ``model`` will be sparsified and weights are updated optimally using an Hessian
+                approximation of the loss function (see SparseGPT paper for details). The mode's
+                config is described in
+                :meth:`SparseGPTConfig<modelopt.torch.sparsity.config.SparseGPTConfig>`.
+
+            If the mode argument is specified as a dictionary, the keys should indicate the mode and
+            the values specify the per-mode configuration. If not provided, then default
+            configuration would be used.
+
+        config: Additional optional arguments to configure the search. Currently, we support:
+
+            * ``verbose``: Whether to print detailed search stats during search.
+            * ``forward_loop``: A ``Callable`` that takes a model as input and runs a forward loop
+                on it. It is recommended to choose the data loader used inside the forward loop
+                carefully to reduce the runtime. Cannot be provided at the same time as
+                ``data_loader`` and ``collect_func``.
+            * ``data_loader``: An iterator yielding batches of data for calibrating the
+              normalization layers in the model or compute gradient scores. It is recommended to use
+              the same data loader as for training but with significantly fewer iterations. Cannot
+              be provided at the same time as ``forward_loop``.
+            * ``collect_func``: A ``Callable`` that takes a batch of data from the data loader as
+              input and returns the input to ``model.forward()`` as described in
+              :meth:`run_forward_loop <modelopt.torch.utils.network.run_forward_loop>`. Cannot
+              be provided at the same time as ``forward_loop``.
+
+            .. note::
+
+                Additional configuration options may be added by individual algorithms. Please
+                refer to the documentation of the individual algorithms for more information.
+
+    Returns: A sparsified model
+
+    .. note::
+
+        The given model is sparsified in-place. The returned model is thus a reference to the same
+        model instance as the input model.
+    """
+    # apply sparsity to the model
+    model = apply_mode(model, mode, registry=SparsityModeRegistry)
+
+    # retrieve searcher class
+    searcher_cls: Type[BaseSearcher] = getattr(get_mode(model), "search_algorithm")
+
+    # run search+sparsification algorithm
+    searcher = searcher_cls()
+    searcher.search(model, {}, tuple(), config)
+
+    # return the sparsified model
+    return model
+
+
+def export(model: nn.Module) -> nn.Module:
+    """Export a sparse dynamic model to a regular model.
+
+    This should be done after the model is fine-tuned and the weights are fixed.
+
+    .. warning::
+
+        After the call to ``export()``, the sparsity mask will no longer be enforced. This means any
+        future weight updates would destroy the sparsity pattern. If you want to continue training,
+        call ``export()`` after training is finished.
+    """
+    # unwrap a DP/DDP model
+    model = unwrap_model(
+        model,
+        warn=True,
+        msg=(
+            f"Unwrapping a {type(model).__name__} model for export! Note that the export is"
+            " in-place and the model wrapper should be re-created after export since the wrapper"
+            " might not support changing parameters after initialization."
+        ),
+    )
+
+    # apply export mode and return model
+    return apply_mode(model, "export_sparse", registry=SparsityModeRegistry)
```

## modelopt/torch/sparsity/plugins/__init__.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Handles sparsity plugins for third-party modules.
-
-Currently, we support plugins for
-
-- :meth:`megatron<modelopt.torch.sparsity.plugins.megatron>`
-
-"""
-
-import warnings
-
-try:
-    from .megatron import *
-
-    has_megatron_core = True
-except ImportError:
-    has_megatron_core = False
-except Exception as e:
-    has_megatron_core = False
-    warnings.warn(f"Failed to import megatron plugin due to: {repr(e)}")
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Handles sparsity plugins for third-party modules.
+
+Currently, we support plugins for
+
+- :meth:`megatron<modelopt.torch.sparsity.plugins.megatron>`
+
+"""
+
+import warnings
+
+try:
+    from .megatron import *
+
+    has_megatron_core = True
+except ImportError:
+    has_megatron_core = False
+except Exception as e:
+    has_megatron_core = False
+    warnings.warn(f"Failed to import megatron plugin due to: {repr(e)}")
```

## modelopt/torch/sparsity/plugins/megatron.py

 * *Ordering differences only*

```diff
@@ -1,74 +1,74 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-
-"""Support megatron parallel linear."""
-
-from megatron.core.tensor_parallel.layers import ColumnParallelLinear, RowParallelLinear
-from megatron.core.transformer.utils import make_sharded_tensors_for_checkpoint
-
-from modelopt.torch.sparsity.config import SparseGPTConfig, SparseMagnitudeConfig
-from modelopt.torch.sparsity.module import SparseModule, SpDMRegistry
-
-
-class _MegatronParallelLinear(SparseModule):
-    def _get_shard_axis_dict(self):
-        raise NotImplementedError
-
-    def sharded_state_dict(self, prefix="", sharded_offsets=(), metadata=None):
-        sharded_state_dict = super().sharded_state_dict(prefix, sharded_offsets)
-
-        sparse_state_dict, sharded_axis_dict = {}, self._get_shard_axis_dict()
-        for k, v in self.state_dict(prefix="", keep_vars=True).items():
-            if k == "_weight_mask":
-                sparse_state_dict[k] = v
-        if sparse_state_dict:
-            sharded_state_dict.update(
-                **make_sharded_tensors_for_checkpoint(
-                    sparse_state_dict, prefix, sharded_axis_dict, sharded_offsets
-                )
-            )
-
-        return sharded_state_dict
-
-
-@SpDMRegistry.register(
-    {ColumnParallelLinear: "megatron.core.tensor_parallel.layers.ColumnParallelLinear"}
-)
-class _MegatronColumnParallelLinear(_MegatronParallelLinear):
-    def _get_shard_axis_dict(self):
-        return {"_weight_mask": 0}
-
-
-@SpDMRegistry.register(
-    {RowParallelLinear: "megatron.core.tensor_parallel.layers.RowParallelLinear"}
-)
-class _MegatronRowParallelLinear(_MegatronParallelLinear):
-    def _get_shard_axis_dict(self):
-        return {"_weight_mask": 1}
-
-
-def _get_extra_rules():
-    """Get the extra rules for megatron."""
-    return {
-        "megatron.core.tensor_parallel.layers.ColumnParallelLinear": {
-            "*": {},
-            "*output_layer*": None,
-        },
-        "megatron.core.tensor_parallel.layers.RowParallelLinear": {
-            "*": {},
-            "*output_layer*": None,
-        },
-    }
-
-
-# Update the default rules
-SparseMagnitudeConfig.register_default(_get_extra_rules())
-SparseGPTConfig.register_default(_get_extra_rules())
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+
+"""Support megatron parallel linear."""
+
+from megatron.core.tensor_parallel.layers import ColumnParallelLinear, RowParallelLinear
+from megatron.core.transformer.utils import make_sharded_tensors_for_checkpoint
+
+from modelopt.torch.sparsity.config import SparseGPTConfig, SparseMagnitudeConfig
+from modelopt.torch.sparsity.module import SparseModule, SpDMRegistry
+
+
+class _MegatronParallelLinear(SparseModule):
+    def _get_shard_axis_dict(self):
+        raise NotImplementedError
+
+    def sharded_state_dict(self, prefix="", sharded_offsets=(), metadata=None):
+        sharded_state_dict = super().sharded_state_dict(prefix, sharded_offsets)
+
+        sparse_state_dict, sharded_axis_dict = {}, self._get_shard_axis_dict()
+        for k, v in self.state_dict(prefix="", keep_vars=True).items():
+            if k == "_weight_mask":
+                sparse_state_dict[k] = v
+        if sparse_state_dict:
+            sharded_state_dict.update(
+                **make_sharded_tensors_for_checkpoint(
+                    sparse_state_dict, prefix, sharded_axis_dict, sharded_offsets
+                )
+            )
+
+        return sharded_state_dict
+
+
+@SpDMRegistry.register(
+    {ColumnParallelLinear: "megatron.core.tensor_parallel.layers.ColumnParallelLinear"}
+)
+class _MegatronColumnParallelLinear(_MegatronParallelLinear):
+    def _get_shard_axis_dict(self):
+        return {"_weight_mask": 0}
+
+
+@SpDMRegistry.register(
+    {RowParallelLinear: "megatron.core.tensor_parallel.layers.RowParallelLinear"}
+)
+class _MegatronRowParallelLinear(_MegatronParallelLinear):
+    def _get_shard_axis_dict(self):
+        return {"_weight_mask": 1}
+
+
+def _get_extra_rules():
+    """Get the extra rules for megatron."""
+    return {
+        "megatron.core.tensor_parallel.layers.ColumnParallelLinear": {
+            "*": {},
+            "*output_layer*": None,
+        },
+        "megatron.core.tensor_parallel.layers.RowParallelLinear": {
+            "*": {},
+            "*output_layer*": None,
+        },
+    }
+
+
+# Update the default rules
+SparseMagnitudeConfig.register_default(_get_extra_rules())
+SparseGPTConfig.register_default(_get_extra_rules())
```

## modelopt/torch/utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,20 +1,20 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions."""
-from ._pytree import *
-from .cpp_extension import *
-from .dataset_utils import *
-from .graph import *
-from .list import *
-from .logging import *
-from .network import *
-from .perf import *
-from .tensor import *
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions."""
+from ._pytree import *
+from .cpp_extension import *
+from .dataset_utils import *
+from .graph import *
+from .list import *
+from .logging import *
+from .network import *
+from .perf import *
+from .tensor import *
```

## modelopt/torch/utils/_pytree.py

 * *Ordering differences only*

```diff
@@ -1,128 +1,128 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions for working with nested python data structure.
-
-This utility is inspired by the PyTree utility from PyTorch:
-https://github.com/pytorch/pytorch/blob/main/torch/utils/_pytree.py
-
-From the PyTree documentation:
-
-    A *pytree* is Python nested data structure. It is a tree in the sense that
-    nodes are Python collections (e.g., list, tuple, dict) and the leaves are
-    Python values. Furthermore, a pytree should not contain reference cycles.
-
-    pytrees are useful for working with nested collections of Tensors. For example,
-    one can use `tree_map` to map a function over all Tensors inside some nested
-    collection of Tensors and `tree_unflatten` to get a flat list of all Tensors
-    inside some nested collection. pytrees are helpful for implementing nested
-    collection support for PyTorch APIs.
-
-We use the same terminology for our pytrees but implement a simpler version. Specifically, our tree
-spec is simply the original data structure with the values eliminated to None instead of storing
-a class-based, nested tree spec object.
-"""
-
-from collections import deque
-from typing import Any, Dict, List, Tuple, Union
-
-
-class TreeSpec:
-    """A simple class to hold a tree spec."""
-
-    def __init__(self, pytree: Any, names: List[str]):
-        self.spec = self._fill_spec(None, pytree)
-        self.names = names
-
-    @staticmethod
-    def _fill_spec(values: Union[List, Tuple, Any], spec: Any) -> Any:
-        """Fill the pytree spec with values."""
-        # put fill_values in a deque container to allow for popping it while traversing data
-        values = deque(values) if isinstance(values, (list, tuple)) else values
-
-        def fill(spec):
-            """Eliminate values from output structure from keep structure."""
-            if isinstance(spec, (tuple, list)):
-                return type(spec)([fill(val) for val in spec])
-            elif isinstance(spec, dict):
-                _check_serializable_keys(spec)
-                return {k: fill(val) for k, val in spec.items()}
-            return values.popleft() if isinstance(values, deque) else values
-
-        # eliminate values from output structure
-        data_structure = fill(spec)
-
-        # return output structure
-        return data_structure
-
-    def generate_pytree(self, values: Union[List, Tuple, Any]) -> Any:
-        """Fill the pytree spec with values (non-static version)."""
-        return self._fill_spec(values, self.spec)
-
-    def __eq__(self, other: Any) -> bool:
-        """Compare two tree specs."""
-        return self.spec == other.spec and self.names == other.names
-
-    def __ne__(self, other: Any) -> bool:
-        return not self.__eq__(other)
-
-
-def _check_serializable_keys(data: Dict):
-    """Check if all keys in the data structure are serializable."""
-    allowed_key_types = (float, int, str, type(None), bool)
-    assert all(isinstance(k, allowed_key_types) for k in data.keys()), "Keys must be serializable!"
-
-
-def unflatten_tree(values: Union[List, Tuple, Any], tree_spec: TreeSpec) -> Any:
-    """Return a pytree according to the tree_spec and values filled according to the fillers.
-
-    Args:
-        values: A list/tuple of values or a single value to fill the pytree with.
-
-            * If ``values`` are a list/tuple, then the values in the data structure will be filled
-              in a sequential fashion while traversing the data structure in a DFS manner.
-
-            * Otherwise, ``values`` will be used for every value field in the pytree.
-
-        tree_spec: A pytree spec describing the pytree.
-
-    Returns:
-        A python object structured according to the tree_spec filled with the provided values.
-    """
-    return tree_spec.generate_pytree(values)
-
-
-def flatten_tree(pytree: Any, prefix: str = "") -> Tuple[List[Any], TreeSpec]:
-    """Flatten given pytree with depth-first search.
-
-    Args:
-        pytree: Data structure to flatten.
-        prefix: Prefix for the flattened keys. Defaults to "".
-
-    Returns: A tuple (values, pytree) where
-        values is a list of values flattened from the provided pytree, and
-        tree_spec is the pytree spec describing the structure of the pytree.
-    """
-
-    def collect_spec(pytree, prefix):
-        if isinstance(pytree, dict):
-            _check_serializable_keys(pytree)
-            for key, value in pytree.items():
-                yield from collect_spec(value, prefix + ("." if prefix else "") + str(key))
-        elif isinstance(pytree, (tuple, list)):
-            for i, value in enumerate(pytree):
-                yield from collect_spec(value, prefix + ("." if prefix else "") + str(i))
-        else:
-            yield prefix, pytree
-
-    # retrieve flattened values and names. Then initialize tree_spec with the flattened names.
-    flattened = {n: v for n, v in collect_spec(pytree, prefix)}
-
-    return list(flattened.values()), TreeSpec(pytree, list(flattened.keys()))
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions for working with nested python data structure.
+
+This utility is inspired by the PyTree utility from PyTorch:
+https://github.com/pytorch/pytorch/blob/main/torch/utils/_pytree.py
+
+From the PyTree documentation:
+
+    A *pytree* is Python nested data structure. It is a tree in the sense that
+    nodes are Python collections (e.g., list, tuple, dict) and the leaves are
+    Python values. Furthermore, a pytree should not contain reference cycles.
+
+    pytrees are useful for working with nested collections of Tensors. For example,
+    one can use `tree_map` to map a function over all Tensors inside some nested
+    collection of Tensors and `tree_unflatten` to get a flat list of all Tensors
+    inside some nested collection. pytrees are helpful for implementing nested
+    collection support for PyTorch APIs.
+
+We use the same terminology for our pytrees but implement a simpler version. Specifically, our tree
+spec is simply the original data structure with the values eliminated to None instead of storing
+a class-based, nested tree spec object.
+"""
+
+from collections import deque
+from typing import Any, Dict, List, Tuple, Union
+
+
+class TreeSpec:
+    """A simple class to hold a tree spec."""
+
+    def __init__(self, pytree: Any, names: List[str]):
+        self.spec = self._fill_spec(None, pytree)
+        self.names = names
+
+    @staticmethod
+    def _fill_spec(values: Union[List, Tuple, Any], spec: Any) -> Any:
+        """Fill the pytree spec with values."""
+        # put fill_values in a deque container to allow for popping it while traversing data
+        values = deque(values) if isinstance(values, (list, tuple)) else values
+
+        def fill(spec):
+            """Eliminate values from output structure from keep structure."""
+            if isinstance(spec, (tuple, list)):
+                return type(spec)([fill(val) for val in spec])
+            elif isinstance(spec, dict):
+                _check_serializable_keys(spec)
+                return {k: fill(val) for k, val in spec.items()}
+            return values.popleft() if isinstance(values, deque) else values
+
+        # eliminate values from output structure
+        data_structure = fill(spec)
+
+        # return output structure
+        return data_structure
+
+    def generate_pytree(self, values: Union[List, Tuple, Any]) -> Any:
+        """Fill the pytree spec with values (non-static version)."""
+        return self._fill_spec(values, self.spec)
+
+    def __eq__(self, other: Any) -> bool:
+        """Compare two tree specs."""
+        return self.spec == other.spec and self.names == other.names
+
+    def __ne__(self, other: Any) -> bool:
+        return not self.__eq__(other)
+
+
+def _check_serializable_keys(data: Dict):
+    """Check if all keys in the data structure are serializable."""
+    allowed_key_types = (float, int, str, type(None), bool)
+    assert all(isinstance(k, allowed_key_types) for k in data.keys()), "Keys must be serializable!"
+
+
+def unflatten_tree(values: Union[List, Tuple, Any], tree_spec: TreeSpec) -> Any:
+    """Return a pytree according to the tree_spec and values filled according to the fillers.
+
+    Args:
+        values: A list/tuple of values or a single value to fill the pytree with.
+
+            * If ``values`` are a list/tuple, then the values in the data structure will be filled
+              in a sequential fashion while traversing the data structure in a DFS manner.
+
+            * Otherwise, ``values`` will be used for every value field in the pytree.
+
+        tree_spec: A pytree spec describing the pytree.
+
+    Returns:
+        A python object structured according to the tree_spec filled with the provided values.
+    """
+    return tree_spec.generate_pytree(values)
+
+
+def flatten_tree(pytree: Any, prefix: str = "") -> Tuple[List[Any], TreeSpec]:
+    """Flatten given pytree with depth-first search.
+
+    Args:
+        pytree: Data structure to flatten.
+        prefix: Prefix for the flattened keys. Defaults to "".
+
+    Returns: A tuple (values, pytree) where
+        values is a list of values flattened from the provided pytree, and
+        tree_spec is the pytree spec describing the structure of the pytree.
+    """
+
+    def collect_spec(pytree, prefix):
+        if isinstance(pytree, dict):
+            _check_serializable_keys(pytree)
+            for key, value in pytree.items():
+                yield from collect_spec(value, prefix + ("." if prefix else "") + str(key))
+        elif isinstance(pytree, (tuple, list)):
+            for i, value in enumerate(pytree):
+                yield from collect_spec(value, prefix + ("." if prefix else "") + str(i))
+        else:
+            yield prefix, pytree
+
+    # retrieve flattened values and names. Then initialize tree_spec with the flattened names.
+    flattened = {n: v for n, v in collect_spec(pytree, prefix)}
+
+    return list(flattened.values()), TreeSpec(pytree, list(flattened.keys()))
```

## modelopt/torch/utils/cpp_extension.py

 * *Ordering differences only*

```diff
@@ -1,61 +1,61 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions for loading CPP / CUDA extensions."""
-
-import warnings
-from pathlib import Path
-from types import ModuleType
-from typing import Any, List, Optional, Union
-
-import torch
-from packaging.specifiers import SpecifierSet
-from packaging.version import Version
-from torch.utils.cpp_extension import load
-
-__all__ = ["load_cpp_extension"]
-
-
-def load_cpp_extension(
-    name: str,
-    sources: List[Union[str, Path]],
-    cuda_version_specifiers: Optional[str],
-    fail_msg="",
-    **load_kwargs: Any,
-) -> Optional[ModuleType]:
-    """Load a C++ / CUDA extension using torch.utils.cpp_extension.load() if the current CUDA version satisfies it.
-
-    Loading first time may take a few mins because of the compilation, but subsequent loads are instantaneous.
-
-    Args:
-        name: Name of the extension.
-        sources: Source files to compile.
-        cuda_version_specifiers: Specifier (e.g. ">=11.8,<12") for CUDA versions required to enable the extension.
-        **load_kwargs: Keyword arguments to torch.utils.cpp_extension.load().
-    """
-    if torch.version.cuda is None:
-        return None
-
-    if cuda_version_specifiers and Version(torch.version.cuda) not in SpecifierSet(
-        cuda_version_specifiers
-    ):
-        return None
-
-    try:
-        print(f"Loading extension {name}...")
-        return load(name, sources, **load_kwargs)
-    except Exception as e:
-        # RuntimeError can be raised if there are any errors while compiling the extension.
-        # OSError can be raised if CUDA_HOME path is not set correctly.
-        # subprocess.CalledProcessError can be raised on `-runtime` images where c++ is not installed.
-        warnings.warn(
-            fail_msg or f"{e}\nUnable to load extension {name} and falling back to CPU version."
-        )
-        return None
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions for loading CPP / CUDA extensions."""
+
+import warnings
+from pathlib import Path
+from types import ModuleType
+from typing import Any, List, Optional, Union
+
+import torch
+from packaging.specifiers import SpecifierSet
+from packaging.version import Version
+from torch.utils.cpp_extension import load
+
+__all__ = ["load_cpp_extension"]
+
+
+def load_cpp_extension(
+    name: str,
+    sources: List[Union[str, Path]],
+    cuda_version_specifiers: Optional[str],
+    fail_msg="",
+    **load_kwargs: Any,
+) -> Optional[ModuleType]:
+    """Load a C++ / CUDA extension using torch.utils.cpp_extension.load() if the current CUDA version satisfies it.
+
+    Loading first time may take a few mins because of the compilation, but subsequent loads are instantaneous.
+
+    Args:
+        name: Name of the extension.
+        sources: Source files to compile.
+        cuda_version_specifiers: Specifier (e.g. ">=11.8,<12") for CUDA versions required to enable the extension.
+        **load_kwargs: Keyword arguments to torch.utils.cpp_extension.load().
+    """
+    if torch.version.cuda is None:
+        return None
+
+    if cuda_version_specifiers and Version(torch.version.cuda) not in SpecifierSet(
+        cuda_version_specifiers
+    ):
+        return None
+
+    try:
+        print(f"Loading extension {name}...")
+        return load(name, sources, **load_kwargs)
+    except Exception as e:
+        # RuntimeError can be raised if there are any errors while compiling the extension.
+        # OSError can be raised if CUDA_HOME path is not set correctly.
+        # subprocess.CalledProcessError can be raised on `-runtime` images where c++ is not installed.
+        warnings.warn(
+            fail_msg or f"{e}\nUnable to load extension {name} and falling back to CPU version."
+        )
+        return None
```

## modelopt/torch/utils/dataset_utils.py

 * *Ordering differences only*

```diff
@@ -1,204 +1,204 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions for getting samples and forward loop function for different datasets."""
-from typing import TYPE_CHECKING, Callable, List, Optional, Union
-
-import torch
-from torch.utils.data import DataLoader
-
-if TYPE_CHECKING:
-    from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
-
-# Use dict to store the config for each dataset.
-# If we want to export more options to user like target languages, we need more standardized approach like dataclass.
-SUPPORTED_DATASET_CONFIG = {
-    "cnn_dailymail": {
-        "config": {"path": "cnn_dailymail", "name": "3.0.0"},
-        "target": "article",
-    },
-    "pile": {
-        "config": {"path": "monology/pile-uncopyrighted"},
-        "target": "text",
-    },
-    "pg19": {
-        "config": {"path": "pg19"},
-        "target": "text",
-    },
-    "wikipedia": {
-        "config": {"path": "wikipedia", "name": "20220301.en"},
-        "target": "text",
-    },
-    "c4": {
-        "config": {"path": "c4", "name": "en"},
-        "target": "text",
-    },
-}
-
-__all__ = ["create_forward_loop", "get_dataset_dataloader"]
-
-
-def _get_dataset_samples(dataset_name: str, num_samples: int) -> List[str]:
-    """Load a portion of train dataset with the dataset name and a given size.
-
-    Args:
-        dataset_name: Name of the dataset to load.
-        num_samples: Number of samples to load from the dataset.
-
-    Returns:
-        Smaples: The list of samples.
-    """
-    # Load the dataset
-    if dataset_name in SUPPORTED_DATASET_CONFIG:
-        try:
-            from datasets import load_dataset
-        except ModuleNotFoundError:
-            print(
-                "The 'datasets' module is not installed. Please install it using 'pip install"
-                " datasets' to continue."
-            )
-        # Use streaming can reduce the downloading time for large datasets
-        dataset = load_dataset(
-            split="train", streaming=True, **SUPPORTED_DATASET_CONFIG[dataset_name]["config"]
-        )
-    else:
-        raise NotImplementedError(
-            f"dataset {dataset_name} is not supported. Check available datasets with"
-            " get_supported_datasets."
-        )
-
-    # Access only the required samples
-    samples = []
-    target_key = SUPPORTED_DATASET_CONFIG[dataset_name]["target"]
-    for i, sample in enumerate(dataset):
-        if i >= num_samples:
-            break
-        samples.append(sample[target_key])
-
-    return samples
-
-
-def get_dataset_dataloader(
-    dataset_name: str = "cnn_dailymail",
-    tokenizer: Union["PreTrainedTokenizer", "PreTrainedTokenizerFast"] = None,
-    batch_size: int = 1,
-    num_samples: int = 512,
-    max_sample_length: int = 512,
-    device: Optional[str] = None,
-) -> DataLoader:
-    """Get a dataloader with the dataset name and toknizer of the target model.
-
-    Args:
-        dataset_name: Name of the dataset to load.
-        tokenizer: Instancne of Hugginface tokenizer.
-        batch_size: Batch size of the returned dataloader.
-        num_samples: Number of samples from the dataset.
-        max_sample_length: Maximum length of a sample.
-        device: Target device for the returned dataloader.
-
-    Returns:
-        A instance of dataloader.
-    """
-    assert tokenizer is not None, "Please provide a tokenizer."
-
-    dataset = _get_dataset_samples(dataset_name, num_samples=num_samples)
-
-    batch_encoded = tokenizer.batch_encode_plus(
-        dataset, return_tensors="pt", padding=True, truncation=True, max_length=max_sample_length
-    )
-    if device:
-        batch_encoded = batch_encoded.to(device)
-    batch_encoded = batch_encoded["input_ids"]
-
-    calib_dataloader = DataLoader(batch_encoded, batch_size=batch_size, shuffle=False)
-
-    return calib_dataloader
-
-
-def get_supported_datasets():
-    """Retrieves a list of datasets supported.
-
-    Returns:
-    - list[str]: A list of strings, where each string is the name of a supported dataset.
-
-    Example usage:
-
-    ```python
-    print("Supported datasets:", get_supported_datasets())
-    ```
-    """
-    return SUPPORTED_DATASET_CONFIG.keys()
-
-
-def create_forward_loop(
-    model: torch.nn.Module = None,
-    dataset_name: str = "cnn_dailymail",
-    tokenizer: Union["PreTrainedTokenizer", "PreTrainedTokenizerFast"] = None,
-    batch_size: int = 1,
-    num_samples: int = 512,
-    max_sample_length: int = 512,
-    device: Optional[str] = None,
-) -> Callable:
-    """Creates and returns a forward loop function configured for a specific model, dataset, and tokenizer.
-
-    This function initializes a forward loop function tailored to process batches of data from the specified dataset
-    using the given model and tokenizer. The forward loop function, when called, iterates over the dataset, applies the
-    tokenizer to prepare the input data, feeds it into the model, and returns the model's predictions.
-
-    Parameters:
-    - model: The PyTorch model for inference.
-    - dataset_name: The name of the dataset to be used.
-    - tokenizer: The tokenizer used to preprocess text data into a format suitable
-    for the model.
-    - batch_size: Batch size of the returned dataloader.
-    - num_samples: Number of samples from the dataset.
-    - max_sample_length: Maximum length of a sample.
-    - device: Target device for the returned dataloader.
-
-    Example usage for quantization:
-
-    .. code-block:: python
-
-        import modelopt.torch.quantization as mtq
-
-        # Initialize model and tokenizer
-        # ...
-
-        # Create forward loop for calibration
-        forward_loop = create_forward_loop(model=model, dataset_name="cnn_dailymail", tokenizer=tokenizer)
-
-        # Quantize the model with the calibration dataset
-        mtq.quantize(model, quant_cfg, forward_loop=forward_loop)
-
-    Returns:
-    - function: A forward loop function that can be called with no arguments. When called, this function iterates over
-    the dataset specified by `dataset_name`.
-    """
-    dataloader = get_dataset_dataloader(
-        dataset_name=dataset_name,
-        tokenizer=tokenizer,
-        batch_size=batch_size,
-        num_samples=num_samples,
-        max_sample_length=max_sample_length,
-        device=device,
-    )
-
-    def forward_loop(*args, **kwargs):
-        """Adjusts weights and scaling factors based on selected algorithms."""
-        if args or kwargs:
-            print("Warning: The following arguments will not be used in the forward loop:")
-            for i, arg in enumerate(args):
-                print(f"- Positional argument {i}: {arg}")
-            for key, value in kwargs.items():
-                print(f"- Keyword argument '{key}': {value}")
-        for idx, data in enumerate(dataloader):
-            model(data)
-
-    return forward_loop
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions for getting samples and forward loop function for different datasets."""
+from typing import TYPE_CHECKING, Callable, List, Optional, Union
+
+import torch
+from torch.utils.data import DataLoader
+
+if TYPE_CHECKING:
+    from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
+
+# Use dict to store the config for each dataset.
+# If we want to export more options to user like target languages, we need more standardized approach like dataclass.
+SUPPORTED_DATASET_CONFIG = {
+    "cnn_dailymail": {
+        "config": {"path": "cnn_dailymail", "name": "3.0.0"},
+        "target": "article",
+    },
+    "pile": {
+        "config": {"path": "monology/pile-uncopyrighted"},
+        "target": "text",
+    },
+    "pg19": {
+        "config": {"path": "pg19"},
+        "target": "text",
+    },
+    "wikipedia": {
+        "config": {"path": "wikipedia", "name": "20220301.en"},
+        "target": "text",
+    },
+    "c4": {
+        "config": {"path": "c4", "name": "en"},
+        "target": "text",
+    },
+}
+
+__all__ = ["create_forward_loop", "get_dataset_dataloader"]
+
+
+def _get_dataset_samples(dataset_name: str, num_samples: int) -> List[str]:
+    """Load a portion of train dataset with the dataset name and a given size.
+
+    Args:
+        dataset_name: Name of the dataset to load.
+        num_samples: Number of samples to load from the dataset.
+
+    Returns:
+        Smaples: The list of samples.
+    """
+    # Load the dataset
+    if dataset_name in SUPPORTED_DATASET_CONFIG:
+        try:
+            from datasets import load_dataset
+        except ModuleNotFoundError:
+            print(
+                "The 'datasets' module is not installed. Please install it using 'pip install"
+                " datasets' to continue."
+            )
+        # Use streaming can reduce the downloading time for large datasets
+        dataset = load_dataset(
+            split="train", streaming=True, **SUPPORTED_DATASET_CONFIG[dataset_name]["config"]
+        )
+    else:
+        raise NotImplementedError(
+            f"dataset {dataset_name} is not supported. Check available datasets with"
+            " get_supported_datasets."
+        )
+
+    # Access only the required samples
+    samples = []
+    target_key = SUPPORTED_DATASET_CONFIG[dataset_name]["target"]
+    for i, sample in enumerate(dataset):
+        if i >= num_samples:
+            break
+        samples.append(sample[target_key])
+
+    return samples
+
+
+def get_dataset_dataloader(
+    dataset_name: str = "cnn_dailymail",
+    tokenizer: Union["PreTrainedTokenizer", "PreTrainedTokenizerFast"] = None,
+    batch_size: int = 1,
+    num_samples: int = 512,
+    max_sample_length: int = 512,
+    device: Optional[str] = None,
+) -> DataLoader:
+    """Get a dataloader with the dataset name and toknizer of the target model.
+
+    Args:
+        dataset_name: Name of the dataset to load.
+        tokenizer: Instancne of Hugginface tokenizer.
+        batch_size: Batch size of the returned dataloader.
+        num_samples: Number of samples from the dataset.
+        max_sample_length: Maximum length of a sample.
+        device: Target device for the returned dataloader.
+
+    Returns:
+        A instance of dataloader.
+    """
+    assert tokenizer is not None, "Please provide a tokenizer."
+
+    dataset = _get_dataset_samples(dataset_name, num_samples=num_samples)
+
+    batch_encoded = tokenizer.batch_encode_plus(
+        dataset, return_tensors="pt", padding=True, truncation=True, max_length=max_sample_length
+    )
+    if device:
+        batch_encoded = batch_encoded.to(device)
+    batch_encoded = batch_encoded["input_ids"]
+
+    calib_dataloader = DataLoader(batch_encoded, batch_size=batch_size, shuffle=False)
+
+    return calib_dataloader
+
+
+def get_supported_datasets():
+    """Retrieves a list of datasets supported.
+
+    Returns:
+    - list[str]: A list of strings, where each string is the name of a supported dataset.
+
+    Example usage:
+
+    ```python
+    print("Supported datasets:", get_supported_datasets())
+    ```
+    """
+    return SUPPORTED_DATASET_CONFIG.keys()
+
+
+def create_forward_loop(
+    model: torch.nn.Module = None,
+    dataset_name: str = "cnn_dailymail",
+    tokenizer: Union["PreTrainedTokenizer", "PreTrainedTokenizerFast"] = None,
+    batch_size: int = 1,
+    num_samples: int = 512,
+    max_sample_length: int = 512,
+    device: Optional[str] = None,
+) -> Callable:
+    """Creates and returns a forward loop function configured for a specific model, dataset, and tokenizer.
+
+    This function initializes a forward loop function tailored to process batches of data from the specified dataset
+    using the given model and tokenizer. The forward loop function, when called, iterates over the dataset, applies the
+    tokenizer to prepare the input data, feeds it into the model, and returns the model's predictions.
+
+    Parameters:
+    - model: The PyTorch model for inference.
+    - dataset_name: The name of the dataset to be used.
+    - tokenizer: The tokenizer used to preprocess text data into a format suitable
+    for the model.
+    - batch_size: Batch size of the returned dataloader.
+    - num_samples: Number of samples from the dataset.
+    - max_sample_length: Maximum length of a sample.
+    - device: Target device for the returned dataloader.
+
+    Example usage for quantization:
+
+    .. code-block:: python
+
+        import modelopt.torch.quantization as mtq
+
+        # Initialize model and tokenizer
+        # ...
+
+        # Create forward loop for calibration
+        forward_loop = create_forward_loop(model=model, dataset_name="cnn_dailymail", tokenizer=tokenizer)
+
+        # Quantize the model with the calibration dataset
+        mtq.quantize(model, quant_cfg, forward_loop=forward_loop)
+
+    Returns:
+    - function: A forward loop function that can be called with no arguments. When called, this function iterates over
+    the dataset specified by `dataset_name`.
+    """
+    dataloader = get_dataset_dataloader(
+        dataset_name=dataset_name,
+        tokenizer=tokenizer,
+        batch_size=batch_size,
+        num_samples=num_samples,
+        max_sample_length=max_sample_length,
+        device=device,
+    )
+
+    def forward_loop(*args, **kwargs):
+        """Adjusts weights and scaling factors based on selected algorithms."""
+        if args or kwargs:
+            print("Warning: The following arguments will not be used in the forward loop:")
+            for i, arg in enumerate(args):
+                print(f"- Positional argument {i}: {arg}")
+            for key, value in kwargs.items():
+                print(f"- Keyword argument '{key}': {value}")
+        for idx, data in enumerate(dataloader):
+            model(data)
+
+    return forward_loop
```

## modelopt/torch/utils/distributed.py

 * *Ordering differences only*

```diff
@@ -1,269 +1,269 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-
-"""Utility functions for using torch.distributed."""
-
-import functools
-import io
-import os
-import time
-from typing import Any, List, Optional, Union
-
-import torch
-import torch.distributed
-from packaging.version import Version
-
-__all__ = [
-    "backend",
-    "size",
-    "rank",
-    "is_master",
-    "barrier",
-    "set_data_parallel_group",
-    "set_tensor_parallel_group",
-    "get_data_parallel_group",
-    "get_tensor_parallel_group",
-]
-
-
-def backend() -> Optional[str]:
-    """Returns the distributed backend."""
-    if torch.distributed.is_available() and torch.distributed.is_initialized():
-        return "torch"
-    return None
-
-
-def size(group=None) -> int:
-    """Returns the number of processes."""
-    if backend() == "torch":
-        return torch.distributed.get_world_size(group=group)
-    return 1
-
-
-def rank(group=None) -> int:
-    """Returns the rank of the current process."""
-    if backend() == "torch":
-        return torch.distributed.get_rank(group=group)
-    return 0
-
-
-def is_master(group=None) -> bool:
-    """Returns whether the current process is the master process."""
-    return rank(group=group) == 0
-
-
-def _serialize(obj: Any) -> torch.Tensor:
-    buffer = io.BytesIO()
-    torch.save(obj, buffer)
-    if Version(torch.__version__) >= Version("1.13"):
-        storage = torch.UntypedStorage.from_buffer(buffer.getvalue(), dtype=torch.uint8)
-    else:
-        storage = torch._UntypedStorage.from_buffer(buffer.getvalue(), dtype=torch.uint8)
-    tensor = torch.ByteTensor(storage)
-    return tensor
-
-
-def _deserialize(tensor: torch.Tensor, size: Optional[int] = None) -> Any:
-    buffer = tensor.numpy().tobytes()
-    if size is not None:
-        buffer = buffer[:size]
-    obj = torch.load(io.BytesIO(buffer))
-    return obj
-
-
-def _broadcast(tensor: torch.Tensor, src: int = 0, group=None) -> None:
-    if backend() == "torch":
-        torch.distributed.broadcast(tensor, src, group)
-
-
-def broadcast(obj: Any, src: int = 0, group=None) -> Any:
-    """Broadcasts an object from the source to all other processes."""
-    if size() == 1:
-        return obj
-
-    # serialize
-    if rank() == src:
-        tensor = _serialize(obj).cuda()
-
-    # broadcast the tensor size
-    if rank() == src:
-        tensor_size = torch.LongTensor([tensor.numel()]).cuda()
-    else:
-        tensor_size = torch.LongTensor([0]).cuda()
-    _broadcast(tensor_size, src=src, group=group)
-
-    # broadcast the tensor
-    if rank() != src:
-        tensor = torch.ByteTensor(size=(tensor_size.item(),)).cuda()
-    _broadcast(tensor, src=src, group=group)
-
-    # deserialize
-    if rank() != src:
-        obj = _deserialize(tensor.cpu())
-    return obj
-
-
-def _allgather(tensors: List[torch.Tensor], tensor: torch.Tensor) -> None:
-    if backend() == "torch":
-        torch.distributed.all_gather(tensors, tensor)
-
-
-def allgather(obj: Any) -> List[Any]:
-    """Gathers an object from all processes into a list."""
-    if size() == 1:
-        return [obj]
-
-    # serialize
-    tensor = _serialize(obj).cuda()
-
-    # gather the tensor size
-    tensor_size = torch.LongTensor([tensor.numel()]).cuda()
-    tensor_sizes = [torch.LongTensor([0]).cuda() for _ in range(size())]
-    _allgather(tensor_sizes, tensor_size)
-    tensor_sizes = [int(tensor_size.item()) for tensor_size in tensor_sizes]
-    max_size = max(tensor_sizes)
-
-    # gather the tensor
-    tensors = [torch.ByteTensor(size=(max_size,)).cuda() for _ in tensor_sizes]
-    if tensor_size != max_size:
-        padding = torch.ByteTensor(size=(max_size - tensor_size,)).cuda()
-        tensor = torch.cat((tensor, padding), dim=0)
-    _allgather(tensors, tensor)
-
-    # deserialize
-    objs = []
-    for tensor_size, tensor in zip(tensor_sizes, tensors):
-        obj = _deserialize(tensor.cpu(), size=tensor_size)
-        objs.append(obj)
-    return objs
-
-
-def allreduce(obj: Any, reduction: str = "sum") -> Any:
-    """Reduces an object from all processes."""
-    objs = allgather(obj)
-    if reduction == "sum":
-        return sum(objs)
-    else:
-        raise NotImplementedError(reduction)
-
-
-def barrier() -> None:
-    """Synchronizes all processes."""
-    if size() == 1:
-        return
-    if backend() == "torch":
-        torch.distributed.barrier()
-
-
-def master_only(func):
-    """Decorator to run a function only on the master process and broadcast the result."""
-
-    @functools.wraps(func)
-    def wrapper(*args, **kwargs):
-        return broadcast(func(*args, **kwargs) if is_master() else None)
-
-    return wrapper
-
-
-class DistributedProcessGroup:
-    """A class to manage the distributed process group for data parallel, tensor parallel etc."""
-
-    def __init__(self, group=None):
-        """Initialize the distributed process group."""
-        self.group = group
-
-    def is_initialized(self) -> bool:
-        """Check if the distributed process group is initialized."""
-        return backend() == "torch" and self.group != -1
-
-    def rank(self):
-        """Get the rank of the current process group."""
-        if not self.is_initialized():
-            return -1
-        return backend() == "torch" and rank(group=self.group)
-
-    def world_size(self):
-        """Get the world size of the current process group."""
-        if not self.is_initialized():
-            return -1
-        return backend() == "torch" and size(group=self.group)
-
-
-_DATA_PARALLEL_GROUP = DistributedProcessGroup(None)
-_TENSOR_PARALLEL_GROUP = DistributedProcessGroup(-1)
-
-
-def set_data_parallel_group(group: Optional[Union[torch.distributed.ProcessGroup, int]]):
-    """Set the data parallel group."""
-    _DATA_PARALLEL_GROUP.group = group
-
-
-def set_tensor_parallel_group(group: Optional[Union[torch.distributed.ProcessGroup, int]]):
-    """Set the tensor parallel group."""
-    _TENSOR_PARALLEL_GROUP.group = group
-
-
-def get_data_parallel_group() -> DistributedProcessGroup:
-    """Get the data parallel group."""
-    return _DATA_PARALLEL_GROUP
-
-
-def get_tensor_parallel_group() -> DistributedProcessGroup:
-    """Get the tensor parallel group."""
-    return _TENSOR_PARALLEL_GROUP
-
-
-class FileLock:
-    """Mutex object for writing to a file atomically using the O_EXCL directive on Unix filesystems."""
-
-    def __init__(
-        self,
-        lockfile_path: str,
-        all_acquire: bool = False,
-        poll_time: float = 0.25,
-    ):
-        """Constructor.
-
-        Args:
-            lockfile_path: Path to a nonexistent file to be used as the locking mechanism.
-            all_acquire: Will keep retrying to acquire a lock if True.
-            poll_time: Sleep interval between retries.
-        """
-        self.lockfile_path = lockfile_path
-        self.all_acquire = all_acquire
-        self.poll_time = poll_time
-        self.handle = None
-
-    def try_acquire(self):  # noqa: D102
-        try:
-            self.handle = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL)
-            return True
-        except FileExistsError:
-            return False
-
-    def wait(self):  # noqa: D102
-        while os.path.exists(self.lockfile_path):
-            time.sleep(self.poll_time)
-
-    def release(self):  # noqa: D102
-        if self.handle is not None:
-            os.close(self.handle)
-        os.remove(self.lockfile_path)
-
-    def __enter__(self):
-        while True:
-            if self.try_acquire() or not self.all_acquire:
-                break
-            self.wait()
-        return self
-
-    def __exit__(self, exc_type, exc_value, traceback):
-        self.release()
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+
+"""Utility functions for using torch.distributed."""
+
+import functools
+import io
+import os
+import time
+from typing import Any, List, Optional, Union
+
+import torch
+import torch.distributed
+from packaging.version import Version
+
+__all__ = [
+    "backend",
+    "size",
+    "rank",
+    "is_master",
+    "barrier",
+    "set_data_parallel_group",
+    "set_tensor_parallel_group",
+    "get_data_parallel_group",
+    "get_tensor_parallel_group",
+]
+
+
+def backend() -> Optional[str]:
+    """Returns the distributed backend."""
+    if torch.distributed.is_available() and torch.distributed.is_initialized():
+        return "torch"
+    return None
+
+
+def size(group=None) -> int:
+    """Returns the number of processes."""
+    if backend() == "torch":
+        return torch.distributed.get_world_size(group=group)
+    return 1
+
+
+def rank(group=None) -> int:
+    """Returns the rank of the current process."""
+    if backend() == "torch":
+        return torch.distributed.get_rank(group=group)
+    return 0
+
+
+def is_master(group=None) -> bool:
+    """Returns whether the current process is the master process."""
+    return rank(group=group) == 0
+
+
+def _serialize(obj: Any) -> torch.Tensor:
+    buffer = io.BytesIO()
+    torch.save(obj, buffer)
+    if Version(torch.__version__) >= Version("1.13"):
+        storage = torch.UntypedStorage.from_buffer(buffer.getvalue(), dtype=torch.uint8)
+    else:
+        storage = torch._UntypedStorage.from_buffer(buffer.getvalue(), dtype=torch.uint8)
+    tensor = torch.ByteTensor(storage)
+    return tensor
+
+
+def _deserialize(tensor: torch.Tensor, size: Optional[int] = None) -> Any:
+    buffer = tensor.numpy().tobytes()
+    if size is not None:
+        buffer = buffer[:size]
+    obj = torch.load(io.BytesIO(buffer))
+    return obj
+
+
+def _broadcast(tensor: torch.Tensor, src: int = 0, group=None) -> None:
+    if backend() == "torch":
+        torch.distributed.broadcast(tensor, src, group)
+
+
+def broadcast(obj: Any, src: int = 0, group=None) -> Any:
+    """Broadcasts an object from the source to all other processes."""
+    if size() == 1:
+        return obj
+
+    # serialize
+    if rank() == src:
+        tensor = _serialize(obj).cuda()
+
+    # broadcast the tensor size
+    if rank() == src:
+        tensor_size = torch.LongTensor([tensor.numel()]).cuda()
+    else:
+        tensor_size = torch.LongTensor([0]).cuda()
+    _broadcast(tensor_size, src=src, group=group)
+
+    # broadcast the tensor
+    if rank() != src:
+        tensor = torch.ByteTensor(size=(tensor_size.item(),)).cuda()
+    _broadcast(tensor, src=src, group=group)
+
+    # deserialize
+    if rank() != src:
+        obj = _deserialize(tensor.cpu())
+    return obj
+
+
+def _allgather(tensors: List[torch.Tensor], tensor: torch.Tensor) -> None:
+    if backend() == "torch":
+        torch.distributed.all_gather(tensors, tensor)
+
+
+def allgather(obj: Any) -> List[Any]:
+    """Gathers an object from all processes into a list."""
+    if size() == 1:
+        return [obj]
+
+    # serialize
+    tensor = _serialize(obj).cuda()
+
+    # gather the tensor size
+    tensor_size = torch.LongTensor([tensor.numel()]).cuda()
+    tensor_sizes = [torch.LongTensor([0]).cuda() for _ in range(size())]
+    _allgather(tensor_sizes, tensor_size)
+    tensor_sizes = [int(tensor_size.item()) for tensor_size in tensor_sizes]
+    max_size = max(tensor_sizes)
+
+    # gather the tensor
+    tensors = [torch.ByteTensor(size=(max_size,)).cuda() for _ in tensor_sizes]
+    if tensor_size != max_size:
+        padding = torch.ByteTensor(size=(max_size - tensor_size,)).cuda()
+        tensor = torch.cat((tensor, padding), dim=0)
+    _allgather(tensors, tensor)
+
+    # deserialize
+    objs = []
+    for tensor_size, tensor in zip(tensor_sizes, tensors):
+        obj = _deserialize(tensor.cpu(), size=tensor_size)
+        objs.append(obj)
+    return objs
+
+
+def allreduce(obj: Any, reduction: str = "sum") -> Any:
+    """Reduces an object from all processes."""
+    objs = allgather(obj)
+    if reduction == "sum":
+        return sum(objs)
+    else:
+        raise NotImplementedError(reduction)
+
+
+def barrier() -> None:
+    """Synchronizes all processes."""
+    if size() == 1:
+        return
+    if backend() == "torch":
+        torch.distributed.barrier()
+
+
+def master_only(func):
+    """Decorator to run a function only on the master process and broadcast the result."""
+
+    @functools.wraps(func)
+    def wrapper(*args, **kwargs):
+        return broadcast(func(*args, **kwargs) if is_master() else None)
+
+    return wrapper
+
+
+class DistributedProcessGroup:
+    """A class to manage the distributed process group for data parallel, tensor parallel etc."""
+
+    def __init__(self, group=None):
+        """Initialize the distributed process group."""
+        self.group = group
+
+    def is_initialized(self) -> bool:
+        """Check if the distributed process group is initialized."""
+        return backend() == "torch" and self.group != -1
+
+    def rank(self):
+        """Get the rank of the current process group."""
+        if not self.is_initialized():
+            return -1
+        return backend() == "torch" and rank(group=self.group)
+
+    def world_size(self):
+        """Get the world size of the current process group."""
+        if not self.is_initialized():
+            return -1
+        return backend() == "torch" and size(group=self.group)
+
+
+_DATA_PARALLEL_GROUP = DistributedProcessGroup(None)
+_TENSOR_PARALLEL_GROUP = DistributedProcessGroup(-1)
+
+
+def set_data_parallel_group(group: Optional[Union[torch.distributed.ProcessGroup, int]]):
+    """Set the data parallel group."""
+    _DATA_PARALLEL_GROUP.group = group
+
+
+def set_tensor_parallel_group(group: Optional[Union[torch.distributed.ProcessGroup, int]]):
+    """Set the tensor parallel group."""
+    _TENSOR_PARALLEL_GROUP.group = group
+
+
+def get_data_parallel_group() -> DistributedProcessGroup:
+    """Get the data parallel group."""
+    return _DATA_PARALLEL_GROUP
+
+
+def get_tensor_parallel_group() -> DistributedProcessGroup:
+    """Get the tensor parallel group."""
+    return _TENSOR_PARALLEL_GROUP
+
+
+class FileLock:
+    """Mutex object for writing to a file atomically using the O_EXCL directive on Unix filesystems."""
+
+    def __init__(
+        self,
+        lockfile_path: str,
+        all_acquire: bool = False,
+        poll_time: float = 0.25,
+    ):
+        """Constructor.
+
+        Args:
+            lockfile_path: Path to a nonexistent file to be used as the locking mechanism.
+            all_acquire: Will keep retrying to acquire a lock if True.
+            poll_time: Sleep interval between retries.
+        """
+        self.lockfile_path = lockfile_path
+        self.all_acquire = all_acquire
+        self.poll_time = poll_time
+        self.handle = None
+
+    def try_acquire(self):  # noqa: D102
+        try:
+            self.handle = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL)
+            return True
+        except FileExistsError:
+            return False
+
+    def wait(self):  # noqa: D102
+        while os.path.exists(self.lockfile_path):
+            time.sleep(self.poll_time)
+
+    def release(self):  # noqa: D102
+        if self.handle is not None:
+            os.close(self.handle)
+        os.remove(self.lockfile_path)
+
+    def __enter__(self):
+        while True:
+            if self.try_acquire() or not self.all_acquire:
+                break
+            self.wait()
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        self.release()
```

## modelopt/torch/utils/graph.py

 * *Ordering differences only*

```diff
@@ -1,119 +1,119 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions for computational graph."""
-import itertools
-from typing import Callable, Dict, Optional, Sequence, Union
-
-import torch
-from torch import nn
-from torch.fx import Node, symbolic_trace
-
-__all__ = ["match"]
-
-NodeTarget = Union[nn.Module, Callable]
-
-
-def _get_node_target(node: Node, root: nn.Module) -> Optional[NodeTarget]:
-    """Return node target depending on node operator."""
-    target_extractor = {
-        "call_module": lambda t: root.get_submodule(t),
-        "call_method": lambda t: getattr(torch.Tensor, t),
-        "call_function": lambda t: t,
-        "output": lambda _: None,
-        "placeholder": lambda _: None,
-        "get_attr": lambda _: None,
-    }
-    return target_extractor[node.op](node.target)
-
-
-def _local_match(nx: Node, ny: Node, mx: nn.Module, my: nn.Module) -> bool:
-    # Check if input and output degrees match.
-    if len(nx.all_input_nodes) != len(ny.all_input_nodes):
-        return False
-    if len(nx.users) != len(ny.users):
-        return False
-
-    # Check if the node operator matches.
-    if nx.op != ny.op:
-        return False
-
-    # Check if the node target matches.
-    tx = _get_node_target(nx, mx)
-    ty = _get_node_target(ny, my)
-    if isinstance(tx, nn.Module) and isinstance(ty, nn.Module):
-        tx, ty = type(tx), type(ty)
-    if tx != ty:
-        return False
-
-    return True
-
-
-def _recursive_match(
-    nx: Node, ny: Node, mx: nn.Module, my: nn.Module, maps: Dict[Node, Node]
-) -> bool:
-    # Check if we have already matched these nodes in the current traversal.
-    if nx in maps:
-        return maps[nx] == ny
-
-    # Terminate early if the nodes do not match locally.
-    if not _local_match(nx, ny, mx, my):
-        return False
-
-    # Optimistically mark `nx` as a match for `ny`.
-    maps[nx] = ny
-
-    # Both nodes are inputs. We have a match!
-    if nx.op == "placeholder":
-        return True
-
-    # Enumerate all possible input node matches.
-    ixs = nx.all_input_nodes
-    for iys in itertools.permutations(ny.all_input_nodes):
-        if not all(_local_match(ix, iy, mx, my) for ix, iy in zip(ixs, iys)):
-            continue
-        if all(_recursive_match(ix, iy, mx, my, maps) for ix, iy in zip(ixs, iys)):
-            return True
-
-    # No match found.
-    del maps[nx]
-    return False
-
-
-def match(module: nn.Module, patterns: Sequence[nn.Module]) -> bool:
-    """Check if a module matches any of the patterns.
-
-    Args:
-        module: The module to be checked.
-        patterns: The patterns to be matched.
-
-    Returns:
-        True if the module matches any of the patterns, False otherwise.
-    """
-    try:
-        module_g = symbolic_trace(module).graph
-    except Exception:
-        return False
-
-    for pattern in patterns:
-        pattern_g = symbolic_trace(pattern).graph
-
-        # Check if the number of nodes match.
-        if len(module_g.nodes) != len(pattern_g.nodes):
-            continue
-
-        # Extract output nodes from the graphs.
-        *_, pattern_o = pattern_g.nodes
-        *_, module_o = module_g.nodes
-
-        # Check whether two graphs match recursively from the output node.
-        if _recursive_match(pattern_o, module_o, pattern, module, {}):
-            return True
-    return False
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions for computational graph."""
+import itertools
+from typing import Callable, Dict, Optional, Sequence, Union
+
+import torch
+from torch import nn
+from torch.fx import Node, symbolic_trace
+
+__all__ = ["match"]
+
+NodeTarget = Union[nn.Module, Callable]
+
+
+def _get_node_target(node: Node, root: nn.Module) -> Optional[NodeTarget]:
+    """Return node target depending on node operator."""
+    target_extractor = {
+        "call_module": lambda t: root.get_submodule(t),
+        "call_method": lambda t: getattr(torch.Tensor, t),
+        "call_function": lambda t: t,
+        "output": lambda _: None,
+        "placeholder": lambda _: None,
+        "get_attr": lambda _: None,
+    }
+    return target_extractor[node.op](node.target)
+
+
+def _local_match(nx: Node, ny: Node, mx: nn.Module, my: nn.Module) -> bool:
+    # Check if input and output degrees match.
+    if len(nx.all_input_nodes) != len(ny.all_input_nodes):
+        return False
+    if len(nx.users) != len(ny.users):
+        return False
+
+    # Check if the node operator matches.
+    if nx.op != ny.op:
+        return False
+
+    # Check if the node target matches.
+    tx = _get_node_target(nx, mx)
+    ty = _get_node_target(ny, my)
+    if isinstance(tx, nn.Module) and isinstance(ty, nn.Module):
+        tx, ty = type(tx), type(ty)
+    if tx != ty:
+        return False
+
+    return True
+
+
+def _recursive_match(
+    nx: Node, ny: Node, mx: nn.Module, my: nn.Module, maps: Dict[Node, Node]
+) -> bool:
+    # Check if we have already matched these nodes in the current traversal.
+    if nx in maps:
+        return maps[nx] == ny
+
+    # Terminate early if the nodes do not match locally.
+    if not _local_match(nx, ny, mx, my):
+        return False
+
+    # Optimistically mark `nx` as a match for `ny`.
+    maps[nx] = ny
+
+    # Both nodes are inputs. We have a match!
+    if nx.op == "placeholder":
+        return True
+
+    # Enumerate all possible input node matches.
+    ixs = nx.all_input_nodes
+    for iys in itertools.permutations(ny.all_input_nodes):
+        if not all(_local_match(ix, iy, mx, my) for ix, iy in zip(ixs, iys)):
+            continue
+        if all(_recursive_match(ix, iy, mx, my, maps) for ix, iy in zip(ixs, iys)):
+            return True
+
+    # No match found.
+    del maps[nx]
+    return False
+
+
+def match(module: nn.Module, patterns: Sequence[nn.Module]) -> bool:
+    """Check if a module matches any of the patterns.
+
+    Args:
+        module: The module to be checked.
+        patterns: The patterns to be matched.
+
+    Returns:
+        True if the module matches any of the patterns, False otherwise.
+    """
+    try:
+        module_g = symbolic_trace(module).graph
+    except Exception:
+        return False
+
+    for pattern in patterns:
+        pattern_g = symbolic_trace(pattern).graph
+
+        # Check if the number of nodes match.
+        if len(module_g.nodes) != len(pattern_g.nodes):
+            continue
+
+        # Extract output nodes from the graphs.
+        *_, pattern_o = pattern_g.nodes
+        *_, module_o = module_g.nodes
+
+        # Check whether two graphs match recursively from the output node.
+        if _recursive_match(pattern_o, module_o, pattern, module, {}):
+            return True
+    return False
```

## modelopt/torch/utils/list.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utils for operating on lists."""
-from typing import Any, Dict, List, Tuple, Union
-
-import numpy as np
-
-__all__ = [
-    "list_closest_to_median",  # used
-    "val2list",  # used
-    "val2tuple",  # used
-    "stats",  # used
-]
-
-
-def list_closest_to_median(x: List) -> Any:
-    """Return element from list that's closest to list mean."""
-    median = np.median(x)
-    diff = [abs(elem - median) for elem in x]
-    return x[diff.index(min(diff))]
-
-
-def val2list(val: Union[List, Tuple, Any], repeat_time=1) -> List:
-    """Repeat `val` for `repeat_time` times and return the list or val if list/tuple."""
-    if isinstance(val, (list, tuple)):
-        return list(val)
-    return [val for _ in range(repeat_time)]
-
-
-def val2tuple(val: Union[List, Tuple, Any], min_len: int = 1, idx_repeat: int = -1) -> Tuple:
-    """Return tuple with min_len by repeating element at idx_repeat."""
-    # convert to list first
-    val = val2list(val)
-
-    # repeat elements if necessary
-    if len(val) > 0:
-        val[idx_repeat:idx_repeat] = [val[idx_repeat] for _ in range(min_len - len(val))]
-
-    return tuple(val)
-
-
-def stats(vals: List[float]) -> Dict[str, float]:
-    """Compute min, max, avg, std of vals."""
-    stats = {"min": np.min, "max": np.max, "avg": np.mean, "std": np.std}
-    return {name: fn(vals) for name, fn in stats.items()} if vals else {}
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utils for operating on lists."""
+from typing import Any, Dict, List, Tuple, Union
+
+import numpy as np
+
+__all__ = [
+    "list_closest_to_median",  # used
+    "val2list",  # used
+    "val2tuple",  # used
+    "stats",  # used
+]
+
+
+def list_closest_to_median(x: List) -> Any:
+    """Return element from list that's closest to list mean."""
+    median = np.median(x)
+    diff = [abs(elem - median) for elem in x]
+    return x[diff.index(min(diff))]
+
+
+def val2list(val: Union[List, Tuple, Any], repeat_time=1) -> List:
+    """Repeat `val` for `repeat_time` times and return the list or val if list/tuple."""
+    if isinstance(val, (list, tuple)):
+        return list(val)
+    return [val for _ in range(repeat_time)]
+
+
+def val2tuple(val: Union[List, Tuple, Any], min_len: int = 1, idx_repeat: int = -1) -> Tuple:
+    """Return tuple with min_len by repeating element at idx_repeat."""
+    # convert to list first
+    val = val2list(val)
+
+    # repeat elements if necessary
+    if len(val) > 0:
+        val[idx_repeat:idx_repeat] = [val[idx_repeat] for _ in range(min_len - len(val))]
+
+    return tuple(val)
+
+
+def stats(vals: List[float]) -> Dict[str, float]:
+    """Compute min, max, avg, std of vals."""
+    stats = {"min": np.min, "max": np.max, "avg": np.mean, "std": np.std}
+    return {name: fn(vals) for name, fn in stats.items()} if vals else {}
```

## modelopt/torch/utils/logging.py

 * *Ordering differences only*

```diff
@@ -1,91 +1,91 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions for logging."""
-import contextlib
-import os
-from contextlib import contextmanager
-from inspect import signature
-
-import tqdm
-
-from . import distributed as dist
-
-__all__ = ["num2hrb", "no_stdout", "print_rank_0", "DeprecatedError"]
-
-
-def num2hrb(num: float, suffix="") -> str:
-    """Convert big floating number to human readable string."""
-    step = 1000  # step between units is 1000
-    units = ["", "K", "M", "G", "T", "P", "E"]
-    while abs(num) >= step and len(units) > 1:
-        num /= step
-        units.pop(0)
-    return f"{num:3.2f}{units[0]}{suffix}"
-
-
-@contextmanager
-def _monkeypatched(obj, name, patch):
-    """Temporarily monkeypatch."""
-    old_attr = getattr(obj, name)
-    setattr(obj, name, patch(old_attr))
-    try:
-        yield
-    finally:
-        setattr(obj, name, old_attr)
-
-
-@contextmanager
-def _disable_tqdm():
-    """Context manager to disable tqdm.
-
-    tqdm progress bar outputs to sys.stderr.
-    Silencing sys.stderr to silence tqdm will also prevent error messages from streamed out.
-
-    Therefore, monkey patching is used to silence tqdm module without silencing sys.stderr.
-    """
-
-    def _patch(old_init):
-        def _new_init(self, *args, **kwargs):
-            # get the index of disable from function signature
-            index_disable = list(signature(old_init).parameters.keys()).index("disable") - 1
-
-            if len(args) >= index_disable:
-                # if arg "disable" is passed as a positional arg,
-                # overwrite pos args with disable = False
-                args = args[: index_disable - 1] + (True,) + args[index_disable:]
-            else:
-                kwargs["disable"] = True
-
-            # initialize tqdm with updated args reflecting "disable" = True
-            old_init(self, *args, **kwargs)
-
-        return _new_init
-
-    with _monkeypatched(tqdm.std.tqdm, "__init__", _patch):
-        yield
-
-
-@contextmanager
-def no_stdout():
-    """Silences stdout within the invoked context."""
-    # Special disable for tqdm
-    with contextlib.redirect_stdout(open(os.devnull, "w")), _disable_tqdm():
-        yield
-
-
-def print_rank_0(*args, **kwargs):
-    """Prints only on the master process."""
-    if dist.is_master():
-        print(*args, **kwargs, flush=True)
-
-
-class DeprecatedError(NotImplementedError):
-    """Error for deprecated functions."""
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions for logging."""
+import contextlib
+import os
+from contextlib import contextmanager
+from inspect import signature
+
+import tqdm
+
+from . import distributed as dist
+
+__all__ = ["num2hrb", "no_stdout", "print_rank_0", "DeprecatedError"]
+
+
+def num2hrb(num: float, suffix="") -> str:
+    """Convert big floating number to human readable string."""
+    step = 1000  # step between units is 1000
+    units = ["", "K", "M", "G", "T", "P", "E"]
+    while abs(num) >= step and len(units) > 1:
+        num /= step
+        units.pop(0)
+    return f"{num:3.2f}{units[0]}{suffix}"
+
+
+@contextmanager
+def _monkeypatched(obj, name, patch):
+    """Temporarily monkeypatch."""
+    old_attr = getattr(obj, name)
+    setattr(obj, name, patch(old_attr))
+    try:
+        yield
+    finally:
+        setattr(obj, name, old_attr)
+
+
+@contextmanager
+def _disable_tqdm():
+    """Context manager to disable tqdm.
+
+    tqdm progress bar outputs to sys.stderr.
+    Silencing sys.stderr to silence tqdm will also prevent error messages from streamed out.
+
+    Therefore, monkey patching is used to silence tqdm module without silencing sys.stderr.
+    """
+
+    def _patch(old_init):
+        def _new_init(self, *args, **kwargs):
+            # get the index of disable from function signature
+            index_disable = list(signature(old_init).parameters.keys()).index("disable") - 1
+
+            if len(args) >= index_disable:
+                # if arg "disable" is passed as a positional arg,
+                # overwrite pos args with disable = False
+                args = args[: index_disable - 1] + (True,) + args[index_disable:]
+            else:
+                kwargs["disable"] = True
+
+            # initialize tqdm with updated args reflecting "disable" = True
+            old_init(self, *args, **kwargs)
+
+        return _new_init
+
+    with _monkeypatched(tqdm.std.tqdm, "__init__", _patch):
+        yield
+
+
+@contextmanager
+def no_stdout():
+    """Silences stdout within the invoked context."""
+    # Special disable for tqdm
+    with contextlib.redirect_stdout(open(os.devnull, "w")), _disable_tqdm():
+        yield
+
+
+def print_rank_0(*args, **kwargs):
+    """Prints only on the master process."""
+    if dist.is_master():
+        print(*args, **kwargs, flush=True)
+
+
+class DeprecatedError(NotImplementedError):
+    """Error for deprecated functions."""
```

## modelopt/torch/utils/network.py

 * *Ordering differences only*

```diff
@@ -1,542 +1,542 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions for PyTorch models."""
-import inspect
-import warnings
-from collections import abc, deque
-from typing import Any, Callable, Dict, Iterable, Optional, Set, Tuple, Type, Union
-
-import torch
-import torch.distributed.fsdp
-import torch.nn as nn
-from torch.nn.modules.batchnorm import _BatchNorm
-from tqdm import tqdm
-
-from .tensor import torch_to
-
-__all__ = [
-    "ModelLike",
-    "compare_dict",
-    "get_model_attributes",
-    "get_module_device",
-    "get_same_padding",
-    "init_model_from_model_like",
-    "is_channels_last",
-    "is_parallel",
-    "make_divisible",
-    "model_to",
-    "param_num",
-    "param_num_from_forward",
-    "remove_bn",
-    "set_submodule",
-    "standardize_model_args",
-    "standardize_model_like_tuple",
-    "standardize_named_model_args",
-    "standardize_constructor_args",
-    "unwrap_model",
-    "zero_grad",
-    "run_forward_loop",
-]
-
-ModelLike = Union[nn.Module, Type[nn.Module], Tuple, Callable]
-ConstructorLike = Union[Callable, Tuple]
-
-
-def is_parallel(model: nn.Module) -> bool:
-    """Check if a PyTorch model is parallelized."""
-    return isinstance(model, (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel))
-
-
-def get_module_device(module: nn.Module) -> torch.device:
-    """Get the device of a PyTorch module."""
-    try:
-        return next(module.parameters()).device
-    except StopIteration:
-        # For modules without parameters
-        return torch.device("cpu")
-
-
-def param_num(network: nn.Module, trainable_only: bool = False, unit=1e6) -> float:
-    """Get the number of parameters of a PyTorch model.
-
-    Args:
-        network: The PyTorch model.
-        trainable_only: Whether to only count trainable parameters. Default is False.
-        unit: The unit to return the number of parameters in. Default is 1e6 (million).
-
-    Returns:
-        The number of parameters in the model in the given unit.
-    """
-    return (
-        sum(
-            p.numel() if not trainable_only or p.requires_grad else 0
-            for mod in network.modules()
-            for p in mod.parameters(recurse=False)
-            if not isinstance(mod, _BatchNorm)
-        )
-        / unit
-    )
-
-
-# TODO: we could also use the same approach as in inference_flops to get the number of params,
-# which might be more accurate. Another approach could be to run a backwards pass and use a hook
-# on the tensor directly.
-def param_num_from_forward(
-    model: nn.Module,
-    trainable_only: bool = False,
-    args: Union[torch.Tensor, Tuple, None] = None,
-    unit: float = 1e6,
-):
-    """Get the number of parameters of a PyTorch model from a forward pass.
-
-    Args:
-        network: The PyTorch model.
-        trainable_only: Whether to only count trainable parameters. Default is False.
-        unit: The unit to return the number of parameters in. Default is 1e6 (million).
-
-    Returns:
-        The number of parameters from the model's forward pass in the given unit.
-
-    This can helpful for dynamic modules, where the state dict might contain extra parameters that
-    is not actively used in the model, e.g., because of a DynamicModule that is deactivated for the
-    forward pass. We circumvent this issue by just counting parameters of modules that appear in a
-    forward pass.
-    """
-    params = {}
-
-    def count_hook(m: nn.Module, *_):
-        if m not in params:  # don't double-count parameters
-            params[m] = sum(
-                getattr(m, n).numel()  # use getattr to retrieve param since it might be dynamic
-                for n, p in m.named_parameters(recurse=False)  # don't recurse!
-                if not trainable_only or p.requires_grad
-            )
-
-    # add hook to count parameters to all modules except _BatchNorm
-    hooks = [
-        m.register_forward_hook(count_hook)
-        for m in model.modules()
-        if not isinstance(m, _BatchNorm)
-    ]
-
-    # run forward pass
-    args = standardize_model_args(model, args, use_kwargs=True)
-    args = torch_to(args, get_module_device(model))
-    model(*args[:-1], **args[-1])
-
-    # remove hooks
-    for h in hooks:
-        h.remove()
-
-    # count parameters and return
-    return sum(params.values()) / unit
-
-
-def get_same_padding(kernel_size: Union[int, Tuple[int, int]]) -> Union[int, tuple]:
-    """Get the same padding for a given kernel size."""
-    if isinstance(kernel_size, tuple):
-        assert len(kernel_size) == 2, f"invalid kernel size: {kernel_size}"
-        p1 = get_same_padding(kernel_size[0])
-        p2 = get_same_padding(kernel_size[1])
-        return p1, p2
-    else:
-        assert isinstance(kernel_size, int), "kernel size should be either `int` or `tuple`"
-        assert kernel_size % 2 == 1, "kernel size should be odd number"
-        return kernel_size // 2
-
-
-def make_divisible(v: Union[int, float], divisor: Optional[int], min_val=None) -> Union[int, float]:
-    """Function taken from the original tf repo.
-
-    It ensures that all layers have a channel number that is divisible by 8
-    It can be seen here:
-    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
-    """
-    if divisor is None:
-        return v
-
-    if min_val is None:
-        min_val = divisor
-    new_v = max(min_val, int(v + divisor / 2) // divisor * divisor)
-    # Make sure that round down does not go down by more than 10%.
-    if new_v < 0.9 * v:
-        new_v += divisor
-    return int(new_v)
-
-
-def is_channels_last(model: nn.Module):
-    """Check if the model is using channels last memory format."""
-    # Infer target_model's memory_format
-    # from https://github.com/pytorch/tutorials/blob/444fbd16f2ddf9967baf8b06e83867a141b071c2/
-    # intermediate_source/memory_format_tutorial.py#L283
-    has_channels_last = any(
-        p.is_contiguous(memory_format=torch.channels_last) and not p.is_contiguous()
-        for p in model.parameters()
-    )
-    return has_channels_last
-
-
-def model_to(model: nn.Module, target_model: nn.Module):
-    """Convert model to the same device, dtype and memory layout as the target_model."""
-    has_channels_last = is_channels_last(target_model)
-    # return model with same device, dtype, memory_format as self
-    return model.to(
-        tensor=next(target_model.parameters()),
-        memory_format=torch.channels_last if has_channels_last else torch.contiguous_format,
-    )
-
-
-def set_submodule(model: nn.Module, target: str, target_submodule: nn.Module):
-    """The set function that complements nn.Module.get_submodule()."""
-    assert target != "", "Cannot set root module"
-
-    # Verify the original submodule exists
-    model.get_submodule(target)
-    parent_module = model.get_submodule(target.rpartition(".")[0])
-    child_name = target.split(".")[-1]
-    parent_module.add_module(child_name, target_submodule)
-
-
-def remove_bn(model: nn.Module):
-    """Remove all batch normalization layers in the network."""
-    for m in model.modules():
-        if isinstance(m, _BatchNorm):
-            m.weight = m.bias = None
-            m.forward = lambda x: x
-
-
-def _preprocess_args(args: Union[Any, Tuple]) -> Tuple:
-    """Return args in standardized format as tuple with last entry as kwargs."""
-    # Re: torch.onnx.utils._decide_input_format, which is used in torch.onnx.export:
-    # Starting in pytorch >= 1.12 tuplification is done in the beginning instead of at the end
-    # for the args. We want to be consistent with that behavior here.
-    # Specifically, this affects the case of just passing in one dict as args:
-    #   * In torch < 1.12, this will be treated as a single positional argument; see here:
-    #     https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/onnx/utils.py#L336
-    #   * In torch >= 1.12, this will be treated as variable keyword argument (**kwargs), see here:
-    #     https://github.com/pytorch/pytorch/blob/91e754b268c1869df5b2836f15c73e6ec1e265f1/torch/onnx/utils.py#L774
-    if torch.__version__ < "1.12" and isinstance(args, abc.Mapping):
-        args = (args, {})
-
-    # now we can safely tuplify the args and add kwargs if necessary
-    if not isinstance(args, tuple):
-        args = (args,)
-    if not isinstance(args[-1], abc.Mapping):
-        args = args + ({},)
-
-    return args
-
-
-def standardize_named_model_args(
-    model_or_fw_or_sig: Union[nn.Module, Callable, inspect.Signature], args: Union[Any, Tuple]
-) -> Tuple[Dict[str, Any], Set[str]]:
-    """Standardize model arguments according to torch.onnx.export and give them a name.
-
-    Args:
-        model_or_fw_or_sig: A nn.Module, its forward method, or its forward method's signature.
-        args: A tuple of args/kwargs or torch.Tensor feed into the model's ``forward()`` method.
-
-    Returns: A tuple (args_normalized, args_with_default) where
-        args_normalized is a dictionary of ordered model args where the key represents a unique
-            serialized string based on the the argument's name in the function signature and the
-            value contains the actual argument,
-        args_with_default is a set indicating whether the argument was retrieved from the default
-            value in the function signature of the model's ``forward()`` method or whether the
-            argument exactly corresponds to the default value.
-
-    .. note::
-
-        See :meth:`standardize_model_args() <modelopt.torch.utils.network.standardize_model_args>` for
-        more info as well.
-    """
-    # pre-process args
-    args = _preprocess_args(args)
-
-    # extract parameters from model signature
-    if isinstance(model_or_fw_or_sig, nn.Module):
-        model_or_fw_or_sig = inspect.signature(model_or_fw_or_sig.forward)
-    elif callable(model_or_fw_or_sig):
-        model_or_fw_or_sig = inspect.signature(model_or_fw_or_sig)
-    params = model_or_fw_or_sig.parameters
-
-    # we now continue to process the parameters in the function signature and classify them according
-    # to their kind (see https://docs.python.org/3/library/inspect.html#inspect.Parameter.kind for
-    # an overview of the different kinds of parameters in a function signature)
-
-    # sanity-check: kw-only must have default value and cannot be provided by user
-    kw_only = [
-        n
-        for n, p in params.items()
-        if p.kind == inspect.Parameter.KEYWORD_ONLY and (p.default == p.empty or n in args[-1])
-    ]
-    if kw_only:
-        raise AssertionError(f"Keyword-only args ({kw_only}) can only be used w/ default values.")
-
-    # sanity-check: kwargs in signature are okay but cannot be used by user!
-    has_kwargs = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in params.values())
-    kwargs_unexpected = any(kw not in params for kw in args[-1])
-    if has_kwargs and kwargs_unexpected:
-        raise AssertionError("Variable kwargs (**kwargs) are not supported.")
-
-    # sanity-check: no unexpected kwargs provided by user
-    assert not kwargs_unexpected, "Cannot provide unexpected keyword args!"
-
-    # now sort in args_dict and default values
-    args_queue = deque(args[:-1])
-    args_dict = args[-1]
-    args_normalized = {}
-    args_with_default = set()
-    for pname, param in params.items():
-        # we peel off all positional/keyword arguments and fill them accordingly
-        if param.kind in [param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD]:
-            if args_queue:
-                args_normalized[pname] = args_queue.popleft()
-            elif pname in args_dict:
-                args_normalized[pname] = args_dict[pname]
-            elif param.default != param.empty:
-                args_normalized[pname] = param.default
-            else:
-                # sanity check: any args without default value must be provided by the user
-                raise AssertionError(f"Argument {pname} must be provided by the user.")
-            # check if provided arg is exactly (``is``) the default value
-            if args_normalized[pname] is param.default:
-                args_with_default.add(pname)
-        # when we have a var-positional arg (*args) we fill in the rest of the args
-        elif param.kind == param.VAR_POSITIONAL:
-            idx = 0
-            while args_queue:
-                args_normalized[f"{pname}.{idx}"] = args_queue.popleft()
-                idx += 1
-            # we also do not need to process further since everything following a var-positional
-            # argument is keyword-only, which we don't allow!
-            break
-
-    # sanity-check: no positional arguments left
-    assert not args_queue, "Positional arguments left unprocessed; too many provided!"
-
-    # return the args (without kw-only args and kwargs) and set to indicate which args were
-    # retrieved from default value in the function signature and which args exactly correspond to
-    # the default value
-    return args_normalized, args_with_default
-
-
-def standardize_model_args(
-    model_or_fw_or_sig: Union[nn.Module, Callable, inspect.Signature],
-    args: Union[Any, Tuple],
-    use_kwargs=False,
-) -> Tuple:
-    """Standardize model arguments according to torch.onnx.export.
-
-    Args:
-        model_or_fw_or_sig: A nn.Module, its forward method, or its forward method's signature.
-        args: Refer to the ``dummy_input`` parameter in
-            :meth:`mtn.profile() <modelopt.torch.nas.algorithms.profile>`.
-        use_kwargs: Affects the return value, see below. For ``use_kwargs==False``, the returned
-            args are also compatible with ``torch.onnx.export``.
-
-    Returns:
-        Standardized model args that can be used in ``model.forward()`` in the same standardized
-        way no matter how they were provided, see below for more info.
-
-    * If ``use_kwargs == False``, the returned args can be used as
-
-      .. code-block:: python
-
-            args = standardize_model_args(model, args, use_kwargs=False)
-            model(*args)
-
-    * If ``use_kwargs == True``, the returned args can be used as
-
-      .. code-block:: python
-
-            args = standardize_model_args(model, args, use_kwargs=True)
-            model.forward(*args[:-1], **args[-1])
-
-    .. warning::
-
-        If ``use_kwargs == False`` the model's ``forward()`` method **cannot** contain keyword-only
-        arguments (e.g. ``forward(..., *, kw_only_args)``) without default values and you must not
-        provide them in ``args``.
-
-    .. warning::
-
-        If ``use_kwargs == False`` you must not provide variable keyword arguments in ``args`` that
-        are processed via variable keyword arguments in the model's ``forward()`` method
-        (e.g. ``forward(..., **kwargs)``).
-
-    """
-    # preprocess args
-    args = _preprocess_args(args)
-
-    # simply return as args/kwargs in this case
-    if use_kwargs:
-        return args
-
-    # return sorted args without names in this case
-    return tuple(standardize_named_model_args(model_or_fw_or_sig, args)[0].values())
-
-
-def get_model_attributes(model: nn.Module) -> Dict[str, Any]:
-    """Get the key attributes of a PyTorch model."""
-    attrs = {}
-    attrs["type(model)"] = type(model).__name__
-    attrs["model.forward"] = getattr(model.forward, "__name__", None)
-    keys = ["training"]
-    for key in keys:
-        attrs[key] = getattr(model, key)
-    return attrs
-
-
-def compare_dict(dict1: Dict[str, Any], dict2: Dict[str, Any]) -> Tuple[str, ...]:
-    """Compare two dictionaries and return keys with unmatched values."""
-    keys_unmatched = tuple(k for k in dict1.keys() & dict2.keys() if dict1[k] != dict2[k])
-    keys_unmatched += tuple(dict1.keys() ^ dict2.keys())
-    return keys_unmatched
-
-
-def unwrap_model(
-    model: nn.Module, warn: bool = False, raise_error: bool = False, msg: str = ""
-) -> nn.Module:
-    """Unwrap a model that is wrapped by supported wrapper module or return original model."""
-    # NOTE: can be extended in the future for other frameworks and wrappers
-    supported_wrappers = {
-        nn.parallel.DataParallel: "module",  # indicating attribute key to unwrap
-        nn.parallel.DistributedDataParallel: "module",
-    }
-    unsupported_wrappers = {
-        torch.distributed.fsdp.FullyShardedDataParallel: None,
-    }
-    if type(model) in supported_wrappers:
-        if raise_error:
-            raise ValueError(msg or f"Model {model} is wrapped by {type(model)}!")
-        elif warn:
-            warnings.warn(msg or f"Model {model} is wrapped by {type(model)}; unwrapping...")
-        return getattr(model, supported_wrappers[type(model)])
-    elif type(model) in unsupported_wrappers:
-        raise ValueError(
-            f"Automatically unwrapping {type(model)} is not supported at this time! Please manually"
-            " unwrap the model before passing it in."
-        )
-    return model
-
-
-def zero_grad(model: nn.Module) -> None:
-    """Set any gradients in the model's parameters to None."""
-    for p in model.parameters():
-        if p.grad is not None:
-            p.grad = None
-
-
-def standardize_model_like_tuple(model: ModelLike) -> Tuple[Type[nn.Module], Tuple, Dict]:
-    """Standardize a model-like tuple."""
-    if not (isinstance(model, (type, tuple)) or callable(model)):
-        raise ValueError(f"Expected type or tuple or callable but got {model}")
-    return standardize_constructor_args(model)  # type: ignore[return-value]
-
-
-def standardize_constructor_args(constructor_args: ConstructorLike) -> Tuple[Callable, Tuple, Dict]:
-    """Standardize a constructor-like tuple."""
-    if not isinstance(constructor_args, tuple):
-        constructor_args = (constructor_args,)
-
-    if len(constructor_args) == 1:
-        constructor_args = (*constructor_args, (), {})
-    elif len(constructor_args) == 2:
-        constructor_args = (*constructor_args, {})
-
-    cls_or_callable, args, kwargs = constructor_args
-    assert isinstance(args, (tuple, list)), f"Invalid model args: {args}"
-    assert isinstance(kwargs, dict), f"Invalid model kwargs: {kwargs}"
-    return cls_or_callable, tuple(args), kwargs
-
-
-def init_model_from_model_like(model: ModelLike) -> nn.Module:
-    """Initialize a model from a model-like object.
-
-    Args:
-        model: A model-like object. Can be a nn.Module (returned as it is), a model class or callable, or a tuple.
-            If a tuple, it must be of the form (model_cls_or_callable,) or (model_cls_or_callable, args) or
-            (model_cls_or_callable, args, kwargs).
-            Model will be initialized as ``model_cls_or_callable(*args, **kwargs)``.
-    """
-    if isinstance(model, nn.Module):
-        return model
-
-    model_cls, args, kwargs = standardize_model_like_tuple(model)
-    return model_cls(*args, **kwargs)
-
-
-def run_forward_loop(
-    model,
-    data_loader: Iterable,
-    max_iters: Optional[int] = None,
-    collect_func: Optional[Callable[[Any], Union[Any, Tuple]]] = None,
-    progress_bar: Optional[str] = None,
-):
-    """Run multiple forward passes with a model according to the provided data loader.
-
-    Args:
-        model: The model with which we run forward.
-        data_loader: An iterator with data samples.
-        max_iters: Number of batches to run; by default it is infiinite or until ``data_loader``
-            is exhausted.
-        collect_func: A ``Callable`` that takes a batch of data from the ``data_loader``
-              as input and returns the input to ``model.forward()`` such that the return value
-              (``input``) is either:
-
-                #. a single argument (``type(input) != tuple``) corresponding to
-
-                   .. code-block:: python
-
-                        model.forward(input)
-
-                #. a tuple of arguments corresponding to
-
-                   .. code-block:: python
-
-                        model.forward(*input)
-
-                #. a tuple of arguments such that ``type(input[-1]) == dict`` corresponding to
-
-                   .. code-block:: python
-
-                        model.forward(*input[:-1], **input[-1])
-
-                .. note::
-
-                    In order to pass a dict as last non-keyword argument, you need to use a tuple as
-                    ``input`` and add an *empty* dict as the last element, e.g.,
-
-                    .. code-block:: python
-
-                        input = (x, {"y": y, "z": z}, {})
-
-                    The empty dict at the end will then be interpreted as the keyword args.
-
-                See the ``args`` argument of
-                `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
-                for more info on the format of the return value of ``collect_func`` (``input``).
-
-                The default ``collect_func`` assumes that the data loader returns a tuple, e.g.,
-                ``(images, labels, ...)``, and returns the first element of the tuple.
-
-        progress_bar: Set to a description string to see the progress bar.
-    """
-    device = get_module_device(model)
-    collect_fn = collect_func or (lambda x: x[0])
-    with tqdm(total=max_iters, desc=progress_bar, disable=(not progress_bar)) as t:
-        for idx, data in enumerate(data_loader):
-            if isinstance(max_iters, int) and idx >= max_iters:
-                break
-            args = standardize_model_args(model, collect_fn(data), use_kwargs=True)
-            args = torch_to(args, device)
-            model(*args[:-1], **args[-1])
-            t.update()
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions for PyTorch models."""
+import inspect
+import warnings
+from collections import abc, deque
+from typing import Any, Callable, Dict, Iterable, Optional, Set, Tuple, Type, Union
+
+import torch
+import torch.distributed.fsdp
+import torch.nn as nn
+from torch.nn.modules.batchnorm import _BatchNorm
+from tqdm import tqdm
+
+from .tensor import torch_to
+
+__all__ = [
+    "ModelLike",
+    "compare_dict",
+    "get_model_attributes",
+    "get_module_device",
+    "get_same_padding",
+    "init_model_from_model_like",
+    "is_channels_last",
+    "is_parallel",
+    "make_divisible",
+    "model_to",
+    "param_num",
+    "param_num_from_forward",
+    "remove_bn",
+    "set_submodule",
+    "standardize_model_args",
+    "standardize_model_like_tuple",
+    "standardize_named_model_args",
+    "standardize_constructor_args",
+    "unwrap_model",
+    "zero_grad",
+    "run_forward_loop",
+]
+
+ModelLike = Union[nn.Module, Type[nn.Module], Tuple, Callable]
+ConstructorLike = Union[Callable, Tuple]
+
+
+def is_parallel(model: nn.Module) -> bool:
+    """Check if a PyTorch model is parallelized."""
+    return isinstance(model, (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel))
+
+
+def get_module_device(module: nn.Module) -> torch.device:
+    """Get the device of a PyTorch module."""
+    try:
+        return next(module.parameters()).device
+    except StopIteration:
+        # For modules without parameters
+        return torch.device("cpu")
+
+
+def param_num(network: nn.Module, trainable_only: bool = False, unit=1e6) -> float:
+    """Get the number of parameters of a PyTorch model.
+
+    Args:
+        network: The PyTorch model.
+        trainable_only: Whether to only count trainable parameters. Default is False.
+        unit: The unit to return the number of parameters in. Default is 1e6 (million).
+
+    Returns:
+        The number of parameters in the model in the given unit.
+    """
+    return (
+        sum(
+            p.numel() if not trainable_only or p.requires_grad else 0
+            for mod in network.modules()
+            for p in mod.parameters(recurse=False)
+            if not isinstance(mod, _BatchNorm)
+        )
+        / unit
+    )
+
+
+# TODO: we could also use the same approach as in inference_flops to get the number of params,
+# which might be more accurate. Another approach could be to run a backwards pass and use a hook
+# on the tensor directly.
+def param_num_from_forward(
+    model: nn.Module,
+    trainable_only: bool = False,
+    args: Union[torch.Tensor, Tuple, None] = None,
+    unit: float = 1e6,
+):
+    """Get the number of parameters of a PyTorch model from a forward pass.
+
+    Args:
+        network: The PyTorch model.
+        trainable_only: Whether to only count trainable parameters. Default is False.
+        unit: The unit to return the number of parameters in. Default is 1e6 (million).
+
+    Returns:
+        The number of parameters from the model's forward pass in the given unit.
+
+    This can helpful for dynamic modules, where the state dict might contain extra parameters that
+    is not actively used in the model, e.g., because of a DynamicModule that is deactivated for the
+    forward pass. We circumvent this issue by just counting parameters of modules that appear in a
+    forward pass.
+    """
+    params = {}
+
+    def count_hook(m: nn.Module, *_):
+        if m not in params:  # don't double-count parameters
+            params[m] = sum(
+                getattr(m, n).numel()  # use getattr to retrieve param since it might be dynamic
+                for n, p in m.named_parameters(recurse=False)  # don't recurse!
+                if not trainable_only or p.requires_grad
+            )
+
+    # add hook to count parameters to all modules except _BatchNorm
+    hooks = [
+        m.register_forward_hook(count_hook)
+        for m in model.modules()
+        if not isinstance(m, _BatchNorm)
+    ]
+
+    # run forward pass
+    args = standardize_model_args(model, args, use_kwargs=True)
+    args = torch_to(args, get_module_device(model))
+    model(*args[:-1], **args[-1])
+
+    # remove hooks
+    for h in hooks:
+        h.remove()
+
+    # count parameters and return
+    return sum(params.values()) / unit
+
+
+def get_same_padding(kernel_size: Union[int, Tuple[int, int]]) -> Union[int, tuple]:
+    """Get the same padding for a given kernel size."""
+    if isinstance(kernel_size, tuple):
+        assert len(kernel_size) == 2, f"invalid kernel size: {kernel_size}"
+        p1 = get_same_padding(kernel_size[0])
+        p2 = get_same_padding(kernel_size[1])
+        return p1, p2
+    else:
+        assert isinstance(kernel_size, int), "kernel size should be either `int` or `tuple`"
+        assert kernel_size % 2 == 1, "kernel size should be odd number"
+        return kernel_size // 2
+
+
+def make_divisible(v: Union[int, float], divisor: Optional[int], min_val=None) -> Union[int, float]:
+    """Function taken from the original tf repo.
+
+    It ensures that all layers have a channel number that is divisible by 8
+    It can be seen here:
+    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
+    """
+    if divisor is None:
+        return v
+
+    if min_val is None:
+        min_val = divisor
+    new_v = max(min_val, int(v + divisor / 2) // divisor * divisor)
+    # Make sure that round down does not go down by more than 10%.
+    if new_v < 0.9 * v:
+        new_v += divisor
+    return int(new_v)
+
+
+def is_channels_last(model: nn.Module):
+    """Check if the model is using channels last memory format."""
+    # Infer target_model's memory_format
+    # from https://github.com/pytorch/tutorials/blob/444fbd16f2ddf9967baf8b06e83867a141b071c2/
+    # intermediate_source/memory_format_tutorial.py#L283
+    has_channels_last = any(
+        p.is_contiguous(memory_format=torch.channels_last) and not p.is_contiguous()
+        for p in model.parameters()
+    )
+    return has_channels_last
+
+
+def model_to(model: nn.Module, target_model: nn.Module):
+    """Convert model to the same device, dtype and memory layout as the target_model."""
+    has_channels_last = is_channels_last(target_model)
+    # return model with same device, dtype, memory_format as self
+    return model.to(
+        tensor=next(target_model.parameters()),
+        memory_format=torch.channels_last if has_channels_last else torch.contiguous_format,
+    )
+
+
+def set_submodule(model: nn.Module, target: str, target_submodule: nn.Module):
+    """The set function that complements nn.Module.get_submodule()."""
+    assert target != "", "Cannot set root module"
+
+    # Verify the original submodule exists
+    model.get_submodule(target)
+    parent_module = model.get_submodule(target.rpartition(".")[0])
+    child_name = target.split(".")[-1]
+    parent_module.add_module(child_name, target_submodule)
+
+
+def remove_bn(model: nn.Module):
+    """Remove all batch normalization layers in the network."""
+    for m in model.modules():
+        if isinstance(m, _BatchNorm):
+            m.weight = m.bias = None
+            m.forward = lambda x: x
+
+
+def _preprocess_args(args: Union[Any, Tuple]) -> Tuple:
+    """Return args in standardized format as tuple with last entry as kwargs."""
+    # Re: torch.onnx.utils._decide_input_format, which is used in torch.onnx.export:
+    # Starting in pytorch >= 1.12 tuplification is done in the beginning instead of at the end
+    # for the args. We want to be consistent with that behavior here.
+    # Specifically, this affects the case of just passing in one dict as args:
+    #   * In torch < 1.12, this will be treated as a single positional argument; see here:
+    #     https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/onnx/utils.py#L336
+    #   * In torch >= 1.12, this will be treated as variable keyword argument (**kwargs), see here:
+    #     https://github.com/pytorch/pytorch/blob/91e754b268c1869df5b2836f15c73e6ec1e265f1/torch/onnx/utils.py#L774
+    if torch.__version__ < "1.12" and isinstance(args, abc.Mapping):
+        args = (args, {})
+
+    # now we can safely tuplify the args and add kwargs if necessary
+    if not isinstance(args, tuple):
+        args = (args,)
+    if not isinstance(args[-1], abc.Mapping):
+        args = args + ({},)
+
+    return args
+
+
+def standardize_named_model_args(
+    model_or_fw_or_sig: Union[nn.Module, Callable, inspect.Signature], args: Union[Any, Tuple]
+) -> Tuple[Dict[str, Any], Set[str]]:
+    """Standardize model arguments according to torch.onnx.export and give them a name.
+
+    Args:
+        model_or_fw_or_sig: A nn.Module, its forward method, or its forward method's signature.
+        args: A tuple of args/kwargs or torch.Tensor feed into the model's ``forward()`` method.
+
+    Returns: A tuple (args_normalized, args_with_default) where
+        args_normalized is a dictionary of ordered model args where the key represents a unique
+            serialized string based on the the argument's name in the function signature and the
+            value contains the actual argument,
+        args_with_default is a set indicating whether the argument was retrieved from the default
+            value in the function signature of the model's ``forward()`` method or whether the
+            argument exactly corresponds to the default value.
+
+    .. note::
+
+        See :meth:`standardize_model_args() <modelopt.torch.utils.network.standardize_model_args>` for
+        more info as well.
+    """
+    # pre-process args
+    args = _preprocess_args(args)
+
+    # extract parameters from model signature
+    if isinstance(model_or_fw_or_sig, nn.Module):
+        model_or_fw_or_sig = inspect.signature(model_or_fw_or_sig.forward)
+    elif callable(model_or_fw_or_sig):
+        model_or_fw_or_sig = inspect.signature(model_or_fw_or_sig)
+    params = model_or_fw_or_sig.parameters
+
+    # we now continue to process the parameters in the function signature and classify them according
+    # to their kind (see https://docs.python.org/3/library/inspect.html#inspect.Parameter.kind for
+    # an overview of the different kinds of parameters in a function signature)
+
+    # sanity-check: kw-only must have default value and cannot be provided by user
+    kw_only = [
+        n
+        for n, p in params.items()
+        if p.kind == inspect.Parameter.KEYWORD_ONLY and (p.default == p.empty or n in args[-1])
+    ]
+    if kw_only:
+        raise AssertionError(f"Keyword-only args ({kw_only}) can only be used w/ default values.")
+
+    # sanity-check: kwargs in signature are okay but cannot be used by user!
+    has_kwargs = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in params.values())
+    kwargs_unexpected = any(kw not in params for kw in args[-1])
+    if has_kwargs and kwargs_unexpected:
+        raise AssertionError("Variable kwargs (**kwargs) are not supported.")
+
+    # sanity-check: no unexpected kwargs provided by user
+    assert not kwargs_unexpected, "Cannot provide unexpected keyword args!"
+
+    # now sort in args_dict and default values
+    args_queue = deque(args[:-1])
+    args_dict = args[-1]
+    args_normalized = {}
+    args_with_default = set()
+    for pname, param in params.items():
+        # we peel off all positional/keyword arguments and fill them accordingly
+        if param.kind in [param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD]:
+            if args_queue:
+                args_normalized[pname] = args_queue.popleft()
+            elif pname in args_dict:
+                args_normalized[pname] = args_dict[pname]
+            elif param.default != param.empty:
+                args_normalized[pname] = param.default
+            else:
+                # sanity check: any args without default value must be provided by the user
+                raise AssertionError(f"Argument {pname} must be provided by the user.")
+            # check if provided arg is exactly (``is``) the default value
+            if args_normalized[pname] is param.default:
+                args_with_default.add(pname)
+        # when we have a var-positional arg (*args) we fill in the rest of the args
+        elif param.kind == param.VAR_POSITIONAL:
+            idx = 0
+            while args_queue:
+                args_normalized[f"{pname}.{idx}"] = args_queue.popleft()
+                idx += 1
+            # we also do not need to process further since everything following a var-positional
+            # argument is keyword-only, which we don't allow!
+            break
+
+    # sanity-check: no positional arguments left
+    assert not args_queue, "Positional arguments left unprocessed; too many provided!"
+
+    # return the args (without kw-only args and kwargs) and set to indicate which args were
+    # retrieved from default value in the function signature and which args exactly correspond to
+    # the default value
+    return args_normalized, args_with_default
+
+
+def standardize_model_args(
+    model_or_fw_or_sig: Union[nn.Module, Callable, inspect.Signature],
+    args: Union[Any, Tuple],
+    use_kwargs=False,
+) -> Tuple:
+    """Standardize model arguments according to torch.onnx.export.
+
+    Args:
+        model_or_fw_or_sig: A nn.Module, its forward method, or its forward method's signature.
+        args: Refer to the ``dummy_input`` parameter in
+            :meth:`mtn.profile() <modelopt.torch.nas.algorithms.profile>`.
+        use_kwargs: Affects the return value, see below. For ``use_kwargs==False``, the returned
+            args are also compatible with ``torch.onnx.export``.
+
+    Returns:
+        Standardized model args that can be used in ``model.forward()`` in the same standardized
+        way no matter how they were provided, see below for more info.
+
+    * If ``use_kwargs == False``, the returned args can be used as
+
+      .. code-block:: python
+
+            args = standardize_model_args(model, args, use_kwargs=False)
+            model(*args)
+
+    * If ``use_kwargs == True``, the returned args can be used as
+
+      .. code-block:: python
+
+            args = standardize_model_args(model, args, use_kwargs=True)
+            model.forward(*args[:-1], **args[-1])
+
+    .. warning::
+
+        If ``use_kwargs == False`` the model's ``forward()`` method **cannot** contain keyword-only
+        arguments (e.g. ``forward(..., *, kw_only_args)``) without default values and you must not
+        provide them in ``args``.
+
+    .. warning::
+
+        If ``use_kwargs == False`` you must not provide variable keyword arguments in ``args`` that
+        are processed via variable keyword arguments in the model's ``forward()`` method
+        (e.g. ``forward(..., **kwargs)``).
+
+    """
+    # preprocess args
+    args = _preprocess_args(args)
+
+    # simply return as args/kwargs in this case
+    if use_kwargs:
+        return args
+
+    # return sorted args without names in this case
+    return tuple(standardize_named_model_args(model_or_fw_or_sig, args)[0].values())
+
+
+def get_model_attributes(model: nn.Module) -> Dict[str, Any]:
+    """Get the key attributes of a PyTorch model."""
+    attrs = {}
+    attrs["type(model)"] = type(model).__name__
+    attrs["model.forward"] = getattr(model.forward, "__name__", None)
+    keys = ["training"]
+    for key in keys:
+        attrs[key] = getattr(model, key)
+    return attrs
+
+
+def compare_dict(dict1: Dict[str, Any], dict2: Dict[str, Any]) -> Tuple[str, ...]:
+    """Compare two dictionaries and return keys with unmatched values."""
+    keys_unmatched = tuple(k for k in dict1.keys() & dict2.keys() if dict1[k] != dict2[k])
+    keys_unmatched += tuple(dict1.keys() ^ dict2.keys())
+    return keys_unmatched
+
+
+def unwrap_model(
+    model: nn.Module, warn: bool = False, raise_error: bool = False, msg: str = ""
+) -> nn.Module:
+    """Unwrap a model that is wrapped by supported wrapper module or return original model."""
+    # NOTE: can be extended in the future for other frameworks and wrappers
+    supported_wrappers = {
+        nn.parallel.DataParallel: "module",  # indicating attribute key to unwrap
+        nn.parallel.DistributedDataParallel: "module",
+    }
+    unsupported_wrappers = {
+        torch.distributed.fsdp.FullyShardedDataParallel: None,
+    }
+    if type(model) in supported_wrappers:
+        if raise_error:
+            raise ValueError(msg or f"Model {model} is wrapped by {type(model)}!")
+        elif warn:
+            warnings.warn(msg or f"Model {model} is wrapped by {type(model)}; unwrapping...")
+        return getattr(model, supported_wrappers[type(model)])
+    elif type(model) in unsupported_wrappers:
+        raise ValueError(
+            f"Automatically unwrapping {type(model)} is not supported at this time! Please manually"
+            " unwrap the model before passing it in."
+        )
+    return model
+
+
+def zero_grad(model: nn.Module) -> None:
+    """Set any gradients in the model's parameters to None."""
+    for p in model.parameters():
+        if p.grad is not None:
+            p.grad = None
+
+
+def standardize_model_like_tuple(model: ModelLike) -> Tuple[Type[nn.Module], Tuple, Dict]:
+    """Standardize a model-like tuple."""
+    if not (isinstance(model, (type, tuple)) or callable(model)):
+        raise ValueError(f"Expected type or tuple or callable but got {model}")
+    return standardize_constructor_args(model)  # type: ignore[return-value]
+
+
+def standardize_constructor_args(constructor_args: ConstructorLike) -> Tuple[Callable, Tuple, Dict]:
+    """Standardize a constructor-like tuple."""
+    if not isinstance(constructor_args, tuple):
+        constructor_args = (constructor_args,)
+
+    if len(constructor_args) == 1:
+        constructor_args = (*constructor_args, (), {})
+    elif len(constructor_args) == 2:
+        constructor_args = (*constructor_args, {})
+
+    cls_or_callable, args, kwargs = constructor_args
+    assert isinstance(args, (tuple, list)), f"Invalid model args: {args}"
+    assert isinstance(kwargs, dict), f"Invalid model kwargs: {kwargs}"
+    return cls_or_callable, tuple(args), kwargs
+
+
+def init_model_from_model_like(model: ModelLike) -> nn.Module:
+    """Initialize a model from a model-like object.
+
+    Args:
+        model: A model-like object. Can be a nn.Module (returned as it is), a model class or callable, or a tuple.
+            If a tuple, it must be of the form (model_cls_or_callable,) or (model_cls_or_callable, args) or
+            (model_cls_or_callable, args, kwargs).
+            Model will be initialized as ``model_cls_or_callable(*args, **kwargs)``.
+    """
+    if isinstance(model, nn.Module):
+        return model
+
+    model_cls, args, kwargs = standardize_model_like_tuple(model)
+    return model_cls(*args, **kwargs)
+
+
+def run_forward_loop(
+    model,
+    data_loader: Iterable,
+    max_iters: Optional[int] = None,
+    collect_func: Optional[Callable[[Any], Union[Any, Tuple]]] = None,
+    progress_bar: Optional[str] = None,
+):
+    """Run multiple forward passes with a model according to the provided data loader.
+
+    Args:
+        model: The model with which we run forward.
+        data_loader: An iterator with data samples.
+        max_iters: Number of batches to run; by default it is infiinite or until ``data_loader``
+            is exhausted.
+        collect_func: A ``Callable`` that takes a batch of data from the ``data_loader``
+              as input and returns the input to ``model.forward()`` such that the return value
+              (``input``) is either:
+
+                #. a single argument (``type(input) != tuple``) corresponding to
+
+                   .. code-block:: python
+
+                        model.forward(input)
+
+                #. a tuple of arguments corresponding to
+
+                   .. code-block:: python
+
+                        model.forward(*input)
+
+                #. a tuple of arguments such that ``type(input[-1]) == dict`` corresponding to
+
+                   .. code-block:: python
+
+                        model.forward(*input[:-1], **input[-1])
+
+                .. note::
+
+                    In order to pass a dict as last non-keyword argument, you need to use a tuple as
+                    ``input`` and add an *empty* dict as the last element, e.g.,
+
+                    .. code-block:: python
+
+                        input = (x, {"y": y, "z": z}, {})
+
+                    The empty dict at the end will then be interpreted as the keyword args.
+
+                See the ``args`` argument of
+                `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_
+                for more info on the format of the return value of ``collect_func`` (``input``).
+
+                The default ``collect_func`` assumes that the data loader returns a tuple, e.g.,
+                ``(images, labels, ...)``, and returns the first element of the tuple.
+
+        progress_bar: Set to a description string to see the progress bar.
+    """
+    device = get_module_device(model)
+    collect_fn = collect_func or (lambda x: x[0])
+    with tqdm(total=max_iters, desc=progress_bar, disable=(not progress_bar)) as t:
+        for idx, data in enumerate(data_loader):
+            if isinstance(max_iters, int) and idx >= max_iters:
+                break
+            args = standardize_model_args(model, collect_fn(data), use_kwargs=True)
+            args = torch_to(args, device)
+            model(*args[:-1], **args[-1])
+            t.update()
```

## modelopt/torch/utils/perf.py

 * *Ordering differences only*

```diff
@@ -1,83 +1,83 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-"""Utility functions for performance measurement."""
-
-from contextlib import ContextDecorator
-
-import torch
-
-from .logging import print_rank_0
-
-__all__ = ["clear_cuda_cache", "get_cuda_memory_stats", "report_memory", "Timer"]
-
-
-def clear_cuda_cache():
-    """Clear the CUDA cache."""
-    if torch.cuda.is_available():
-        torch.cuda.empty_cache()
-
-
-def get_cuda_memory_stats(device=None):
-    """Get memory usage of specified GPU in Bytes."""
-    return {
-        "allocated": torch.cuda.memory_allocated(device),
-        "max_allocated": torch.cuda.max_memory_allocated(device),
-        "reserved": torch.cuda.memory_reserved(device),
-        "max_reserved": torch.cuda.max_memory_reserved(device),
-    }
-
-
-def report_memory(name="", rank=0):
-    """Simple GPU memory report."""
-    memory_stats = get_cuda_memory_stats()
-    string = name + " memory (MB)"
-    for k, v in memory_stats.items():
-        string += f" | {k}: {v / 2**20: .2e}"
-
-    if torch.distributed.is_initialized():
-        string = f"[Rank {torch.distributed.get_rank()}] " + string
-        if torch.distributed.get_rank() == rank:
-            print(string, flush=True)
-    else:
-        print(string, flush=True)
-
-
-class Timer(ContextDecorator):
-    """A Timer that can be used as a decorator as well."""
-
-    def __init__(self, name=""):
-        """Initialize Timer."""
-        super().__init__()
-        self.name = name
-        self._start_event = torch.cuda.Event(enable_timing=True)
-        self._stop_event = torch.cuda.Event(enable_timing=True)
-        self.estimated_time = 0
-        self.start()
-
-    def start(self):
-        """Start the timer."""
-        self._start_event.record()
-        return self
-
-    def stop(self) -> float:
-        """End the timer."""
-        self._stop_event.record()
-        # Waits for everything to finish running
-        torch.cuda.synchronize()
-        self.estimated_time = self._start_event.elapsed_time(self._stop_event)
-        return self.estimated_time
-
-    def __enter__(self):
-        self.start()
-        return self
-
-    def __exit__(self, type, value, traceback):
-        self.stop()
-        print_rank_0(f"{self.name} took {self.estimated_time:.3e} ms")
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+"""Utility functions for performance measurement."""
+
+from contextlib import ContextDecorator
+
+import torch
+
+from .logging import print_rank_0
+
+__all__ = ["clear_cuda_cache", "get_cuda_memory_stats", "report_memory", "Timer"]
+
+
+def clear_cuda_cache():
+    """Clear the CUDA cache."""
+    if torch.cuda.is_available():
+        torch.cuda.empty_cache()
+
+
+def get_cuda_memory_stats(device=None):
+    """Get memory usage of specified GPU in Bytes."""
+    return {
+        "allocated": torch.cuda.memory_allocated(device),
+        "max_allocated": torch.cuda.max_memory_allocated(device),
+        "reserved": torch.cuda.memory_reserved(device),
+        "max_reserved": torch.cuda.max_memory_reserved(device),
+    }
+
+
+def report_memory(name="", rank=0):
+    """Simple GPU memory report."""
+    memory_stats = get_cuda_memory_stats()
+    string = name + " memory (MB)"
+    for k, v in memory_stats.items():
+        string += f" | {k}: {v / 2**20: .2e}"
+
+    if torch.distributed.is_initialized():
+        string = f"[Rank {torch.distributed.get_rank()}] " + string
+        if torch.distributed.get_rank() == rank:
+            print(string, flush=True)
+    else:
+        print(string, flush=True)
+
+
+class Timer(ContextDecorator):
+    """A Timer that can be used as a decorator as well."""
+
+    def __init__(self, name=""):
+        """Initialize Timer."""
+        super().__init__()
+        self.name = name
+        self._start_event = torch.cuda.Event(enable_timing=True)
+        self._stop_event = torch.cuda.Event(enable_timing=True)
+        self.estimated_time = 0
+        self.start()
+
+    def start(self):
+        """Start the timer."""
+        self._start_event.record()
+        return self
+
+    def stop(self) -> float:
+        """End the timer."""
+        self._stop_event.record()
+        # Waits for everything to finish running
+        torch.cuda.synchronize()
+        self.estimated_time = self._start_event.elapsed_time(self._stop_event)
+        return self.estimated_time
+
+    def __enter__(self):
+        self.start()
+        return self
+
+    def __exit__(self, type, value, traceback):
+        self.stop()
+        print_rank_0(f"{self.name} took {self.estimated_time:.3e} ms")
```

## modelopt/torch/utils/random.py

 * *Ordering differences only*

```diff
@@ -1,154 +1,154 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Random number generator with a deterministic, synchronized seed for sampling."""
-import random as _random
-from contextlib import contextmanager
-from typing import Any, Callable, MutableSequence, Optional, Sequence, TypeVar
-
-import torch
-
-from . import distributed as dist
-from .list import list_closest_to_median
-
-T = TypeVar("T")  # pylint: disable=invalid-name
-FSample = Callable[[Sequence[T]], T]
-
-
-def _get_generator(seed: Optional[int] = None) -> _random.Random:
-    # delete existing generator if manual seed is provided OR if we are now in distributed setting
-    # but weren't previously and so generator is not yet synced across GPUs (manual seed still takes
-    # precedence though).
-    if hasattr(_get_generator, "generator"):
-        if seed is not None or (
-            dist.size() > 1
-            and not getattr(_get_generator, "is_synced", False)
-            and not getattr(_get_generator, "is_manual", False)
-        ):
-            delattr(_get_generator, "generator")
-    if not hasattr(_get_generator, "generator"):
-        # synchronizing random seed and initialize generator
-        seed = dist.broadcast(seed or _random.getrandbits(64))
-        _get_generator.generator = _random.Random(seed)  # type: ignore[attr-defined]
-        _get_generator.is_manual = seed is not None  # type: ignore[attr-defined]
-        _get_generator.is_synced = dist.size() > 1  # type: ignore[attr-defined]
-    return _get_generator.generator  # type: ignore[attr-defined]
-
-
-def _set_deterministic_seed(seed: int = 1):
-    """Set the default seed for the random number generator  and synchronize it across GPUs."""
-    _get_generator(seed=seed)
-
-
-def random() -> float:
-    """Generate a random number from [0, 1) with a deterministic seed."""
-    return _get_generator().random()
-
-
-def choice(seq: Sequence[T]) -> T:
-    """Return a random element from the sequence using a synchronized seed.
-
-    Args:
-        seq: Sequence to sample from.
-
-    Returns:
-        Random element from the sequence.
-
-    This function is synchronized across all GPUs and can be used to sample a random subnet from a
-    search space via :meth:`mtn.sample()<modelopt.torch.nas.utils.sample>` such that the resulting
-    subnet/configuration is the same across all GPUs.
-
-    Example:
-    .. code-block:: python
-
-        from modelopt.torch.nas import random
-        import modelopt.torch.nas as mtn
-
-        # Sample a random subnet of a converted model
-        config = mtn.sample(model, random.choice)
-
-        # random.choice is also the default option for sample
-        config = mtn.sample(model)
-    """
-    return _get_generator().choice(seq)
-
-
-def sample(*args, **kwargs):
-    """Sample elements from a given population with a deterministic seed."""
-    return _get_generator().sample(*args, **kwargs)
-
-
-def centroid(seq: Sequence[T]) -> T:
-    """Reduce each element of the seq via ``torch.prod()`` and then return seq element closest.
-
-    Args:
-        seq: Sequence to determine centroid.
-
-    Returns:
-        Centroid of the sequence.
-
-    This function can be used to sample the centroid subnet of an search space via
-    :meth:`mtn.sample()<modelopt.torch.nas.utils.sample>`. The centroid subnet aims to cheaply
-    approximate the median of the search space defined by the ``model``.
-
-    Example:
-    .. code-block:: python
-
-        from modelopt.torch.nas import random
-        import modelopt.torch.nas as mtn
-
-        # Sample the centroid subnet of a converted model
-        config = mtn.sample(model, random.centroid)
-    """
-    seq_reduced = [torch.prod(torch.tensor(x)).item() for x in seq]
-    return seq[seq_reduced.index(list_closest_to_median(seq_reduced))]
-
-
-def original(seq: Sequence[T]) -> None:
-    """Return an indicator (None) that can be recognized internally to sample the original choice.
-
-    Args:
-        seq: Sequence of choices from which we want to "choose" original choice.
-
-    Returns:
-        None indicating to internally select the original choice from the sequence.
-
-    This function can be used to sample the original subnet of a search space via
-    :meth:`mtn.sample()<modelopt.torch.nas.utils.sample>`. The original subnet corresponds to the
-    model architecture before the conversion process.
-
-    Example:
-    .. code-block:: python
-
-        from modelopt.torch.nas import random
-        import modelopt.torch.nas as mtn
-
-        # Sample the original subnet of a converted model
-        config = mtn.sample(model, random.original)
-    """
-    return None
-
-
-def shuffle(seq: MutableSequence[Any]):
-    """Shuffle the sequence in-place with a deterministic seed."""
-    return _get_generator().shuffle(seq)
-
-
-@contextmanager
-def _deterministic_seed():
-    """Sets a deterministic seed whitin the context.
-
-    Resets the random state to prior upon exit.
-    """
-    old_random_generator = _get_generator()
-    _set_deterministic_seed(1024)
-    yield
-    delattr(_get_generator, "generator")
-    setattr(_get_generator, "generator", old_random_generator)
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Random number generator with a deterministic, synchronized seed for sampling."""
+import random as _random
+from contextlib import contextmanager
+from typing import Any, Callable, MutableSequence, Optional, Sequence, TypeVar
+
+import torch
+
+from . import distributed as dist
+from .list import list_closest_to_median
+
+T = TypeVar("T")  # pylint: disable=invalid-name
+FSample = Callable[[Sequence[T]], T]
+
+
+def _get_generator(seed: Optional[int] = None) -> _random.Random:
+    # delete existing generator if manual seed is provided OR if we are now in distributed setting
+    # but weren't previously and so generator is not yet synced across GPUs (manual seed still takes
+    # precedence though).
+    if hasattr(_get_generator, "generator"):
+        if seed is not None or (
+            dist.size() > 1
+            and not getattr(_get_generator, "is_synced", False)
+            and not getattr(_get_generator, "is_manual", False)
+        ):
+            delattr(_get_generator, "generator")
+    if not hasattr(_get_generator, "generator"):
+        # synchronizing random seed and initialize generator
+        seed = dist.broadcast(seed or _random.getrandbits(64))
+        _get_generator.generator = _random.Random(seed)  # type: ignore[attr-defined]
+        _get_generator.is_manual = seed is not None  # type: ignore[attr-defined]
+        _get_generator.is_synced = dist.size() > 1  # type: ignore[attr-defined]
+    return _get_generator.generator  # type: ignore[attr-defined]
+
+
+def _set_deterministic_seed(seed: int = 1):
+    """Set the default seed for the random number generator  and synchronize it across GPUs."""
+    _get_generator(seed=seed)
+
+
+def random() -> float:
+    """Generate a random number from [0, 1) with a deterministic seed."""
+    return _get_generator().random()
+
+
+def choice(seq: Sequence[T]) -> T:
+    """Return a random element from the sequence using a synchronized seed.
+
+    Args:
+        seq: Sequence to sample from.
+
+    Returns:
+        Random element from the sequence.
+
+    This function is synchronized across all GPUs and can be used to sample a random subnet from a
+    search space via :meth:`mtn.sample()<modelopt.torch.nas.utils.sample>` such that the resulting
+    subnet/configuration is the same across all GPUs.
+
+    Example:
+    .. code-block:: python
+
+        from modelopt.torch.nas import random
+        import modelopt.torch.nas as mtn
+
+        # Sample a random subnet of a converted model
+        config = mtn.sample(model, random.choice)
+
+        # random.choice is also the default option for sample
+        config = mtn.sample(model)
+    """
+    return _get_generator().choice(seq)
+
+
+def sample(*args, **kwargs):
+    """Sample elements from a given population with a deterministic seed."""
+    return _get_generator().sample(*args, **kwargs)
+
+
+def centroid(seq: Sequence[T]) -> T:
+    """Reduce each element of the seq via ``torch.prod()`` and then return seq element closest.
+
+    Args:
+        seq: Sequence to determine centroid.
+
+    Returns:
+        Centroid of the sequence.
+
+    This function can be used to sample the centroid subnet of an search space via
+    :meth:`mtn.sample()<modelopt.torch.nas.utils.sample>`. The centroid subnet aims to cheaply
+    approximate the median of the search space defined by the ``model``.
+
+    Example:
+    .. code-block:: python
+
+        from modelopt.torch.nas import random
+        import modelopt.torch.nas as mtn
+
+        # Sample the centroid subnet of a converted model
+        config = mtn.sample(model, random.centroid)
+    """
+    seq_reduced = [torch.prod(torch.tensor(x)).item() for x in seq]
+    return seq[seq_reduced.index(list_closest_to_median(seq_reduced))]
+
+
+def original(seq: Sequence[T]) -> None:
+    """Return an indicator (None) that can be recognized internally to sample the original choice.
+
+    Args:
+        seq: Sequence of choices from which we want to "choose" original choice.
+
+    Returns:
+        None indicating to internally select the original choice from the sequence.
+
+    This function can be used to sample the original subnet of a search space via
+    :meth:`mtn.sample()<modelopt.torch.nas.utils.sample>`. The original subnet corresponds to the
+    model architecture before the conversion process.
+
+    Example:
+    .. code-block:: python
+
+        from modelopt.torch.nas import random
+        import modelopt.torch.nas as mtn
+
+        # Sample the original subnet of a converted model
+        config = mtn.sample(model, random.original)
+    """
+    return None
+
+
+def shuffle(seq: MutableSequence[Any]):
+    """Shuffle the sequence in-place with a deterministic seed."""
+    return _get_generator().shuffle(seq)
+
+
+@contextmanager
+def _deterministic_seed():
+    """Sets a deterministic seed whitin the context.
+
+    Resets the random state to prior upon exit.
+    """
+    old_random_generator = _get_generator()
+    _set_deterministic_seed(1024)
+    yield
+    delattr(_get_generator, "generator")
+    setattr(_get_generator, "generator", old_random_generator)
```

## modelopt/torch/utils/tensor.py

 * *Ordering differences only*

```diff
@@ -1,50 +1,50 @@
-# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-#
-# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-# property and proprietary rights in and to this material, related
-# documentation and any modifications thereto. Any use, reproduction,
-# disclosure or distribution of this material and related documentation
-# without an express license agreement from NVIDIA CORPORATION or
-# its affiliates is strictly prohibited.
-
-"""Utility functions for PyTorch tensors."""
-from collections import abc
-from typing import List
-
-import numpy as np
-import torch
-
-__all__ = ["torch_to", "torch_detach", "torch_to_numpy", "numpy_to_torch"]
-
-
-def torch_to(data, *args, **kwargs):
-    """Try to recursively move the data to the specified args/kwargs."""
-    if isinstance(data, torch.Tensor):
-        return data.to(*args, **kwargs)
-    elif isinstance(data, (tuple, list)):
-        return type(data)([torch_to(val, *args, **kwargs) for val in data])
-    elif isinstance(data, abc.Mapping):
-        return {k: torch_to(val, *args, **kwargs) for k, val in data.items()}
-    return data
-
-
-def torch_detach(data):
-    """Try to recursively detach the data from the computation graph."""
-    if isinstance(data, torch.Tensor):
-        return torch.detach(data)
-    elif isinstance(data, (tuple, list)):
-        return type(data)([torch_detach(val) for val in data])
-    elif isinstance(data, abc.Mapping):
-        return {k: torch_detach(val) for k, val in data.items()}
-    return data
-
-
-def torch_to_numpy(inputs: List[torch.Tensor]) -> List[np.ndarray]:
-    """Convert torch tensors to numpy arrays."""
-    return [t.detach().cpu().numpy() for t in inputs]
-
-
-def numpy_to_torch(np_outputs: List[np.ndarray]) -> List[torch.Tensor]:
-    """Convert numpy arrays to torch tensors."""
-    return [torch.from_numpy(arr) for arr in np_outputs]
+# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+"""Utility functions for PyTorch tensors."""
+from collections import abc
+from typing import List
+
+import numpy as np
+import torch
+
+__all__ = ["torch_to", "torch_detach", "torch_to_numpy", "numpy_to_torch"]
+
+
+def torch_to(data, *args, **kwargs):
+    """Try to recursively move the data to the specified args/kwargs."""
+    if isinstance(data, torch.Tensor):
+        return data.to(*args, **kwargs)
+    elif isinstance(data, (tuple, list)):
+        return type(data)([torch_to(val, *args, **kwargs) for val in data])
+    elif isinstance(data, abc.Mapping):
+        return {k: torch_to(val, *args, **kwargs) for k, val in data.items()}
+    return data
+
+
+def torch_detach(data):
+    """Try to recursively detach the data from the computation graph."""
+    if isinstance(data, torch.Tensor):
+        return torch.detach(data)
+    elif isinstance(data, (tuple, list)):
+        return type(data)([torch_detach(val) for val in data])
+    elif isinstance(data, abc.Mapping):
+        return {k: torch_detach(val) for k, val in data.items()}
+    return data
+
+
+def torch_to_numpy(inputs: List[torch.Tensor]) -> List[np.ndarray]:
+    """Convert torch tensors to numpy arrays."""
+    return [t.detach().cpu().numpy() for t in inputs]
+
+
+def numpy_to_torch(np_outputs: List[np.ndarray]) -> List[torch.Tensor]:
+    """Convert numpy arrays to torch tensors."""
+    return [torch.from_numpy(arr) for arr in np_outputs]
```

## Comparing `nvidia_modelopt-0.11.1.dist-info/LICENSE` & `nvidia_modelopt-0.11.2.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
-SPDX-License-Identifier: LicenseRef-NvidiaProprietary
-
-NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
-property and proprietary rights in and to this material, related
-documentation and any modifications thereto. Any use, reproduction,
-disclosure or distribution of this material and related documentation
-without an express license agreement from NVIDIA CORPORATION or
-its affiliates is strictly prohibited.
+SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+
+NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+property and proprietary rights in and to this material, related
+documentation and any modifications thereto. Any use, reproduction,
+disclosure or distribution of this material and related documentation
+without an express license agreement from NVIDIA CORPORATION or
+its affiliates is strictly prohibited.
```

## Comparing `nvidia_modelopt-0.11.1.dist-info/METADATA` & `nvidia_modelopt-0.11.2.dist-info/METADATA`

 * *Files 21% similar despite different names*

```diff
@@ -1,64 +1,55 @@
-Metadata-Version: 2.1
-Name: nvidia-modelopt
-Version: 0.11.1
-Summary: Nvidia TensorRT Model Optimizer: a unified model optimization and deployment toolkit.
-Author-email: "Nvidia, Inc." <ammo-support@exchange.nvidia.com>
-License: NVIDIA Proprietary Software
-Project-URL: Homepage, https://nvidia.com
-Classifier: Programming Language :: Python :: 3
-Classifier: Operating System :: OS Independent
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Intended Audience :: Science/Research
-Requires-Python: <3.12,>=3.8
-Description-Content-Type: text/x-rst
-License-File: LICENSE
-Requires-Dist: cloudpickle >=1.6.0
-Requires-Dist: ninja
-Requires-Dist: numpy
-Requires-Dist: packaging
-Requires-Dist: pydantic >=2.0
-Requires-Dist: rich
-Requires-Dist: scipy
-Requires-Dist: tqdm
-Provides-Extra: all
-Requires-Dist: onnx ; extra == 'all'
-Requires-Dist: onnxruntime ~=1.16.3 ; extra == 'all'
-Requires-Dist: onnx-graphsurgeon ; extra == 'all'
-Requires-Dist: onnxruntime-extensions >=0.10.1 ; extra == 'all'
-Requires-Dist: transformers ; extra == 'all'
-Requires-Dist: accelerate ; extra == 'all'
-Requires-Dist: torch >=1.11 ; extra == 'all'
-Requires-Dist: torchvision ; extra == 'all'
-Provides-Extra: deploy
-Provides-Extra: dev-unit
-Requires-Dist: coverage ; extra == 'dev-unit'
-Requires-Dist: parameterized ; extra == 'dev-unit'
-Requires-Dist: pytest ; extra == 'dev-unit'
-Requires-Dist: pytest-asyncio ; extra == 'dev-unit'
-Requires-Dist: pytest-cov ; extra == 'dev-unit'
-Requires-Dist: pytest-timeout ; extra == 'dev-unit'
-Requires-Dist: toml ; extra == 'dev-unit'
-Requires-Dist: tox ==4.14.2 ; extra == 'dev-unit'
-Requires-Dist: tox-current-env ; extra == 'dev-unit'
-Provides-Extra: hf
-Requires-Dist: transformers ; extra == 'hf'
-Requires-Dist: accelerate ; extra == 'hf'
-Provides-Extra: onnx
-Requires-Dist: onnx ; extra == 'onnx'
-Requires-Dist: onnxruntime ~=1.16.3 ; extra == 'onnx'
-Requires-Dist: onnx-graphsurgeon ; extra == 'onnx'
-Requires-Dist: onnxruntime-extensions >=0.10.1 ; extra == 'onnx'
-Provides-Extra: torch
-Requires-Dist: torch >=1.11 ; extra == 'torch'
-Requires-Dist: torchvision ; extra == 'torch'
-
-Model Optimizer
-###############
-
-*Nvidia TensorRT Model Optimizer.*
-
---------
-
-Model Optimizer (ModelOpt) is a library for conveniently optimizing and deploying efficient neural networks that can
-fit a wide range of Nvidia hardware. It is intended for ML engineers to efficiently design, train,
-and deploy their models on Nvidia from within their desired ML training framework, e.g., PyTorch.
+Metadata-Version: 2.1
+Name: nvidia-modelopt
+Version: 0.11.2
+Summary: Nvidia TensorRT Model Optimizer: a unified model optimization and deployment toolkit.
+Author-email: "Nvidia, Inc." <ammo-support@exchange.nvidia.com>
+License: NVIDIA Proprietary Software
+Project-URL: Homepage, https://github.com/NVIDIA/TensorRT-Model-Optimizer
+Classifier: Programming Language :: Python :: 3
+Classifier: Operating System :: OS Independent
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Intended Audience :: Science/Research
+Requires-Python: <3.12,>=3.8
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: cloudpickle >=1.6.0
+Requires-Dist: ninja
+Requires-Dist: numpy
+Requires-Dist: packaging
+Requires-Dist: pydantic >=2.0
+Requires-Dist: rich
+Requires-Dist: scipy
+Requires-Dist: tqdm
+Provides-Extra: all
+Requires-Dist: torch >=1.11 ; extra == 'all'
+Requires-Dist: torchvision ; extra == 'all'
+Requires-Dist: onnx ; extra == 'all'
+Requires-Dist: onnxruntime ; extra == 'all'
+Requires-Dist: onnx-graphsurgeon ; extra == 'all'
+Requires-Dist: onnxruntime-extensions >=0.10.1 ; extra == 'all'
+Requires-Dist: transformers ; extra == 'all'
+Requires-Dist: accelerate ; extra == 'all'
+Provides-Extra: deploy
+Provides-Extra: dev-unit
+Requires-Dist: coverage ; extra == 'dev-unit'
+Requires-Dist: parameterized ; extra == 'dev-unit'
+Requires-Dist: pytest ; extra == 'dev-unit'
+Requires-Dist: pytest-asyncio ; extra == 'dev-unit'
+Requires-Dist: pytest-cov ; extra == 'dev-unit'
+Requires-Dist: pytest-timeout ; extra == 'dev-unit'
+Requires-Dist: toml ; extra == 'dev-unit'
+Requires-Dist: tox ==4.14.2 ; extra == 'dev-unit'
+Requires-Dist: tox-current-env ; extra == 'dev-unit'
+Provides-Extra: hf
+Requires-Dist: transformers ; extra == 'hf'
+Requires-Dist: accelerate ; extra == 'hf'
+Provides-Extra: onnx
+Requires-Dist: onnx ; extra == 'onnx'
+Requires-Dist: onnxruntime ; extra == 'onnx'
+Requires-Dist: onnx-graphsurgeon ; extra == 'onnx'
+Requires-Dist: onnxruntime-extensions >=0.10.1 ; extra == 'onnx'
+Provides-Extra: torch
+Requires-Dist: torch >=1.11 ; extra == 'torch'
+Requires-Dist: torchvision ; extra == 'torch'
+
+Checkout the [documentation](https://nvidia.github.io/TensorRT-Model-Optimizer) for more information.
```

## Comparing `nvidia_modelopt-0.11.1.dist-info/RECORD` & `nvidia_modelopt-0.11.2.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,128 +1,130 @@
-modelopt/__init__.py,sha256=t4cVfYKBeLUteTaxDlZl2sIXnM4AaAfKFFRfUA0XbsA,694
-modelopt/deploy/__init__.py,sha256=kPLOuoLCaV0xwzl2Me4kXwJECUXDIa_YR2ozADCnYbQ,604
-modelopt/deploy/llm/__init__.py,sha256=qczpeDFe-TSphksKq1BWHqMQLCUwLKgVPadw05b-fYY,2390
-modelopt/deploy/llm/generate.py,sha256=DC8o4xl_lH7BqP0_swcLWQDG5Mr6UIa0GStkt0iFEGY,5213
-modelopt/deploy/llm/model_config_trt.py,sha256=svfuyx2PH1pZBCT3VXYvjs9Ozv2LiT382m12cgFMJXo,12337
-modelopt/deploy/llm/nemo_utils.py,sha256=VEN1dz02mwiD8gj-zAG1cRcW-mdDxqfnE-BJxeBaZ3U,6892
-modelopt/onnx/__init__.py,sha256=2FUcmfR-kxw22Gd3a7cFiQrE6BjiUHZ7ZtXlYW0INE4,791
-modelopt/onnx/op_types.py,sha256=KXitAMSzXYlQvvtL9u4553Ov7Ga2yC5Lq3bmTjor9Tk,8003
-modelopt/onnx/utils.py,sha256=GfJDKUxg2kse2NtyKrjwVrwmUQ_PJjwwJBf_pte3DKQ,19828
-modelopt/onnx/quantization/__init__.py,sha256=X4YfnosBlCqdsAjIuYHOYKVdrrGrzSFcouCwn9yMocM,619
-modelopt/onnx/quantization/__main__.py,sha256=x7m_ORCfBwB06MKmBJcczSQD5ohD5ozlqNaWGURCZQU,4605
-modelopt/onnx/quantization/calib_utils.py,sha256=YgzjMcagg-Mp9PNSxVIBxkjzt5UoTppNILEvdBp6La8,4605
-modelopt/onnx/quantization/graph_utils.py,sha256=zcREuVYFLcxKVT8kmS_pYyiFPcEMEfxko-7PHFVJy9Q,18754
-modelopt/onnx/quantization/gs_patching.py,sha256=EqE3DtVsUsaZFNGamPsYLZcYDpeYv_Bdh4NaJYvCEgg,4132
-modelopt/onnx/quantization/int4.py,sha256=S-yVyhxaMrFrid_fqh8XljUQRtGTU9AXI4i_-p1yiLM,18389
-modelopt/onnx/quantization/operators.py,sha256=sP8VMx8tMcjz-WEBvjzAC87oPognbt6c_5lV68agDDY,4306
-modelopt/onnx/quantization/ort_patching.py,sha256=TtCECorZIuknwADi4IUVFgikmru2Fzq1CjFx3WTVH6M,8217
-modelopt/onnx/quantization/ort_utils.py,sha256=YMy7gD7U0kijeA-3-DRxQzk0wr9iOR0-AI4P7y467RE,1053
-modelopt/onnx/quantization/partitioning.py,sha256=D2b16rIOZyNLKCPOZATGZyoxu16a9UrgVR5E2kjkXtI,14046
-modelopt/onnx/quantization/qdq_utils.py,sha256=AzScMs3y6MKs7_Oy63_XSAk-zwwWTgeMBf1YYjEvE6A,7440
-modelopt/onnx/quantization/quant_utils.py,sha256=-pWAKMONfs8y0m54LuCvb6RwSbYfjyKENKsMExiyr7M,2475
-modelopt/onnx/quantization/quantize.py,sha256=jJLm_W_0Di2759e_pX4QEvtFTJFEl5BEkGL9dghyNl4,21116
-modelopt/torch/__init__.py,sha256=jlb5BBtkY93z1952iYPpx-qrVp1iP-2TL4fYl7-4BOg,805
-modelopt/torch/_deploy/__init__.py,sha256=wLswmGXJx9LEBt7qspaRgfZ85WV3PhlOJgZErB5l1K8,876
-modelopt/torch/_deploy/compilation.py,sha256=qc6kZNAiyLr3JGmTHceYxzv782zhFx_a3esv_aSRNnM,4824
-modelopt/torch/_deploy/device_model.py,sha256=Erk23_sh8R9y60J_YDchQyNS6KC0b5q3B3JFA00ovAQ,5522
-modelopt/torch/_deploy/profiling.py,sha256=aO-T_jtNHEk0M9whkFw-aWaq4_bRnIf0jeeTsGLrhcs,7588
-modelopt/torch/_deploy/_runtime/__init__.py,sha256=xVnt845HcVeoBK9RxvivmyEXkROiDrx6gIELsXDEJAE,957
-modelopt/torch/_deploy/_runtime/common.py,sha256=vzqRT2DHbTchiJddXdcrm3TTeYwJ3ZjwWDFeFYKXbIY,2327
-modelopt/torch/_deploy/_runtime/ort_client.py,sha256=wVLcgNPFE5xTCDY77gd4bvWYS72QeZIaiEiR4qIPTys,7266
-modelopt/torch/_deploy/_runtime/registry.py,sha256=BRzIT-Qqrz1bSiIx7OwU7Umf627n_PSByaghL5m8830,2224
-modelopt/torch/_deploy/_runtime/runtime_client.py,sha256=DQpJfwPHdXCB6EiSHbbepvkCqP0aP7yqyGRzCKG0q7k,5865
-modelopt/torch/_deploy/_runtime/trt_client.py,sha256=ClOVYKRyl-n7m_DH1FQkcq7n3jQxtW5w6pAC2AXZ8Lc,7730
-modelopt/torch/_deploy/_runtime/tensorrt/constants.py,sha256=_vqNBMtba0jV9BfBT06F5TcjD0qmQGznXsCJaohopBI,2594
-modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cp39-win_amd64.pyd,sha256=f5dx7ZEQd8CTnYGBHgaWeZNrRB7DhpYNiJ6A25dFG-Y,119296
-modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py,sha256=Om3X9ryKTsqRFWPN0vkdoSDq9DV7wO0-ErVeyFZIWQI,1763
-modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py,sha256=s2-8XGsrlzTtkWVSfZlFNptlyEGQ3gLL0vH8tiJEEn0,6972
-modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py,sha256=k2DcHME2FYj61QYDuI0-hdEaAYWGm0jDgdShNW0jOp8,5691
-modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py,sha256=lWQpy4eFQ7v4wQIT9aPQdIrM2aC2hg6zJasoFUxymcc,6524
-modelopt/torch/_deploy/utils/__init__.py,sha256=IsziBUwYpHo5BDrPdakSq7qYp-UJbiEPZ6i957NSf6A,633
-modelopt/torch/_deploy/utils/onnx_optimizer.py,sha256=i36vl83-Np_WLrpzh28whxSqWf2LFt246NvXtQolo_c,4730
-modelopt/torch/_deploy/utils/torch_onnx.py,sha256=UZNtj8cvfW3gOW7JPUv1yah6mvah2OVnq0zjT-EeBBo,20946
-modelopt/torch/export/__init__.py,sha256=sbUwLSbjSx9mWji8-RFcM6RQuNElqEeAJHfGaxwlykY,848
-modelopt/torch/export/distribute.py,sha256=DWx7kSyo4_0GIM4pspGOuhsczOg8NYec3cCrzg6KLC8,12668
-modelopt/torch/export/layer_utils.py,sha256=Z-Q-kxhfSarp32FquZ5EGmmHRBX5YQtgtvFojQ-kLrg,52089
-modelopt/torch/export/model_config.py,sha256=7U9UwAS2S8MC1k4fQtPCQW9yKIi9Ts66Nvo9qjuv2TA,13780
-modelopt/torch/export/model_config_export.py,sha256=Nx_Jtdyi-2W7jAGJWeC2244nimWcdEarzYLJoj6FPYM,17778
-modelopt/torch/export/model_config_utils.py,sha256=9WPNMtbyB8oqWhLR_0Tzy-7dAtF7BTc2Vp2FixXc7Io,18360
-modelopt/torch/export/postprocess.py,sha256=j-vywTNk2RnmHoYfY0sBIwh1yTMLhzwKJdtWI_V8qr0,29334
-modelopt/torch/export/scaling_factor_utils.py,sha256=Rvie1EqsSt5vhWnQcQ6yR78US_4NK7d7zdDgHin09b0,3182
-modelopt/torch/export/tensorrt_llm_utils.py,sha256=OIgPd7VdaWgK3XYpF0aimq1FK81Ube-NF5HYRuz2w5Y,11289
-modelopt/torch/export/transformer_engine.py,sha256=FXbHlEY5jkzLpWaQeeHqqBhdie7GZvdO0yFJ22FJDMw,4088
-modelopt/torch/opt/__init__.py,sha256=ftGL2EIBmHUHbG15ZwHZGF9HzIr08NwW6NffxlVm6hE,1360
-modelopt/torch/opt/_hooks.py,sha256=PVO8kSihdf46nc-8zU5oqFU3lwm1V68CkNOlgrEFKHQ,3137
-modelopt/torch/opt/config.py,sha256=CBeNoDb6qFw5mgFu4Nl-BLaPCLWsfVFK30sOjeTpz-M,15227
-modelopt/torch/opt/conversion.py,sha256=-ZGqMKTYEulQbLfQmfqhm3K2tLHt8bOGDJtYRK-qaSE,24050
-modelopt/torch/opt/dynamic.py,sha256=Q_iakZ3JHE5ikIcl-LflkWCs4e318rn7VKEsH56WvN8,57369
-modelopt/torch/opt/hparam.py,sha256=DDkdVsC_1LwjIp1aUNf5uq-nOFzREsBKCKqRR8UezJw,9461
-modelopt/torch/opt/mode.py,sha256=9ys9j_HQ0uzUv7rWMQ4MvswMcRTyEDHap2iyNd_mpbI,12723
-modelopt/torch/opt/searcher.py,sha256=H4rv0yHFkMQuZA0kOwRqNmSs-oHmSaLoWlAOJ8ISQoY,9570
-modelopt/torch/opt/utils.py,sha256=XMLCLaOKyZxjdCkWv0jlqf4kNUyEZyocqoLdzWZCh3s,1955
-modelopt/torch/quantization/__init__.py,sha256=ja68_nADm7sL-YuGpElmuu-Ve9DN6uKtONEUpFROYe8,846
-modelopt/torch/quantization/config.py,sha256=y4_JdUB4svxqm8mv6wYQTlNbcX-0NnBqQDYqim23Os0,13912
-modelopt/torch/quantization/conversion.py,sha256=0qoiIAczFtZKJREhVMpTNjwBduXV-g0_ZlkglZl6-pM,10771
-modelopt/torch/quantization/extensions.py,sha256=cYxeCB9z33ooN8XoCuCZTFvl-YLgjNCJSVIe5sI1tcg,1109
-modelopt/torch/quantization/mode.py,sha256=OQcfuY9j8lJRmuMWKGca00HLpOz9ufgyTGoZaZf22j8,4062
-modelopt/torch/quantization/model_calib.cp39-win_amd64.pyd,sha256=DGD25I9CTXFYcVkSDJ9fJpmSEyzTwitNRrQevDh2bHQ,322560
-modelopt/torch/quantization/model_quant.py,sha256=0sm586IppQmULxuGRc4QiyLyy1JZIwKLRAXZFVAmdkg,6302
-modelopt/torch/quantization/optim.py,sha256=SBYwxlDU8YQH8QyEq_GRG_8_NsqdOMoSKcAw3e00sMw,1393
-modelopt/torch/quantization/quant_modules.py,sha256=CI4kw5-cXgQBWotSOM36q3ZxJmlDzAWkT05k4tR5HKI,1725
-modelopt/torch/quantization/tensor_quant.py,sha256=AFHsMkMQgxPDjxHhpIudICQsS4NnheAKT4uCmkeVYlQ,29725
-modelopt/torch/quantization/utils.py,sha256=OAmRGhxYzk-WfXa_vYc5PO9P0ebGk7eREU_GJph75z4,5703
-modelopt/torch/quantization/calib/__init__.py,sha256=QC2CDk13q2ZbfENLeWk4s3ckC8FJBd7pTolMc44Sf4A,816
-modelopt/torch/quantization/calib/calibrator.py,sha256=mOtasRBGoPfYFh4aw2XeqQZmCsj__dHq-5P-3Fxm7s0,1832
-modelopt/torch/quantization/calib/histogram.py,sha256=IqcxhdIFKsMuaAp95GMYu-RzwHL0iJCdOX65T6wO6YQ,17445
-modelopt/torch/quantization/calib/max.py,sha256=ws0NnHv4EeiLr3iVOPoajQM3zmeI2jQ7REj76B3psUI,3436
-modelopt/torch/quantization/nn/__init__.py,sha256=_6vmgxHTsy7UY7aKLJlExoGQ3x8AEjiTh9RNDHbz1kk,945
-modelopt/torch/quantization/nn/functional.py,sha256=1J5SB4T3DpC-KBn9AR473kSwa0zCTKGyT9xxb3eKrlU,2822
-modelopt/torch/quantization/nn/modules/__init__.py,sha256=hiV4Q5QP0y1rl1zOgawSoknb6I-drlZ-Xr6EUmiwyeE,601
-modelopt/torch/quantization/nn/modules/_utils.py,sha256=yWWFcCIz09SKYDIPLusfZ2vJckdC4IiDQF0nuOzwDDM,8338
-modelopt/torch/quantization/nn/modules/clip.py,sha256=-8NWLJoX2kUt4FEvfikX9y5UlL1lDtpFMSQ1VxCH4so,2317
-modelopt/torch/quantization/nn/modules/quant_activations.py,sha256=9qxIwQcQj3oftQR84LfAEs9oOCVdDeBQzVLM3vyZIPM,1389
-modelopt/torch/quantization/nn/modules/quant_batchnorm.py,sha256=oUGuPEjzJIN3LzJlx7BEnLG63QbW-hLNHJXVVq1ZF3w,1568
-modelopt/torch/quantization/nn/modules/quant_conv.py,sha256=QMNBQRk1tPD6YbgiDMfAyWI7bWXfg-jAiGW9fij3bjo,3986
-modelopt/torch/quantization/nn/modules/quant_instancenorm.py,sha256=R2Qr4OFDdCaZLzRX8jlcJGL_kmVujLW01hlJDwopfHg,1556
-modelopt/torch/quantization/nn/modules/quant_linear.py,sha256=oGGrOR75vGSLxPZ_-GEVYeT_IPARnzI7Vjd6_Y2TGRM,2762
-modelopt/torch/quantization/nn/modules/quant_module.py,sha256=meEhgENukyaaU-UBAN3X8JHhbUDWu68fmleJ7C274IY,7849
-modelopt/torch/quantization/nn/modules/quant_pooling.py,sha256=mQfjb0Hao7wKxh4SgrE6_ll4aeVuqE8MA2nM_5IAE8g,3556
-modelopt/torch/quantization/nn/modules/tensor_quantizer.py,sha256=468W-LhHIE_cYwAdu6Cp72hypruTK-M5hMcbE6Wyl7Y,34362
-modelopt/torch/quantization/plugins/__init__.py,sha256=loDBZvUrrpz-xZAuPTix28x9Y7szaB4L23C8iNZnNT4,1892
-modelopt/torch/quantization/plugins/apex.py,sha256=GhgQFLYUW1fWSZ8PPRASs_HYLhITh2woUkNft5Ry_Yo,1484
-modelopt/torch/quantization/plugins/custom.py,sha256=Q-WprnXxeP1rZ3UlMmKgo6a9uiHZXezysFLM1mryx0Q,4226
-modelopt/torch/quantization/plugins/diffusers.py,sha256=18IaJCbQPX84tjWhmApPDPrSPRpB7SPfvaB9uvnQl2A,3453
-modelopt/torch/quantization/plugins/huggingface.py,sha256=sQRyRWua3r5B7-Mi1HcIAxZwfjETf7Ib7j23a6vX5WE,2489
-modelopt/torch/quantization/plugins/megatron.py,sha256=h7nyRTH9rKCBe1MsrraKo30wj5SmRBWyDBXAlGlKDpY,4804
-modelopt/torch/quantization/plugins/nemo.py,sha256=EN21xNTtlY0EVQeeZomMrpSI7V_V-twt0Hg8odB6YzQ,2472
-modelopt/torch/quantization/src/tensor_quant.cpp,sha256=iyjrAdeIlQLI7QTSU2ynpC6l7R04ZyzJ5L8IqixlIBA,3061
-modelopt/torch/quantization/src/tensor_quant_fp8.cpp,sha256=3sZMuzS9Rh92-_sVwos9aPEqRs8-vHULIriOmQSMWYI,1320
-modelopt/torch/quantization/src/tensor_quant_gpu.cu,sha256=ZjyZLEJKzElXsGJMpq5I9qdrK--NT9EPRjgu8P2MAYs,5557
-modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu,sha256=ONDjED7AlcwWFRGKzPuuqMKqYODiJrVlzjBQdbY4_aQ,2082
-modelopt/torch/sparsity/__init__.py,sha256=HZ_QgP_YXBYrek8UTJ_25p5FLEV8QsfADksqI-6V1Ic,671
-modelopt/torch/sparsity/config.py,sha256=lndNOuurv-YHJTu-15W8YgvFwqxSmdZahaTAZ2OvWBE,1641
-modelopt/torch/sparsity/magnitude.py,sha256=GQC2-XHvKbLAjoC8z_6pyLQMBBIrN0sbDhGZCnWOg4M,5352
-modelopt/torch/sparsity/mode.py,sha256=9RJivfnNG-yF9b6lsn91r-0Zc_u37K8jRWmsq_oXmx4,6847
-modelopt/torch/sparsity/module.py,sha256=LVB7t5nNN0Fdh-2HcnzhgH8m6AaPx9s8B5NgId_jN4g,3588
-modelopt/torch/sparsity/searcher.py,sha256=xipaaEWc79FXu0ovpR9O3prLqSVTVT4LptvXx7WX1J4,3199
-modelopt/torch/sparsity/sparsegpt.py,sha256=kRHqKje2DF4kZb4FOrIi6roG0QWJGGfc-8YpJ4cR3jI,10791
-modelopt/torch/sparsity/sparsification.py,sha256=R-oFV0mvzfpB-oaSLX0HMKq92knMgowgjB5mruzBJRY,5740
-modelopt/torch/sparsity/plugins/__init__.py,sha256=YSIFMOUX0q6gzG82Rf9xN6TOP7mTHwrbthWT86FSwnE,990
-modelopt/torch/sparsity/plugins/megatron.py,sha256=Pd77yaBAWzwpHRn2LwhFg1zYlH_gT4vP1vSx-4qCVsY,2762
-modelopt/torch/utils/__init__.py,sha256=_3Fi4SbTPAi3wRkkfZ8WCo7Jj-SkA5XfUnK9hCcEMxg,804
-modelopt/torch/utils/_pytree.py,sha256=AJC3qOtD4ljmGcUjIboGxmI9zmIvyC7sOIMlDsLRL2o,5582
-modelopt/torch/utils/cpp_extension.py,sha256=aDxgP_sml9rLkoFRsirSNNbvNJqPDU_eTNe6YxcPwVo,2419
-modelopt/torch/utils/dataset_utils.py,sha256=U9Qe6mG6Em2ZkmVOIaGb5otlfHjmjzTSXViITEQXPaQ,7448
-modelopt/torch/utils/distributed.py,sha256=onQe2CnrpHkw7X34QBmBB_iJP3TSQHk_Vi9t2Gm7WFs,8262
-modelopt/torch/utils/graph.py,sha256=udBDNDEpv9TPPEsYZemll8-5sksCJthE99XPPAcWWUQ,3894
-modelopt/torch/utils/list.py,sha256=Gof1ysHiU5ZuYQdefa-XZBsfR1233sPj-2k58jutAuk,1951
-modelopt/torch/utils/logging.py,sha256=9TSEoce7ZFYNV72DjcdeI4JP6QU9TymZXxh2EbnoSVg,2991
-modelopt/torch/utils/network.py,sha256=Jfc-9QZhSBJMY8PhF1hYFuX7C6Gg6aYr6XY_CYdJP-U,22299
-modelopt/torch/utils/perf.py,sha256=Mha-bppNORq-VlxdS12ttfm6c7Ca3z_dTxBpPT_0UqQ,2816
-modelopt/torch/utils/random.py,sha256=lIg_8WdFH8plq57tXMK0DYxNoBzR8W2nOR-Gqhu6PWA,5706
-modelopt/torch/utils/tensor.py,sha256=Zxcrtp5Y2wrjHN0tukVz1GDfggRZxI2QNldWrbWcCKU,1999
-nvidia_modelopt-0.11.1.dist-info/LICENSE,sha256=xBMtI6murYnjWr29V05XL75sviTU9Sq1XfxrjhWJ6TI,540
-nvidia_modelopt-0.11.1.dist-info/METADATA,sha256=UPLsbsmztu7gFIVyqoljXvzbgd8EcE4P9PIIVlpWucg,2589
-nvidia_modelopt-0.11.1.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
-nvidia_modelopt-0.11.1.dist-info/entry_points.txt,sha256=bRFL6MJQ9MybG1Ulo2LVylrwaZGnOPt4liQ_5c5Vt6g,72
-nvidia_modelopt-0.11.1.dist-info/top_level.txt,sha256=IMo1VR4nTJCILUd5BAb4Pd5DmXMRn_4W9wobCeTfn5Y,9
-nvidia_modelopt-0.11.1.dist-info/RECORD,,
+nvidia_modelopt-0.11.2.dist-info/LICENSE,sha256=Il5n7fbhbOotbSs3WkWaBPXy6-Sa8pYiPxnZSb3CJ9E,531
+nvidia_modelopt-0.11.2.dist-info/WHEEL,sha256=DRFGfbmk00iwRfDF9fpLQAJbF-b4Wp23htOC54ltOsw,114
+nvidia_modelopt-0.11.2.dist-info/METADATA,sha256=Ki7_4MrQGmHK2Jb03DVNZjgw_P0BHZLtf-wJTOnIwjA,2251
+nvidia_modelopt-0.11.2.dist-info/entry_points.txt,sha256=bRFL6MJQ9MybG1Ulo2LVylrwaZGnOPt4liQ_5c5Vt6g,72
+nvidia_modelopt-0.11.2.dist-info/RECORD,,
+nvidia_modelopt-0.11.2.dist-info/top_level.txt,sha256=IMo1VR4nTJCILUd5BAb4Pd5DmXMRn_4W9wobCeTfn5Y,9
+modelopt/__init__.py,sha256=ujhc3PFYODpM9IEXbtVXDkMsDtu7R9BT_A3B4GpmKBo,680
+modelopt/torch/__init__.py,sha256=IyInIK6hbdMZxdLQQq6vlU93QFkX8SbY4HefFBL2xmY,790
+modelopt/torch/quantization/quant_modules.py,sha256=7i9aIsDASEZ6PLZKEEe5iqilgtVsr5C5F-w8X2JOLeY,1680
+modelopt/torch/quantization/model_quant.py,sha256=pslN2h2LIxSEM-N2CPHyqiUoO2pm_FWFgKzaa91cHFI,6153
+modelopt/torch/quantization/config.py,sha256=UqN6KUaL4i3LGg3ZtPVEsRAnXjTGv-nNHIV45a16wvY,10900
+modelopt/torch/quantization/mode.py,sha256=PZj2NpCFsQ2Xr_MFbmiNWuieCuTbm3hIGZHtBYAJykY,3945
+modelopt/torch/quantization/extensions.py,sha256=HWWH5C3eHQ99S0Fpi3yjuOd4CDTj4lDHnfGrPU8H2_4,1079
+modelopt/torch/quantization/__init__.py,sha256=Q19NE6sJB9UHME31-CM2K132A79ToE3M693FsaGDDco,826
+modelopt/torch/quantization/model_calib.cpython-310-x86_64-linux-gnu.so,sha256=cYmw_OwpHJ6Rn6vIAvd2ssk-juqMF1sSn3oQ1Gggox8,3609088
+modelopt/torch/quantization/conversion.py,sha256=ZdgOVKSHN9PB0z_FTSrrE7sFPc3j7NT5-YyGZr1w1VY,10516
+modelopt/torch/quantization/tensor_quant.py,sha256=keGaHsKZ666Eq_GcYV0ewR5btix0tXbRWmuoslmhuYo,28994
+modelopt/torch/quantization/utils.py,sha256=A2vwbihY1zIrwOEF89we-DtzD5uocMPbYy_CoLpjzLM,5536
+modelopt/torch/quantization/optim.py,sha256=5wZLVH9-ZpF7c9j2TLamzRkMsarfpXdgrDuRuPegP94,1360
+modelopt/torch/quantization/plugins/megatron.py,sha256=6n8svgZWecYslB-AIbI9YRV1OeU1cldPHw94tyHLxIY,4705
+modelopt/torch/quantization/plugins/huggingface.py,sha256=DtogRKEh0xaMuRlhoZC1NxRaJaddzJiOzHBEKaGEQfU,2433
+modelopt/torch/quantization/plugins/custom.py,sha256=5YeW3vNLNFcyUgCY6notQe_X8sk89k5yDeenL7e1L4w,4124
+modelopt/torch/quantization/plugins/__init__.py,sha256=nALVhZXrcu4yXkl5KxR_E0UCAcjxhF569LdEPZY7Xm0,1903
+modelopt/torch/quantization/plugins/nemo.py,sha256=eeVj6IjqEnqBWwm09s4AtHbdYul1wD9Z4BtVti0bDrs,2411
+modelopt/torch/quantization/plugins/diffusers.py,sha256=xiTUCXRv35KeamNEmgKQnvvRRyMDyYNsR9Iz36qpsiE,3367
+modelopt/torch/quantization/plugins/apex.py,sha256=-5F-pL26AjFH4vKaZsacm59a2eH7Klf6macoBHmNzLw,1444
+modelopt/torch/quantization/nn/__init__.py,sha256=kjhrcHKRGEMxbzHHxKSbgBYdGqoT2DN2sFwcKGnDEe0,924
+modelopt/torch/quantization/nn/functional.py,sha256=v8fhOLt8uT6YM5YZDQnnLc15uwnqeK3h2InJ-KcOnOM,2756
+modelopt/torch/quantization/nn/modules/quant_pooling.py,sha256=77GWUXmg_mIi0ceKs5I2yo53LKH9Lj2zEwSNlinPa8w,3444
+modelopt/torch/quantization/nn/modules/quant_module.py,sha256=EfDe770UP7bYQxzmOGxgduR4X0SUXH2ALsbyFl8nOBQ,7672
+modelopt/torch/quantization/nn/modules/clip.py,sha256=khJcF0wVjD5QaeP_fZWMGhQGa9MLn3QXV58HJZFSqw0,2259
+modelopt/torch/quantization/nn/modules/quant_activations.py,sha256=LmfJBNr_XhIAp9kRckVTlNQd0UnLMlQIeDVkSOfHJwI,1350
+modelopt/torch/quantization/nn/modules/__init__.py,sha256=DPuO-IBBsGIlTJ46v-EX4giAC-WMemru74lsPwWykpQ,590
+modelopt/torch/quantization/nn/modules/_utils.py,sha256=mSjsGZBY0VSCLkbgGL5lA128reaiSpy_VQ9hVjDYVXw,8131
+modelopt/torch/quantization/nn/modules/tensor_quantizer.py,sha256=FWqkfKaCnfXnoWJZMtD4CHZju9OhgWZg2Y57SnmI8RA,33520
+modelopt/torch/quantization/nn/modules/quant_batchnorm.py,sha256=j_isUTdcIlFJHiXbuPh2-REdIPebM8-lo-oOvu_Q03I,1527
+modelopt/torch/quantization/nn/modules/quant_instancenorm.py,sha256=79cNyCjiNoPfkWx9IOenfvawKOThc02ByJVVtN19sG4,1516
+modelopt/torch/quantization/nn/modules/quant_linear.py,sha256=YRaq45LECaXshTlt_illTTZ2NW-WCmuaP4Zf9eGIApI,2689
+modelopt/torch/quantization/nn/modules/quant_conv.py,sha256=6aOzUx6Ol0kQbYrcPt8_9ejfU5gZD6mIqkhjgiip8sc,3868
+modelopt/torch/quantization/calib/calibrator.py,sha256=lSwUGj3T734-yQqKTV_VGhMqRz2Q4862l__99Wi_K9E,1774
+modelopt/torch/quantization/calib/__init__.py,sha256=F9eLUv37AdqH-u51y9m2a3n6Y4mLc8w34PqtlbOaFTE,797
+modelopt/torch/quantization/calib/histogram.py,sha256=RdjqTrNrFrWLh-O0DjoRGi46zERku6RM7R9VszNQyVc,17018
+modelopt/torch/quantization/calib/max.py,sha256=XtCRRU5xHrxie6YKca8uYvPaijHwBJRMTOj3BkQSeNA,3338
+modelopt/torch/quantization/src/tensor_quant.cpp,sha256=Ri7H8g8s0nio_xQtM-rYMsMHmIOJQlGfUx_Tz_2nAnM,3004
+modelopt/torch/quantization/src/tensor_quant_gpu.cu,sha256=fQwhUgdtXc2Jtk0cTqhQYyhzlL2_a9_S8traLRShpsI,5427
+modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu,sha256=hCKoAIIJxZ-x0Pz5hiaxHPBJxd0FTqIvhVGB4udSHCE,2037
+modelopt/torch/quantization/src/tensor_quant_fp8.cpp,sha256=F-vIeg_f2-hnHLldiSx6Ty2lXP1BNlO_1uwitvJumGk,1284
+modelopt/torch/_deploy/profiling.py,sha256=RWEsvDgIz0h0Hp8kOzlVBAAcbaX67jao0_XBaVDJxJ8,7399
+modelopt/torch/_deploy/device_model.py,sha256=3a574TJnlsDTZTR8wloxoXOZs3fr-NQ77GvSZ_ZpgJs,5394
+modelopt/torch/_deploy/__init__.py,sha256=go8A5rZnKiAJy-vxja_g7lM6lktTbyjHV6jLa9OThzw,858
+modelopt/torch/_deploy/compilation.py,sha256=5R72lBXUhxdUmfDopEAZoO3LyC4QU-wNIskfPzg7COE,4707
+modelopt/torch/_deploy/_runtime/registry.py,sha256=xpTnthWn1JHWxYZ68PGrmh2i-2FjGI-UcBx_Bh1FPSI,2164
+modelopt/torch/_deploy/_runtime/__init__.py,sha256=dcQpgKSYVbuDjWjN_3zfcUJDpoGsozAWpq39okN1UjA,932
+modelopt/torch/_deploy/_runtime/ort_client.py,sha256=u_LGp8RvvJeIl66xj3psc6chW4JbVn-FbSlgCCt-oCE,7090
+modelopt/torch/_deploy/_runtime/common.py,sha256=S4l5FQB7BUWQtk5t3XlgELicsBogBUx4V9hvn00pZNQ,2259
+modelopt/torch/_deploy/_runtime/trt_client.py,sha256=Zkno5K_vR--JrpJgeu1ITA6kX-aQmACudumAxY_sFUk,7560
+modelopt/torch/_deploy/_runtime/runtime_client.py,sha256=dfu-pd6VZuTQ3MzgDODgBJNkcBTA_UFxG-cW88luF30,5726
+modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py,sha256=JMiJ2hyK3BXpovWKgJKpfxBYLw7uydrP7Hb2nJFOR1k,5539
+modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py,sha256=Axwn-OiB77hA2aj5qq9r2zyDHtkhpRVhSu8fdoPnM_k,1718
+modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py,sha256=aVfsjJR762CPDUvEFJd-1RnqaGEiN9gN0OUnCjG0K84,6796
+modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py,sha256=-DW_purcuQtz0c-T0hhbOqYpx1mwnYwAUzEHxlQyh6A,6328
+modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cpython-310-x86_64-linux-gnu.so,sha256=L5vhbjX1esLzOk_vjsXjZY4a1QMwgjRPa-w2kxd8Pfo,1135008
+modelopt/torch/_deploy/_runtime/tensorrt/constants.py,sha256=b1fuzShMGwv8i8O7YbXIsY-dXJsyKNaFUdE2SWZQ9d0,2503
+modelopt/torch/_deploy/utils/onnx_optimizer.py,sha256=s6bFk3n3qOnNFvHslVBdUlIauGTY21qOZ0RxHWNOf7g,4618
+modelopt/torch/_deploy/utils/__init__.py,sha256=VTT1JZ8zKbDXprdb1MnAyNDKBMPyGQDHS1xVE6NIYPI,620
+modelopt/torch/_deploy/utils/torch_onnx.py,sha256=ZCFPiX9P7dE6VItZDZjkkyNnqDiJXZBzqeqz26UDIsQ,20466
+modelopt/torch/export/model_config_utils.py,sha256=0edUHvPYGy_1EisGCeC5XqAS0aveWexhapQk1bUFSqE,17911
+modelopt/torch/export/layer_utils.py,sha256=ScpNUUgLW6h53oe0rgZRrMp_AX9Fr3ykiQu3Q3jhGCY,50776
+modelopt/torch/export/tensorrt_llm_utils.py,sha256=W3OQ9WzMCZ7vTtps0NgcxCjn2AWRstN8ALMVNSABz1A,10999
+modelopt/torch/export/postprocess.py,sha256=8bkHTCLkN-5ug0xHxGNyOxNDrtCUf87ZUtKt-XWbw9Q,28642
+modelopt/torch/export/__init__.py,sha256=6YNp-ZcPBk5jnHyZClb5vkEJaWjwinZIKT2yQjWV3ak,831
+modelopt/torch/export/distribute.py,sha256=mZUXchDAmQNw09BB1Ebol0L5d0R24P22FR886zFtU18,12338
+modelopt/torch/export/transformer_engine.py,sha256=aB07wVUvpQcX9j3mWPs34hz0f0iph9QGrdWGS70wWYE,3998
+modelopt/torch/export/model_config.py,sha256=GDqanUCvoqCaKNCqc-3hlD4TzddyB8ZH0_W-6lood_U,13332
+modelopt/torch/export/model_config_export.py,sha256=wES2Eyr4ZZY5IwE1DUaQAcpKknNladlxE24k5ZEbdN0,17383
+modelopt/torch/export/scaling_factor_utils.py,sha256=hFNJnp86Z-ThpPJ9HIu0mTgE_H36e-pUyZcxv-Cm-xY,3094
+modelopt/torch/opt/config.py,sha256=9zcIkw2geUgR4y0KjJ7etpd1PUyi6wSJdp1x8PBjOzg,14834
+modelopt/torch/opt/mode.py,sha256=KiX_KCXbDRzykgJWaxasNonoSXQ45DCRSTBS_Pqn9MU,12405
+modelopt/torch/opt/__init__.py,sha256=BRuOUA3HuZLUKWgLF3tGFAVD7b50KZV9_4Gr8du_wp4,1335
+modelopt/torch/opt/searcher.py,sha256=LVim6vgT9dLn3ntLbO30bZCsgfsPcApdi-BxChcgPRM,9316
+modelopt/torch/opt/conversion.py,sha256=Xi2ojzcqR-pdkl-3i1s7zQ0Cs-bqGJ7UPbuQ0dmwXFE,23486
+modelopt/torch/opt/_hooks.py,sha256=7w4TLRiyKa7yJXC5OGDEsABCZYa_JMVOllc9yXxN5bA,3074
+modelopt/torch/opt/dynamic.py,sha256=I8SPUXy2FVTRS1bE0TWgJroSV4GJo-XwqVw8lZZg6iQ,56103
+modelopt/torch/opt/utils.py,sha256=ZpZ84qO5EYvppfzcxWR7vzPgQD3fWFKNZRqc-5Im2Sk,1902
+modelopt/torch/opt/hparam.py,sha256=ahzp2KuAurSaLvD12s618ZgZ2QqWx38R4DamysN23Jc,9234
+modelopt/torch/opt/plugins/__init__.py,sha256=zEz28fo0toM94tOtFCDPoXMJ0lbQj6VqQbhrQkp2mB8,812
+modelopt/torch/opt/plugins/mcore_dist_checkpointing.py,sha256=td_IXjSfIS8SjbeS_HjmpYINb8AsDKqBCZSc2DV8WX0,13411
+modelopt/torch/utils/dataset_utils.py,sha256=jxlkn3qvT8AT6UDX7ZLbvsoPDqp_WJ1spqcZtRgPcNU,7244
+modelopt/torch/utils/distributed.py,sha256=IAsCQ5_5UAdvkHEvdmcBIPePn55mERXYBkrd1jn7bBs,7993
+modelopt/torch/utils/logging.py,sha256=R9kGY3a6tiS5wZKwUkRdg2WyeQ72FyRCcYLyPWkSuKs,2900
+modelopt/torch/utils/graph.py,sha256=Nnp9Z_KlkL4g07H3QQFLdDvui1JJGdo98tLbayEqN5o,3775
+modelopt/torch/utils/__init__.py,sha256=cV-v6HjkC9Q7mzdMFbUJaf50u9mAv7OazASjenDOOsE,784
+modelopt/torch/utils/network.py,sha256=l6ANotNC6uIk278cVMIZ3vZodkqxDmBvXNcGSTtwjpM,21757
+modelopt/torch/utils/perf.py,sha256=ojamODwBf0n4zyHU10rpxD2p9hdcvrBmWKpBcUlub74,2733
+modelopt/torch/utils/_pytree.py,sha256=RfxyikkQq0CImyMIUXHO5vOC5rO5yoPrdQ8PSYCAP60,5454
+modelopt/torch/utils/cpp_extension.py,sha256=Vh2q6BaDaLsHrLLgHed5nFTja6FVFGwbiS3-vJwGGTI,2358
+modelopt/torch/utils/list.py,sha256=ZbNKslugXauueHSMBUJn-Zfaf6va17MopNs66EaN5s0,1898
+modelopt/torch/utils/tensor.py,sha256=rr0d87oWLTUrEoAiiS6yPtdrNMvM8YSupHkVrjSkies,1949
+modelopt/torch/utils/random.py,sha256=THF_O5UoBsiBW9e89JSFBUIAf7NVts2_nsT8kkMopR4,5552
+modelopt/torch/sparsity/magnitude.py,sha256=o_F5TQ0WDj_5MfxiK6ah-Pc8MybOHaCVjOW9fOJ7K8w,5208
+modelopt/torch/sparsity/config.py,sha256=AIAdyYk0bQDgKV2udEahPJjbIWuDKL0L3BP1qizuksE,1594
+modelopt/torch/sparsity/mode.py,sha256=5ksdZpE27C5r9I5ockeqHZdeHSSa4jvevQXx0bSahAU,6646
+modelopt/torch/sparsity/__init__.py,sha256=W-aGBn5SiPIBjB4KaMa32JbT5psrQzSKM7ko3cAZJrc,657
+modelopt/torch/sparsity/searcher.py,sha256=Ijw8KOeRzoz95clN2FRnpGcgp2--nn-MDWf73OsKzSg,3116
+modelopt/torch/sparsity/sparsification.py,sha256=mgLK-QJgbZQ5mB8SN17BzmxvU1vaENAkvydzNDPd-Qo,5623
+modelopt/torch/sparsity/sparsegpt.py,sha256=cWueyS6U8Cja8Owve_f80eWBpTsjXCYit4YeSI8uvWw,10523
+modelopt/torch/sparsity/module.py,sha256=Ys0GXiifJYaL29ER_qSXD-AKBiVXvXFp0ze74oRkc8E,3504
+modelopt/torch/sparsity/plugins/megatron.py,sha256=pw_0-dq5qwYbxvOzKHMAIbb7Te_5XFBzqUpa_BGXzfs,2688
+modelopt/torch/sparsity/plugins/__init__.py,sha256=2Ur44uRB62XLFxD-CMjd2Def0aSrOLfhQGprENGkNjY,961
+modelopt/onnx/__init__.py,sha256=-dDsCbgqgETZwq-PvZilkoFLzl5WFVtdJ1atNFy1g-8,775
+modelopt/onnx/op_types.py,sha256=JLWCWL4YlrW-IT4DAwJcsyQC8qrjQwKu9ZEtLGFVAKQ,7712
+modelopt/onnx/utils.py,sha256=jzhxFxzkG5zmgUApcPfbQrZSgOlcU9sHHdX3yTmbxcs,19251
+modelopt/onnx/quantization/ort_patching.py,sha256=aDqRZDCy2hJgFXwhQQJpApHj_ZDi0QAyZVD40h1rjwg,7986
+modelopt/onnx/quantization/quantize.py,sha256=z1hfe8hUin9OQESrB2viZXcU6n1hQ1unftWPOKbZKxg,21538
+modelopt/onnx/quantization/gs_patching.py,sha256=Iye8bu0LZmJTdwI1MtaN-P6CyvcCTQy1ZVUrMdUKb58,4025
+modelopt/onnx/quantization/__init__.py,sha256=YGmYzssZECWawsrSaCAJNgfxafqouZUDor0RczN2_AY,608
+modelopt/onnx/quantization/__main__.py,sha256=G7aFICqg3Ip4Dvx0W8El7-d6VgzeFyJj5-vf6hU13PI,4467
+modelopt/onnx/quantization/calib_utils.py,sha256=BOLekU2-9DA9XwoqDnIRUT4gOjhBDvm3et9uPmRleqg,4505
+modelopt/onnx/quantization/quant_utils.py,sha256=AVVaOk8C_o70STauaCsB3WhHJUuZKb7adk-ipzSBaP0,2418
+modelopt/onnx/quantization/operators.py,sha256=U-Q0PEISFpdgHzy_TVdrOtEH3MULWQwMJDGhdPfm4zk,4215
+modelopt/onnx/quantization/ort_utils.py,sha256=S7y0iNajBPbrtDZSpUU5mYuHIxaiCEraUAv9wwyyPRw,1054
+modelopt/onnx/quantization/partitioning.py,sha256=MQ5d_GSzIw1YSRPqYwIe5pbXFClhJGDjw61nsMZKQq0,13673
+modelopt/onnx/quantization/int4.py,sha256=NFpquF4Bo0S3wsmTl1mBUcB_XKmXhfxF705xj_UQqxc,17915
+modelopt/onnx/quantization/graph_utils.py,sha256=Yk7qIxcvC7Rg2H9yDfJdq2TRndf7M-BgABeZsMzOYcY,18295
+modelopt/onnx/quantization/qdq_utils.py,sha256=hmjj2m52LaJJqFA9Bog8mUmisteBZeZTY54WsNlxFUI,7198
+modelopt/deploy/__init__.py,sha256=duhegEd1OSrG5RzIhV1raqtwyS-qJlHX28cp1JOa7R8,593
+modelopt/deploy/llm/generate.py,sha256=-pfc8vvRqdFdJCCuOLdTLM-d7s9WWq-3eh7EmPdP-Uc,5083
+modelopt/deploy/llm/__init__.py,sha256=-TPZONMk1OY3r-1oNNuQkaBhvDKdA3TZ9kJSDzg9Fro,2320
+modelopt/deploy/llm/model_config_trt.py,sha256=Pz5M-ZvdWRGW9eaQ_uatnSlmVEY-DecIfaD-EKo7B3s,12040
+modelopt/deploy/llm/nemo_utils.py,sha256=UiGhdENlqJ18Qb96ir6NgUDsbpmKpgZdnzENdbr9s2k,6722
```

